arXiv:2510.24450v1 [cs.CL] 28 Oct 2025

Charting the European LLM Benchmarking Landscape:
A New Taxonomy and a Set of Best Practices

Spela Vintar*', Taja Kuzman PungerSek*, Mojca Brglez*', Nikola Ljubesi¢*
*Jozef Stefan Institute
Jamova 39, Ljubljana, Slovenia
{spela.vintar, taja.kuzman, mojca.brglez, nikola.|jubesic}@ijs.si
'Faculty of Arts, University of Ljubljana
ASkeréeva 2, Ljubljana, Slovenia

Abstract
While new benchmarks for large language models (LLMs) are being developed continuously to catch up with the
growing capabilities of new models and Al in general, using and evaluating LLMs in non-English languages remains
a little-charted landscape. We give a concise overview of recent developments in LLM benchmarking, and then
propose a new taxonomy for the categorization of benchmarks that is tailored to multilingual or non-English use
scenarios. We further propose a set of best practices and quality standards that could lead to a more coordinated
development of benchmarks for European languages. Among other recommendations, we advocate for a higher

language and culture sensitivity of evaluation methods.

Keywords: large language models, benchmarking,
tions

1. Introduction

The rapid advancement of large language mod-
els (LLMs) has brought unprecedented capabili-
ties in natural language understanding and gen-
eration, reasoning, coding, and more. With the
global race in raising the bar, commercial models
are approaching artificial general intelligence (AGI)
and exhibit more and more agency as they engage
in strategic planning, independently interact with
other applications, and carry out larger tasks. While
open-source models generally score lower on most
leaderboards, they too grow larger and smarter.

In the brief history of LLMs, many evaluation
frameworks have been set up — both human and
automatic — to assess their evolving performance
across different linguistic and non-linguistic tasks
of growing complexity. While new benchmarks
emerge almost on a daily basis in order to measure
these expanding abilities, the overwhelming major-
ity of evaluation datasets are developed primarily
for English, creating a significant evaluation gap for
other languages and varieties.

To evaluate the performance of LLMs in non-
English contexts, a widely used approach so far
has been to translate existing English benchmarks
using machine translation, with or without human
revision. This might seem reasonable: several
major international benchmarks or benchmark col-
lections (e.g., SuperGLUE (Wang et al., 2019),
MMLU (Hendrycks et al., 2021), Hellaswag (Zellers
et al., 2019) exist together with their parallel (trans-
lated) versions, and this allows for a direct com-
parison of LLMs across a wide range of tasks and
languages. However, Global-MMLU (Singh et al.,

taxonomy, cultural competence, quality recommenda-

2024) revealed that success in MMLU depends
heavily on learning Western-centric concepts, with
28% of all questions requiring culturally sensitive
knowledge. Moreover, for questions requiring geo-
graphic knowledge, an astounding 84.9% focus on
either North American or European regions. Such
cultural biases are not uncommon in other widely
used benchmarks, and their machine-translated
versions potentially overlook language- and culture-
specific phenomena, exhibit skewed performance
which does not accurately reflect true multilingual
capabilities, or simply fail to address issues which
may be critical for users of an LLM in a particular
language.

On the other spectrum of multilingual evaluation,
there are several cases of language- and culture-
specific benchmarks, such as those included in the
Hungarian HuLu (Ligeti-Nagy et al., 2024) evalua-
tion framework, BenCzechMark (Fajcik et al., 2025),
PLCC (Dadas et al., 2025), or BertaQA (Etxaniz
et al., 2024), which have been developed specif-
ically for a particular language or community of
speakers. Such benchmarks provide a deeper in-
sight into a model’s performance for that language,
but typically do not allow us to assess the model’s
multilingual capacities.

Despite the proliferation of evaluation platforms,
research projects, and benchmarking initiatives
across the multilingual landscape, the field lacks
a comprehensive overview that synthesizes cur-
rent practices, identifies critical gaps, and provides
clear guidance for developing more inclusive and
effective evaluation methodologies for LLMs in non-
English contexts. We thus make a case for the
creation of a European benchmarking registry that


===== PAGE BREAK =====

collects structured benchmark data according to a
universal and inclusive taxonomy of benchmarks.
The proposed registry would include rich descrip-
tive metadata and provide a clear overview of both
existing benchmarks and the gaps in the current
European benchmarking landscape.

The remainder of this paper is structured as fol-
lows: In Section 2, we present some recent devel-
opments in LLM benchmarking, focusing on eval-
uations of linguistic and cultural competence in a
multilingual context and on emerging trends. In
Section 3, we proceed with a proposal for a cat-
egorization of benchmarks that could serve as a
foundation for charting ongoing and future LLM
evaluation activities in Europe and beyond. We
finally propose some recommendations as to how
existing benchmarks can be documented, and how
future benchmarks can be made both more culture-
aware and more in tune with the needs of different
language communities.

2. Recent Developments in LLM
Benchmarking

Several comprehensive overviews of LLM bench-
marking have recently been published, including
Chang et al. (2023) and Ni et al. (2025). Both sur-
veys clearly show trends in both the development
of datasets and the evolution of evaluation metrics.
However, they lack a focus on non-English and mul-
tilingual scenarios, which is the main motivation for
this work.

2.1. Major Benchmarks

By major or global, we refer to benchmarks most
frequently used in current evaluation platforms and
leaderboards. These benchmarks are without ex-
ception in English. The ones that evaluate generic
language understanding and commonsense rea-
soning have their origins in the 2018-2022 period,
when the challenges still roughly corresponded to
natural language processing (NLP) research areas.
Some of these benchmarks have seen multiple re-
visions, extensions and updates, and can be seen
as “parent” datasets on which many adaptations,
translations, or local versions are based.

Some of the most prominent for language under-
standing and reasoning include MMLU (Hendrycks
et al., 2021) and its derivatives MMLU-Pro (Wang
et al., 2024), MMLU-Prox (Xuan et al., 2025) and
Global MMLU (Singh et al., 2024); the Super-
GLUE benchmark collection comprising BoolQ
(yes/no questions, Clark et al., 2019), Commit-
mentBank (textual entailment, De Marneffe et al.,
2019), COPA (Choice of Plausible Alternatives for
causal reasoning, Roemmele et al., 2011), MultiRC
(multi-sentence reading comprehension, Khashabi

et al., 2018), ReCoRD (reading comprehension
with commonsense reasoning, Zhang et al., 2018),
RTE (Recognizing Textual Entailment, Giampiccolo
et al., 2007), WiC (Words in Context, Pilehvar and
Camacho-Collados, 2019), and WSC (Winograd
Schema Challenge, Levesque et al., 2012); ARC
(Clark et al., 2018) with multiple-grade science
questions; Hellaswag (Zellers et al., 2019) and
its recent derivative GoldenSwag (Chizhov et al.,
2025). An attempt to create a more challenging
benchmark collection is BIG-bench (Beyond the Im-
itation Game Benchmark, Srivastava et al., 2023),
a massive collaborative benchmark consisting of
204 tasks contributed by more than 450 authors
across 132 institutions, designed to probe large lan-
guage models on tasks believed to be beyond their
current capabilities. Finally, SUPERB (Speech pro-
cessing Universal PERformance Benchmark, Yang
et al., 2021) is a unified speech-focused benchmark
for evaluating self-supervised and general-purpose
speech representations across a wide spectrum
of speech processing tasks. It organizes 10 core
tasks — including automatic speech recognition,
speaker identification, keyword spotting, emotion
recognition, and intent classification — spanning
content, speaker, semantics, and paralinguistics.

2.2. Multilingual Benchmarks

Most state-of-the art models have multilingual ca-
pabilities, but since the precise amounts of non-
English data used in their pre-training are usually
obscure, it is hard to say to what extent the lan-
guage competence of a model in a particular lan-
guage is in correlation with the amount of language-
specific data it has seen. In addition to this, models
differ in their representations of intermediate layers,
which may result in cultural conflicts between latent
internal and target output language (Zhong et al.,
2024).

Since many authors observe a marked decline
in performance for low-resource languages, bench-
marks are now being developed both as parallel
evaluation sets based on existing “parent” datasets
to allow for direct comparison of LLM capabilities
across a number of languages, and as language-
specific benchmarks, usually aimed at assessing
LLM performance in a particular linguistic commu-
nity and/or culture (see Section 2.4 for the latter).

Although the datasets in the first category are
parallel, they may differ considerably in the meth-
ods used for their creation. Some were translated
using machine translation or LLMs, for example,
EU20-MMLU, EU20-HellaSwag, EU20-ARC, EU20-
TruthfulQA, and EU20-GSM8K (Thellmann et al.,
2024); or MMLU-Prox (Xuan et al., 2025). Other
multilingual benchmarks were created with a spe-
cial focus on cultural sensitivity by dividing the orig-
inal subsets into culturally sensitive and culturally


===== PAGE BREAK =====

agnostic ones (Global MMLU, Singh et al., 2024), or
by using professional translators or multiple rounds
of revision to raise the quality of the dataset, e.g.,
BenchMax (Huang et al., 2025), Flores-101 and
FLORES-200 (Goyal et al., 2022) and Belebele
(Bandarkar et al., 2024).

For speech, ML-SUPERB (Multilingual Speech
processing Universal PERformance Benchmark,
Shi et al., 2023) extends the English SUPERB
speech benchmark to 143 languages, evaluating
self-supervised speech representations on auto-
matic speech recognition and language identifica-
tion. FLEURS (Conneau et al., 2022) is a speech-
based extension of the FLORES multilingual bench-
mark, with focus on language identification, auto-
matic speech recognition, and retrieval evaluation.
DIALECTBENCH (Faisal et al., 2024) is the first
large-scale benchmark for language variety under-
standing, aggregating 10 text-level tasks for 281
varieties.

2.3. Dynamic Benchmarks

Recent developments emphasize dynamic and
contamination-resistant evaluation. The period
2022-2025 has witnessed fundamental shifts to-
ward more sophisticated evaluation approaches.
One such attempt is LiveBench (White et al., 2024),
the first benchmark designed to resist training data
contamination through frequently updated ques-
tions from recent sources, automatic scoring, and
monthly updates. This dynamic approach remains
challenging, with the top models achieving accu-
racy below 80%.

2.4. Language- and Culture-Specific
Benchmarks

Focusing on different approaches to the evaluation
of LLM performance in non-English European lan-
guages, we find a broad array of language- and
culture-specific benchmarks developed with vari-
ous methodologies and serving different purposes,
of which we present the ones we find most interest-
ing.

Many European languages have established
evaluation frameworks dedicated to language-
specific benchmarks, and in most cases such
frameworks combine traditional datasets translated
from English, and native more culture-aware bench-
marks. Examples include HuLu' for Hungarian
(Ligeti-Nagy et al., 2024), which covers a number of
well-known tasks such as plausible alternatives (Hu-
CoPa), textual entailment (HURTE) and linguistic
acceptability (HuCoLa), of which the latter was orig-
inally constructed using sentences from selected
Hungarian linguistics books. BenCzechMark for

Thttps://hulu.nytud. hu

Czech (Fajcik et al., 2025) is a complex bench-
mark collection comprising 50 tasks, of which 14
were newly created and only 10% of the collective
instances were machine-translated. The authors
also employ multiple evaluation metrics including
duel scoring. Another recent benchmark for Czech,
Ukrainian and Slovak called CUS-QA (Libovicky
et al., 2025) focuses specifically on cultural compe-
tence and crafts questions, both textual and visual,
from Wikipedia articles which exist in only one of
the languages.

For Iberian languages, a comprehensive and ex-
tensible framework has been established under
IberBench (Gonzalez et al., 2025), spanning 22
task categories and addressing both generic and
industry-relevant tasks. In parallel and under a sim-
ilar name, lberoBench (Baucells et al., 2025) offers
62 tasks of which several were created from scratch
from native data, and others were included only if
they satisfied rather strict quality criteria.

For Slovenian, the SloBENCH? evaluation frame-
work offers natural language inference (SI-NLI),
machine translation, speech recognition, Slovene
SuperGLUE, and two pragmatics benchmarks: Slo-
PragMega and SloPragEval. To create the latter,
full localization of the originally English dataset was
performed, by adapting cultural references and oc-
casionally completely rewriting examples to better
match the linguistic and cultural context. A simi-
lar approach has been taken while translating and
adapting the COPA benchmark (Roemmele et al.,
2011) to four standard languages and three dialects
of the South Slavic language group, resulting in
the DIALECT-COPA (Ljube$ié et al., 2024) bench-
mark collection. While proper cultural adaptation
did take place, the overall popularity and age of
the parent COPA benchmark surely makes it prone
to LLM contamination. Regardless of that, today’s
best-performing proprietary models still score only
halfway between random and optimal on dialectal
data (Chifu et al., 2024).

A fully native benchmark is ITALIC for Italian
(Seveso et al., 2025), which comprises 10,000 in-
stances from 12 domains and was built entirely from
exam materials offered by various public institutions
or government bodies.

An exceptionally active approach to language-
and culture-specific benchmarking can be ob-
served for Polish®, with a range of generic and
domain-specific evaluations for Polish including
multi-turn conversation (MT-Bench), emotional in-
telligence (EQ-Bench), comprehensive text under-
standing (CPTUB‘), medical domain benchmark,

*https://slobench.cjvt.si

Shttps://huggingface.co/spaces/
speakleash/polish-1llm-benchmarks

*https://huggingface.co/spaces/
speakleash/cptu_bench


===== PAGE BREAK =====

linguistic and cultural competency (PLCC, Dadas
et al., 2025), educational (LLMs Behind the School
Desk), cultural vision benchmark, and legal QA
tasks. Most of these benchmarks were developed
anew, by carefully selecting tasks and examples,
verifying them by experts and collecting human an-
notations. The CPTUB is composed of two parts,
the first evaluating implicatures (implied meaning,
sarcasm, idiomatic expressions) and the second
testing on tricky questions (logical puzzles, seman-
tic ambiguity, logical inconsistencies, absurdity, hu-
mor). Similarly, the PLCC consists of 600 manually
crafted questions and is divided into six categories:
history, geography, culture and tradition, art and en-
tertainment, grammar, and vocabulary. The leader-
board results® indicate that even the largest models
still reach mediocre performance in Polish gram-
mar and vocabulary, thus justifying the need for
detailed assessment of linguistic competence for
other European languages as well. A final exam-
ple of a culturally specific benchmark is the Polish
Cultural Vision Benchmark®, a collection of images
with text descriptions to evaluate the cultural com-
petence of multimodal models. The dataset is part
of acitizen science project aimed at collecting 1 mil-
lion culturally specific images’ and recruiting user
donations under the slogan of “technopatriotism.”
While similar platforms have been established be-
fore to collect text data, this is a positive example of
a contemporary and at the same time participatory
benchmark.

3. Categorization of Benchmarks

Since this paper makes a case for a European
database of LLM benchmarks, we propose a new
taxonomy which would allow a better categoriza-
tion and labeling of benchmarks for non-English
languages. This would allow us to better compare
LLMs across languages; gain deeper insight into
the strengths and weaknesses of current and future
LLMs in a specific language, use case, modality or
domain; set common priorities and work towards
filling the evaluation and performance gaps.

The proposed taxonomy should serve as a (ten-
tative) hierarchy of labels to organize or classify
benchmarks; many (or indeed most) belong to more
than a single category. For this reason, a natural
choice to store and query the European benchmark-
ing activities is a database, in which benchmarks
are described according to this proposal. Thus, the

Shttps://huggingface.co/spaces/sdadas/
plcc

Shttps://huggingface.co/spaces/
speakleash/Polish_Cultural_Vision_
Benchmark

"https://obywatel.bielik.ai

benchmarks are assigned non-exclusive categories
and are richly described with metadescriptions.

While alignment, including trustworthiness, truth-
fulness and safety of LLMs, are central topics to
the development of LLMs, they constitute another
level of evaluation. Many elements of Al ethics
partly overlap, or are entailed, in other benchmarks
(e.g., bias is revealed in translation or language
generation; trustworthiness is related to reasoning
performance, etc.). This is another reason why
multiple categories per benchmark are considered
typical and expected.

Since LLMs tend to perform worse in non-English
languages, and especially non-standard varieties,
over a spectrum of tasks, we propose that the
language- and culture-related abilities receive more
attention, and therefore a more fine-grained taxon-
omy than they do in existing taxonomies. The list
can be expanded as needed.

3.1.

As new benchmarks are continuously presented to
evaluate the emerging capabilities of LLMs, many
attempts have been made to organize them in a
structured and logical way.

The Al Verify Foundation has established one
of the most systematic approaches to LLM bench-
mark categorization globally. In their October 2023
publication “Cataloguing LLM Evaluations” (Al Ver-
ify Foundation, 2023), LLM benchmarks are orga-
nized into 5 top categories (further divided into
subcategories). These are General Capabilities
(natural language understanding, natural language
generation, reasoning, knowledge and factuality,
tool use effectiveness, multilingualism, and con-
text length handling); Domain Specific Capabilities
(specialized industry performance across various
domains); Safety and Trustworthiness; Extreme
Risks; and Undesirable Use Cases.

The catalogue represents a comprehensive and
valuable contribution to the field, and has many
positive features: The taxonomy is based on LLM
capabilities, occasionally also referred to as tasks,
which seems intuitively most pragmatic as this is
usually the way we think about (and evaluate) hu-
man performance too. Complex benchmarks can
appear in several categories simultaneously (e.g.,
BigBench as a massive collaborative benchmark
appears in almost all taxonomy categories), and the
recommendations for future LLM evaluations are
a solid starting point to reinforce minimum quality
standards for fair and trustworthy LLM assessment.

However, the catalogue also has some draw-
backs which render it unsuitable for our purposes.
Firstly, to no fault of its authors, it has not been
updated since 2023 and hence does not include
many benchmarks which have since become main-
stream, nor does it address recent developments

Existing Taxonomies


===== PAGE BREAK =====

in LLMs and Al in general. Secondly, although it
includes Multilinguality as a separate category, it
falls short in capturing some aspects of LLM perfor-
mance which may be critical for the evaluation of
European models; i.e., models specifically devel-
oped to be used in region-, language-, culture- or
domain-specific contexts. Thirdly, and this is less
of a drawback but simply an observation, the taxon-
omy and the quality recommendations are primarily
focused on the safety and trustworthiness of LLMs,
in the context of Al governance and alignment re-
search. While these are indeed crucial priorities
especially for the so-called "frontier models” and
capabilities, the European landscape of LLM de-
velopment and evaluation is — at least for now —
gyrating around a different set of goals, such as
how to reach state-of-the-art levels of understand-
ing and generation in non-English languages, or
how to de-bias English-centric models.

Other approaches to taxonomization include
HELM (Holistic Evaluation of Language Models),
also referred to as the Stanford approach (Liang
et al., 2022). The authors introduce the concept of
scenarios (what we want to evaluate) and metrics
(which performance aspects are measured, and
how), then propose a taxonomy of scenarios and
desiderata. Today, the framework? includes a num-
ber of leaderboards with support for multimodality
and model-graded evaluation. While the scenarios
proposed in HELM and the framework itself leave
room for continuous extension, they do not in fact
offer a hierarchical structure with sufficient focus
on multilinguality and issues related to the use of
LLMs in non-English contexts.

Similarly, Chang et al. (2023) provide an overview
of existing LLM evaluations, which they examine
from three aspects: what, where, and how to eval-
uate. They divide the evaluations tasks into eight
top-level non-exclusive categories, namely Natural
language processing; Robustness, ethics, biases
and trustworthiness; Social sciences; Natural sci-
ence and engineering; Medical applications; Agent
applications; and Other applications.

Huber and Niklaus (2025) present a cognitive-
based view on benchmarking by mapping the well-
known Bloom’s taxonomy of cognitive abilities to
LLM capabilities across six hierarchical cognitive
levels: Remember, Understand, Apply, Analyze,
Evaluate, and Create, revealing significant gaps in
the coverage of higher-order thinking skills.

Another comprehensive attempt at taxonomizing
benchmarks is by Guo et al. (2023) who introduce
a three-pillar framework that categorizes LLM eval-
uation into three major groups: Knowledge and
capability evaluation, Alignment evaluation, and
Safety evaluation.

Shttps://crfm. stanford. edu/helm/

3.2. New Taxonomy Proposal

As already outlined in the sections above, the pro-
posed taxonomy is intended for the categoriza-
tion of (mainly) European LLM benchmarks and
is based on Al Verify Foundation’s catalogue, but
with the following modifications:

« We merge all language-related tasks and
scenarios under a single top-level category
called Language capabilities.

We further merge the traditional NLP divide
between natural language understanding
and natural language generation into a sin-
gle subcategory. The fact is that state-of-the-
art LLMs more often than not combine these
two capabilities, and even straight-forward
tasks such as question answering or text sum-
marization involve both.

We expand the category for general linguis-
tic competence with further subcategories
for style, conversation and pragmatics, and
allow for other more fine-grained aspects of
measuring the grammaticality, stylistic appro-
priateness or coherence of generated outputs.

We expand the category of specific linguis-
tic competence to include creativity, atypical
communication, the use of domain-specific lan-
guage etc.

We also expand and redefine the category
of multilinguality to include code-switching,
multilingual interaction, and dialectal flexibility.

We introduce cultural competence as a sep-
arate category.

We introduce speech as a separate category
to include benchmarks specifically aimed at
performing tasks unique to spoken language
as input or output.°

We add agency as a form of long-term, con-
sistent or strategic reasoning.

Figure 1 shows the four top-level taxonomy cate-
gories with subcategories. The full taxonomy along
with fine-grained third-level categories will be added
as Appendix A into the camera-ready version of this

paper.

°We propose Modality as one of the metadescriptors,
allowing for any benchmark to be implemented in any of
the modes. The Speech category refers to evaluations
targeted at speech-related activities.


===== PAGE BREAK =====

li
Language         Knowledge          Safety and        A eee
capabilities      and reasoning     trustworthiness       capabilities
Natural
language
understanding
and generation

General
knowledge

Instruction
following

M

Domain-
specific
knowledge

General
linguistic
competence

Misinformation

sg       >)
Specific
linguistic
competence
S

Reasoning                    Toxicity

i     >)
UJ

Multilinguality         Agency                Bias

4]

J

Speech                                 Machine ethics

Cultural

Psychological
competence                                                      i

Multimodality                               Robustness

Data
governance

Figure 1: Top-level categories with subcategories.

4. Quality Standards and Metadata

If we conceive of a European benchmarking reg-
istry as a searchable database which may help the
LLM development community in setting priorities,
exchanging knowledge and standardizing evalua-
tion practices, we should be able to define some
desiderata around how a particular benchmark is
presented, described and distributed. Below we
discuss some of these aspects.

4.1.

While it is clear that the development of original
and sufficiently complex benchmarking datasets is
highly time-consuming and costly, the drawbacks of
automatically translated and culturally maladapted
benchmarks have been clearly pointed out (Singh
et al., 2024; Xuan et al., 2025). We should thus
strive towards clearer — even if more complex —
descriptors which indicate how a dataset or bench-
mark was created. We propose the following de-
scriptors:

Provenance

* Original Applies to datasets which have been
originally created in the language they appear
in, by any method other than translation (e.g.,
collecting original exam questions, employ-
ing experts to provide domain-specific tasks,
adapting authentic texts in a particular lan-
guage to create tasks in that language).

Machine translation Applies to datasets cre-
ated by any automated translation service, in-
cluding those created by LLMs using any kind

of prompt scenario, and workflows with ma-
chine revision. The tools and workflows must
be specified.

Machine translation and Human revision
Applies to datasets where the result of auto-
mated translation was revised by a human pro-
fessional or non-professional translator or re-
viewer. Since many non-English benchmarks
are created by machine translating the — usu-
ally — English original, followed by human re-
vision of only a small portion of the dataset,
the recommendation would be to use all labels
that apply.

Human translation Used only for datasets
which have been fully translated and revised
by humans. Only a few European non-English
datasets satisfy this criterion.

Full localization Used for datasets which have
not only been translated professionally, but
for which a full linguistic and cultural adap-
tation was performed. This might mean the
replacement of culturally-specific or untrans-
latable tasks with new ones, or removing parts
of the dataset deemed culturally unsuitable.

Other If several of the above scenarios were
used, the dataset should be labeled with all
that apply. Other methods and scenarios used
in the creation of a dataset should be specified
here.

4.2. Accessibility

The tension between open benchmarking and data
contamination poses a significant challenge for Al
evaluation. While public datasets enable repro-
ducible research and fair comparisons, they risk
contamination when models train on test data, inflat-
ing performance scores and undermining bench-
mark validity. Private evaluation sets offer a po-
tential solution by keeping data hidden from train-
ing processes, ensuring cleaner performance mea-
surements.

* Public Applies to fully open datasets shared
with labels through common platforms.

Public without labels Applies to datasets
where labels are not distributed to prevent di-
rect training.

Private (academic/research access) Where
authors encourage reproducibility but wish to
prevent contamination.

Private (closed/proprietary) Where datasets
are typically not shared as they are used for
internal or industry-specific evaluation.


===== PAGE BREAK =====

¢ Other This may include dynamic benchmarks
where tasks are continually updated (such as
White et al., 2024).

4.3. Language Coverage

This category indicates the prominence, reach or
scale of a benchmark in terms of its presence on
major leaderboards, coverage, global spread, but
also purpose. We are aware that the boundaries
between the proposed buckets may be fuzzy.

¢ Major global benchmark See section 2.1 for
examples.

Multilingual benchmark This category can be
used for benchmarks derived from the above,
for instance by developing a multilingual vari-
ant of a well-known benchmark for a set of new
languages. Examples include XCOPA (Ponti
et al., 2020), MMLU-Prox (Xuan et al., 2025)
or xHumanEval (Raihan et al., 2025).

Language-group or region-specific bench-
mark This category is used for language-
specific benchmarks as well as benchmarks
that cover multiple languages from a simi-
lar language group or target a certain ge-
ographical region. Examples include Iber-
oOBENCH (Baucells et al., 2025) and DIALECT-
COPA (Ljube$Sié et al., 2024).

¢ Other

4.4. Evaluation Type

An important factor for present and future bench-
marks is the divide between closed-ended types of
tasks, most prominently multiple-choice questions,
but also other types of tasks where the solution is in-
cluded in the task, and open-ended tasks, typically
generation of text, speech, image, or multimodal
output. Few benchmarks to date address the lat-
ter, despite the fact that generative LLMs are now
mainstream and the vast majority of application
scenarios exploit generative abilities.

4.5. Evaluation Metrics and Frameworks

The performance of an LLM can be evaluated in
several ways, depending on the type of task. For
multiple choice questions, text classification tasks
or cloze tests, where the correct answer is deter-
ministic, accuracy or F1 can be used. However, to
evaluate longer, more complex responses resulting
from generative tasks, many other methods were
proposed. In reference-based evaluation, the LLM
response is compared to a reference using various
distance measures (e.g., BERTScore, Rouge-1,
METEOR), while in reference-less contexts, the

quality of the response is directly assessed (e.g.,
by an LLM-as-a-judge). As we have seen, re-
cently developed benchmarks employ more com-
plex evaluation methodologies, and a common al-
ternative to algorithmic benchmarks is human pref-
erence voting (in so-called chatbot arenas, e.g.,
https://lmarena.ai/).

Another important element is the existence of
a human baseline, and its quality. Important fac-
tors to consider are participant selection (demo-
graphic sampling, expertise), participant training,
task design and instructions (to ensure fair com-
parison between humans and LLMs), control for
attention, bias or fatigue, and the collection of par-
ticipant demographics to facilitate reproducibility
and interpretation of results. Human annotators or
participants can also provide relevant insight into
the overall quality of the benchmark, thus some
campaigns actively collect human feedback to be
used in revised versions of the evaluation dataset.

If the benchmark or dataset is integrated into
an evaluation framework, this should be indicated
together with a link or other reference to the evalu-
ation site.

4.6. Metadescriptors

We propose collecting rich metadata for each
benchmark that allows researchers to quickly under-
stand its content, characteristics, and provenance.
Such metadescriptors facilitate dataset discovery,
comparison, and reuse. Below, we summarize the
metadata fields currently envisaged in the proposed
registry:

* Description: A short summary of the dataset’s
content and purpose.

Benchmark family: The broader benchmark
initiative or collection to which the dataset
belongs. For example, the COPA bench-
mark family would include the English COPA
(Roemmele et al., 2011) and its many par-
allel variants such as the COPA datasets in
Hungarian (Ligeti-Nagy et al., 2024), Croat-
ian (Ljubesi€é and Lauc, 2021), South Slavic
dialects (LjubeSic et al., 2024) etc.

Number of test instances: The total num-
ber of instances in the test set, enabling quick
comparison of dataset scale.

Language: The language(s) in which the
dataset is provided.

Language type: Specification of whether
the language of the dataset is standard, non-
standard, or a dialect.

Modality: The input modality of the dataset,
such as text, speech, sign language, or audio-
visual signal.


===== PAGE BREAK =====

Authors: The creators or curators of the
dataset.

Paper link: Reference to the main publication
describing the dataset.

Access info: Information on how to obtain the
dataset, e.g., a link to a website from where the
dataset can be acquired, or an e-mail of the
dataset owner if the dataset is not published
online.

Last revised: The date of the most recent
update or revision of the dataset.

More information: Additional notes, links, or
resources relevant to the dataset.

5. Recommendations and Future
Directions

Several challenges of LLM evaluation have been
pointed out by a number of studies (e.g., Laskar
et al., 2024 or Al Verify Foundation, 2023: p. 16-
22), most notably reproducibility, reliability (includ-
ing contamination, obscure evaluation methods
and unfair comparisons), and robustness. There
are numerous parallel activities in progress to set
the course of the European LLM evaluation land-
scape, agree on common principles and establish
a dialogue between different stakeholders. While a
full review of the above-mentioned challenges is be-
yond the scope of this paper, we list some priorities
which apply in particular for the European bench-
marking landscape and evaluations in non-English
settings.

1. Cultural sensitivity The prevalent framing of
toxicity, bias, but also factual knowledge and
reasoning, tends to be Western-centric. Evalu-
ation concepts should be expanded to include
diverse global perspectives and values.

2. Language sensitivity Under this term, we re-
fer to the fact that LLMs find applications in
multilingual and multicultural settings, and that
the performance of an LLM may depend heav-
ily on the language it is used in. Benchmarks
for bias and toxicity, but also some common
benchmarks for language understanding and
inference, are not trivial to transfer across lan-
guages and linguistic structures.

3. Comparability vs. specificity Many Euro-
pean and other non-English languages still
have a limited number of benchmark datasets.
To provide a common ground for the evalua-
tion of LLMs across languages, major global
benchmarks are still being translated to facil-
itate international comparability. It is recom-
mended that such translation and adaptation

procedures acknowledge the complexity of the
task and, even if performed automatically, con-
sider employing different tools, prompt strate-
gies, LLM revision techniques and human eval-
uation of random samples.

4. Additional modalities Many benchmarks
have been developed recently to include differ-
ent modalities, but there is still a gap in bench-
marks for speech, sign language, audiovisual
communication, and combinations thereof.

5. Human baseline While frontier LLMs already
surpass humans in many areas, it is of
paramount importance, especially in high-risk
domains, to have a solid foundation for the
evaluation of LLMs by comparing the results
to human performance on a specific task, ac-
knowledging that every benchmark is a limited
representation of a task.

6. Transparent implementation The details of
the evaluation process should be published
in a transparent manner, including the exact
specification of prompting strategies and hyper-
parameters used, such as temperature. If pos-
sible, evaluation scripts should also be made
public, and results validated through multiple
runs in the case of smaller datasets.

7. Context-specific evaluation There is a lack
of nuanced, context-specific evaluations that
address the multi-faceted nature of real-world
LLM deployments. This includes domain-
specificity, but also other elements of attuning
evaluation to the users it serves.

We believe that the rapidly evolving benchmark-
ing landscape for European languages would ben-
efit from a catalogue where each benchmark is cat-
egorized and documented according to the above
principles. We conceive of such a catalogue in the
form of a database, accommodating the fact that
the proposed taxonomy categories and metade-
scriptors may overlap, so that each entry can be
labeled with all which apply.

6. Conclusion

We have presented recent trends in LLM bench-
marking for European languages and proposed a
new taxonomy for their categorization, intended to
be implemented alongside a range of metadescrip-
tors in the context of a European catalogue of LLM
benchmarks. Our taxonomization strategy focuses
on the linguistic, cultural, factual and reasoning ca-
pabilities of models and also incorporates emerging
abilities. The proposed considerations follow the
wide-spread belief amongst European researchers
and developers that the traditional Western-centric,


===== PAGE BREAK =====

likely contaminated and linguistically inappropriate
datasets no longer satisfy our needs, and that tar-
geted efforts should be invested in filling the evalu-
ation gaps for all European languages.

The initiative presented in this paper is the result
of a series of discussions and reflections within the
framework of several international research com-
munities, collecting and integrating feedback from
a number of researchers, developers and bench-
mark creators. With the rapid advancement of the
field, we envisage continuous extensions and revi-
sions both of the taxonomy and the associated set
of metadescriptors and recommendations.

Acknowledgements

This work is supported by the LLM4DH project,
funded by the Slovenian Research and Innovation
Agency (ARIS) under the grant agreement GC-
0002.

7. Bibliographical References

Al Verify Foundation. 2023. Catalogue of LLM Eval-
uations.

Lucas Bandarkar, Davis Liang, Benjamin Muller,
Mikel Artetxe, Satya Narayan Shukla, Donald
Husa, Naman Goyal, Abhinandan Krishnan,
Luke Zettlemoyer, and Madian Khabsa. 2024.
The Belebele Benchmark: a Parallel Reading
Comprehension Dataset in 122 Language Vari-
ants. In Proceedings of the 62nd Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 749-775,
Bangkok, Thailand. Association for Computa-
tional Linguistics.

Irene Baucells, Javier Aula-Blasco, Iria de Dios-
Flores, Silvia Paniagua Suarez, Naiara Perez,
Anna Salles, Susana Sotelo Docio, Julia Fal-
cao, Jose Javier Saiz, Robiert Sepulveda Torres,
Jeremy Barnes, Pablo Gamallo, Aitor Gonzalez-
Agirre, German Rigau, and Marta Villegas. 2025.
IberoBench: A benchmark for LLM evaluation in
Iberian languages. In Proceedings of the 31st
International Conference on Computational Lin-
guistics, pages 10491-10519, Abu Dhabi, UAE.
Association for Computational Linguistics.

Yupeng Chang, Xu Wang, Jindong Wang, Yuan
Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xi-
aoyuan Yi, Cunxiang Wang, Yidong Wang, Wei
Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang
Yang, and Xing Xie. 2023. A survey on evalu-
ation of large language models. arXiv preprint
arXiv:2307.03109.

Adrian-Gabriel Chifu, Goran GlavaS, Radu Tudor
lonescu, Nikola Ljubesi¢é, Aleksandra Miletic,
Filip Mileti€é, Yves Scherrer, and Ivan Vulié. 2024.
VarDial evaluation campaign 2024: Common-
sense reasoning in dialects and multi-label sim-
ilar language identification. In Proceedings of
the Eleventh Workshop on NLP for Similar Lan-
guages, Varieties, and Dialects (VarDial 2024),
pages 1-15, Mexico City, Mexico. Association
for Computational Linguistics.

Pavel Chizhov, Mattia Nee, Pierre-Carl Langlais,
and Ivan P Yamshchikov. 2025. What the
HellaSwag? On the Validity of Common-
Sense Reasoning Benchmarks. arXiv preprint
arXiv:2504.07825.

Christopher Clark, Kenton Lee, Ming-Wei Chang,
Tom Kwiatkowski, Michael Collins, and Kristina
Toutanova. 2019. BoolQ: Exploring the Surpris-
ing Difficulty of Natural Yes/No Questions. In
Proceedings of the 2019 Conference of the North
American Chapter of the Association for Compu-
tational Linguistics: Human Language Technolo-
gies, Volume 1 (Long and Short Papers), pages
2924-2936, Minneapolis, Minnesota. Associa-
tion for Computational Linguistics.

Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar
Khot, Ashish Sabharwal, Carissa Schoenick, and
Oyvind Tafjord. 2018. Think you have Solved
Question Answering? Try ARC, the Al2 Reason-
ing Challenge. arXiv preprint arXiv:1803.05457.

Alexis Conneau, Min Ma, Simran Khanuja,
Yu Zhang, Vera Axelrod, Siddharth Dalmia, Ja-
son Riesa, Clara Rivera, and Ankur Bapna. 2022.
Fleurs: Few-shot learning evaluation of univer-
sal representations of speech. arXiv preprint
arXiv:2205. 12446.

Stawomir Dadas, Matgorzata Grebowiec, Michat
Peretkiewicz, and Rafat Poswiata. 2025. Eval-
uating Polish linguistic and cultural compe-
tency in large language models. arXiv preprint
arXiv:2503.00995.

Marie-Catherine De Marneffe, Mandy Simons, and
Judith Tonhauser. 2019. The CommitmentBank:
Investigating projection in naturally occurring dis-
course. In Proceedings of Sinn und Bedeutung,
volume 23, pages 107-124.

Julen Etxaniz, Gorka Azkune, Aitor Soroa, Oier La-
calle, and Mikel Artetxe. 2024. BertaQA: How
Much Do Language Models Know About Local
Culture? Advances in Neural Information Pro-
cessing Systems, 37:34077-34097.

Fahim Faisal, Orevaoghene Ahia, Aarohi Srivas-
tava, Kabir Ahuja, David Chiang, Yulia Tsvetkov,


===== PAGE BREAK =====

and Antonios Anastasopoulos. 2024. DIALECT-
BENCH: An NLP benchmark for dialects, vari-
eties, and closely-related languages. In Proceea-
ings of the 62nd Annual Meeting of the Associ-
ation for Computational Linguistics (Volume 1:
Long Papers), pages 14412-14454, Bangkok,
Thailand. Association for Computational Linguis-
tics.

Martin Fajcik, Martin Docekal, Jan Dolezal, Karel
Ondrej, Karel Benes, Jan Kapsa, Pavel Smrz,
Alexander Polok, Michal Hradis, Zuzana Never-
ilova, et al. 2025. BenCzechMark : A Czech-
centric Multitask and Multimetric Benchmark for
Large Language Models with Duel Scoring Mech-
anism. Transactions of the Association for Com-
putational Linguistics, 13:1068—1095.

Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and William B Dolan. 2007. The third pascal
recognizing textual entailment challenge. In Pro-
ceedings of the ACL-PASCAL workshop on tex-
tual entailment and paraphrasing, pages 1-9.

José Angel Gonzalez, lan Borrego Obrador, Al-
varo Romo Herrero, Areg Mikael Sarvazyan,
Mara Chinea-Rios, Angelo Basile, and Marc
Franco-Salvador. 2025. IberBench: LLM Eval-
uation on Iberian Languages. arXiv preprint
arXiv:2504. 16921.

Naman Goyal, Cynthia Gao, Vishrav Chaudhary,
Peng-Jen Chen, Guillaume Wenzek, Da Ju, San-
jana Krishnan, Marc’Aurelio Ranzato, Francisco
Guzman, and Angela Fan. 2022. The Flores-
101 evaluation benchmark for low-resource and
multilingual machine translation. Transactions
of the Association for Computational Linguistics,
10:522-538.

Zishan Guo, Renren Jin, Chuang Liu, Yufei Huang,
Dan Shi, Linhao Yu, Yan Liu, Jiaxuan Li, Bojian
Xiong, Deyi Xiong, et al. 2023. Evaluating Large
Language Models: A Comprehensive Survey.
arXiv preprint arXiv:2310.19736.

Dan Hendrycks, Collin Burns, Steven Basart, Andy
Zou, Mantas Mazeika, Dawn Song, and Jacob
Steinhardt. 2021. Measuring Massive Multitask
Language Understanding. Proceedings of the
International Conference on Learning Represen-
tations (ICLR).

Xu Huang, Wenhao Zhu, Hanxu Hu, Conghui
He, Lei Li, Shujian Huang, and Fei Yuan. 2025.
BenchMAx: A Comprehensive Multilingual Eval-
uation Suite for Large Language Models. arXiv
preprint arXiv:2502.07346.

Thomas Huber and Christina Niklaus. 2025. LLMs
Meet Bloom’s Taxonomy: A Cognitive View on

Large Language Model Evaluations. In Proceed-
ings of the 31st International Conference on Com-
putational Linguistics, pages 5211-5246.

Daniel Khashabi, Snigdha Chaturvedi, Michael
Roth, Shyam Upadhyay, and Dan Roth. 2018.
Looking Beyond the Surface: A Challenge Set
for Reading Comprehension over Multiple Sen-
tences. In Proceedings of North American Chap-
ter of the Association for Computational Linguis-
tics (NAACL).

Md Tahmid Rahman Laskar, Sawsan Algahtani,
M Saiful Bari, Mizanur Rahman, Mohammad
Abdullah Matin Khan, Haidar Khan, Israt Ja-
han, Amran Bhuiyan, Chee Wei Tan, Md Rizwan
Parvez, Enamul Hoque, Shafiq Joty, and Jimmy
Huang. 2024. A systematic survey and critical re-
view on evaluating large language models: Chal-
lenges, limitations, and recommendations. In
Proceedings of the 2024 Conference on Empir-
ical Methods in Natural Language Processing,
pages 13785-13816, Miami, Florida, USA. As-
sociation for Computational Linguistics.

Hector J Levesque, Ernest Davis, and Leora Mor-
genstern. 2012. The Winograd Schema Chal-
lenge. In 13th International Conference on the
Principles of Knowledge Representation and
Reasoning, KR 2012, pages 552-561. Institute
of Electrical and Electronics Engineers Inc.

Percy Liang, Rishi Bommasani, Tony Lee, Dimitris
Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian
Zhang, Deepak Narayanan, Yuhuai Wu, Ananya
Kumar, et al. 2022. Holistic Evaluation of Lan-
guage Models. arXiv preprint arXiv:221 1.09110.

Jindfich Libovicky, Jindfich Helcl, Andrei Manea,
and Gianluca Vico. 2025. CUS-QA: Local-
Knowledge-Oriented Open-Ended Ques-
tion Answering Dataset.   arXiv preprint
arXiv:2507.22752.

Noémi Ligeti-Nagy, Gerg6 Ferenczi, Enik6é Héja,
Laszlé6 Janos Laki, Noémi Vadasz, Zijian Gy6z6
Yang, and Tamas Varadi. 2024. HuLU: Hun-
garian language understanding benchmark kit.
In Proceedings of the 2024 Joint Interna-
tional Conference on Computational Linguistics,
Language Resources and Evaluation (LREC-
COLING 2024), pages 8360-8371.

Nikola LjubeSi¢, Nada Galant, Sonja Benéina, Jaka
Cibej, Stefan Milosavljevicé, Peter Rupnik, and
Taja Kuzman. 2024. DIALECT-COPA: Extend-
ing the standard translations of the COPA causal
commonsense reasoning dataset to South Slavic
dialects. In Proceedings of the Eleventh Work-
shop on NLP for Similar Languages, Varieties,


===== PAGE BREAK =====

and Dialects (VarDial 2024), pages 89-98, Mex-
ico City, Mexico. Association for Computational
Linguistics.

Nikola Ljubesi¢ and Davor Lauc. 2021. BERTic-
The Transformer Language Model for Bosnian,
Croatian, Montenegrin and Serbian. In Proceed-
ings of the 8th Workshop on Balto-Slavic Natural
Language Processing, pages 37-42.

Shiwen Ni, Guhong Chen, Shuaimin Li, Xuanang
Chen, Siyi Li, Bingli Wang, Qiyao Wang, Xingjian
Wang, Yifan Zhang, Liyang Fan, et al. 2025. A
Survey on Large Language Model Benchmarks.
arXiv preprint arXiv:2508. 15361.

Mohammad Taher Pilehvar and Jose Camacho-

Collados. 2019. WiC: the word-in-context dataset
for evaluating context-sensitive meaning repre-
sentations. In Proceedings of the 2019 Confer-
ence of the North American Chapter of the As-
sociation for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and
Short Papers), pages 1267-1273, Minneapolis,
Minnesota. Association for Computational Lin-
guistics.

Edoardo Maria Ponti, Goran Glavas, Olga Majew-
ska, Qianchu Liu, Ivan Vulié, and Anna Korho-
nen. 2020. XCOPA: A multilingual dataset for
causal commonsense reasoning. In Proceed-
ings of the 2020 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
pages 2362-2376, Online. Association for Com-
putational Linguistics.

Nishat Raihan, Antonios Anastasopoulos, and Mar-
cos Zampieri. 2025. mHumanEval - a multilin-
gual benchmark to evaluate large language mod-
els for code generation. In Proceedings of the
2025 Conference of the Nations of the Americas
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies (Vol-
ume 1: Long Papers), pages 11432-11461, Al-
buquerque, New Mexico. Association for Com-
putational Linguistics.

Melissa Roemmele, Cosmin Adrian Bejan, and An-
drew S. Gordon. 2011. Choice of Plausible Alter-
natives: An Evaluation of Commonsense Causal
Reasoning. In Proceedings of the 2011 AAAI
Spring Symposium on Logical Formalizations of
Commonsense Reasoning.

Andrea Seveso, Daniele Poterti, Edoardo Federici,
Mario Mezzanzanica, and Fabio Mercorio. 2025.
ITALIC: An Italian culture-aware natural language
benchmark. In Proceedings of the 2025 Con-
ference of the Nations of the Americas Chapter
of the Association for Computational Linguistics:
Human Language Technologies (Volume 1: Long

Papers), pages 1469-1478, Albuquerque, New
Mexico. Association for Computational Linguis-
tics.

Jiatong Shi, Dan Berrebbi, William Chen, Ho-Lam
Chung, En-Pei Hu, Wei-Ping Huang, Xuankai
Chang, Shang-Wen Li, Abdelrahman Mohamed,
Hung yi Lee, and Shinji Watanabe. 2023. MI-
superb: Multilingual speech universal perfor-
mance benchmark. In Proceedings of the Annual
Conference of the International Soeech Com-
munication Association, INTERSPEECH 2023,
pages 884-888.

Shivalika Singh, Angelika Romanou, Clémen-
tine Fourrier, David | Adelani, Jian Gang Ngui,
Daniel Vila-Suero, Peerat Limkonchotiwat, Kelly
Marchisio, Wei Qi Leong, Yosephine Susanto,
et al. 2024. Global MMLU: Understanding
and Addressing Cultural and Linguistic Bi-
ases in Multilingual Evaluation. arXiv preprint
arXiv:2412.03304.

Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,
Abu Awal Md Shoeb, Abubakar Abid, Adam
Fisch, Adam R. Brown, Adam Santoro, Aditya
Gupta, Adria Garriga-Alonso, Agnieszka Kluska,
Aitor Lewkowycz, Akshat Agarwal, Alethea
Power, Alex Ray, Alex Warstadt, et al. 2023. Be-
yond the imitation game: Quantifying and ex-
trapolating the capabilities of language models.
Transactions on Machine Learning Research.

Klaudia Thellmann, Bernhard Stadler, Michael
Fromm, Jasper Schulze Buschhoff, Alex Jude,
Fabio Barth, Johannes Leveling, Nicolas Flores-
Herr, Joachim Kohler, René Jakel, et al. 2024. To-
wards Multilingual LLM Evaluation for European
Languages. arXiv preprint arXiv:2410.08928.

Alex Wang, Yada Pruksachatkun, Nikita Nangia,
Amanpreet Singh, Julian Michael, Felix Hill,
Omer Levy, and Samuel Bowman. 2019. Su-
perGLUE: A Stickier Benchmark for General-
Purpose Language Understanding Systems. Ad-
vances in neural information processing systems,
32.

Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng
Ni, Abhranil Chandra, Shiguang Guo, Weiming
Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al.
2024. MMLU-Pro: A More Robust and Challeng-
ing Multi-Task Language Understanding Bench-
mark. Advances in Neural Information Process-
ing Systems, 37:95266-95290.

Colin White, Samuel Dooley, Manley Roberts,
Arka Pal, Ben Feuer, Siddhartha Jain, Ravid
Shwartz-Ziv, Neel Jain, Khalid Saifullah, Sree-
manti Dey, et al. 2024. LiveBench: A Challenging,
Contamination-Limited LLM Benchmark. arXiv
preprint arXiv:2406. 19314.


===== PAGE BREAK =====

Weihao Xuan, Rui Yang, Heli Qi, Qingcheng Zeng,
Yunze Xiao, Aosong Feng, Dairui Liu, Yun Xing,
Junjue Wang, Fan Gao, et al. 2025. MMLU-
ProX: A Multilingual Benchmark for Advanced
Large Language Model Evaluation. arXiv preprint
arXiv:2503. 10497.

Shu-wen Yang, Po-Han Chi, Yung-Sung Chuang,
Cheng-| Jeff Lai, Kushal Lakhotia, Yist Y. Lin,
Andy T. Liu, Jiatong Shi, Xuankai Chang, Guan-
Ting Lin, Wei-Cheng Huang, Wei-Cheng Tseng,
Da-Rong Liu, Zili Huang, Shuyan Dong, Shang-
Wen Li Wang, Zhaoheng Ni Ma, Benjamin Chen,
Chih-Liang Chang, Kevin Lin, Wen-Chin Huang,
Andy T. Wu, Po-Chun Hsu, Chun-Lin Chen,
Han Lu Cheng, Yu Tsao, Hung-Yi Hsieh, and
Hung-Yi Lee. 2021. Superb: Speech processing
universal performance benchmark. In Proceed-
ings of Interspeech.

Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali
Farhadi, and Yejin Choi. 2019. HellaSwag: Can
a Machine Really Finish Your Sentence? — In
Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics, pages
4791-4800.

Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng
Gao, Kevin Duh, and Benjamin Van Durme. 2018.
Record: Bridging the gap between human and
machine commonsense reading comprehension.
arXiv preprint arXiv:1810.12885.

Chengzhi Zhong, Fei Cheng, Qianying Liu, Jun-
feng Jiang, Zhen Wan, Chenhui Chu, Yugo Mu-
rawaki, and Sadao Kurohashi. 2024. Beyond
English-centric LLMs: What language do multi-
lingual language models think in? arXiv preprint
arXiv:2408. 10811.
