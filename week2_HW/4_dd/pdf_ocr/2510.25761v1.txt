arX1iv:2510.25761v1 [cs.CL] 29 Oct 2025

DiagramEval: Evaluating LLM-Generated Diagrams via Graphs

Chumeng Liang’, Jiaxuan You!
"University of Illinois Urbana-Champaign

chumeng] @illinois.edu, jiaxuan @illinois.edu

Abstract

Diagrams play a central role in research papers
for conveying ideas, yet they are often notori-
ously complex and labor-intensive to create. Al-
though diagrams are presented as images, stan-
dard image generative models struggle to pro-
duce clear diagrams with well-defined structure.
We argue that a promising direction is to gen-
erate demonstration diagrams directly in tex-
tual form as SVGs, which can leverage recent
advances in large language models (LLMs).
However, due to the complexity of components
and the multimodal nature of diagrams, suffi-
ciently discriminative and explainable metrics
for evaluating the quality of LLM-generated
diagrams remain lacking. In this paper, we
propose DiagramEval, a novel evaluation met-
ric designed to assess demonstration diagrams
generated by LLMs. Specifically, DiagramEval
conceptualizes diagrams as graphs, treating text
elements as nodes and their connections as di-
rected edges, and evaluates diagram quality us-
ing two new groups of metrics: node alignment
and path alignment. For the first time, we ef-
fectively evaluate diagrams produced by state-
of-the-art LLMs on recent research literature,
quantitatively demonstrating the validity of our
metrics. Furthermore, we show how the en-
hanced explainability of our proposed metrics
offers valuable insights into the characteristics
of LLM-generated diagrams. Code: https:
//github.com/ulab-uiuc/diagram-eval.

1 Introduction

Diagrams play a central role in research papers
for conveying ideas, e.g., the diagram of Trans-
former (Vaswani et al., 2017) has played a pivotal
role in presenting and publicizing the idea, mak-
ing it one of the most cited deep learning papers.
However, generating high-quality diagrams is often
notoriously complex and labor-intensive to create.
Consequently, automated diagram generation is a
central challenge in AlI-assisted scientific discov-
ery (Eger et al., 2025), potentially saving millions

of hours for researchers while improving the qual-
ity of research publications. Although diagrams
are presented as images, standard image generative
models struggle to produce clear diagrams with
well-defined structure (Zala et al., 2023). We argue
that a promising direction is to generate demon-
stration diagrams directly in textual form as SVGs,
which can leverage recent advances in large lan-
guage models (LLMs).

Existing methods on automated diagram genera-
tion with LLMs rely heavily on proprietary LLMs,
either through direct planning (Mondal et al., 2024;
Zhang et al., 2024; Cui et al., 2025) or as assistance
in diagram generation (Belouadi et al., 2023, 2024;
Cui et al., 2025). Given this reliance, advance-
ments in cutting-edge proprietary LLMs, such as
Claude 3.7 Sonnet (Anthropic, 2025) and Gem-
ini 2.5 Pro (Google DeepMind, 2025), directly en-
hance automated diagram generation through SVG
code generation capabilities (Blecher et al., 2023).

However, existing evaluation metrics lack the
expressiveness needed to differentiate diagrams of
varying quality. Most benchmarks employ model-
based, diagram-level metrics (Hessel et al., 2021;
Fu et al., 2023), which were originally designed
for general text-to-image tasks rather than text-
within-image generation specific to diagrams (Ro-
driguez et al., 2023). These metrics evaluate en-
tire diagrams using general vision models, which
limits their explainability and their ability to ac-
curately capture detailed logical correctness, re-
sulting in only moderate correlation with human
judgments (Eger et al., 2025). Thus, there is a
critical need for new metrics.

In this paper, we propose DiagramEval, a novel
evaluation metric designed to assess demonstration
diagrams generated by LLMs. As demonstrated in
Figure 1, DiagramEval conceptualizes diagrams as
graphs, treating text elements as nodes and their
connections as directed edges, and evaluates di-
agram quality using two new groups of metrics:


===== PAGE BREAK =====

Preprocessing: Diagram
Graph Extraction

—_ee---  --—_

_-—

Reference Text Reference Diagram

New Metric 2
Path Alignment

New Metric 1
Node Alignment

’ 7 >> \ Excluded
1
\

x
oC | &

/

_

/
I
I
\   J
yA                      :
1      \
\

or Diagram       Graph (Graph A)              \              F                   !
aR FSS SS SS             \      \          7                  I
[Onn
\
        5
3                          1
Generated Generated Diagram Prec.=7=1         Prec.= —— = 0.5
Diagram       Graph (Graph B) — Recal.= — = 0.75 Recal.= x = 0.33

Figure 1: DiagramEval framework overview. By considering both research paper context and paper diagrams as
directed graphs, we use information precision and recall among nodes and edges of the generated diagram graph
and those of the reference graph (from reference diagrams or paper context) to measure the generation quality of

paper diagrams.

node alignment and path alignment. Specifically,
we transform diagrams into SVG format, parsing
text elements as nodes. Connections between el-
ements are then extracted as edges. Our metrics
quantify the node and path (multi-hop edges) align-
ment between generated and ground-truth diagrams
using precision, recall, and F1 scores. For the first
time, we effectively evaluate diagrams produced
by state-of-the-art LLMs on recent research liter-
ature, validating the proper consistency between
our metrics and existing metrics. We also demon-
strate metric statistics and case studies that reveal
some previously unrecognized shortcomings of ex-
isting metrics due to their moderate explainabil-
ity, for example, over-sensitivity to spatial layouts
and other visual elements and metric hacking. We
furthermore show empirically how the enhanced
explainability of our metrics helps overcome these
shortcomings and meanwhile offers valuable in-
sights into the characteristics of LLM-generated
diagrams.

Based on our proposed metrics, we build
a benchmark to evaluate the quality of LLM-
generated paper diagrams. We implement an au-
tomated pipeline to extract context and diagrams
from preprint versions of research papers on Arxiv
only with their titles. With this highly available

data source, we construct a simple agent for dia-
gram generation with Gemini 2.5 Flash Image, the
proprietary image generative models from Google.
Our metrics are then used to evaluate the quality of
generated diagrams. To the best of our knowledge,
this is the first benchmark specified for evaluating
LLM-generated paper diagrams. Code: https:
//github.com/ulab-uiuc/diagram-eval.

2 Related Works

Extensive research has addressed both the under-
standing (Han et al., 2023; Liu et al., 2023; Wang
et al., 2024b; Hu et al., 2024) and generation (Mad-
digan and Susnjak, 2023; Yang et al., 2024) of
scientific images, with specific efforts targeting di-
agram generation (Zala et al., 2023; Zhang et al.,
2024; Mondal et al., 2024; Cui et al., 2025). Cur-
rent automated evaluation metrics fall into two
main categories: 1) diagram-to-diagram similar-
ity (Fu et al., 2023), and 2) caption-to-diagram
similarity (Zhang et al., 2019; Hessel et al., 2021).
Both metric types primarily measure overall simi-
larity between generated diagrams and references
in latent space, neglecting detailed logical accuracy,
such as verifying element-wise connections against
the paper context. Closely related to our approach
is the VPEval metric (Cho et al., 2023), which


===== PAGE BREAK =====

evaluates element and connection accuracy via vi-
sual question answering (VQA). However, VPEval
often struggles to accurately identify connections
even in diagrams involving common knowledge,
requiring manual annotations (Zala et al., 2023).
Our approach extends the concept of VPEval by
leveraging vision-language models (VLMs) and
file parsing to reliably extract elements and con-
nections, thus better suited for evaluating diagrams
generated from academic papers.

3 Preliminary

Problem Definition Following existing research
working on diagram generation (Zala et al., 2023;
Mondal et al., 2024), we define the task of auto-
mated generation of scientific diagrams from aca-
demic papers: Given the input corpus {T,c, 7},
which includes original paper context T, origi-
nal diagram captions Co,;,, and layout captions
Clayout, Our goal is to generate a diagram D which
demonstrates the overall idea of research paper
T’. We have a groundtruth diagram D,;, which
we use to generated the layout captions with an
independent vision language model. D,; also serve
as reference to evaluate generated diagrams. As
mentioned, the diagram generation is done in the
SVG format (Blecher et al., 2023), which is both
editable and widely adopted by proprietary LLMs.

4 Methodology

As illustrated in Figure 2, our evaluation framework
for LLM-generated diagrams is based on the core
idea of treating diagrams as text-attributed graphs
(TAGs) (Yang et al., 2021). In these graphs, dia-
gram elements associated with text are defined as
nodes, while connections between elements form
directed edges. We introduce novel metrics to eval-
uate these diagrams from two complementary per-
spectives: Node Alignment, which assesses the
matching of nodes between the generated and ref-
erence diagrams, and Path Alignment, which eval-
uates the consistency of paths in both diagrams
connecting matched nodes.

The primary challenge in performing this de-
tailed evaluation lies in effectively extracting nodes
and edges from SVG diagram files, which we ad-
dress in the following section.

4.1 Constructing Diagram Graphs

Node Extraction: Text exists in SVG files in the
form of SVG text items. Since they are accessible

by scanning SVG files, the key problem of node
extraction is to determine what text items belong
to one node. We combine the spatial and semantic
factors in a two-step process to tackle this process.

The upper half of Figure 2 outlines the node
extraction process. First, we form a draft node
list by parsing the SVG file. We collect spatial
coordinates and span lengths of all text items in
the file. Then, we investigate every item pair about
whether 1) their y-coordinates differ by less than
x font size, and 2) their spans in the x-coordinate
overlap for more than 7. If both conditions are
fulfilled, two items are considered as one node. We
apply above parsing to get the draft node list.

Second, we exploit a light-weight multi-modal
LLM to refine the draft node list according to
the text semantic. This step takes the rendered
SVG image and draft node list as input. The LLM
is tasked with several refinement operations: (1)
Merging spatially contiguous and semantically re-
lated text nodes; (2) Adding missing conceptual
nodes, which may include non-textual elements
(such as icons or logos); and (3) Removing nodes
with unclear semantics. We obtain the final node
list V = {v;} from the LLM output.

Edge Extraction: Edges are represented implic-
itly in the diagram, making their identification
challenging. Manually defining rules to capture
diverse visual forms of connections (e.g., arrows
with various styles, lines, and spatial arrangements
indicating logical flows) from raw SVG data is
highly complex. Thus, we again employ an LLM
with vision capabilities. As depicted in the lower
half of Figure 2, this model is provided with the
rendered diagram image and a curated list of nodes,
each with a unique identifier and textual content.
The LLM analyzes visual cues—such as arrows,
lines, and proximities—to identify all directed
connections between node identifiers. This yields
a set of directed edges EF = (vj, v;),..., where
uj,v; © V. Combining edge set E’ with node set
V results in the complete extracted graph G(V, E).

This node and edge extraction process is indepen-
dently applied to both the LLM-generated diagram
D, resulting in the graph Gye, and the groundtruth
diagram Dgt, yielding the reference graph G;e.
We evaluate its accuracy in Section 5.4.


===== PAGE BREAK =====

Metric 1:  Node Alignment

\         a
Ws: Diffusion Transformer  =)  ‘ug: Diffusion Transformer  =; Matching ~      9: Diffusion Transformer ) ~        vs: Diffusion Transformer         =
=    ==                                       p                                          = E
FES 57) Extipet text                 Merge or add    u,: VAE Decoder           mec            4: VAE Encoder    Merge or add
Recal.                               8

Extract text [2
by parsing

| Generated” ane                                        nodes paved                                                          Fl                             Da) VAE Decoder          nodes based
| Diagram (SVG)

ee

Diagram Graph Nodes                   ee Graph Nodes

Metric 2: Path signet

Text Input

nodes according
to the image

Generated
Diagram (PDF)

MLLM                          Diagram Graph

Image                       Prompt: Extract                                                                                                —
Input                        edges between

Reference
Diagram (SVG)

on PDF

Diagram Graph Nodes

Diagram Graph Nodes

Text Input

Prompt: Extract
edges between

Diagram Graph es according aT M
to the image

Reference
— _Diagram (PDF)

Figure 2: The detailed pipeline of DiagramEval framework. Intuitively, Node Alignment measures the correctly
matched text elements between generated and groundtruth diagrams while Path Alignment measures the correctly

matched connections upon matched elements.

Model                         Node Alignment                  Path Alignment               CLIPScore
prec. — recal.       Fl        prec. — recal.       Fil        Text      Image

Llama 4 Maverick 0.4737 0.3121 0.3470 0.2260 0.2506 0.2005 0.6962 0.6950

Gemini 2.5 Pro       0.3600 0.3741 0.3341 0.2503 0.2817) 0.2261 0.6090 0.7021

Claude 3.7 Sonnet 0.2921

0.5087 0.3500 0.3353 (0.2108

0.2419 0.6206 0.7324

Table 1: Results on diagram generation of three LLMs over our metrics and two CLIPScore metrics (Hessel et al.,
2021), the most widely-used evaluation measurement. Half or more nodes and edges in the reference diagrams are
missing or not recognizable, indicating the bad quality of LLM-generated paper diagrams.

4.2 Evaluation Metrics

Node Alignment These metrics appraise the fi-
delity of the textual content within the generated
diagram. This involves assessing the degree to
which the set of text elements V,,,, in the gener-
ated diagram aligns with the set V,-¢ from the
reference diagram. The core of this compari-
son is a node-matching procedure: each node in
Vgen is compared against all unmatched nodes in
Vref A match is established if the textual simi-
larity between a pair of nodes, surpasses a prede-
fined threshold. Denoting My as the set of suc-
cessfully matched node pairs (vgen, Vref), Where
Ugen © Vgen and Vre¢ © Vref, We quantify perfor-
mance as follows:

¢ True Positives (TPy): The cardinality of the
set of matched node pairs, |My].

¢ False Positives (FPy): The count of nodes in
Veen that remain unmatched, |Vgen| — TPv.

¢ False Negatives (FN): The count of nodes in
V-ef that remain unmatched, |V;-.f| — TPv.

Based on these quantities, we compute informa-
tion retrieval metrics—Precisiony, Recally, and
Fl-scorey as our proposed metrics for node align-
ment.

Path Alignment We also assess the node-wise
structural fidelity encoded in Ggen corresponds with
that in G,-f in addition to node alignment. We
choose to investigate path, whose existence is a
reachability indicator between two nodes. Not lim-
ited to neighborhood, paths reveal all relationships
in the diagram, thus being a better feature for rela-
tional information comparison.

Crucially, this comparison is constrained to the
subgraph induced by the previously matched nodes
My, because node appearance has been evaluated
in the Node Alignment. Specifically, we exclude
those unmatchable nodes in one graph that are not
possible to get involved a path in the other graph.

Let My = (vgen, Vref) denote the set of
matched node pairs. We define Vjy as the set of
matched nodes, i.e., Vig = {Ugen | (Ugens Vref) €
My} = {ref | (Ugens Vref) € My}. For sim-
plicity, we maintain a one-to-one correspondence
between nodes in Vin and Vie via the mapping
defined by My.

We then induce subgraphs Gen and gM fon the
matched nodes Vj, in both the generated and refer-
ence graphs, respectively.

For each ordered pair of distinct matched nodes
(u,v) where u,v € Vay and u F v, we assess: 1)
whether there exists a path from u to v in Gens a   and
2) whether there exists a path from the correspond-
ing node wu’ to v’ in Geer. where (wu, u’) € My and


===== PAGE BREAK =====

(v,v’) € My. Formally, we define:

Pgen ={(u,v) | UF v,

path from u to v exists in G
Pret ={(u,v) | uF v,

path from w’ to v’ exists in GY ai

M
mm ()

where, for each (u,v) € Pyen or Pref, u and v are
matched nodes and wu’ and v’ are their respective
counterparts in the other graph according to My.
We can then compare Pgen and P,¢f by defining:

¢ True Positives (TPp): The number of node
pairs for which a path exists in both induced
subgraphs, i.e., |Pgen M Pref].

¢ False Positives (FPp): Node pairs where a
path exists only in Gane ie., |Pgen \ Pref]:

¢ False Negatives (FNp): Node pairs where a
path exists only in Gree ie., | Prep \ Pyen|-

Based on these quantities, we compute
Precisionp, Recall p, and Fl-score p as our metrics
for path alignment.

In concert, metrics for Node Alignment and
Path Alignment furnish a fine-grained and explain-
able evaluation of LLM-generated diagrams. They
discriminate diagrams by exactly telling the mis-
matching of text elements and their relationship.
Our advantage in interpretablity also provides guid-
ance on where the generation could be improved.
In the next section, we support this point by results.

5 Experiment

This section discuss our experiments conducted
to validate our metrics by comparison with CLIP-
Score (Hessel et al., 2021), the most widely-used
evaluation metrics for diagram generation. We first
explain our experiment setup in Section 5.1. Then,
we give the main result of our experiment in Sec-
tion 5.2. Section 5.3 demonstrates the statistics
of metrics in the experiment. Section 5.4 vali-
dates our metrics by comparing to human evalua-
tion. Section 5.5 explains with cases what happen
when our metrics and CLIPScore differ, respec-
tively. Throughout our experiment, we show that
our metrics provide unique and beneficial informa-
tion towards better evaluation of automated dia-
gram generation.

5.1 Experimental Setup

As mentioned in Section 3, we first prompt state-
of-the-art LLMs to generate diagrams based on the
text input. Then, we evaluate the generated dia-
grams over our 6 metrics (3x Node Alignment,
3x Path Alignment) and CLIPScore (Hessel et al.,
2021), the most common metric for diagram gener-
ation. Following are our detailed setup:

Diagram Generation We pick three cutting-
edge LLMs for diagram generation: Llama 4 Mav-
erick, Gemini 2.5 Pro, and Claude 3.7 Sonnet,
which we access by their official APIs. The unified
prompts we use to generate diagrams are omitted
to Appendix A.3. The layout caption is generated
by Gemini-2.0-Flash-lite by prompting to generate
a layout caption for the diagram image. The used
prompts are omitted to Appendix A.3.

DiagramEval We use Gemini-2.0-Flash-lite as
the LLM used in our evaluation pipeline, includ-
ing node extraction refinement and edge extraction.
The used prompts are omitted to Appendix A.3. Kv
and 7 in the node extraction are empirically set to
1.5 and 0.2.

Baseline We use the code ! of Diagram-

merGPT (Zala et al., 2023) for computing CLIP-
Score. Following their implementation, we use
SigLIP (Zhai et al., 2023) * as the vision and
language encoder in CLIPScore. LLM-generated
layout captions and groundtruth diagrams are
selected as text reference and image reference
for CLIPScore, respectively. Following Dia-
grammerGPT (Zala et al., 2023), we use model-
generated captions for computing CLIPScore be-
cause original captions may not cover enough de-
tails of the groundtruth diagram. Notably, VPE-
val (Cho et al., 2023) is not suitable for evaluat-
ing paper diagrams. First, the object detection
in VPEval needs text to explicitly provide the ob-
jects. However, in paper diagrams, there are often
dozens of objects while few of them are explic-
itly described by paper context. Hence, we cannot
use VPEval to evaluate paper diagrams. Second,
VPEval only counts direct connections between ob-
jects. However, generated diagrams tend to skip
some connections in the reference diagrams, for
example, a-c compared to the original a-b-c. This

‘https: //github.com/aszala/DiagrammerGPT
*https: //huggingface.co/google/
siglip-so40@m-patch14-384


===== PAGE BREAK =====

Node_Precision

PDFs of Node-Level Metrics - Group: Overall

Node_Recall

Node_F1

2.0                                  2.0      _ |
A

pis         =   =              Bis    [7
2                          5          =
B10       =                        B10   |            |

:     yy            im               ba |

WA
|
054 <4             Loy      0.5
Sy _i
0.0                        0.0

2.5        _
20, ay

fis    wy      \

a

Nl.            B19   /         \
N                    Vi               ”
“a |    05                     a

0.0                      =

“0.0         0.2          0.4          0.6         0.8          1.0       “0.0         0.2          0.4
Score Value

PDFs of Path-Level Metrics - Group: Overall

Path_Precision

Score Value

Path_Recall

0.6      08      1.0      0.0      0.2      0.4      0.6      0.8      1.0
Score Value

Path_F1

0                                                             oO
0.0          0.2          0.4          0.6          0.8          1.0        0.0          0.2          0.4
Score Value

Score Value

0.6       08       1.0      0.0       0.2       0.4       0.6       0.8       1.0
Score Value

PDFs of Vision-Based Similarity Metrics - Group: Overall

ImageText_Sim

Imagelmage_Sim

Density
coo HN WwW RU DS &

Density
Now

a

oor

0       0.2       0.4       0.6       0.8       1.0
Score Value

Figure 3:

is normal in most cases when b is not very impor-
tant compared to the correct connection between
a and c, for example, b is a linear layer between
two backbone models. However, VPEval cannot
capture such indirect connections, thus losing dis-
crimination.

Dataset To avoid knowledge leakage, we collect
papers accepted by CVPR2025 as the source of our
evaluation dataset, because they are released after
the data cutoff date of three LLMs. We use an au-
tomated pipeline to select diagrams with abundant
text annotation and collect them with captions and
corresponding paper context. A total number of
361 items are included in our dataset. Data consen-
sus and licenses are omitted to Appendix A.1.

5.2 Quantitative Result

Table 1 shows our quantitative result. Three mod-
els have similar performance in diagram generation
over our metrics and CLIPScore, proving our basic
soundness. All metrics agree that Claude generates
diagrams that best align with the groundtruth dia-
grams, for it performs the best over 4 out of our 6
metrics and CLIPScore (Image).

0.0      0.2       0.4      0.6      0.8      1.0
Score Value

Statistic results: probability density functions (PDFs) of our 6 novel metrics and 2 CLIPScore.

One interesting observation is that Claude suffers
from poor node alignment precision while having
the outstanding node recall. One potential expla-
nation is that its generated diagrams tend to in-
clude extraneous nodes compared to groundtruth
diagrams. To validate this assumption, we count
the average number of nodes in generated diagrams
by three LLMs. The result fits our assumption that
Claude produces diagrams with 31.67 nodes on
average, much more than 21.42 nodes of Gemini
and 10.68 nodes of Llama. This may also explain
the poor performance of Claude over CLIPScore
(Text): While CLIPScore (Image) puts weights on
the layout and visual elements, CLIPScore (Text)
is More sensitive to unexpected text. Hence, irrel-
evant text elements greatly affect Claude’s perfor-
mance over CLIPScore (Text).

This observation highlights how our metrics pro-
vide interpretable insights into diagram generation
performance, complementing coarse-grained met-
rics with fine-grained, structure-aware evaluation.

5.3 Statistics

We compute the probability density functions of
our 6 novel metrics and 2 CLIPScore (Hessel et al.,


===== PAGE BREAK =====

2021), whose result is given in Figure 3. We also
analyze their correlation and demonstrate the result
in Figure 4.

Node Alignment metrics showed a notable pos-
itive correlation with CLIPScore metrics, likely
due to their common focus on text element con-
sistency. However, Node Alignment exhibited
healthier score distributions. This improvement
stems from isolating node-level textual alignment
from factors like spatial layouts, colors, and styles,
which significantly affect CLIPScore despite be-
ing irrelevant to textual content accuracy. Simply
changing the layout from vertical to horizontal can
partially offset the drop in CLIPScore caused by
removing all connections in the diagram. Addition-
ally, CLIPScore’s sensitivity to superficial image
elements also inherently limits scoring extremes,
constraining its effectiveness in diagram evalua-
tions. Given the fact that novel methods often re-
port only 0.01 improvements on CLIPScore (Zala
et al., 2023; Mondal et al., 2024), Node Alignment
is a good complement and a potential alternate
to CLIPScore.

Conversely, Path Alignment metrics displayed
minimal correlation with CLIPScore. The case
study in the next section will show that this is be-
cause many diagrams generated by three LLMs,
even they are state-of-the-art ones, perform poorly
in expressing relationship. This finding is co-
validated by existing research on whether LLM can
understand graph structures within text (Wang et al.,
2023). However, this shortcoming has never been
explicitly revealed by any existing metrics. Path
Alignment provides fine-grained, interpretable in-
sights into missing or incorrect connections, being
a novel perspective of evaluation not offered by
existing metrics.

5.4 Human Evaluation

Accuracy of Node Extraction: Our node extrac-
tion simply transfers one single text element in
the SVG format into one node. This is accurate
because phrases are naturally placed in one text
element in both reference and generated diagrams.
2) Edge extraction: We conduct new experiments
to examine the accuracy of edge extraction. We
randomly pick 1 edge for every diagram and let
two machine learning researchers judge whether
this edge exists in the diagram.

The result shows that 85.87% of the nodes in
reference diagrams and 90% in generated diagrams
are extracted accurately, where 361/361 of refer-

Correlation Matrix of Metrics - Group: Overall

Figure 4: Correlation map of our 6 novel metrics and 2
CLIPScore metrics. Metrics of Node Alignment show
considerable positive correlation with 2 CLIPScore met-
rics. Metrics of Path Alignment appear to be indifferent
with 2 CLIPScore metrics.

ence diagrams and 300/361 of generated diagrams
are evaluated (no edge cannot be extracted from the
rest 61 generated diagrams because of the low qual-
ity). While it is hard to know how many edges are
there in the diagram, these precision scores make
sure that extracted edges are highly possible to ex-
ist in the diagram. This validates the accuracy of
our edge extraction process. We will add this result
to our new draft to complement the paper.
Correlation with human evaluation: We fol-
low the human evaluation of Cho et al. (2023) and
select a subset of 50 reference diagrams (the first
50 by the name order) and corresponding generated
diagrams by Gemini-2.5-Pro. Two senior machine
learning researchers then evaluate the semantic sim-
ilarity between reference and generated diagrams
by answering the question: do two diagrams ex-
press the same logic? Our interface offers three
options for the human evaluators: good (1.0), fair
(0.5), and bad (0). The average similarity score
is 0.3298, with nearly half of the results being 0.
The following Table 2 shows the correlation be-
tween the human-evaluated similarity score and
metrics in our papers: Our new metrics show

Metric                            Correlation
Node F1                                       0.4316
Path Fl                           0.4052
CLIPScore-Text                   0.1065
CLIPScore-Image               0.0831

Table 2: Correlation with human evaluation scores of
different metrics. Our metrics align better with the
judgment of human experts.


===== PAGE BREAK =====

(Grourn Truth LaTex                                                       Predicted LaTex                                               Step 1: Element Localization                               Step 2: Element Region Matching

T                                                                                   T
Input             1. Calculate Match Cost Between _—_2. Find the Best Match by
Every Element Pairs                    Hungarian Algorithm

1. LaTeX Source Nermalization

| || legge <gemsal
a.           >
=]
ea OP  = Sra
Cheers PDT        ee    ,

T]| Xsin J[\beta
Weft JL \sioma |_| C[2 ID ]-+
\sigme || — JC JD [vright) | T      Step 3: Invalid Match Elimination        Step 4: Metric Calculation

lement Region Localization        Token Consistency Chec!
2, Element Region Localizati       1, Token Consistency Check                  aaa

Pate foment

er          :
B
one imaid match

20
Z1P-FPEN

Fiscore=

(@) Invalid Match Elimination
‘Token Consistoncy Chock

Position Relationship Consistency Chock
(PANSAC, ate vartomaten: variation, sean)

mi-(a)f =

Valdated fiched Pars

(4) Metric Calculation                                                                                   Sy
(COM = (2° 1P)/(2"TP+FP + FN)                                                       o
ExpRate@CDM = (1IN) £ (COMI = 1)

Generated                                                                                                   Groundtruth

_ 2x13
2x13+3-3

+0125

Figure 5: Case: Low CLIPScore (Text) and high Path F1. CLIPScore (Text): 0.2558. Path F1: 1.

(c) AeroGen Architecture

(a) Layout Embedding

Layout Embedding Module                         Layout Info Injection                                                                                                                                             3
Bounding Box Coordinates.                               Noise Level Injection                                                                                                                                                            3
Vectorized Semantic info                                         Local Mask Control                                                                                                                                                                                                s

Fourier + MLP Layers

Local Mask
CLIP Text Encoder
MILP Layers }-[CLIP Encoder

AeroGen Overall Architecture

Class             yea)
Embedding                                      DownBlock
Residual Block

Global
Control

An aerial image with a Ground
‘track field and Tennis court.

‘Timestep: Image Denoising                                                                                                                                                                                                                                                               xN

Layout Info injection               Denoising Medule                 Generated Image                                                                                       4 oe           NXHxW

iG

UOT3Ua33y
§S0u9-ySeW

Se
@ trainable

%B frozen

Class
Embedding.                            Kav

Generated                                                                                                   Groundtruth

Figure 6: Case: High CLIPScore (Text) and low Path Fl. CLIPScore (Text): 1. Path F1: 0.

(a) Existing setting                                   (b) L2RW setting                           Privacy-invasive  xX                              Privacy-preserved A
| Centralized training | | Centralized training |                                  | Decentralized training | | Decentralized training |
Centralized training                                                                                     Decentralized training

Privacy-nvasive                                                                         Privacy-qreserved

Entity vette eee             Entity N

a                              Camera
Entity N                                 Entity N                   SS  <—\——_       7              Level

(a) Existing setting                                (b) L2RW setting

Generated                                                                                                   Groundtruth

Figure 7: Case: Low CLIPScore (Image) and high Node F1. CLIPScore (Image): 0.6007. Node F1: 0.8696

pat

CSPT: Confident Points                      DCPG: Dynamic Cluster                     DS score: Distribution                                                                                                                                                                           E       se
Semantic Transfer                                   Pseudo-label Generation                         Shape score                                                                                                                                                                                                                                                            a       4
‘Training          Pus
High-confidence semantic               Se             mically clusters neighbor          Pselch fatal Ateopoality of
masks transfer                                     points of seed points                              pseudo-label proposals                                                                                                                                                                                                              1
i

{|__|

3D Object Detector Training
First stage: Train with pseudo-labels

Second stage: Fine-tune with sparse accurate labels ff wenn nnd OU
DS score for NMS to suppress low-quality proposals

Sparse labels

[]Pseudo-label Semantic seed point © Other point        é Cosine Similarity   a Filtering

Generated                                                                                                   Groundtruth

Figure 8: Case: High CLIPScore (Image) and low Node Fl. CLIPScore (Image): 0.8094. Node F1: 0.16


===== PAGE BREAK =====

better alignment with human evaluation. We ob-
serve that CLIPScore-Image may give a generated
diagram a relatively good score even if its element-
wise logic is definitely different from the reference,
while CLIPScore-Text is too sensitive to the text
description. While our metrics are more aligned to
human judgment compared to CLIPScore, the most
widely-used metric in the field, we believe they are
trustworthy enough to provide another perspective
of evaluation to generated diagrams.

5.5 Case Study: When and how our metrics
and CLIPScore differ

The metric statistics show that our new metrics does
not fully consistent with CLIPScore, the classical
evaluation metric. This raises a research question:
What happen when our metrics and CLIPScore
differ? In this section, we select four cases with
distinct scores by our metrics and CLIPScore and
explain why the difference takes place.

Figure 5 shows a case with low CLIPScore (Text)
and high Path Fl. We can see an explicit data flow
in the generated diagram, with good alignment with
the semantic of groundtruth diagram as well as the
original paper (Wang et al., 2024a). This is well
captured by our Path Fl. However, the spatial lay-
out and icons in the generated diagram are very dif-
ferent from those in the groundtruth. While layout
captions used in CLIPScore (Text) include detailed
description to these visual elements, the generated
diagram performs poorly over CLIPScore (Text).
This case consolidates the point that the sensitivity
to visual elements of CLIPScore hinder its recogni-
tion of good diagrams. More robust to the interfer-
ence of layouts and icons, our metrics complement
this disadvantage.

Figure 6 shows a case with high CLIPScore
(Text) and low Path Fl. In contrast to the above
case, there is no clear data flow. Hence, our Path F1
assigns 0 to this diagram. However, the generated
diagram includes all text mentioned in the layout
caption. For this reason, it has perfect alignment
with the layout caption under CLIPScore (Text).
This exposes an inherent problem of depending
evaluation on models: the generator may hack the
metric. Only by placing all text elements men-
tioned in the layout caption (rather than those exist-
ing in the original diagram), the generated diagram
yields perfect score in the model-based compari-
son with the layout caption. Our metrics ease this
problem by obtaining intermediates from models.

Figure 7 shows a case with low CLIPScore (Im-

age) and high Node Fl. Similar to the case in
Figure 5, the evaluation of CLIPScore is interfered
by the difference in spatial layout and non-textual
icons. By contrast, our Node F1 focuses only on
the existence of text elements, thus giving a more
objective result. Figure 8 suffers similar problems,
where CLIPScore (Image) ignores detailed logic
and evaluates the diagram unreasonably.

To conclude, our metrics overcome two short-
comings of CLIPScore: visual element interfer-
ence and metric hacking in these four cases. This
proves that our metrics constitute good comple-
ments to CLIPScore.

6 Conclusion

This paper introduces DiagramEval, a novel set
of metrics for evaluating large language model
(LLM)-generated scientific diagrams. Unlike ex-
isting evaluation methods, DiagramEval represents
diagrams as graphs and performs automated extrac-
tion of graph structures from diagrams. Evaluation
metrics are then computed through fine-grained
comparisons between the nodes and paths in the
generated and reference graphs. DiagramEval
addresses the current lack of fine-grained, inter-
pretable, and structure-aware metrics in the assess-
ment of automated diagram generation.

Limitations

Our proposed metrics have limitations primarily
due to uncertainties in LLM performance. Specifi-
cally, the LLM may not reliably identify all edges
in the reference graph, potentially causing inaccu-
rate or underestimated evaluations. Despite this,
our metrics reduce dependency on complex models
and offer more interpretable outcomes compared
to existing methods. Addressing these limitations
is an important direction for future work.

As our work focuses on evaluating diagram gen-
eration, it does not raise new potential risks other
than those general ones by using LLMs to generate
contents.

Acknowledgments

We sincerely appreciate the support from Amazon
grant funding project #120359, "GRAG: Enhance
RAG Applications with Graph-structured Knowl-
edge", and Meta gift funding project "PERM: To-
ward Parameter Efficient Foundation Models for
Recommenders".


===== PAGE BREAK =====

References

Anthropic. 2025. Claude 3.7 sonnet system card.

Jonas Belouadi, Anne Lauscher, and Steffen Eger.
2023. Automatikz: Text-guided synthesis of sci-
entific vector graphics with tikz. arXiv preprint
arXiv:2310.00367.

Jonas Belouadi, Simone Ponzetto, and Steffen Eger.
2024. Detikzify: Synthesizing graphics programs for
scientific figures and sketches with tikz. Advances in
Neural Information Processing Systems, 37:85074—
85108.

Lukas Blecher, Guillem Cucurull, Thomas Scialom, and
Robert Stojnic. 2023. Nougat: Neural optical un-
derstanding for academic documents. arXiv preprint
arXiv:2308. 13418.

Jaemin Cho, Abhay Zala, and Mohit Bansal. 2023. Vi-
sual programming for step-by-step text-to-image gen-
eration and evaluation. Advances in Neural Informa-
tion Processing Systems, 36:6048—6069.

Zhiqing Cui, Jiahao Yuan, Hanging Wang, Yanshu
Li, Chenxu Du, and Zhenglong Ding. 2025. Draw
with thought: Unleashing multimodal reasoning
for scientific diagram generation. arXiv preprint
arXiv:2504.09479.

Steffen Eger, Yong Cao, Jennifer D’Souza, Andreas
Geiger, Christian Greisinger, Stephanie Gross, Yu-
fang Hou, Brigitte Krenn, Anne Lauscher, Yizhi Li,
and | others. 2025. Transforming science with large
language models: A survey on ai-assisted scientific
discovery, experimentation, content generation, and
evaluation. arXiv preprint arXiv:2502.05151.

Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy
Chai, Richard Zhang, Tali Dekel, and Phillip Isola.
2023. Dreamsim: Learning new dimensions of hu-
man visual similarity using synthetic data. arXiv
preprint arXiv:2306.09344.

Google DeepMind. 2025. Gemini 2.5: Our most intelli-
gent ai model.

Yucheng Han, Chi Zhang, Xin Chen, Xu Yang,
Zhibin Wang, Gang Yu, Bin Fu, and Hanwang
Zhang. 2023. Chartllama: A multimodal Ilm for
chart understanding and generation. arXiv preprint
arXiv:2311,16483.

Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le
Bras, and Yejin Choi. 2021. Clipscore: A reference-
free evaluation metric for image captioning. arXiv
preprint arXiv:2104.08718.

Anwen Hu, Yaya Shi, Haiyang Xu, Jiabo Ye, Qinghao
Ye, Ming Yan, Chenliang Li, Qi Qian, Ji Zhang, and
Fei Huang. 2024. mplug-paperowl: Scientific dia-
gram analysis with the multimodal large language
model. In Proceedings of the 32nd ACM Interna-
tional Conference on Multimedia, pages 6929-6938.

Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen,
Kaiqiang Song, Sangwoo Cho, Yaser Yacoob, and
Dong Yu. 2023. Mme: Advancing multimodal chart
understanding with large-scale instruction tuning.
arXiv preprint arXiv:2311.10774.

Paula Maddigan and Teo Susnjak. 2023. Chat2vis: Gen-
erating data visualizations via natural language using
chatgpt, codex and gpt-3 large language models. Ieee
Access, 11:45181-45193.

Ishani Mondal, Zongxia Li, Yufang Hou, Anandhavelu
Natarajan, Aparna Garimella, and Jordan Boyd-
Graber. 2024. Scidoc2diagrammer-maf: Towards
generation of scientific diagrams from documents
guided by multi-aspect feedback refinement. arXiv
preprint arXiv:2409. 19242.

Juan A Rodriguez, David Vazquez, Issam Laradji,
Marco Pedersoli, and Pau Rodriguez. 2023. Ocr-
vqgan: Taming text-within-image generation. In
Proceedings of the IEEE/CVF winter conference on
applications of computer vision, pages 3689-3698.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Ilia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing
systems, 30.

Bin Wang, Fan Wu, Linke Ouyang, Zhuangcheng Gu,
Rui Zhang, Renqiu Xia, Bo Zhang, and Conghui He.
2024a. Cdm: A reliable metric for fair and accu-
rate formula recognition evaluation. arXiv preprint
arXiv:2409.03643.

Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan
Tan, Xiaochuang Han, and Yulia Tsvetkov. 2023.
Can language models solve graph problems in natural
language? Advances in Neural Information Process-

ing Systems, 36:30840-—30861.

Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen,
Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu,
Haotian Liu, Sadhika Malladi, and 1 others. 2024b.
Charxiv: Charting gaps in realistic chart understand-
ing in multimodal Ilms. Advances in Neural Informa-
tion Processing Systems, 37:113569-113697.

Cheng Yang, Chufan Shi, Yaxin Liu, Bo Shui, Jun-
jie Wang, Mohan Jing, Linran Xu, Xinyu Zhu, Si-
heng Li, Yuxiang Zhang, and | others. 2024. Chart-
mimic: Evaluating Imm’s cross-modal reasoning ca-
pability via chart-to-code generation. arXiv preprint
arXiv:2406.09961.

Junhan Yang, Zheng Liu, Shitao Xiao, Chaozhuo
Li, Defu Lian, Sanjay Agrawal, Amit Singh,
Guangzhong Sun, and Xing Xie. 2021. Graphform-
ers: Gnn-nested transformers for representation learn-
ing on textual graph. Advances in Neural Information
Processing Systems, 34:28798-28810.

Abhay Zala, Han Lin, Jaemin Cho, and Mohit Bansal.
2023. Diagrammergpt: generating open-domain,
open-platform diagrams via Ilm planning. arXiv
preprint arXiv:2310.12128.


===== PAGE BREAK =====

Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov,
and Lucas Beyer. 2023. Sigmoid loss for language
image pre-training. In Proceedings of the IEEE/CVF
international conference on computer vision, pages

11975-11986.

Leixin Zhang, Steffen Eger, Yinjie Cheng, Weihe Zhai,
Jonas Belouadi, Christoph Leiter, Simone Paolo
Ponzetto, Fahimeh Moafian, and Zhixue Zhao. 2024.
Scimage: How good are multimodal large language
models at scientific text-to-image generation? arXiv
preprint arXiv:2412.02368.

Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q
Weinberger, and Yoav Artzi. 2019. Bertscore: Eval-
uating text generation with bert. arXiv preprint
arXiv: 1904.09675.

A Appendix

A.1 Data License and Consensus

In this study, we utilize a collection of research
papers from arXiv as our primary data source. To
ensure ethical and legal reuse, we include only
papers published under open-access licenses that
permit redistribution, including Creative Commons
Attribution (CC BY 4.0), Attribution-ShareAlike
(CC BY-SA 4.0), and Attribution-NonCommercial-
ShareAlike (CC BY-NC-SA 4.0). While the Share-
Alike and NonCommercial terms impose certain
restrictions—such as requiring derivative works
to be shared under the same license or prohibit-
ing commercial use—we fully comply with these
conditions, using the materials only for academic,
non-commercial research with appropriate attribu-
tion. Due to the fact that we only conduct text
mining on the papers and that the number of papers
is huge, we do not cite these papers.

A.2 Usage of AI Assistant
We use ChatGPT to polish the text of our paper.

A.3 Prompts

Following are four prompts we use in our experi-
ment for 1) layout caption generation, 2) diagram
generation, 3) node extraction refinement, and 4)
edge extraction, respectively.

"Do_not interpret _the_
meaning _of_the_diagram,.,
only its visual.
structure_and_element.
arrangement. Be concise.

pdf_blob_part

J

Listing 1: Prompt: Layout Caption Generation

prompt_parts = [

"Describe_the_spatial_layout
of _the_ components in,
this document ,_ focusing.
on their relative.
positions and,
connections.”,

"For example: _’ Component A.
is _above,Component..B,.
and.an_iarrow.connects. Bu
to _C.uwhich_is_to the,
right _of_A’.”,

prompt = f"""
<INSTRUCTION>

Generate_an_SVG_diagram_based_on_the.
following information.

*xkRules :*x*

1...Create_clean, well-structured_SVG_
code. _Keep_the_diagram_width="1000"_
height="700".

2... Use_main_concepts.and_expressions,,
given_in_the_original_paper_context,
for_element_text_(very important).

3... Ensure _elements.(shapes , text) do,
not _overlap.

4._._Do_**xnotx*x*x_include_any legends.

5..Arrows must start and endiprecisely.
on_the_border_of_the_elements, they.
connect. Arrows,,should avoid.
crossing_other_elements by _using.
vertical and horizontal corner.
arrows. Do not use any. sloping.
arrows.

6... Represent _the_core_mechanisms,
described_in_the_context. Avoid,
using _a_single_large block _ for ia
complex_mechanism,_that should be.
broken_down. But _also_keep._the.
mechanism_representation_intuitive.
and_easy-to-understand,_enough.

7..u**Never**x_use_any characters, leading
_to_SVG_rendering issues, for.
example ,_& (Ampersand).

8..Keep_ proper _layout tightness. Don’t.
leave_a_lot_meaningless_blank space,
between.elements.

9._ .Add_font-size_independently _to_every
 .Single_text element.

10. _ Avoid_generating_ problematic. svg.
code, _for_example ,_svg.code_with.
invalid._xml_characters_or_duplicate.
attributes.

{’11.. Adhere strictly _to_the_spatial.
layout in. the layout and element.
text.’ if spatial_layout_prompt else
wy

Please_output_xonly*_the_SVG_code block ,
 starting with, ‘<svg‘ and ending,
with. ‘</svg>‘.

<END_OF_INSTRUCTION>

*xkPaper_Context:x**
{paper_context}

*xxDiagram_Caption/Focus: xx
{diagram_caption}
{layout_section}



===== PAGE BREAK =====

Now, .output_the_SVG_code_block:

nun

Listing 2: Prompt: Diagram Generation

prompt_parts = [

"You_are.an expert diagram,
analysis assistant.
specializing _in_text.
element _coherence.”,

f"The_ following _image_isia.
*{diagram_type_name}’. I
have ialready performed.
an_initial_text.
extraction_from_its.
source, resulting _in_the
wlistlof text elements.
below.”,

"\nxxImage of the diagram: x*x

pil_image,

"\n\n«e*Currently Extracted.
Textual _ Elements. (-.
Element CID]: \"CTEXT
J\") pee",

element_list_str,

"\n\n«x* Your Task: **",

"Analyze_the_image andthe.
provided_list of.
elements. Your goal is.
to .improve,_the_element.
list by identifying.
necessary. merges ,.
additions, or removals.”

"\nl.o**Mergesxx: Identify.
if_any_listed_elements.
are parts _of_a_single,.
continuous._text block Jin
_the_image_and_should be
_merged.”,

"_JoFor example, if.’ Element
 ~ID_A: Hello’ and.’
Element_ID_B: World’.
visually form,’ Hello.
World’, they. shouldbe.
merged.”,

"\n2.o.**xAdditions**: _
Identify _two_specific.
types_of.missing._nodes:”

"_o.a) Duplicate nodes:

Nodes, that_have.the_same
text as existing nodes,
but represent _different.
instances _in_the_diagram

”
’

perererereFor example, if _there
ware_two.’mask’ nodes_in
_the_diagram_but only.
one_is inthe current.
list.”,

"_.b) Non-text _nodes: Nodes
_that.use icons wor,
images instead _of text.
to.represent._concepts.”,

oeFor_example, ian.
OpenAI_logo_representing
 LLMs, Jor ca _neural,

network icon,
representing.a,model.”,

SoooFor_these nodes ,.
generate appropriate,
text_descriptions_based,
on_their visual,
representation.”,

"\n3..**Removals*x: Identify

 nodes, that_should_be.
removed,,based.on_the.
following strict.
policies:”,

"_44a) Non-English/Non-Math,
Text: Remove._nodes. that.
contain_**ONLY**_non-
English _and_non-
mathematical.characters.

Se eoueFor example, ifiaw
node_contains_**xonlyx**_
Chinese characters, Jit.
should _be.removed.”,

eeHowever ,_if_the node.
contains.aimix of.
English _and_non-English,
text, keep _it.”,

 wutb) Numbers Only: Remove.
nodes_that_contain_**
ONLY** numbers, (
including _decimal_points
 cand_basic math symbols)

”
’

ererereneFor example ,_’123’,.
73.14’ , or.’1+2’ should,

be_removed.”,

uouoi¢) Non-conceptual.,

elements: Remove.nodes,
not_representing,,
concepts_in_the_diagram,
such _as_ texte
explanation, description
,.or,examples.”

"\nxxImportant_Notes:*x*",

"- _Do_not_consider.general,,
diagram_elements_( like.
arrows, lines, or.
decorative_elements). as.
nodes_to_be_added.”,

"- For duplicate nodes ,.
ensure,they are truly.
separate _instances _in,
the_diagram.”,

"-— For non-text nodes,

generate_clear_and,

concise_descriptions,,
that _ capture their.

meaning.",
"- For. removals, strictly.
follow the _ three.

policies_above._**Do not
_remove,nodes. for any.
other reasons .**_",

"\nxxOutput Format :**",

"First, provide_your,
analysis ina.’ Thinking.
Phase’ _section,.
explaining your,
observations.and,,
reasoning.”,

"Then, after_the signal,’



===== PAGE BREAK =====

FINAL_ANSWER:’, provide.
your, findings _as.a JSON.
object with three,
optional_keys:_’merges’,
_’adds’, and.’ removes’.”
"- ?merges’: A list Tof.
objects, where._each,,
object _has.,’ keep_id’_(
the_IDLof_ the element to
 retain _and_append._to).
and,’ remove_id’_(the_ID.
of the_element_whose,,
text_will_be_appended,
and_then_the _element,
removed).”,
"- adds’: _A list of objects
, where each_object has,
a.’ text’ key for ithe.
newly_identified_text.
string.”,
-_’ removes’: A list Toff.
objects, where._each,,
object _has_a_’id’ key.
for_the element _ID_to ibe
_removed.”,
"Example JSON output: {\"
merges\": [{\"keep_id\":
~\"G_1\",.\"remove_id\":
uN"G_2\"}],.\"adds\" 22
C{\"text\": \"LLM Model.
COpenAI)\"},.{\"text\": 2
\"Input Image\"}],.\"
removes\": [C{\"id\": .\"
G_3\"},0{\"id\": i\"G_4
\"JI}.",

"If _ no_operations.are_needed
, provide_an_empty JSON,
object _{} _or.omit keys.”

”

"Only vinclude_IDs_from_the.
provided,list for.
merging and_removing..
Ensure.’ keep_id’and,,’
remove_id’ are_different

"\nx*xResponse_Structure:*x*”",

"1, Start with _’ THINKING,
PHASE: ’ and. provide.your
detailed analysis”,

"2. After your analysis ,_
write.’ FINALLANSWER:’ on
anew line”,

"3. Then provide_the_JSON.
output”

J

Element CID]: \"[TEXT
JV") ree",
element_list_str,

"\n\n«xxTask:**",

Ff” Analyze xxallxx_
connections.(e.g.,.
arrows, lines indicating
flow) in _the_{
diagram_type}_Diagram,
image.”,

"Tdentify_**all**_DIRECTED.
one-to-one_connections,.
BETWEEN, the, provided,
element_IDs.”,

"Every element._should.
involve _in_at least _one.
connection.”

"All straight or _corner,
arrows indicate,
connections”,

"First, think step-by-step.
about the_connections..
Then, .on,a_new_line,.
provide_the_final_list.
of.connections.”,

"Output _your_findings asia,
JSON list of lists, J
where_each_inner list is
ua _pair_of_element IDs.
representing._a.directed,
connection_from,_the,
first_ID_to.the_second.
ID.”,

"For example: CL\"ID1I\",.\"
ID2\"], .[\"ID1\", .\" 1D3
\"J, CDN" ID4\", .\"1D2
\"J1.",

"Only include i**IDs**,_ (not.
the_text)_from_the_list.
provided _above. _Ensure,
the_source_and_target.
IDs .are.correct_based._on
_the_diagram’s_flow.”,

"If there are_no_connections
, return an empty listo
[].”,

"Start iyour,final_ JSON,
output _with_the_signal,,’
Final _Answer._JSON:’.”,

"\n\nx*{diagram_type}.
Diagram_Image:*x”,

pil_image,

"\n\n«x«Thinking Process and,
JSON_Output of.
Connections:x*x"

]

Listing 3: Prompt: Node Extraction Refinement

prompt_parts = [

"You_are.an expert diagram,
analysis assistant.”,

f"The_ following _image_isia.
diagram_({diagram_type})
. Ihave _already.
extracted _the_text.
elements_from_it.”,

"\n\nxxIdentified_Textual.
Elements in _the_{
diagram_type}_Diagram,(-

Listing 4: Prompt: Edge Extraction

