arXiv:2510.26484v1 [cs.CL] 30 Oct 2025

Highlights
Bayesian Network Fusion of Large Language Models for Sentiment Analysis

Rasoul Amirzadeh, Dhananjay Thiruvady, Fatemeh Shiri

e Proposes a Bayesian Network fusion framework for integrating multiple LLMs in fi-
nancial sentiment analysis.

e Models probabilistic dependencies among LLM predictions to enhance interpretability.

e BNLF improves accuracy across diverse financial sentiment datasets by up to 6%.


===== PAGE BREAK =====

Bayesian Network Fusion of Large Language Models for
Sentiment Analysis

Rasoul Amirzadeh?, Dhananjay Thiruvady”, Fatemeh Shiri?

* School of Information Technology, Deakin University, 221 Burwood
Highway, Melbourne, 8125, Victoria, Australia
’ School of Computing Technologies, RMIT University, 124 La Trobe
Street, Melbourne, 8000, Victoria, Australia

Abstract

Large language models (LLMs) continue to advance, with an increasing number of domain-
specific variants tailored for specialised tasks. However, these models often lack transparency
and explainability, can be costly to fine-tune, require substantial prompt engineering, yield
inconsistent results across domains, and impose significant adverse environmental impact
due to their high computational demands. To address these challenges, we propose the
Bayesian network LLM fusion (BNLF) framework, which integrates predictions from three
LLMs, including FinBERT, RoBERTa, and BERTweet, through a probabilistic mechanism
for sentiment analysis. BNLF performs late fusion by modelling the sentiment predictions
from multiple LLMs as probabilistic nodes within a Bayesian network. Evaluated across three
human-annotated financial corpora with distinct linguistic and contextual characteristics,
BNLF demonstrates consistent gains of about six percent in accuracy over the baseline LLMs,
underscoring its robustness to dataset variability and the effectiveness of probabilistic fusion
for interpretable sentiment classification.

Keywords: Large language model, Bayesian networks, Financial sentiment analysis, Late
fusion, Interpretable AI

1. Introduction

Advancements in deep learning, computational power, and large-scale corpora have en-
abled the development of LLMs. LLMs with billions of parameters, trained through self-
supervised learning, achieve state-of-the-art performance in tasks such as text generation,
summarization, and sentiment analysis (2024). Unlike earlier methods requir-
ing labelled data and manual feature engineering, LLMs leverage prompt-based instructions
and pretrained knowledge to classify sentiment with minimal supervision, improving both ac-

curacy and accessibility, especially in zero and few-shot settings (Krugmann and Hartmann
2024),

Sentiment analysis identifies and interprets opinions in text to assess the author’s attitude

toward an entity (Ali et al.|/2025). It is typically performed at three levels of document level,

sentence level, and aspect level, which focuses on the sentiment toward specific attributes

or components mentioned in the text (Wijayanto and Khodra\ |2018). Recent advances


===== PAGE BREAK =====

in transformer-based LLMs such as BERT and GPT-3 have significantly enhanced senti-
ment classification by capturing richer contextual information, leading to the development
of domain-adapted models fine-tuned on specific data to better reflect the vocabulary and
tone of domains (Mughal et al.| (2024).  There are various domain-specific models, such as
SciBERT (Beltagy et al.||2019) for scientific literature and BioBERT 2020) for
biomedical and healthcare-related texts.

Although domain-specific LLMs have shown considerable potential, important challenges

persist (Pattnayak et al.| |2025). They often underperform compared to general-purpose

LLMs in complex tasks, particularly those requiring reasoning or interaction with external

tools such as scientific software (2025).  Additionally, domain-specific LLMs are
prone to overfitting on the narrow vocabulary and stylistic patterns, which may hinder gen-
eralisation and reduce robustness on noisy or heterogeneous inputs
 (2024).  Another key challenge, particularly in financial

applications, is the misalignment between informal, user-generated content and structured
reference data. Social media posts, such as tweets, often include sarcasm, slang, or frag-
mented language, complicating interpretation compared to formal texts such as news articles
or reports (2024).

Beyond these data and structural challenges, scalability and energy efficiency represent
major bottlenecks, as modern LLMs demand extensive GPU resources and even domain-
specific fine-tuning incurs substantial overhead]! These costs raise concerns about the prac-

ticality of deploying LLMs in inference-only or resource-constrained settings (Gogineni et al.|
2025} |Xia et al.||2024). Furthermore, prompt-based interaction with LLMs is often proposed

as a lightweight alternative to fine-tuning. However, the performance of domain-specific
LLMs remains highly sensitive to prompt phrasing, with minor variations leading to incon-
sistent outputs (2025). While fine-tuning can improve adaptation, it introduces
trade-offs in as domain-specific gains often reduce generalisation and degrade performance on
out-of-domain tasks (2024). These limitations further emphasise the need for trustwor-
thy AI principles, particularly non-maleficence through resource efficiency and explicability
through interpretable models (2021).

To address these challenges, we introduce the Bayesian network LLM fusion (BNLF)
framework, which uses predictions from three different LLMs, incluing FinBERT, RoBERTa,
and BERTweet, to perform sentiment analysis across various curated financial corpora.
BNLF leverages the complementary strengths of individual LLMs and dynamically adjusts
their influences on sentiment prediction based on the probabilistic dependencies learned
through a Bayesian network (BN). While fusion approaches offer a promising direction, the
use of BNs for this purpose remains underexplored. BNs are particularly appropriate for
this task, as they provide a principled approach for probabilistic reasoning, naturally handle
uncertainty and bias, adapt to heterogeneous data sources, and offer interpretable graphical

structures for analysing model behaviour (Amirzadeh et al.| |2023a).

The main contributions of this work are as follows:

e We propose a novel framework, called BNLF, that integrates sentiment predictions

'For example, training costs for GPT-4 were reported at over $100M, while a single fine-tuning run of
BLOOMZ-7B consumed 7571 kW, roughly the annual electricity use of an average U.S. household.


===== PAGE BREAK =====

from multiple LLMs through probabilistic modelling. It is a modular and scalable
framework that enhances interpretability and transparent reasoning.

e BNLF is designed to be lightweight, leveraging medium-sized LLMs without requiring
additional fine-tuning or large-scale GPU resources, making it practical for inference-
only sentiment classification tasks.

e We provide a transparent mechanism to analyse and interpret the behaviour of multiple
LLMs through inference analysis and influence strength analysis.

e We provide empirical evidence that LLMs trained on particular datasets produce dif-
ferent predictions across data sources, and show how BNLF captures and reveals these
effects through probabilistic inference.

The remainder of the paper is structured as follows: Section [2| reviews related work, and
Section [3] outlines the fundamental methods used in the study. Section [4]introduces BNLF
framework. Section [5] describes the experimental setup and data. Section [6] presents and
discusses the results. Finally, Section |7| concludes the paper and outlines future research
directions.

2. Related Work

Recent studies have systematically benchmarked LLMs against traditional and domain-
specific transformers to assess their effectiveness in sentiment analysis.
 benchmark GPT-3.5, GPT-4, and Llama 2 against transfer-learning models
such as SIEBERT and RoBERTa on 3 900 annotated texts, examining prompting strate-
gies and explainability via human evaluation. Their findings show that LLMs can achieve
accurate sentiment classification without fine-tuning but struggle with noisy social-media
data, and exhibit systematic biases such as GPT-3.5’s positivity and Llama 2’s tendency to
reject offensive content. In another study, compare GPT-4 and GPT-3.5
with domain-specific transformers (FinBERT, RoBERTa, DistilBERT) across five annotated
corpora in finance, healthcare, and social media. The results show that GPT-4 matches fine-
tuned models in finance and healthcare, but underperforms on noisy social media data. The
study concludes that LLMs provide an alternative to fine-tuning, while still exhibiting biases
such as overconfidence and positivity.

Recent work investigates hybrid strategies that combine domain-specific models, LLMs,
and traditional classifiers for sentiment analysis. propose a hybrid frame-
work that integrates a transformer-based NLP pipeline with an LLM/chatbot API (GPT-4)
to enable centralized access and retrieval of educational materials. The system leverages em-
beddings, local database search, and interactive dialogue for efficient information access in
e-learning environments. Similarly, propose a multimodal ensemble frame-
work that combines transformer-based models (Twitter-RoBERTa, multilingual BERT) with
an LLM (GPT-3) for cross-lingual sentiment analysis. Using datasets in four languages trans-
lated into English, the ensemble achieves higher accuracy than individual pre-trained models,
demonstrating the benefit of integrating LLMs with traditional transformer architectures for

sentiment prediction. Finally, Tekin et al.) (2024) propose an ensemble framework that inte-

grates outputs from multiple LLMs, such as Llama 2, using diversity-based fusion strategies


===== PAGE BREAK =====

rather than simple majority voting. Evaluations on multiple reasoning and summarisation
benchmarks demonstrate that the ensemble achieves greater robustness and mitigates biases
present in single-model predictions.

Building on these ensemble and hybrid approaches, recent research has explored more ad-
vanced fusion frameworks within LLM applications. For instance, propose
FinMSG, an order-aware multimodal transformer architecture that integrates text, audio,
and video signals to generate concise financial summaries. The framework introduces a
curated dataset (FAV) of over 400 annotated financial expert videos and benchmarks mul-
tiple LLMs, including BART, T5, LLaMA-2, and GPT-3.5. Their results highlight the
benefit of fusion-based reasoning, achieving around a 7% improvement. Another fusion
approach is MedTsLLM by (2024), a framework that extends LLM reasoning
to time-series analysis in healthcare. The model fuses physiological signals, such as ECG
and respiratory data, with textual patient context through a reprogramming layer that per-
forms early feature-level fusion without additional fine-tuning. The framework was evaluated
across multiple public medical datasets and achieved performance improvements over exist-
ing transformer-based and statistical baselines.

In summary, these studies highlight the growing trend of LLM-centric fusion architectures
that integrate heterogeneous data sources to enhance robustness, generalisation, and inter-
pretability. However, most existing approaches remain deterministic, focusing on embedding-
level or model-level combinations with limited attention to probabilistic reasoning and de-
pendency modelling, which motivates the design of the BNLF framework.

3. Preliminaries

In this section, we briefly introduce the key concepts of LLMs and BNs that form the
methodological foundation of this study. For a comprehensive overview, interested readers

are refereed to (Minace et al.||2024) for LLMs and (Polotskaya et al. (2024) for BNs.

3.1. Large Language Models

Natural language processing (NLP) enables computers to interpret and generate human
language for tasks such as sentiment analysis, translation, and summarisation. Early NLP
approaches relied on rule-based methods and statistical models, including n-grams, which
depended heavily on handcrafted features and struggled to capture the complexity of natural
language (Muhammad et al.|/2025).  The emergence of neural networks improved performance
through word embeddings and sequential models such as long short-term memory (LSTM)
networks, but their sequential nature made training slow and limited their ability to represent
long documents effectively 2015).

The breakthrough in transformer architecture replaced recurrent computations with a
self-attention mechanism, in which dependencies among all tokens are computed in paral-
lel to capture contextual relationships across the entire sequence (2024).  This
mechanism enables efficient modelling of long-range dependencies and scalability to very
large datasets, thereby establishing the foundation for subsequent advances in large-scale
language modelling. Building on this foundation, LLMs are transformer-based neural net-

works trained on massive text corpora (Ridley et al.||2023). With billions or even trillions of


===== PAGE BREAK =====

parameters, they encode statistical regularities in language and capture semantic and con-
textual patterns, enabling the model, given an input sequence, to generate the most probable
continuation (2024).

To effectively apply these models, it is necessary to consider how LLMs can be adapted
for specific tasks. Two commonly used strategies are fine-tuning and prompt engineering.
Fine-tuning is the process in which a pretrained model, such as an LLM, is further trained on
task-specific data to adapt it for specialised use cases . The core
objective of fine-tuning is to enhance the adaptability of LLMs, enabling them to exhibit
behaviour tailored to particular domains or tasks, rather than relying solely on their general
pretrained knowledge (2025).

Prompt engineering, the second strategy, is the process of designing and structuring
inputs to guide the output of LLMs for specific tasks. As a relatively new area of re-
search, it focuses on refining prompts or instructions to maximise the utility and accuracy
of these models (2023).  Common prompt engineering approaches include zero-shot
and few-shot prompting for in-context learning without retraining, instruction-based and
role-defining prompts that frame tasks explicitly, and advanced methods such as chain-of-
thought, self-consistency, and generated knowledge prompting to improve reasoning and
factual grounding (2024).  These techniques highlight the
adaptability of prompt engineering across domains. However, they also make model outputs
highly sensitive to prompt design.

3.2. Bayesian Networks

Bayesian networks are a class of probabilistic graphical models designed to represent sys-
tems characterised by uncertainty, interdependence, and causal structure (2025).
Unlike models that capture only statistical correlations, BNs explicitly encode conditional
dependencies between variables, which provide interpretability and make them particularly
valuable for reasoning and decision-making in various domains such as finance (Amirzadeh|
 023), healtheat 025).

Formally, a BN consists of two main components: a directed acyclic graph (DAG) and
a set of conditional probability distributions represented by conditional probability tables
(CPTs). In the DAG, nodes correspond to variables in the system, while directed edges
denote conditional dependencies from parent (cause) to child (effect). Each node is associated
with a CPT that provides the probability of each possible outcome given the states of its
parent variables. The joint probability distribution of all variables in a BN can be factorised
as follows:

P(X1, X2,..., Xn) = |] P(Xi|parents(X;))
i=1
where parents(X;) is the set of parents of X; in a DAG representation (Heckerman| |2008).
BNs can be constructed using expert knowledge, where domain expertise defines the
structure, or through data-driven approaches that automatically learn the DAG and pa-
rameters from data. Expert-driven methods may be limited by factors such as subjectiv-

ity (Rique et al.||2025), while data-driven methods are often computationally expensive. A

hybrid approach integrates both strategies to achieve a balance between interpretability and

accuracy (/Polotskaya et al. /2024).


===== PAGE BREAK =====

4. Proposed Framework: Bayesian Network LLMs Fusion

In this section, we introduce the Bayesian network LLM Fusion (BNLF) framework as an
end-to-end pipeline for sentiment analysis, as illustrated in Figure[1] The framework has four
components. First, the texts are drawn from various corpus sources. Second, these texts are
processed by three LLMs (FinBERT, RoBERTa, and BERTweet), each producing sentiment
predictions. Third, predictions are fused through a BN to produce a probabilistic sentiment
inference. Finally, the fused inference produces a discrete sentiment label. Thus, BNLF
represents the entire pipeline from input text to final classification, while BN specifically
constitutes the probabilistic fusion mechanism within it.

LLM Module                                         BN Module
FinBERT                                  FinBERT
Model                                  Prediction
CN                (a
Input Text |                        RoBERTa                                     RoBERTa                              Probabilistic                   Output Sentiment Label
li                :                    Model                                 Prediction                   Sentiment Prediction                [NEG, NEU, POS]
EEE                  EEE
(__ ne)                ae
BERTweet                                BERTweet
Model                                  Prediction
Sl

Figure 1: Each input text, drawn from multiple financial and social media corpora, is processed by three
LLM-based classifiers: FinBERT, RoBERTa, and BERTweet. These models generate individual sentiment
predictions that are fused within a BN through probabilistic inference. The BN outputs a posterior sentiment
distribution, which is then mapped to a discrete sentiment label: negative (NEG), neutral (NEU), or positive
(POS).

To illustrate the role of each component, we begin with the inputs. Each Input Text,
drawn from multiple financial corpora, serves as the entry point to the BNLF pipeline.
The corpora span both formal financial documents and informal user-generated content.
Financial text poses unique challenges for sentiment analysis because it differs from nat-
ural conversational language and often combines technical vocabulary with alphanumeric
expressions and context-dependent cues. This diversity introduces systematic variation in
the sentiment distribution of financial texts, which the Input Text is designed to capture
as a contextual factor within the framework.

In addition to the dataset-level signal, the framework includes an LLM Module that pro-
cesses input texts to generate sentiment predictions. As shown in Figure [I] three models of
RoBERTa, BERTweet, and FinBERT are employed. RoBERTa is a general purpose trans-
former pretrained on a large and diverse corpus and later fine-tuned for sentiment tasks,

including Twitter-based datasets (Mohawesh et al.| |2024). BERTweet follows a similar pre-

training procedure but is trained on a large-scale Twitter data, enabling it to capture the

informal and noisy language characteristic of social media (Baker et al. 2022). FinBERT

adapts the BERT architecture by training on both general text and financial documents,


===== PAGE BREAK =====

making it sensitive to finance-specific sentiment cues (Karadas et al.||2025). Together, these

models capture complementary aspects of sentiment across financial and social media cor-
pora, forming the predictive foundation for BNLF.

These models are chosen for their complementary coverage, efficiency, and practicality.
FinBERT provides domain-specific knowledge of the financial language, RoBERTa serves as
a strong general-purpose model, and BERTweet captures the informal style of user-generated
content. A key design goal of BNLF is to remain lightweight while robust, enabling efficient
inference without the high GPU costs of larger models. Each of the three models is medium-
sized (~110M-135M parameters} allowing deployment on CPUs or single-GPU systems.
BNLF thus combines computational efficiency with practical robustness.

Next, after defining the inputs and the LLMs, we specify the fusion mechanism, where
predictions from the LLMs are combined using a BN. Fusion strategies are generally classified
as early (data-level), deep (feature-level), late (decision-level), and hybrid fusion (Yi et al.|
2025). BNLF employs a late fusion strategy, where sentiment predictions from individual
LLMs are fused using a BN that integrates multiple predictions into a probabilistic estimate.
Unlike standard ensemble methods that aggregate predictions via majority voting or aver-
aging, the BN explicitly models the joint probability distribution over the input variables,
enabling a principled probabilistic fusion that captures uncertainty in the final sentiment
inference?

Having established the role of BNLF as a late-fusion pipeline, we now describe the inter-
nal structure of its BN component in more detail. The BN fuses LLM predictions through
probabilistic inference, while also capturing how dataset context and individual model out-
puts jointly influence the final sentiment decision. To achieve this, the BN is represented
by a DAG, where arcs encode conditional relationships among the Input Text, the LLM
prediction nodes, and the Probabilistic Sentiment Prediction node.

In the DAG, the Input Text is connected to each LLM prediction node to model sys-
tematic domain effects on model behaviour, and the prediction nodes are connected to the
Probabilistic Sentiment Prediction node so their outputs directly inform sentiment
inference. No arcs are allowed between the LLM prediction nodes, enforcing conditional
independence and preventing correlations among them from biasing results. This structure
ensures interpretability, reflects domain-specific relationships, and provides a principled ba-
sis for probabilistic fusion. Based on the DAG specified from domain knowledge, CPTs are
subsequently learned from the annotated training dataf]

To provide a clearer understanding of how BNLF performs, Figure |2| presents a real

?For comparison, FinLLaMA variants range from 7B to 13B parameters, requiring 13-24 GB of storage
in full precision and high-end GPUs (> 24 GB VRAM) for fine-tuning (2025). Similarly, models
such as FinSoSent have been reported in the literature but remain without publicly available pretrained
weights, making them unsuitable for inference-only setups.

3In preliminary experiments, we also evaluated fuzzy integration of confidence scores, which yielded
suboptimal results. Such baselines collapse multiple signals into a single score and fail to capture dataset-
conditioned dependencies or offer an interpretable joint model, motivating the BN-based fusion.

4It should be noted that DAG described is distinct from the pipeline architecture of BNLF shown in
Figure[l|  The BN specifies probabilistic dependencies among variables, while the framework figure illustrates
the overall flow from input text to final sentiment classification. The two views are complementary but not
equivalent.


===== PAGE BREAK =====

example from the dataset, showing how sentiment predictions from three LLMs are fused
through the BN to produce a final probabilistic sentiment output.

LLM Module                                       BN Module

FinBERT                                                 POs

This is a gorgeous vehicle. It
kinda looks like a hallucination.

RoBERTa

NEU (0.6513)                            va |

Figure 2: A real example from the dataset showing how BNLF fuses sentiment predictions from FinBERT,
RoBERTa, and BERTweet. BN integrates these individual predictions to generate final sentiment probabil-
ities of (POS = 0.3436, NEU = 0.6513, NEG = 0.0051), where the neutral class is eventually selected with
the highest probability.

at
i

BERTweet                                          POS

5. Datasets and Experimental Design

To assess the proposed framework, we require diverse, annotated datasets that capture
different styles of financial text. To this end, we evaluate BNLF using three publicly available,
human-annotated corpora that provide gold-standard sentiment labels (positive, neutral,
negative) assigned by expert or crowd-sourced annotators, as follows.

e Financial PhraseBank: A news-based dataset for financial sentiment classification,
annotated by professional financial analysts. It consists of 4,840 short text fragments
drawn from corporate announcements, market news, and financial reports. Since the
data originates from authentic financial news sources, it reflects policy discourse and
risk-related language. Annotations were provided by 5-8 experts, and the dataset is

also divided based on the agreement rate among annotators (Malo et al.| |2014).

e Twitter Financial News Sentiment (TFNS): An English-language dataset of
12,424 finance-related tweets annotated with three sentiment classes: bearish, neutral,
and bullish. This dataset reflects the informal, concise, and time-sensitive nature of
financial discourse on Twitter where human annotations ensure reliable categorisation

of sentiment (Zeroshot} |2024).

e FIQA: Originally developed for the financial opinion mining and question answering
challenge (FIQA-2018), this dataset contains 961 entries covering diverse financial top-
ics, including market events, company performance, and investment opinions. It is
included in the BEIR benchmark, which standardises preprocessing and format across
multiple retrieval and sentiment datasets. Each entry is labelled as positive, neutral,

or negative (Thakur et al.| 2021).
All three datasets are publicly available on Hugging Face (Wolf et al. |2019) , ensuring

transparent and reproducible experimentation. They span formal corporate disclosures to
informal social media posts, enabling assessment across diverse linguistic and contextual


===== PAGE BREAK =====

settings. Their combination supports robust LLM evaluation and reliable BN parameter
learning.

Before experimentation, the datasets were merged into a single corpus and underwent
essential preprocessing, including removal of missing or empty texts, deletion of zero-width
characters, spacing normalisation, and duplication removal. Sentiment labels were remapped
to a consistent three-class scheme (0 = negative, 1 = neutral, 2 = positive) to ensure com-
parability across datasets. For instance, in the TFNS dataset the original classes bearish,
bullish, and neutral were mapped to negative, positive, and neutral, respectively. Table
summarises the class distributions for each dataset.

Table 1: Summary of sentiment dataset statistics showing the number and proportion of negative, neutral,
and positive samples in each corpus.

Dataset                   Negative        Neutral        Positive
Financial PhraseBank —303 (13.38%) 1,391 (61.44%) 570 (25.18%)
TFNS                 1,789 (14.99%) 7,744 (64.91%) 2,398 (20.10%)
FIQA                  716 (59.03%)     118 (9.73%)    379 (31.24%)
Total                 2,808 (18.22%) 9,253 (60.05%) 3,347 (21.72%)

This unified dataset was then split into training and test partitions using an 80/20 ratio,
a common practice in BN studies, applied consistently for both BN construction and LLM
evaluation. Following the BNLF design, the BN structure was constrained by domain knowl-
edge and its CPT parameters were learned from the training set. To ensure comparability,
the BNLF model was evaluated on the test set, which was also used for standalone LLM
evaluation.

Alongside the three models used within BNLF, we also evaluate the performance of
DistilRoBERTa-financial-sentiment as an external baseline (2024).  This model
serves as an independent benchmark due to its fine-tuning on financial data, computationally
efficient (82M parameters), and is widely adopted in Hugging Face. Its popularity, high

performance in financial sentiment tasks (Kadasi et al.\!2025), and adoption in various recent
studies (Banka and Chudziak}|2025) ensure both robustness and practical accessibility. Thus,

BNLF is compared both against its constituent LLMs and with an external benchmark |
In addition to the individual LLMs and the external DistilRoBERTa baseline, two ensem-
ble baselines were included for comparison. The first, majority voting, assigns each instance
the sentiment label predicted by at least two of the three constituent LLMs—FinBERT,
RoBERTa, and BERTweet. In cases where no majority exists, FinBERT’s prediction is
used, reflecting its financial-domain specialisation. The second, averaging, aggregates class
probabilities across three LLMs and selects the sentiment with the highest mean probability.
The model’s performance is evaluated using multiple complementary metrics. Accuracy
captures overall correctness, while macro- and weighted F1-scores provided balanced assess-
ments of performance across sentiment classes. In addition, model agreement is assessed
through a pairwise agreement matrix and Cohen’s Kappa, which analyse consensus and

®We also tested additional models, but their results were substantially weaker. The complete results are

reported in for completeness.


===== PAGE BREAK =====

divergence across models.

Experiments were conducted on a Windows 10 Enterprise system equipped with an Intel
Core i7 CPU and 16 GB of RAM. All implementations were developed in Python. LLM
experiments were carried out with the Hugging Face Transformers library. The three LLMs
were applied through sentiment-analysis pipelines in inference-only mode, using fixed pre-
trained weights to generate sentiment predictions without task-specific fine-tuning. This
setup isolates model comparison from fine-tuning effects and evaluates the inherent general-
isability of pretrained LLMs across financial corpora.

BNs were implemented and evaluated using GeNle and pySMILE 1999). To
construct the structure according to the proposed BNLF design, we employed GeNle’s
Knowledge Editor to incorporate domain-specific background knowledge. The default hy-
perparameter settings for the BN are reported in Table

Table 2: Hyper-parameters used for BN construction.

Parameter              Value

Learning Algorithm      Bayesian Search
Discrete Threshold      20

Max Parent Count      8

Iterations                20

Sample Size             50

Seed                      0

Link Probability         0.1

Prior Link Probability 0.001

6. Results and Discussion

This section summarises key experimental results and comparative analyses of BNLF and
baseline models. Table [3] presents the results in a zero-shot setting across three metrics for
the combined test set derived from all three sources. It shows that BNLF achieves the best
results, with 78.6% accuracy, about 5.3% higher than the DistilRoBERTa baseline. It also
improves both macro- and weighted-F1 scores, indicating that the improvement is consistent
across classes. Furthermore, DistilRoBERTa outperforms all of the individual LLMs included
in BNLF, underscoring its strength as a competitive baseline. In addition, among the three
models that form the basis of BNLF, FinBERT is the best performer (72.1% accuracy),
reflecting its financial-domain specialisation. RoBERTa and BERTweet achieve comparable
accuracies (65.8% and 66.2%, respectively) but lower Fl-scores, which is consistent with
their shared transformer architecture and broader training domains.

The ensemble baselines achieve moderate performance, with majority voting attaining
72.6% accuracy and probability averaging reaching 74.8%. Both results are less than BNLF’s
78.6%, suggesting that simple aggregation methods offer limited improvement over individual
models. In contrast, BNLF delivers more consistent and substantial gains across all metrics.
Compared with traditional ensemble methods, BNLF provides a more principled integration
of multiple LLMs. Instead of combining outputs linearly, it models conditional dependencies

10


===== PAGE BREAK =====

among LLMs through a probabilistic structure. This results in a more informed fusion pro-
cess and greater robustness.For better comparison, we also visualise the overall performance
metrics in Figure

Table 3: Performance comparison of individual LLMs, ensemble baselines, and BNLF on the combined
test set. DistilRoBERTa serves as the external baseline, while majority voting and averaging are ensemble
baselines combining FinBERT, RoBERTa, and BERTweet. Best results are shown in bold.

Model                 Accuracy (%) Macro-F1 (%) Weighted-F1 (%)
DistilRoBERTa (baseline)        73.3              68.3                73.8
RoBERTa                     65.8            54.8              63.6
BER Tweet                    66.2            57.4              65.0
FinBERT                     72.1            66.7              72.5
Majority Voting                      72.6                 65.1                    71.9
Averaging                     74.8            68.5              74.4
BNLF                        78.6            71.5              77.7

®@ DistiRoBERTa WM RoBERTa “| BERTweet ( FinBERT ® Majority Voting ™ Averaging ™ BNLF
1.00

0.75

0.50

0.25

0.00

Accuracy                                             Macro F1                                          Weighted F1

Figure 3: Overall performance comparison of BNLF, individual LLMs, ensemble baselines, and the external
DistilRoBERTa model across accuracy, macro-Fl, and weighted-F1 metrics. The blue bars representing
BNLF consistently exceed those of all baselines, including the ensemble methods (majority voting and
averaging), demonstrating the effectiveness of its probabilistic fusion approach.

Class-level analysis is important for assessing whether a model’s performance is balanced
across majority and minority classes. To this end, Table |4| reports precision, recall, and
F1-scores for each sentiment class. As can be seen in the table, BNLF achieves balanced
performance across classes, with the highest scores in neutral sentiment (F1 = 0.850, recall
= 0.912) and competitive results in negative (F1 = 0.639) and positive (F1 = 0.667) classes.
Overall, DistilRoBERTa performs better across most classes than the other individual LLMs,
but its performance remains lower than that of BNLF. While it is strong on the neutral class,
its results on the negative and positive categories are weaker compared to BNLF.

11


===== PAGE BREAK =====

Table 4: Class-level precision, recall, and F1-score for LLM models and BNLF. Classes correspond to senti-
ment categories, including negative, neutral, and positive. Best values in each column are shown in bold.

Negative                                       Neutral                                       Positive
Model                             Precision Recall      Fl      Precision Recall      Fl      Precision Recall     Fl
DistilRoBERTa (baseline)      0.576       0.596 0.586      0.860      0.789 0.823      0.595      0.715 0.649
RoBERTa                   0.525     0.353 0.422    0.692     0.850 0.763     0.585     0.375 0.457
BERTweet                          0.515       0.452 0.482      0.707      0.797 0.749      0.537      0.402 0.460
FinBERT                   0.500    0.643 0.563    0.840     0.762 0.799     0.620     0.631 0.626
BNLF                         0.684     0.600 0.639     0.796     0.912 0.850    0.848     0.548 0.667

While overall results provide a high-level view of performance, a closer examination across
datasets further reveals how the models adapt to varying linguistic styles and domain charac-
teristics. To assess BNLF’s performance across different datasets, Table[5]reports the results
for each data source individually. Our framework outperforms all LLMs on FIQA and TFNS,
while DistilRoBERTa achieves slightly higher accuracy on Financial PhraseBank. In par-
ticular, on Financial PhraseBank, DistilRoBERTa achieves the highest accuracy (99.6%),
while BNLF also performs strongly with 98.2% accuracy and balanced F'1-scores, surpassing
FinBERT’s already strong results. For FIQA, which contains more diverse and nuanced
financial question—answer content, BNLF improves accuracy to 65.8%, higher accuracy over
the best individual model (BERTweet at 60.1%). On TFNS, the fusion approach reaches
75.3% accuracy, outperforming FinBERT’s 73.2% and showing consistent improvements in
both macro- and weighted-F1.

Table 5: Comparison of model performance across individual datasets (Financial PhraseBank, FIQA, and
TFNS). Results are reported with accuracy, macro-F1, and weighted-F1, with best values highlighted in
bold.

Source          Model                       Acc Macro-F1 Weighted-F1
DistilRoBERTa (baseline) 0.9956 0.9919        0.9956
RoBERTa                   0.7035     0.5182         0.6463
Financial PhraseBank BERTweet                   0.7788     0.6873         0.7565
FinBERT                    0.9690     0.9593         0.9694
BNLF                       0.9823     0.9811         0.9820
DistilRoBERTa (baseline) 0.4571      0.4179         0.5393
FIQA             RoBERTa                      0.5247      0.3823           0.5442
BERTweet                   0.6009     0.4581         0.6374
FinBERT                    0.5561      0.4491         0.6114
BNLF                       0.6583     0.5224        0.6586
DistilRoBERTa (baseline) 0.7452     0.7015         0.7529
TFNS           RoBERTa                   0.7030     0.6101         0.6947
BERTweet                   0.6921      0.6153         0.6877
FinBERT                    0.7323     0.6794         0.7400
BNLF                       0.7525     0.6997        0.7501

To visualise dataset-level performance, we include Figure [4] which compares the accuracy
of all models based on the datasets.

12


===== PAGE BREAK =====

® DistiiIRoBERTa M™ RoBERTa /) BERTweet @@ FinBERT @® BNLF

0.75

0.00
Financial PhraseBank             FIQA                      TFNS

Figure 4: Accuracy comparison across datasets, where each group of bars corresponds to one dataset (Finan-
cial PhraseBank, FIQA, TFNS), with bars representing indivicual LLMs, and BNLF. It shows that BNLF
achieves the highest accuracy on FIQA and TFNS, while DistilRoBERTa reaches the highest accuracy on
Financial PhraseBank.

6.1. Model Agreement Analysis

Pairwise agreement scores quantify how closely two models align in their predictions,
offering a straightforward measure of similarity in classification behaviour. To this end, this
section compares the prediction patterns of individual LLMs and BNLF by measuring the
proportion of documents assigned identical sentiment labels. We also report Cohen’s Kappa,
which adjusts for chance agreement and provides a more robust measure of alignment. The
results are presented in Table |6|

Table 6: Pairwise model agreement in which each cell shows the proportion of matching sentiment labels.
Cohen’s Kappa is presented in parentheses. The “Mean” column reports the average agreement for each
model.

Model          DistiIRoBERTa FinBERT    RoBERTa BERTweet      BNLF     Mean
DistilRoBERTa        1.000        0.783 (0.648) 0.671 (0.423) 0.674 (0.440) 0.736 (0.543) | 0.716
FinBERT                            1.000      0.659 (0.396) 0.682 (0.448) 0.831 (0.704) | 0.739
RoBERTa                                             1.000      0.824 (0.643) 0.799 (0.580) | 0.738
BERTweet                                                           1.000      0.838 (0.677) | 0.754
BNLF                                                                             1.000       0.801

As can be seen in the table, DistilRoBERTa shows strong alignment with FinBERT
(78.3%) and BNLF (73.6%), while RoBERTa and BERTweet demonstrate the closest rela-
tionship (82.4%), reflecting their shared architecture and training domains. BNLF exhibits
consistently high agreement with all three transformer-based models, suggesting that the
fusion effectively integrates their complementary patterns. On average, BERTweet (75.4%)

13


===== PAGE BREAK =====

and BNLF achieves the highest mean consistency (80.1% based on proportions), whereas
DistilRoBERTa (71.6%) is slightly less aligned with the other models. Furthermore, the
results are shown in Figure [5] for better comparison of the models.

- 1.00

DistiIRoBERTa
FinBERT
RoBERTa
BERTweet

BNLF

Figure 5: Heatmap of pairwise agreement scores between individual LLMs and BNLF. Darker shades indicate
stronger agreement, corresponding to higher proportions of matching sentiment labels.

Cohen’s Kappa values, reported in Table [6] adjust for chance agreement and provide
a stricter measure of alignment |] BNLF achieves the highest mean consistency (0.801),
reinforcing its role as a balanced integrator of individual model predictions, and Distil-
RoBERTa’s agreements are moderate across all models. The strongest alignment is between
FinBERT and BNLF (« = 0.704), while BERTweet also shows substantial agreement with
both RoBERTa (« = 0.643) and BNLF (« = 0.677). These agreement values reflect dif-
ferences in domain adaptation rather than inconsistency among models. For instance, the
RoBERTa-—DistilRoBERTa pair shows lower agreement, while BERTweet’s relatively high
overall agreement suggests that it captures linguistic features shared across both formal and
informal text sources. Furthermore, the difference between raw agreement and Kappa val-
ues indicates that the observed agreement patterns, whether high or low, are statistically
meaningful and not due to chance.

6.2. Analysing Sentiment Predictions of BNLF

In this section, we conduct additional analyses on the BNLF model, including inference
behaviour and influence strength assessment. These evaluations aim to reveal how individual
LLMs and dataset characteristics contribute to the BNLF’s final sentiment predictions, pro-
viding a clearer understanding of its decision-making process, and supporting transparency
and explicability, key aspects of trustworthy AI.

°Kappa values are generally lower than raw proportions because they account for agreement that could
occur randomly.

14


===== PAGE BREAK =====

6.2.1. Inference Analysis

Inference analysis is a standard step in BN studies, as it reveals how the model updates
beliefs under different evidence settings [2025b).  We explore scenario-
based inference to see how BNLF combines LLM predictions with contextual factors such as
corpus type. In each scenario, we set specific nodes in the BN to fixed states and observe the
resulting posterior probabilities for the sentiment output node. These controlled cases show
whether the BN treats all model predictions equally or instead changes its output based on
the characteristics of the dataset.

In the BN design, the Input Text is implemented as a deterministic node, since the
dataset type is an observed fact rather than a probabilistic variable. Its inputs from various
soucres (FinancialPhraseBank, FIQA, TFNS) are fixed for each experiment and conditions
the behaviour of the LLM prediction nodes. This ensures that data-level effects are explicitly
incorporated into inference without adding additional uncertainty at the dataset level.

Scenario 1 — Corpus Type Effect with Identical Model Predictions

In the first scenario, we fix all LLM sentiment predictions to Negative while changing
the corpus type to investigate how the origin of the dataset influences the final sentiment
distribution of the BNLF. Table || presents the resulting posterior probabilities for each
corpus setting.

Table 7: BNLF posteriors with all LLMs fixed to negative, under different corpus type settings. Values are
percentages for negative, neutral, and positive sentiment.

Corpus Type           Negative (%) Neutral (%) Positive (%)
Financial PhraseBank       96.55           1.72           1.72
FIQA                     95.06          3.70          1.23
TFNS                     66.54         31.52         1.94

Even with identical negative predictions from all LLMs, BNLF’s outputs vary consider-
ably by corpus. In particular, For Financial PhraseBank and FIQA, the outcome remains
almost deterministic (over 95% negative), whereas TFNS shifts toward greater uncertainty,
with 66.5% negative and 31.5% neutral. This shows that corpus type can influence the
model’s certainty and also the balance between sentiment classes. This effect is further illus-
trated in Figure |6]which visualises the TFNS case where neutral probability rises noticeably
despite all models predicting negative.

Scenario 2 — Corpus Type Variation with Model Disagreement

In the second scenario, each LLM is set to a different sentiment state (FinBERT =
negative, RoBERTa = neutral, BERTweet = positive) while varying the corpus type. This
configuration creates maximum disagreement among the individual models, allowing us to
examine how BNLF resolves conflicting evidence across different dataset domains. Table
and Figure |7| summarise the resulting posterior probabilities for BNLF under each corpus
setting.

15


===== PAGE BREAK =====

[© _ FinBERT Prediction

© Probabilistic Sentiment Prediction

Corpus Source
|FinancialPhraseBank — 0%
|FIOA                        0%
ITENS                    100°

Figure 6: BNLF inference for the TFNS corpus with all LLMs fixed to negative. The model outputs 67%
negative, 32% neutral, and 2% positive, showing a clear rise in neutral probability even though all models
give negative as input.

Table 8: BNLF posteriors with FinBERT = Negative, RoBERTa = Neutral, and BERTweet = Positive,
under different corpus type settings. Values are percentages.

Corpus Type          NEG (%) NEU (%) POS (%)
Financial PhraseBank     66.67       16.67       16.67
FIQA                   16.67      16.67      66.67
TFNS                    4.55       36.36      59.09

Even with the disagreement pattern from all models, BNLF’s outputs change significantly
with corpus type. For Financial PhraseBank, the prediction is mostly negative (66.7%).
For FIQA, positive becomes the dominant sentiment (66.7%). For TFNS, positive remains
highest (59.1%), while neutral is also relatively high (36.4%), showing greater uncertainty.
This shows that corpus type can affect both the model’s certainty and which sentiment class
becomes dominant when models disagree. As shown in Figure ]7| the FIQA case illustrates
that positive sentiment has the highest prediction probability despite mixed model inputs.
NEG 100%

NEU 0%
POS 0%

Corpus Source                                                                                                  Probabilistic Sentiment Prediction
FinancialPhraseBank 0%
FIQA                                       100%
NS                         0%

RoBERTa Prediction

Figure 7: BNLF inference for the FIQA corpus with FinBERT = Negative, RoBERTa = Neutral, and
BERTweet = Positive. The model outputs 17% negative, 17% neutral, and 67% positive, showing a clear
shift toward positive sentiment despite conflicting inputs.

16


===== PAGE BREAK =====

6.2.2. Influence Strength

To further analyse the BNLF model, we explore the strength of influence, a measure of
how strongly changes in a parent node affect the probability distribution of its child node.
It is calculated from the CPTs as the average difference between the child’s probability
distributions under different parent states. The strength of influence results are shown in
Figure |8} where the arc annotations indicate influence values and thicker arcs represent
stronger relationships between connected nodes.

FinBERT

0.102

Probabilistic
Sentiment
Prediction

Corpus Source

0.320

RoBERTa
Prediction

0.309

BERTweet
Prediction

Figure 8: Strength of influence diagram for BNLF. Arc thickness is proportional to the magnitude of influence
between nodes.

The figure shows that FinBERT has the strongest direct influence on BNLF (0.364),
followed by RoBERTa (0.320) and BERTweet (0.309). The corpus type also directly affects
BNLF (0.327) and indirectly influences it through the LLMs, most notably RoBERTa (0.156),
then FinBERT (0.102) and BERTweet (0.096). This indicates that, although all models
contribute, FinBERT and RoBERTa are the main drivers of the final sentiment predictions,
likely because FinBERT benefits from domain-specific financial training and RoBERTa offers
strong general-purpose coverage. This aligns with earlier results showing that these two
models have the largest effect on BNLF outputs under different corpus types.

The two inference scenarios and the influence analysis together demonstrate the benefits
of BNLF in enhancing reasoning and interpretability. The results show that BNLF dy-
namically integrates varying model predictions and contextual factors, generating sentiment
outcomes that reflect both consensus and uncertainty in a transparent way. This behaviour
underscores its ability to combine evidence probabilistically, providing a clear understanding
of how sentiment decisions are formed and revealing the mechanisms behind them.

Beyond interpretability, these analyses also highlight BNLF’s advantage in causal reason-
ing. A major limitation of current LLMs lies in their restricted capacity for causal reasoning,
as they often struggle to capture underlying cause-effect dependencies within data, a critical

17


===== PAGE BREAK =====

gap in domains such as financial analysis, where understanding the drivers behind sentiments
is essential for interpretability. For instance, (2023) show that LLMs still fail to
reason consistently about causality, while (2023) demonstrate that GPT models
can identify correlations between events and sentiments but often lack logical coherence in
counterfactual reasoning. However, BNLF explicitly models directional dependencies, en-
abling inference from effect to cause and vice versa. As demonstrated by the inference and
influence analyses, the Bayesian structure systematically identifies which LLM or contex-
tual factor most plausibly contributes to a given outcome, even when model predictions

disagree or appear counterintuitive. This causal interpretability enhances the framework’s

transparency and directly supports the explicability principle of Trustworthy AI (Thiebes|

et al.) 2021),

7. Conclusion and Future Work

This study addresses key challenges in applying large language models for financial senti-
ment analysis across formal financial data and social media posts. While LLMs have achieved
strong performance, they often require costly fine-tuning, extensive prompt engineering, and
still produce inconsistent results across domains and corpora. To overcome these limita-
tions, we propose the Bayesian network LLM fusion framework, which integrates sentiment
outputs from multiple LLMs. The framework shows that different LLMs contribute vary-
ing influences to sentiment prediction, and their probabilistic combination enhances overall
performance.

Experimental results in three diverse, human-annotated financial datasets show that
BNLF generally outperforms all individual LLM baselines. Moreover, it improves accuracy
and weighted Fl1-score by around six percentage over the baseline LLM, with gains observed
across all datasets, including the challenging FIQA corpus. Class-level analysis shows that
BNLF achieves balanced performance. Furthermore, the inference and influence-strength
analyses reveal that FinBERT and RoBERTa exert the strongest influence, with BERTweet
providing complementary signals. The source corpus of each text also plays an important
role in shaping the certainty and sentiment distributions.

Beyond quantitative gains, BNLF offers broader design advantages for developing reliable
and adaptable LLM-based systems. It addresses two main challenges in this domain, includ-
ing limited interpretability and scalable integration, by incorporating causal and probabilis-
tic reasoning within a modular architecture. This design allows the framework to integrate
heterogeneous models while maintaining transparency in its decision process.

While scalability in LLMs is traditionally defined by increasing model parameters, data
size, and computational resources (2025), BNLF achieves
both structural and functional scalability, where adaptability is achieved without retraining
large models. It allows the seamless addition or replacement of LLMs (structural scalability)
and, more importantly, supports adaptation to different tasks (functional scalability). This
modular architecture enables the framework to adapt or extend dynamically, representing a
shift from parametric scaling to architectural scalability. Building on this scalable design,
BNLF offers a flexible foundation for several promising research directions aimed at enhanc-
ing its scalability, interpretability, and applicability across broader financial and generative
contexts.

18


===== PAGE BREAK =====

In line with this scalable design, future research could extend BNLF along two comple-
mentary directions, addressing some of the current framework’s limitations. First, structural
scalability can be enhanced by incorporating additional LLM architectures or multilingual
financial corpora to strengthen cross-domain robustness and adaptability. Moreover, con-
structing the BN directly from the confidence scores of sentiment prediction, rather than
discrete sentiment classifications, using structure-learning algorithms such as the PC algo-
rithm, could reveal more nuanced dependencies among model outputs. Second, functional
scalability involves expanding the framework beyond sentiment classification. A promising
direction is to extend BNLF to generative modelling, where multiple GPT-style models con-
tribute next-token probabilities and the BN fuses them to select the most likely output.
Another direction is to employ dynamic BNs to capture temporal sentiment evolution and

its influence on financial markets (Amirzadeh et al.| |2025a)).

Data availability

The datasets used in this study were obtained from publicly available resources on Hug-

ging Face. The Bayesian network model is available at: https://doi.org/10.59381/
ceglcyvwla

Appendix A. Supplementary Results

In addition to the models discussed in the main text, we evaluated other widely used
and trending sentiment models from Hugging Face, including ALSGYU General and TabSA
Multilingual. Despite their popularity and accessibility, their performance on the combined
dataset was notably weaker, as shown in Table

Table A.9: Performance of additional baseline models on the combined test set. Best results among these
supplementary models are shown in bold.

Model                         Accuracy Macro-F1 Weighted-F1

ALSGYU General          0.452           0.312               0.434

TabSA Multilingual         0.302              0.299                 0.293
References

Ali, H.M.U., Farooq, Q., Imran, A., El Hindi, K., 2025. A systematic literature review on
sentiment analysis techniques, challenges, and future trends. Knowledge and Information
Systems , 1-68.

Amirzadeh, R., Nazari, A., Thiruvady, D., Ee, M.S., 2023a. Modelling determinants of
cryptocurrency prices: A bayesian network approach. arXiv preprint arXiv:2303.16148 .

Amirzadeh, R., Thiruvady, D., Nazari, A., Ee, M.S., 2023b. A framework for empowering
reinforcement learning agents with causal analysis: Enhancing automated cryptocurrency
trading. arXiv preprint arXiv:2310.09462 .

19


===== PAGE BREAK =====

Amirzadeh, R., Thiruvady, D., Nazari, A., Ee, M.S., 2025a. Dynamic bayesian networks for
predicting cryptocurrency price directions: Uncovering causal relationships: R. amirzadeh
et al. Annals of Data Science , 1-31.

Amirzadeh, R., Thiruvady, D., Nazari, A., Ee, M.S., 2025b. Dynamic evolution of causal
relationships among cryptocurrencies: an analysis via bayesian networks. Knowledge and
Information Systems 67, 355-370.

Amjad, F., Korotko, T., Rosin, A., 2025. Review of llms applications in electrical power &
energy systems. IEEE Access .

Anisuzzaman, D., Malins, J.G., Friedman, P.A., Attia, Z.I., 2025. Fine-tuning large language
models for specialized use cases. Mayo Clinic Proceedings: Digital Health 3, 100184.

Asadi, G.H., Hamzeh, A., Mozafari, N., 2025. Pars: A position-based attention for rumor
detection using feedback from source news. IEEE Access .

Baker, W., Colditz, J.B., Dobbs, P.D., Mai, H., Visweswaran, S., Zhan, J., Primack, B.A.,
2022. Classification of twitter vaping discourse using bertweet: comparative deep learning
study. JMIR Medical Informatics 10, e33678.

Balaskas, G., Papadopoulos, H., Pappa, D., Loisel, Q., Chastin, S., 2025. A framework for
domain-specific dataset creation and adaptation of large language models. Computers 14,
172.

Barika, F., Chudziak, J.A., 2025. Options pricing platform with neural networks, Ilms and
reinforcement learning, in: Asian Conference on Intelligent Information and Database
Systems, Springer. pp. 202-216.

Beltagy, I., Lo, K., Cohan, A., 2019. Scibert: A pretrained language model for scientific
text. arXiv preprint arXiv:1903.10676 .

Brati¢, D., Sapina, M., Jureci¢, D., Ziljak Grsi¢é, J., 2024. Centralized database access:
transformer framework and llm/chatbot integration-based hybrid model. Applied System
Innovation 7, 17.

Chan, N., Parker, F., Bennett, W., Wu, T., Jia, M.Y., Fackler, J., Ghobadi, K., 2024.
Leveraging Ilms for multimodal medical time series analysis, in: Machine Learning for
Healthcare Conference. PMLR.

Chen, H., Tian, C., He, Z., Yu, B., Liu, Y., Cao, J., 2025. Inference performance evaluation
for llms on edge devices with a novel benchmarking framework and metric. arXiv preprint
ar Xiv:2508.11269 .

Chien, J., Danks, D., 2024. Beyond behaviorist representational harms: A plan for measure-
ment and mitigation, in: Proceedings of the 2024 ACM Conference on Fairness, Account-
ability, and Transparency, pp. 933-946.

20


===== PAGE BREAK =====

contributors, H.F., 2024. Distilroberta-financial-sentiment model. https://huggingface.

co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis

Das, S., Ghosh, S., Tiwari, A., Lynghoi, R.Z.M., Saha, $., Murad, Z., Maurya, A., 2025. Pre-
senting an order-aware multimodal fusion framework for financial advisory summarization
with an exclusive video dataset. IEEE Access .

Dewi, C., Amirzadeh, R., Thiruvady, D., Zaidi, N., 2025. Knowledge-guided object de-
tection via bayesian networks and knowledge graphs (kgbncnet). Expert Systems with
Applications , 129385.

Druzdzel, M.J., 1999. Smile: Structural modeling, inference, and learning engine and genie:
a development environment for graphical decision-theoretic models, in: Aaai/Iaai, pp.
902-903.

Gogineni, K., Suvizi, A., Venkataramani, G., 2025. Llms on a budget: System-level ap-
proaches to power-efficient and scalable fine-tuning. IEEE Open Journal of the Computer
Society .

Heckerman, D., 2008. A tutorial on learning with bayesian networks. Innovations in Bayesian
networks , 33-82.

Jeong, C., 2024. Fine-tuning and utilization methods of domain-specific lms. arXiv preprint
arXiv:2401.02981 .

Jin, Z., Chen, Y., Leeb, F., Gresele, L., Kamal, O., Lyu, Z., Blin, K., Gonzalez Adauto,
F., Kleiman-Weiner, M., Sachan, M., et al., 2023. Cladder: Assessing causal reasoning in
language models. Advances in Neural Information Processing Systems 36, 31038-31065.

Kadasi, P., Kondam, $.R., Chaturvedula, $.V., Sen, R., Saha, A., Sikdar, S., Sarkar, S., Mit-
tal, S., Jindal, R., Singh, M., 2025. Model hubs and beyond: Analyzing model popularity,
performance, and documentation, in: Proceedings of the International AAAI Conference
on Web and Social Media, pp. 965-993.

Karadas, F., Eravei, B., Ozbayoglu, A.M., 2025. Multimodal stock price prediction. arXiv
preprint arXiv:2502.05186 .

Kong, Y., Nie, Y., Dong, X., Mulvey, J.M., Poor, H.V., Wen, Q., Zohren, S., 2024. Large
language models for financial and investment management: Applications and benchmarks.
Journal of Portfolio Management 51.

Krugmann, J.O., Hartmann, J., 2024. Sentiment analysis in the age of generative ai. Cus-
tomer Needs and Solutions 11, 3.

Kyrimi, E., Mossadegh, $., Wohlgemut, J.M., Stoner, R.S., Tai, N.R., Marsh, W., 2025.
Counterfactual reasoning using causal bayesian networks as a healthcare governance tool.
International journal of medical informatics 193, 105681.

21


===== PAGE BREAK =====

Lee, J., Yoon, W., Kim, 8., Kim, D., Kim, S., So, C.H., Kang, J., 2020. Biobert: a pre-trained
biomedical language representation model for biomedical text mining. Bioinformatics 36,
1234-1240.

Liu, P., Joty, S., Meng, H., 2015. Fine-grained opinion mining with recurrent neural networks
and word embeddings, in: Proceedings of the 2015 conference on empirical methods in
natural language processing, pp. 1433-1443.

Liu, S., Xu, J., Ye, B., Hu, B., Srolovitz, D.J., Wen, T., 2025. Mattools: Benchmarking
large language models for materials science tools. arXiv preprint arXiv:2505.10852 .

Liu, Y., Yao, Y., Ton, J.F., Zhang, X., Guo, R., Cheng, H., Klochkov, Y., Taufiq, M.F., Li,
H., 2023. Trustworthy Ilms: a survey and guideline for evaluating large language models’
alignment. arxiv. Preprint posted online August 10.

Malo, P., Sinha, A., Korhonen, P., Wallenius, J., Takala, P., 2014. Good debt or bad
debt: Detecting semantic orientations in economic texts. Journal of the Association for
Information Science and Technology 65.

Mesk6, B., 2023. Prompt engineering as an important emerging skill for medical professionals:
tutorial. Journal of medical Internet research 25, e50638.

Miah, M.S.U., Kabir, M.M., Sarwar, T.B., Safran, M., Alfarhood, S., Mridha, M., 2024. A
multimodal approach to cross-lingual sentiment analysis with ensemble of transformer and
Ilm. Scientific Reports 14, 9603.

Minaee, S., Mikolov, T., Nikzad, N., Chenaghlu, M., Socher, R., Amatriain, X., Gao, J.,
2024. Large language models: A survey. arXiv preprint arXiv:2402.06196 .

Mohawesh, R., Salameh, H.B., Jararweh, Y., Alkhalaileh, M., Maqsood, S., 2024. Fake review
detection using transformer-based enhanced Istm and roberta. International Journal of
Cognitive Computing in Engineering 5, 250-258.

Mudrik, A., Nadkarni, G.N., Efros, O., Soffer, S., Klang, E., 2025. Prompt engineering in
large language models for patient education: A systematic review. medRxiv , 2025-03.

Mughal, N., Mujtaba, G., Shaikh, S., Kumar, A., Daudpota, $.M., 2024. Comparative
analysis of deep natural networks and large language models for aspect-based sentiment
analysis. IEEE Access 12, 60943-60959.

Muhammad, Y.I., Salim, N., Muchtar, F.B., Sharma, A., Sidik, M.K.B.M., Khairuddin,
A.R.B., Shah, Z.B.A., 2025. Similarity-based intent detection using an enhanced siamese
network. Procedia Computer Science 259, 1719-1727.

Narashiman, $.S., Chandrachoodan, N., 2024. Alphazip: Neural network-enhanced lossless
text compression. arXiv preprint arXiv:2409.15046 .

Pattnayak, A., Ramkumar, A., Khetarpaul, S., Vuthoo, K., 2025. Lawmate: Leveraging
domain-specific llms for the indian legal ecosystem, in: Asian Conference on Intelligent
Information and Database Systems, Springer. pp. 188-201.

22


===== PAGE BREAK =====

Polotskaya, K., Munoz-Valencia, C.S., Rabasa, A., Quesada-Rico, J.A., Orozco-Beltran, D.,
Barber, X., 2024. Bayesian networks for the diagnosis and prognosis of diseases: A scoping
review. Machine Learning and Knowledge Extraction 6, 1243-1262.

Raiaan, M.A.K., Mukta, M.S.H., Fatema, K., Fahad, N.M., Sakib, S., Mim, M.M.J., Ah-
mad, J., Ali, M.E., Azam, S., 2024. A review on large language models: Architectures,
applications, taxonomies, open issues and challenges. IEEE access 12, 26839-26874.

Ridley, N., Branca, E., Kimber, J., Stakhanova, N., 2023. Enhancing code security through
open-source large language models: A comparative study, in: International Symposium
on Foundations and Practice of Security, Springer. pp. 233-249.

Rique, T., Perkusich, M., Gorgénio, K., Almeida, H., Perkusich, A., 2025. Constructing the
graphical structure of expert-based bayesian networks in the context of software engineer-
ing: A systematic mapping study. Information and Software Technology 177, 107586.

Shao, M., Basit, A., Karri, R., Shafique, M., 2024. Survey of different large language model
architectures: Trends, benchmarks, and challenges. IEEE Access .

Singh, A., Ehtesham, A., Gupta, G.K., Chatta, N.K., Kumar, S., Khoei, T.T., 2024. Ex-
ploring prompt engineering: A systematic review with swot analysis. arXiv preprint
arXiv:2410.12843 .

Tekin, S.F., Ilhan, F., Huang, T., Hu, S., Liu, L., 2024. Llm-topla: Efficient Ilm ensemble
by maximising diversity. arXiv preprint arXiv:2410.03958 .

Thakur, N., Reimers, N., Riicklé, A., Srivastava, A., Gurevych, I., 2021. BEIR: A heteroge-
neous benchmark for zero-shot evaluation of information retrieval models, in: Thirty-fifth
Conference on Neural Information Processing Systems Datasets and Benchmarks Track

(Round 2). URL: https: //openreview.net/forum?id=wCu6T5xFjeJ

Thiebes, S., Lins, $., Sunyaev, A., 2021. Trustworthy artificial intelligence. Electronic
Markets 31, 447-464.

Wijayanto, S., Khodra, M.L., 2018. Business intelligence according to aspect-based sentiment
analysis using double propagation, in: 2018 3rd International Conference on Information
Technology, Information System and Electrical Engineering (ICITISEE), IEEE. pp. 105—
109.

Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T.,
Louf, R., Funtowicz, M., et al., 2019. Huggingface’s transformers: State-of-the-art natural
language processing. arXiv preprint arXiv:1910.03771 .

Wu, X.K., Chen, M., Li, W., Wang, R., Lu, L., Liu, J., Hwang, K., Hao, Y., Pan, Y., Meng,
Q., et al., 2025. Llm fine-tuning: Concepts, opportunities, and challenges. Big Data and
Cognitive Computing 9, 87.

23


===== PAGE BREAK =====

Xia, Y., Kim, J., Chen, Y., Ye, H., Kundu, S., Hao, C.C., Talati, N., 2024. Understanding
the performance and estimating the cost of Ilm fine-tuning, in: 2024 IKEE International
Symposium on Workload Characterization (IISWC), IEEE. pp. 210-223.

Xiong, Y., Chen, X., Ye, X., Chen, H., Lin, Z., Lian, H., Su, Z., Huang, W., Niu, J.,
Han, J., et al., 2024. Temporal scaling law for large language models. arXiv preprint
arXiv:2404.17785 .

Yi, Z., Xiao, T., Albert, M.V., 2025. A survey on multimodal large language models in
radiology for report generation and visual question answering. Information 16, 136.

Zeroshot, 2024. Twitter financial news sentiment. https://huggingface.co/datasets/
zeroshot/twitter-financial-news-sentiment, Hugging Face Datasets.

24
