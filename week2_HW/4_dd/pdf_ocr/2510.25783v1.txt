2510.25783v1 [cs.CL] 28 Oct 2025

arXiv

LASTIST: LArge-Scale Target-Independent STance dataset

DongJae Kim', Yaejin Lee', Minsu Park!, Eunil Park!

‘Sungkyunkwan University
bronze.ash@ g.skku.edu, 19lyaejin@ gmail.com, alstn7 @ gmail.com, eunilpark @ skku.edu

Abstract

Stance detection has emerged as an area of research in
the field of artificial intelligence. However, most research
is currently centered on the target-dependent stance detec-
tion task, which is based on a person’s stance in favor of
or against a specific target. Furthermore, most benchmark
datasets are based on English, making it difficult to develop
models in low-resource languages such as Korean, espe-
cially for an emerging field such as stance detection. This
study proposes the LArge-Scale Target-Independent STance
(LASTIST) dataset to fill this research gap. Collected from
the press releases of both parties on Korean political par-
ties, the LASTIST dataset uses 563,299 labeled Korean sen-
tences. We provide a detailed description of how we col-
lected and constructed the dataset and trained state-of-the-
art deep learning and stance detection models. Our LASTIST
dataset is designed for various tasks in stance detection, in-
cluding target-independent stance detection and diachronic
evolution stance detection. We deploy our dataset on https:
//anonymous.4open.science/t/LASTIST-3721/.

Introduction

Stance Detection (SD) is a newly emerging area in the arti-
ficial intelligence research domain. As seen in the SemEval-
2016 dataset, stance detection has been mainly formulated
as a Classification problem for given sentences into Favor,
Against optionally neutral or none toward a specific tar-
get (Mohammad et al. 2016). Backed by the growing interest
in web technology and social media platforms, the number
of studies focusing on the stance detection problem has in-
creased in recent years (Kiiciik and Can 2020; Alturayeif,
Luqman, and Ahmed 2023; Gera and Neal 2025).

Although stance detection problems have become one of
the most prominent problems in Machine Learning (ML),
stance detection studies today share several commonalities
that hinder their performance. First, stance detection datasets
constructed for model development tend to possess a lim-
ited number of instances, making it difficult to train a model
that can accurately classify stance. A recent survey paper
on stance detection (Kii¢tik and Can 2020; Alturayeif, Luq-
man, and Ahmed 2023) revealed that the majority of studies
consist of a limited number of data instances, varying from

Copyright © 2026, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.

1k to 50k, with the exception of Ajjour et al. (2019), which
has 387k. Compared to datasets in other Natural Language
Processing (NLP) fields, such as sentiment analysis (Jim
et al. 2024) or question answering (Rogers, Gardner, and
Augenstein 2023), the insufficiency of large-scale datasets
contributes to the lack of performance of stance detection
models.

Furthermore, stance detection models tend to be target-
specific because of their formulation methodology, which
means that they are bound to one or more specific targets.
While some datasets incorporate multiple targets to miti-
gate this limitation, well-known datasets on stance detection
tasks are usually bound to the particular events such as the
U.S. presidential election (Caceres-Wright et al. 2024; Sob-
hani, Inkpen, and Zhu 2017; Li et al. 2021; Niu et al. 2024),
major societal issues such as the feminist movement or cli-
mate change (Mohammad et al. 2016; Mohammad, Sobhani,
and Kiritchenko 2017; Choi, Shang, and Wang 2025; Upad-
hyaya, Fisichella, and Nejdl 2023; Wang et al. 2024). Al-
though this tendency could be effective when predicting the
stance of a tweet or an article for a specific topic, it leads
to another problem of target dependency. Because the mod-
els are bound to specific targets, their performance toward
unseen targets or a universal stance could be limited. This
problem could be even more critical in the case of NLP for
low-resource languages such as Korean, as the resources for
training ML models are even scarcer.

To overcome this research gap, we propose the LArge-
Scale Target-Independent STance (LASTIST) dataset,
which is a large-scale dataset that can be used in the Ko-
rean stance detection task. This study makes the following
contributions: 1) We present LATIST, a large-scale Korean
stance detection dataset consisting of 563,299 sentences that
can be classified as pro-left or pro-right. 2) We propose a
framework for dataset construction based on active learn-
ing, which suggests an efficient means of extracting biased
sentences from press releases. 3) We conduct several exper-
iments on our LASTIST dataset, including benchmark and
Large Language Models (LLMs), which provide a suitable
benchmark for further stance detection research.

The remainder of this paper is organized as follows. In
Section 2, we present previous studies that attempted to
build stance detection datasets, classifying them into single-
target-specific, multi-target-specific, and target-independent


===== PAGE BREAK =====

datasets. Section 3 provides a detailed description of how
we collected the data and filtered the properly biased ex-
pressions. Section 4 reports the experimental results using
baselines and LLMs on the stance detection dataset. Finally,
we conclude the paper by stating the implications and limi-
tations of our work.

Related Works

Stance detection tasks typically classify the viewpoint ex-
pressed in a text toward one or more specific targets, which
refers to an entity or short noun-phrase that a given doc-
ument or sentence aims to express opinion or attitude, for
example, a political figure or controversial topic (Zotova
et al. 2020; Conforti et al. 2020). Depending on the nature
or number of targets involved, stance detection studies can
be classified into three categories: single-target, multi-target,
and target-independent.

Single-Target Stance Detection

Single-target stance detection refers to predicting the stance
expressed in a given text toward a predefined single target.
Owing to the ease of the construction approach, prior stud-
ies mostly centered on single-target stance detection, and
most existing datasets have been designed accordingly (Al-
turayeif, Luqman, and Ahmed 2023; ALDayel and Magdy
2021). One of the most famous examples is Mohammad
et al. (2016), which introduced an English dataset on top-
ics such as atheism or climate change, aiming to identify
rumors and the stance of Twitter users through their textual
replies. More recently, Li et al. (2021) presented a single-
target stance detection dataset for several U.S. presiden-
tial candidates in the political domain, composed of 21,574
tweets.

Although such datasets are effective for training mod-
els that can classify stances toward a specific target, sev-
eral challenges often arise while building them. First, re-
taining sufficient annotated data for each specific target is
challenging, which leads to datasets lacking the size needed
to effectively train deep learning models, such as LLMs. As
shown in Table 1, most target-specific datasets consist of up
to 50k data instances, making it difficult for the stance de-
tection model to be trained without additional training meth-
ods, such as fine-tuning. Moreover, the model’s narrow fo-
cus on single-target approaches hinders its ability to learn
the underlying relationships among targets, capture domain-
general features, and limit generalization to unseen targets.
Finally, a notable limitation of existing resources is their
dominant focus on English-language data, which highlights
a significant resource gap for other languages, such as Ko-
rean (Mohammad et al. 2016; Conforti et al. 2020; Grim-
minger and Klinger 2021; Gyawali et al. 2024; Zhao and
Caragea 2024; Li and Zhang 2024).

Multi-Target Stance Detection

These drawbacks of single-target approaches have spurred
interest in multi-target stance detection (MTSD) and the de-
velopment of a corresponding dataset to support such a task.

MTSD focuses on identifying stances toward multiple tar-
gets within the input text, thereby enabling a more compre-
hensive and realistic understanding of the expressed stance.
One early effort in this direction was the Multi-Target SD
dataset (Sobhani, Inkpen, and Zhu 2017), which simultane-
ously introduced annotations for two U.S. political targets.
Similarly, the Trump-Hillary dataset (Darwish, Magdy, and
Zanouda 2017) provided multi-target annotations for both
U.S. presidential candidates—Hillary and Trump as targets—
for instance, “supporting Hillary” and “opposing Trump”.
More recently, Niu et al. (2024) constructed a stance de-
tection dataset based on conversational data to address real-
world applications. Such datasets effectively ascertain the
practical challenges of MTSD, including coreference rela-
tions or implicit target references that frequently occur in
the real world. Despite its relevance, MTSD has received
limited attention in recent years owing to the scarcity of
high-quality datasets containing multiple annotated targets.
Moreover, most MTSD datasets are constructed in English,
further highlighting the lack of resources available for other
languages.

Target-Independent Stance Detection

Target-independent stance detection aims to identify the
stance expressed in a text without relying on explicit or pre-
defined target entities, thereby hindering the constraint on
the number of target types. The IBM Debater (Bar-Haim
et al. 2017) dataset leveraged Wikipedia articles and was la-
belled with stances on 55 claims, making it compatible with
target-independent tasks. Similarly, RumourEval-19 (Gor-
rell et al. 2019) extended stance detection to cover different
topic entities as target-independent entities related to natural
disasters from Tweets and Reddit posts.

To the best of our knowledge, there are currently no avail-
able Korean stance detection datasets that support target-
independent tasks. Furthermore, the effectiveness of stance
detection models heavily depends on access to sufficiently
large and well-annotated datasets that are also balanced.
Therefore, this study proposes the LASTIST dataset, aimed
at supporting target-independent stance detection while en-
suring a sufficient data size and extending accessibility to
the low-resource language, Korean.

Dataset Construction

This section describes how we collected the data to construct
the LASTIST dataset and preprocessed it to retain only the
sufficiently biased sentences. This is followed by topic label-
ing and quality assurance of our dataset, which could be used
in various stance detection tasks to work on further stance
detection tasks. Figure 1 illustrates the overall dataset con-
struction.

Data Collection

To collect the biased articles that could be classified into
pro-left and pro-right, we collected the press release arti-
cles from the webpages of The Minjoo party ' and People

‘https://theminjoo.kr/main/


===== PAGE BREAK =====

Dataset                                                                   Target # Target(s)              Source              Dataset Size Language
SemEval-2016 Task 6 (Mohammad et al. 2016)               ST                     6              Twitter                       4,870       English
P-stance (Li et al. 2021)                                             ST                    3             Twitter                     21,574      English
WT-WT (Conforti et al. 2020)                                    ST                    5             Twitter                    51,284      English
EZ-Stance (Zhao and Caragea 2024)                              ST              40,678              Twitter                      47,316       English
Multi-Target SD (Sobhani, Inkpen, and Zhu 2017)          MT                  4            Twitter                     4,455      English
Trump-Hillary (Darwish, Magdy, and Zanouda 2017)       MT                     2              Twitter                        3,450       English
MT-CSD (Niu et al. 2024)                                    MT                 5            Reddit                   15,876     English
C-Stance (Zhao, Li, and Caragea 2023)                        MT                   7              Weibo                     48,126      Chinese
IBM Debater (Bar-Haim et al. 2017)                             TI                   55           Wikipedia                    2,934      English
RumourEval-19 (Gorrell et al. 2019)                                        TI                    8,574           Twitter, Reddit                        8,574         English
ORCHID (Zhao, Wang, and Peng 2023)                         TI               2,436         Debate videos                 14,091      Chinese
Arabic News Stance (Khouja 2020)                            TI              3,786             News                      3,786      Arabic
LASTIST (Ours)                                               ST, TI                 - Press release articles          563,299      Korean

Table 1: Comparison of existing stance detection datasets across various targets and languages. Target denotes the dependency
of target (ST: single-target, MT : multi-target, T7 : target-independent)

68,718 articles

Sentence Split

1,054,200
sentences

Basic Filtering

602,147
sentences

Active Learning
Filtering

563,299

sentences

Figure 1: Dataset construction process.

Power party *. Both parties are regarded as the pro-left and
pro-right sides of the Korean political spectrum, and they
are sufficient to be used as markers for Korean stance detec-
tion studies, as in previous studies on English-based stance
detection tasks that tend to classify the labels as pro-left
and pro-right (Kim, Kim, and Park 2025; He et al. 2024;
Kovacs, Cotfas, and Delcea 2024). We used Selenium * to
crawl the content of press releases from the websites, and
regarded the articles collected from the Minjoo party as pro-
left, and those collected from the People Power party as pro-

*https://www.peoplepowerparty.kr/
+https://www.selenium.dev/

right. From this procedure, we collected 21,378 pro-right
and 47,340 pro-left articles for analysis.

Data Preprocessing

After collecting a dataset that would serve as a basis for our
dataset, we preprocessed the data for our targeted sentence-
level stance-detection task. First, we split the documents col-
lected into sentences. This is because stances are typically
expressed in specific sentences rather than across an en-
tire document, making document-level labels prone to noise.
In addition, sentence-level inputs are better suited for pre-
trained models, such as BERT, and allow for more straight-
forward annotation guidelines, leading to higher consistency
and better interpretability. This preprocessing resulted in ap-
proximately 1,054,317 sentences, of which 774,327 were la-
beled pro-left and 279,990 pro-right.

We then filtered out data that were inappropriate for
stance detection tasks. We first excluded boilerplates, such
as signature lines and publication dates. Sentences that were
excessively short or long for stance analysis were excluded,
retaining only those between five and 30 words. We obtained
602,147 sentences in total, of which 431,822 were labeled
pro-left and 188,325 pro-right.

Active Learning Filtering

In this study, we assumed that subjective expressions inher-
ently entail stances. This is supported by the assumption
that only sentences containing subjective content, such as
opinions, evaluations, or emotions, can meaningfully con-
vey a stance toward an arbitrary target. As theorized in stud-
ies in linguistics, sociolinguistics, and humanities regard-
ing stance detection (Kockelman 2004; Du Bois 2008; Mets
et al. 2024), subjectivity is not an independent concept from
stance, but rather an indexical effect that emerges as a result
of stance expression. Therefore, subjectivity can be used as
a valid indicator of the existence of a stance, which justifies
the use of subjectivity to filter out non-stance-related sen-
tences.

Therefore, we employed a_ subjectivity-based auto-
labeling framework to refine the dataset by filtering out in-


===== PAGE BREAK =====

Pro-Left Pro-Right

Number of sentences 394,763      168,536
Avg. #Tokens               15.734       15.274
Avg. Length               68.054      63.159

Table 2: Dataset statistics for LASTIST.

stances that were still misaligned with stance detection. In
particular, we used Small-Text (Schréder et al. 2023), an ac-
tive learning framework designed to easily supplement la-
bels for text classification tasks. Based on Small-Text, we
used KPF-BERT ¢, which is pre-trained by the Korean Press
Foundation using BigKinds news data made up of Korean
news articles. For the annotation guidelines, we used an ex-
cerpt from Antici et al. (2024), which conveys the definition
of subjectivity in news articles. We trained the KPF-BERT
model using 10 iterations and filtered out sentences inap-
propriate for stance detection, such as quotations or factual
sentences. We applied this method because quotations are
likely to convey the stance of the person referred to in the
sentence, which might not align with the overall document’s
stance.

Dataset Distribution

Following the procedures mentioned above, we finally ob-
tained 563,299 data points. The final LASTIST dataset con-
sists of 394,763 pro-left and 168,536 pro-right sentences.
The detailed statistics of the LASTIST dataset are presented
in Table 2.

Experiment

Following the construction of the LASTIST dataset, we
conducted an empirical study to evaluate its effectiveness
and validity by benchmarking a bidirectional encoder rep-
resentation from transformers (BERT)-based stance detec-
tion method (Devlin et al. 2019). Our experiments aimed to
validate LASTIST as a reliable benchmark suitable for con-
ventional stance detection tasks and the more challenging
target-independent context.

Experiment Setting

To assess the validity of the LASTIST dataset across both
target-independent and target-dependent stance detection
tasks, we conducted experiments under two different set-
tings:

1) Target-Independent Setting (Full LASTIST): The com-
plete LASTIST dataset encompasses a broad and diverse
range of targets, positioning it as an appropriate benchmark
for evaluating target-independent stance detection. In this
setting, the entire dataset was leveraged without providing
the model with explicit information regarding the target en-
tities. This approach enabled a comprehensive evaluation of
the dataset’s suitability for the target-independent stance de-
tection task.

“https://github.com/KPFBERT/kpfbert

2) Single-Target Setting (LASTIST subset): To further ex-
amine the dataset’s applicability in narrowly scoped sce-
narios, we conducted additional experiments focused on
a single-target stance detection task, specifically centered
on one target, party leadership. A refined subset of the
LASTIST dataset was rendered using the Latent Dirichlet
Allocation (LDA) topic modeling method to exclude irrele-
vant and dissimilar topics. Thus, this setting yielded a coher-
ent dataset that evaluated the stance detection performance
under constrained, single-target conditions.

Model Construction

In the context of the BERT-based baseline, our model com-
prises a pretrained KoBERT-based encoder to derive the ini-
tial token-level embeddings for each input sentence. These
token embeddings are then aggregated into a fixed-size sen-
tence embedding using a mean pooling layer.

For the learning strategy, the model adopts a contrastive
learning approach on these sentence embeddings to achieve
the following two goals:

1. Encourage the model to capture stance-specified contex-
tual features, and

2. Effectively differentiates between sentences belonging to
divergent stances by pulling sentence embeddings from
the same label closer while pushing different labels away.

This contrastive learning framework fosters the sentence
embeddings to become stance-discriminative, improving
their utility for downstream stance detection tasks, such as
political bias classification. Unlike a simple token combi-
nation, the pooling layer effectively aggregates token-level
embeddings into a coherent sentence-level embedding. The
resulting sentence embeddings are then passed into a simple
one-layer classifier, which performs the final prediction of
the political bias class.

Configuring the model, we jointly trained the contrastive
learning loss and the political bias classification loss. A
weighting parameter a@ was introduced to balance the two
objectives, which was impirically set to 0.5 to ensure the
equal emphasis during training. The model was optimized
using Adam with a batch size of 8 and le-5 learning rate.
To prevent overfitting, a dropout rate of 0.5 was also ap-
plied to the classifier layer. For the Single-Target model,
most hyperparameters remained consistent with the previous
setup. However, to account for the smaller dataset size, the
batch size was reduced to 4, the learning rate was adjusted
to 2e-5, and the number of training epochs was set to 30. All
the experiments were performed on a single GPU, NVIDIA
GeForce RTX 2080 Ti (11GB VRAM) under CUDA 11.6
and driver version 510.54, ensuring sufficient computational
resources for BERT-based stance detection training. Also,
we fixed the random seed to 42 across all training and eval-
uation runs.

Evaluation Metrics

For our evaluation, we used three evaluation metrics to re-
port the performance of the model. The evaluation metrics
used were as follows:


===== PAGE BREAK =====

Dataset                                 Pro-Left                Pro-Right                     Total
Target-Independent            394,273                  168,536                   563,299
Single-Target        67,584 (17.1%) 37,866 (22.5%) 105,450 (18.7%)

Table 3: Size of dataset in target-independent and single-target settings

Accuracy We used accuracy as a metric to assess the over-
all performance and evaluate how well the model correctly
classifies pro-left and pro-right sentences. The calculation is
as follows, where Ty. ¢; and T’pignz denote the correct pre-
dictions for pro-left and pro-right sentences, respectively. In
addition, F,.¢¢ and f’Rignz denote incorrect predictions for
pro-left and pro-right sentences, respectively.

Treft + Tright

()

Accuracy =

Trest + Tright + Prefe + FRight

Fl-score We also used the macro-average of the Fl-score
to mitigate the effect of label imbalance. We first calculated
the Fl-score for each label and then computed the average
as follows, where P and R refer to Precision and Recall,
respectively.

2: Prett: Rre
Flpept = 40 A Left’ “Left            (2)
Prefi t+ Rreft
2+ Pright * Rrigh
Fl right _—      Right    Right             (3)
Prignt + Rright
Fire    Flip;
Fl qyg = et tia             (4)

2

Area Under ROC Finally, we used the area under ROC
(AUROC) curve as our final evaluation metric. This metric
evaluates the model performance in terms of classification
separability.

Classification Result

Table 4 presents the performance of BERT-based model
across both the target-independent and single-target stance
detection tasks.

The performance of the BERT-based baseline model on
the entire LASTIST dataset was not outstanding but re-
mained understandable. Undoubtedly, the outcome shows
the inherent complexity and contextual difficulty of the
target-independent stance detection task, particularly when
applied to a large-scale dataset such as the LASTIST.

The performance of the single-target stance detection
showed a significant improvement, outperforming other
baselines. This outcome underscores the relative simplicity
of the single-target detection task compared with the target-
independent scenario. The observed performance gap fur-
ther demonstrates the necessity for more advanced model-
ing strategies capable of capturing implicit stance in the ab-
sence of predefined target references and target-independent
stance detection. Moreover, the strong performance of our
BERT-based model reflects its ability to generate stance rep-
resentations effectively, capturing affective contextual fea-
tures from carefully annotated labels. This, in turn, confirms

the quality and reliability of the dataset labels, implying
their value as benchmarks for both single-target and target-
independent stance detection tasks.

Experiment Setting Accuracy Fl AUC-ROC

Target-Independent            0.666          0.372           0.623
Single-Target                       0.976          0.956           0.995

Table 4: Performance comparison of BERT-based baseline
under different experimental settings

Discussion
Contribution of LASTIST dataset

As shown in the experimental results, our study proposes
a dataset construction framework and the validity of the
new large-scale dataset that enables the training of target-
independent stance classifiers, particularly in low-resource
languages such as Korean. Our classification results reveal
that although frequently used SOTA models, such as BERT,
excel at stance detection when the scope of the classifica-
tion is restricted. However, given that the same model cannot
classify the stance when the scope of the dataset is expanded,
it is reasonable to define a new task that enables stance clas-
sification even without an explicitly defined target. In this
context, our study proposes the LASTIST dataset, which
can be used to train and evaluate a stance detection model
designed to detect stances or biases that are not explicitly
given.

Moreover, our study introduces a new benchmark dataset
that can facilitate model training in low-resource languages,
such as Korean. Although stance detection is a rapidly grow-
ing domain in ML for politics, most datasets focus on a cou-
ple of languages, such as English or Chinese, leading to a
lack of multilingual support in stance detection research.
This lack of linguistic diversity limits the generalizability
of existing stance detection models to other political con-
texts and languages. Our study enables the development of
a large-scale stance detection dataset from Korean political
press releases to address this gap. Therefore, our dataset is
a valuable resource for evaluating cross-linguistic transfer-
ability and bias in stance classification tasks.

Limitation

Although this study introduced a new stance detection
dataset for a large-scale target-independent task, it has sev-
eral limitations. First, using a subjectivity filter may not
entirely reflect the nature of the stance represented in real


===== PAGE BREAK =====

life. While subjectivity could serve as a valid filter to ex-
clude sentences that take a stance toward an arbitrary tar-
get, it could also filter out sentences with a more implicit
stance. Modern studies on stance detection (Hamborg 2023;
Zhukova et al. 2023) imply that while subjectivity could in-
dicate the existence of a stance, it fails to provide a complete
viewpoint on whether a stance exists in the data. In other
words, our dataset is limited because it does not include data
that conveys objective information but has a specific stance.

Next, applying the active learning framework to auto-
label the dataset might make its justification imperfect. The
use of an active learning framework was to respond to the
amount of data collected, but its use still has limitations in
terms of accuracy and reliability. Our dataset is not fully
constructed from manual labels; therefore, the labels might
have internal biases that make them incomplete. Further re-
search with human annotations could follow our study by
providing a more reliable and accurate labeling process for
the collected data.

Conclusion

In this study, we introduce LASTIST, a Korean stance
detection dataset designed to support large-scale target-
independent stance detection. Collected the press release ar-
ticles from The Minjoo Party and People Power Party, our
dataset consists of Our experimental results showed that
target-independent stance detection tasks are much more
complicated than target-specific detection, highlighting the
need for further research on stance detection and machine
learning applications in the political domain.

Data and Code availability

We share our data and code for data collection, prepro-
cessing, and experiment in our GitHub repository (https:
//anonymous.4open.science/t/LASTIST-3721/).

References
Ajjour, Y.; Wachsmuth, H.; Kiesel, J.; Potthast, M.; Ha-
gen, M.; and Stein, B. 2019. Data Acquisition for Argu-
ment Search: The args.me Corpus. In Benzmiiller, C.; and
Stuckenschmidt, H., eds., KJ 2019: Advances in Artificial In-
telligence, 48-59. Cham: Springer International Publishing.
ISBN 978-3-030-30179-8.
ALDayel, A.; and Magdy, W. 2021. Stance detection on
social media: State of the art and trends. Information Pro-
cessing & Management, 58(4): 102597.
Alturayeif, N.; Luqman, H.; and Ahmed, M. 2023. A sys-
tematic review of machine learning techniques for stance de-
tection and its applications. Neural Computing and Appli-
cations, 35(7): 5113-5144.
Antici, F.; Ruggeri, F; Galassi, A.; Korre, K.; Muti, A.;
Bardi, A.; Fedotova, A.; and Barrén-Cedefio, A. 2024. A
Corpus for Sentence-Level Subjectivity Detection on En-
glish News Articles. In Calzolari, N.; Kan, M.-Y.; Hoste,
V.; Lenci, A.; Sakti, S.; and Xue, N., eds., Proceedings of
the 2024 Joint International Conference on Computational
Linguistics, Language Resources and Evaluation (LREC-
COLING 2024), 273-285. Torino, Italia: ELRA and ICCL.

Bar-Haim, R.; Bhattacharya, I.; Dinuzzo, F; Saha, A.;
and Slonim, N. 2017. Stance Classification of Context-
Dependent Claims. In Lapata, M.; Blunsom, P.; and Koller,
A., eds., Proceedings of the 15th Conference of the Euro-
pean Chapter of the Association for Computational Linguis-
tics: Volume I, Long Papers, 251-261. Valencia, Spain: As-
sociation for Computational Linguistics.

Caceres-Wright, A. R.; Udhayasankar, N.; Bunn, G.; Shus-
ter, S. M.; and Joseph, K. 2024. Explicit Stance Detection
in the Political Domain: A New Concept and Associated
Dataset. In Thomson, R.; Hariharan, A.; Renshaw, S.; Al-
khateeb, S.; Burger, A.; Park, P.; and Pyke, A., eds., Social,
Cultural, and Behavioral Modeling, 3-14. Cham: Springer
Nature Switzerland.

Choi, Y.; Shang, L.; and Wang, D. 2025. ClimateMiSt: Cli-
mate Change Misinformation and Stance Detection Dataset.
In Aiello, L. M.; Chakraborty, T.; and Gaito, S., eds., Social
Networks Analysis and Mining, 321-330. Cham: Springer
Nature Switzerland.

Conforti, C.; Berndt, J.; Pilehvar, M. T.; Giannitsarou, C.;
Toxvaerd, F.; and Collier, N. 2020. Will-They-Won’t-They:
A Very Large Dataset for Stance Detection on Twitter. In
Jurafsky, D.; Chai, J.; Schluter, N.; and Tetreault, J., eds.,
Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics, 1715-1724. Online: Associ-
ation for Computational Linguistics.

Darwish, K.; Magdy, W.; and Zanouda, T. 2017. Trump vs.
Hillary: What Went Viral During the 2016 US Presidential
Election. In Social Informatics.

Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.
Bert: Pre-training of deep bidirectional transformers for lan-
guage understanding. In Proceedings of the 2019 conference
of the North American chapter of the association for compu-

tational linguistics: human language technologies, volume 1
(long and short papers), 4171-4186.

Du Bois, J. W. 2008. The stance triangle. In Stancetaking
in discourse: Subjectivity, evaluation, interaction, 139-182.
John Benjamins Publishing Company.

Gera, P.; and Neal, T. 2025. Deep Learning in Stance Detec-
tion: A Survey. ACM Computing Surveys.

Gorrell, G.; Kochkina, E.; Liakata, M.; Aker, A.; Zubiaga,
A.; Bontcheva, K.; and Derczynski, L. 2019. SemEval-2019
Task 7: RumourEval, Determining Rumour Veracity and
Support for Rumours. In May, J.; Shutova, E.; Herbelot, A.;
Zhu, X.; Apidianaki, M.; and Mohammad, S. M., eds., Pro-
ceedings of the 13th International Workshop on Semantic
Evaluation, 845-854. Minneapolis, Minnesota, USA: Asso-
ciation for Computational Linguistics.

Grimminger, L.; and Klinger, R. 2021. Hate Towards the
Political Opponent: A Twitter Corpus Study of the 2020 US
Elections on the Basis of Offensive Speech and Stance De-
tection. In De Clercq, O.; Balahur, A.; Sedoc, J.; Barriere,
V.; Tafreshi, S.; Buechel, S.; and Hoste, V., eds., Proceedings
of the Eleventh Workshop on Computational Approaches to
Subjectivity, Sentiment and Social Media Analysis, 171-180.
Online: Association for Computational Linguistics.


===== PAGE BREAK =====

Gyawali, N.; Sirbu, I.; Sosea, T.; Khanal, S.; Caragea, D.;
Rebedea, T.; and Caragea, C. 2024. GunStance: Stance De-
tection for Gun Control and Gun Regulation. In Ku, L.-W.;
Martins, A.; and Srikumar, V., eds., Proceedings of the 62nd
Annual Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), 12027-12044. Bangkok,
Thailand: Association for Computational Linguistics.
Hamborg, F. 2023. Revealing media bias in news articles:
NLP techniques for automated frame analysis. Springer Na-
ture.

He, Z.; Rao, A.; Guo, S.; Mokhberian, N.; and Lerman,
K. 2024. Reading Between the Tweets: Deciphering Ide-
ological Stances of Interconnected Mixed-Ideology Com-
munities. In Graham, Y.; and Purver, M., eds., Findings of
the Association for Computational Linguistics: EACL 2024,
1523-1536. St. Julian’s, Malta: Association for Computa-
tional Linguistics.

Jim, J. R.; Talukder, M. A. R.; Malakar, P.; Kabir, M. M.;
Nur, K.; and Mridha, M. 2024. Recent advancements and
challenges of NLP-based sentiment analysis: A state-of-
the-art review. Natural Language Processing Journal, 6:
100059.

Khouja, J. 2020. Stance Prediction and Claim Verification:
An Arabic Perspective. In Christodoulopoulos, C.; Thorne,
J.; Vlachos, A.; Cocarascu, O.; and Mittal, A., eds., Proceed-
ings of the Third Workshop on Fact Extraction and VERifica-
tion (FEVER), 8-17. Online: Association for Computational
Linguistics.

Kim, J.; Kim, D.; and Park, E. 2025. I know your stance!
Analyzing Twitter users’ political stance on diverse perspec-
tives. Journal of Big Data, 12(1): 14.

Kockelman, P. 2004. Stance and subjectivity. Journal of
Linguistic Anthropology, 14(2): 127-150.

Kovacs, E.-R.; Cotfas, L.-A.; and Delcea, C. 2024. A
Deep Learning Approach to Fine-Grained Political Ideol-
ogy Classification on Social Media Texts. In Nguyen, N. T.;
Franczyk, B.; Ludwig, A.; Nufiez, M.; Treur, J.; Vossen, G.;
and Kozierkiewicz, A., eds., Computational Collective In-
telligence, 3-14. Cham: Springer Nature Switzerland.
Kiiciik, D.; and Can, F. 2020. Stance detection: A survey.
ACM Computing Surveys (CSUR), 53(1): 1-37.

Li, Y.; Sosea, T.; Sawant, A.; Nair, A. J.; Inkpen, D.; and
Caragea, C. 2021. P-stance: A large dataset for stance de-
tection in political domain. In Findings of the association for
computational linguistics: ACL-IJCNLP 2021, 2355-2365.
Li, Y.; and Zhang, Y. 2024. Pro-Woman, Anti-Man? Identi-
fying Gender Bias in Stance Detection. In Ku, L.-W.; Mar-
tins, A.; and Srikumar, V., eds., Findings of the Associa-
tion for Computational Linguistics: ACL 2024, 3229-3236.
Bangkok, Thailand: Association for Computational Linguis-
tics.

Mets, M.; Karjus, A.; Ibrus, I.; and Schich, M. 2024. Au-
tomated stance detection in complex topics and small lan-
guages: the challenging case of immigration in polarizing
news media. Plos one, 19(4): e0302380.

Mohammad, S.; Kiritchenko, S.; Sobhani, P.; Zhu, X.; and
Cherry, C. 2016. Semeval-2016 task 6: Detecting stance in

tweets. In Proceedings of the 10th international workshop
on semantic evaluation (SemEval-2016), 31-41.

Mohammad, S. M.; Sobhani, P.; and Kiritchenko, S. 2017.
Stance and Sentiment in Tweets. ACM Trans. Internet Tech-
nol., 17(3).

Niu, F; Yang, M.; Li, A.; Zhang, B.; Peng, X.; and Zhang, B.
2024. A challenge dataset and effective models for conver-
sational stance detection. arXiv preprint arXiv:2403.11145.

Rogers, A.; Gardner, M.; and Augenstein, I. 2023. QA
Dataset Explosion: A Taxonomy of NLP Resources for
Question Answering and Reading Comprehension. ACM
Comput. Surv., 55(10).

Schréder, C.; Miiller, L.; Niekler, A.; and Potthast, M.
2023. Small-Text: Active Learning for Text Classification in
Python. In Croce, D.; and Soldaini, L., eds., Proceedings of
the 17th Conference of the European Chapter of the Associa-
tion for Computational Linguistics: System Demonstrations,
84-95. Dubrovnik, Croatia: Association for Computational
Linguistics.

Sobhani, P.; Inkpen, D.; and Zhu, X. 2017. A Dataset for
Multi-Target Stance Detection. In Lapata, M.; Blunsom,
P.; and Koller, A., eds., Proceedings of the 15th Conference
of the European Chapter of the Association for Computa-
tional Linguistics: Volume 2, Short Papers, 551-557. Valen-
cia, Spain: Association for Computational Linguistics.

Upadhyaya, A.; Fisichella, M.; and Nejdl, W. 2023. A multi-
task model for emotion and offensive aided stance detection
of climate change tweets. In Proceedings of the ACM Web
Conference 2023, 3948-3958.

Wang, J.; Zuo, L.; Peng, S.; and Plank, B. 2024. Multi-
Climate: Multimodal Stance Detection on Climate Change
Videos. In Third Workshop on NLP for Positive Impact, 315.

Zhao, C.; and Caragea, C. 2024. EZ-STANCE: A Large
Dataset for English Zero-Shot Stance Detection. In Ku,
L.-W.; Martins, A.; and Srikumar, V., eds., Proceedings of
the 62nd Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), 15697-15714.
Bangkok, Thailand: Association for Computational Linguis-
tics.

Zhao, C.; Li, Y.; and Caragea, C. 2023. C-STANCE: A
Large Dataset for Chinese Zero-Shot Stance Detection. In
Rogers, A.; Boyd-Graber, J.; and Okazaki, N., eds., Pro-
ceedings of the 61st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers),
13369-13385. Toronto, Canada: Association for Computa-
tional Linguistics.

Zhao, X.; Wang, K.; and Peng, W. 2023. ORCHID: A
Chinese Debate Corpus for Target-Independent Stance De-
tection and Argumentative Dialogue Summarization. In
Bouamor, H.; Pino, J.; and Bali, K., eds., Proceedings of
the 2023 Conference on Empirical Methods in Natural Lan-
guage Processing, 9358-9375. Singapore: Association for
Computational Linguistics.

Zhukova, A.; Ruas, T.; Hamborg, F.; Donnay, K.; and Gipp,
B. 2023. What’s in the News? Towards Identification of Bias
by Commission, Omission, and Source Selection (COSS).


===== PAGE BREAK =====

In 2023 ACM/IEEE Joint Conference on Digital Libraries
(JCDL), 258-259.

Zotova, E.; Agerri, R.; Nufiez, M.; and Rigau, G. 2020. Mul-
tilingual Stance Detection in Tweets: The Catalonia Inde-
pendence Corpus. In Calzolari, N.; Béchet, F; Blache, P.;
Choukri, K.; Cieri, C.; Declerck, T.; Goggi, S.; Isahara, H.;
Maegaard, B.; Mariani, J.; Mazo, H.; Moreno, A.; Odijk,
J.; and Piperidis, S., eds., Proceedings of the Twelfth Lan-
guage Resources and Evaluation Conference, 1368-1375.
Marseille, France: European Language Resources Associa-
tion. ISBN 979-10-95546-34-4.
