arXiv:2510.26422v1 [cs.CL] 30 Oct 2025

Preprint

OMNIEDUBENCH:   A COMPREHENSIVE CHINESE
BENCHMARK FOR EVALUATING LARGE LANGUAGE
MODELS IN EDUCATION

Min Zhang'* Hao Chen' HaoChen' Wengqi Zhang? Didi Zhu® Xin Lin’
Bo Jiang'' Aimin Zhou'' Fei Wu? Kun Kuang?
East China Normal University Zhejiang University *Imperial College London

mzhang@cs.ecnu.edu.cn bjiang@deit.ecnu.edu.cn amzhou@cs.ecnu.edu.cn

ABSTRACT

With the rapid development of large language models (LLMs), various LLM-
based works have been widely applied in educational fields. However, most ex-
isting LLMs and their benchmarks focus primarily on the knowledge dimension,
largely neglecting the evaluation of cultivation capabilities that are essential for
real-world educational scenarios. Additionally, current benchmarks are often lim-
ited to a single subject or question type, lacking sufficient diversity. This issue
is particularly prominent within the Chinese context. To address this gap, we in-
troduce OmniEduBench, a comprehensive Chinese educational benchmark.
OmniEduBench consists of 24.602K high-quality question-answer pairs. The data
is meticulously divided into two core dimensions: the knowledge dimension and
the cultivation dimension, which contain 18.121K and 6.481K entries, respec-
tively. Each dimension is further subdivided into 6 fine-grained categories, cover-
ing a total of 61 different subjects (41 in the knowledge and 20 in the cultivation).
Furthermore, the dataset features a rich variety of question formats, including 11
common exam question types, providing a solid foundation for comprehensively
evaluating LLMs’ capabilities in education. Extensive experiments on 11 main-
stream open-source and closed-source LLMs reveal a clear performance gap. In
the knowledge dimension, only Gemini-2.5 Pro surpassed 60% accuracy, while
in the cultivation dimension, the best-performing model, QWQ, still trailed hu-
man intelligence by nearly 30%. These results highlight the substantial room for
improvement and underscore the challenges of applying LLMs in education.

1 INTRODUCTION

With the rapid emergence of large language models (LLMs), evaluation benchmarks have become
increasingly critical, shifting the focus of assessment toward broader and complex skills. To address
the demands of this complex paradigm, a variety of benchmarks have been proposed to evaluate
the diverse capabilities of LLMs. These benchmarks cover a wide spectrum of areas, including
knowledge and language understanding (e.g., MMLU (Hendrycks et al.|[2021), ARC
[2018)), reasoning (¢.¢., GSM8K (Cobbe et al,|[2021), AIME (Patel et al,|[2024)), multi-tumn open-
ended dialogue (e.g., MT-bench (Bai et al.|/2024)), and coding (e.g., MBPP (Austin et al.|/2021)).
Serving as indispensable tools for advancing LLM development, these benchmarks have been widely

adopted in recent influential works (Hurst et al.) |2024} 2024a 2025b
 2025} Taylor etal [2022     :     :

In recent years, a series of powerful Chinese LLMs emerged, such as the Qwen (Yang et al.||2024
2025), DeepSeek 2024a 2025 2024b), achieving performance lev-

els comparable to overseas LLMs. With the growing application of LLMs in education, researchers

“Project leader. Min Zhang (mzhang @cs.ecnu.edu.cn)
tCorresponding authors. Bo Jiang (bjiang@deit.ecnu.edu.cn) and Aimin Zhou (amzhou @cs.ecnu.edu.cn)


===== PAGE BREAK =====

Preprint

Secondary mathematics
Middle school physics.
Middle school biographies
Secondary school chemistry

Nature and _
Higher mathematics
Probability theory and mathematical statistics
Introduction to computers
Computer operating system
Computer network
Plant physiology
Biochemistry
Jurisprudence
Criminal law

Civil law
Politics

Information technol:
Civil engineering material
sychology
Pedagogy

Traditional Chinese medicine integration
Western medicine compre!   parahee
Nursing
Middle school Chinese
Secondary school  Nstory

OmniEdu
Bench

Sense of responsibility & Accountabili
=~;   ier  esty       vy

ba doc re &cre Shing   vy
!
Prefitical thinks
Metacogeitive skills
Guiding knowledge transfer
Reflective Wearing

Personalized learning paths
Interest-driven learning
Heuristic teaching

Emotional regulation skills
Empathy & Sympathy
Self-confidence & Self-efficacy
Psychological resilience & Anti-frustration ability
Growth mindset

Secondary school
titer istory
History of ancient Chinese literature

A brief history of foreign art

History of western philosophy
Introduction to linguistics
Introduction to art
Introduction to archaeology
Ideological and political in rriddte school

Managernent

Economics logic ability

Team collaboration skillls
Effective communication skills

Sseyy                                                     Social responsibility

Constructiveness & Timeliness of feedback

Taxation

Ideological and political theory in universities
Marxist theory

Human resource management

Figure 1: Overview of OmniEduBench. The benchmark comprises two dimensions: 41 subjects
across six categories in the knowledge, and 20 subjects across six categories in the cultivation.

have also begun to propose Chinese education benchmarks, which can be broadly categorized into
two types: (1) datasets translated from other languages and (2) datasets natively constructed from
Chinese education corpora. Specifically, (1) Datasets translated from other languages refer to bench-
marks constructed by directly Hae existing benchmarks from other languages into Chinese.  .
Se OT work is CLUE (Xu et al.  , which was translated from the English GLUE (Wang
fet al.| 2019)  However, a ae oe approach i is insufficient for a rigorous caluatfores of
LLMs in Chinese.  These datasets often fail to reflect the unique linguistic and cultural challenges
of the Chinese education and inherently carry biases from their original environment, thus limiting
their ability to assess LLMs’ understanding of local education knowledge and teacher-student needs.

(2) Datasets natively constructed from Chinese educational corpora refer to pentane directly col-
rected from Chinese educational text resources, such as C- Eva  (Huang et al.|/2023), Edubench HG
 112023), Scieval (Sun et al.|/2024), AGIEval (Zhong et al.|/2023), and StLUE (Xu et al.)
   . However, most existing education enc eee are ey Timited to a single subject or =

fon type. lacking sufficient diversity. Additionally, these datasets typically focus on the knowledge
dimension, overlooking the unique cultivation aspects that are essential in real-world education.

We present OmniEduBench, a comprehensive Chinese education benchmark designed to thoroughly
evaluate LLMs in terms of both knowledge understanding and skill cultivation in educational sce-
narios. OmniEduBench encompasses knowledge and cultivation dimension and comprises a total
of 24.602K high- ved question—answer pairs, covering 11 common exam question types (e.g.,
multiple choice (“2% @1),  multiple answer (47 262), fill-in-the-blank (12.2321), short answer (fail =
fl), composite questions (8 47), term explanation (4% itl f###), True/False (F|Ht@), calculation
(itm), logical reasoning G2 #8 7E FE), case analysis (219) 4) NT wl), and essay (27), as il-
lustrated in Figure[]] The knowledge dimension includes 18.121K question—answer pairs spanning
41 subject areas, from humanities to science and engineering, and covering five difficulty levels:
elementary school, middle school, high school, college, and professional examinations. The cul-
tivation dimension comprises 6.481K question—answer pairs across 20 teaching-related comments,
including guided teaching, student emotional support, and moral education (see Table [i]for details),
aiming to comprehensively assess the diverse competencies required in real-world educational set-
tings. Extensive experiments demonstrate that our proposed OmniEduBench presents a highly chal-
lenging and significant benchmark for Chinese educational evaluation. Additionally, we introduce
OmniEduBench HARD, a high-difficulty subset of OmniEduBench, specifically targeting particu-
larly demanding subjects such as advanced mathematics and competitions that require sophisticated
reasoning skills. Even the state-of-the-art LLMs achieve less than 50% accuracy on this subset,
highlighting the rigor and necessity of our proposed OmniEduBench education benchmark.

2


===== PAGE BREAK =====

Preprint

Dataset collection& cleaning                  aN
Publicly                     ‘ad         >
&A pail    Stage 1:                                 -
data                    Classification \yetadata                   Q8A pairs  awe. 328                          OmniEdu Expert annotator
Eka         Multiple-                            ;          |                                                                      Bench          review
at           &info      &indexing    >
Private       type files | extraction                     =                                                |                                       i
data              Taal                                    Structured                                                                       Ambiguit}                 is
erry       7 Field           L         data                                                                  x)   mbiguity      Challenging
GS                    © Grade        |                               — e— =)                  Incomplete Ncomn  ul ©
LIM-             |_| © Problem type pata                         yd                                 Revise/Re    P              9ful Final
synthesized                                 -                                                                                        OmniEdu
y                                             formattin
data        Markdown files                             9                                                                                                                                  Bench
C29 21k O&A Fis 106k OBA G s00k aaa | 927K 08a a 657K Q&A map   a Q&A mp (fs 24.602K 084 = OmniEduBench
Number of Q&A pairs in the collection process            sum               cleaning             filtering          verification

Figure 2: Overview of the construction process, including collection, cleaning, filtering, verification.

2 OMNIEDUBENCH

Our proposed OmniEduBench education benchmark is designed as a natively Chinese education
evaluation benchmark that captures the unique linguistic and cultural knowledge of Chinese educa-
tion, encompasses diverse question types, and assesses LLMs not only on their knowledge capabili-
ties but also on the distinctive cultivation competencies required in real-world educational scenarios.

2.1 TASK DEFINITION

Knowledge dimension focuses on evaluating the model’s mastery of subject-specific knowledge.
Tasks in this dimension include 1 1 common exam question types (e.g., multiple choice, multiple an-
swer, fill-in-the-blank, short answer, composite questions, term explanation, True/False, calculation,
logical reasoning, case analysis, and essay). These 11 question types span a wide range of disci-
plines, from humanities and history to science, engineering, and professional fields. The primary
goal is to assess the LLM’s problem-solving capabilities within the context of real-world education.

Cultivation dimension assesses LLMs on their ability to support holistic educational objectives
beyond mere knowledge acquisition. This includes guiding students’ thinking processes, fostering
moral and value development, enhancing emotional understanding, and promoting critical reasoning
skills. Tasks in this dimension are designed to reflect realistic learning scenarios, where models must
provide pedagogically sound feedback that aligns with students’ cognitive and emotional needs.

2.2 BENCHMARK CONSTRUCTION

In this section, we provide a detailed overview of the construction process for the proposed Om-
niEduBench education evaluation benchmark, as illustrated in Figure  The process consists of
four key stages: dataset collection, dataset cleaning, dual-machine filtering, and expert verification.

Dataset collection. OmniEduBench is designed to encompass a wide range of diverse scenarios to
enable comprehensive evaluation. To achieve this, we employ three distinct data collection methods,
carefully balancing diversity and efficiency in the construction of the OmniEduBench benchmark.

Manual collection of publicly available data. Existing benchmarks often lack sufficient diversity
in question types and knowledge coverage, making them inadequate for our 41 subjects in knowl-
edge dimensions. To address this gap, we manually collected additional data from publicly available
online resources (e.g., XuekeNet, ZujuanNet, ShijuanNet, ShitiNet) to enrich diversity and ensure
coverage of underrepresented scenarios, such as primary and career education. Furthermore, guided
by the Catalogue of Undergraduate Programs in Regular Higher Education Institutions [issued by
China’s Ministry of Education, we curated a large body of review materials and exam questions
across 13 academic disciplines, including philosophy, education, law, literature, history, science,
engineering, agriculture, medicine, military science, management, and the arts. This effort signifi-
cantly improves distributional balance and provides a more faithful reflection of real applications.

http: //www.moe.gov.cn/srcsite/A08/moe_1034/s4930/202403/
W020240319305498791768.pdf



===== PAGE BREAK =====

Preprint

Manual collection of private data. Data contamination remains one of the most critical challenges in
constructing evaluation datasets for LLMs. To mitigate this risk, we manually collected additional
data from private resources, such as internal school exam papers. Unlike widely circulated national
exams, these materials have never appeared on the public Internet or been included in large-scale
web crawls, effectively reducing the risk of leakage. Incorporating such private data enhances the
reliability and fairness of the benchmark, while providing a more rigorous assessment of models.

LLM- generated data. Given the difficulty of directly obtaining data in the cultivation dimension, we
leveraged LLMs to generate a substantial number of scenario-based question—answer pairs, aiming
to supplement gaps in existing resources. To ensure the quality of the synthetic data, we invited five
education experts to conduct discussions on 20 cultivation subjects and consulted relevant books,
papers, and other materials. The collected content was organized into a database, which was then
provided to the LLM to enhance the fidelity and accuracy of the generated data. For generated
questions, to increase their challenge, we designed highly confounding distractors via prompts and
conducted sampling checks and revisions with expert verification (please see more details in expert
verification). Finally, we collected a total of 927K question—answer (Q&A) pairs, including 21K
from publicly available data, 106K from private data, and 800K generated by LLMs.

Dataset cleaning. The entire data cleaning process consists of multiple steps. (1) We used
MinerU to convert the collected 927K Q&A pairs into Markdown (md) format,
enabling structured management and efficient information extraction. (2) Detailed metadata were
extracted for each question, including subject, grade level, question type, and knowledge tags, to
construct comprehensive question profiles that facilitate data management and subsequent analysis.
(3) Standard data cleaning procedures were applied, including deduplication, removal of questions
with missing key content, filtering of sensitive or inappropriate content, and exclusion of questions
that rely on external information.After the cleaning process, we obtained a total of 657K Q&A pairs.

Dual-machine filtering. To ensure OmniEduBench is a high-quality and challenging benchmark,
we implemented a dual-model filtering mechanism on an initial set of 657K Q&A pairs. Specifically,
we first evaluated all questions using QWQ32B (Team}|2025c), retaining only those that the model
answered incorrectly. This initial filtering resulted in a subset of 430K Q&A pairs. These questions
then underwent a second filtering stage with the same strategy, this time using Qwen3-235B
let al.|/2025), ultimately yielding the final set of SOK high-quality and challenging data.

Expert verification. We recruited 50 master’s students to perform an initial quality check on the
dataset based on five predefined dimensions (as shown in Table 2p. removing any data that did not
meet the criteria, which resulted in a final set of 24.602K Q&A pairs for OmniEduBench. Sub-
sequently, we invited 5 senior annotation experts to conduct a rigorous quality review on a 15%
random sample of the OmniEduBench. The review results, shown in Table |2| indicate that the
dataset maintains high overall quality, demonstrating both reliability and applicability.

2.3. EVALUATION CRITERIA

Based on the characteristics of different question types, we adopt two evaluation metrics: (1) Choice.
For questions with a standard answer, we directly evaluate the provided answer. This simplifies the
scoring process, as the model only needs to select the most appropriate option, thereby reducing
ambiguity in assessment. (2) LLM-assisted scoring. For short-answer questions that may have
multiple valid forms but are semantically equivalent, we employ an LLM-assisted scoring method.
This approach provides greater flexibility, avoids imposing unnecessary constraints on the model,
and allows for a more accurate evaluation of the model’s semantic understanding and expression.

2.4 STATISTICS

Through rigorous data filtering and expert validation, we collected 18.121K high-quality ques-
tion—answer pairs for the knowledge and 6.481K for the cultivation. As illustrated in Figure
and summarized in Table[I] with more detailed per-subject statistics provided in the Appendix, the
dataset spans 12 major categories, as shown in Table[I] including K-12, higher school, university-
level courses, and cultivation aspects such as emotion and reasoning, covering a total of 61 specific
scenarios. Figures 3]and [4]present some representative examples in different dimensions and ques-


===== PAGE BREAK =====

Preprint

Table 1: Statistics of OmniEduBench and more detailed per-subject information are shown in the
Appendix. Bilingual names and abbreviations of six knowledge and six cultivation dimensions.

Knowledge dimension                                                                          Cultivation dimension
English name                                        Abbreviation         Chinese name         English name                        Abbreviation      Chinese name
Law & Politics                                               LP                 ES BCR           Character & Values                        CV             ant 5 (MEM
Foundational Disciplines                                    FD                    FETAL             Personalized Development                PD               MEM AE
Humanities & History                                    HH                ASE          Social & Interpersonal Skills            SIS          HAS AME
Medicine & Health                                      MH                Fee (ERE          Thinking & Cognitive Skills           TCS          FES UAIRED
Interdisciplinary & Integrated Subjects                IIs             TRE SMB       Teaching Feedback & Support         TFS         Be ini SH)
Social Sciences & Economics Management        SSEM        He ES Ait FH | Emotional & Mental Health            EMH         TRS DBE RE
Category              Subjects Questions | Category Subjects Questions | Category Subjects Questions
In terms of dimension                      In terms of Knowledge                   In terms of Cultivation

Knowledge       41       18,121    LP           4        1,455     CV           2         694
Cultivation                  20                 6,481            FD                          11                  7,918           PD                          3                   1,031

In terms of different level              HH                  10            5,331        SIS                   3               736
K-12 Schools           10               4,384          MH                     3                  918           TCS                    6                1,900
High school                11                 6,735           IIs                          4                    914             TFS                        1                     193
College                      30               6,364          SSEM                 9                1,643          EMH                  5                 1,833
Total                          61              24,602         Total                  41              18,179         Total                  20               6,387

Table 2: Expert validation results for the OmniEduBench dataset.

Metric English name Metric Chinese name Average Standard deviation  Inter-rater agreement

Overall quality                   ISK               4.8                 0.1                         0.90
Clarity                               [a] ears mT EE              4.5                   0.2                           0.85
Option perplexity                 CTH A ERE                4.8                     0.3                               0.83
Accuracy                       BAUME            4.8                0.1                       0.90
Cultivation value               BAUME              4.6               0.2                      0.88

tion types. The questions exhibit wide variability in type and difficulty and are sourced from diverse
origins, primarily newly collected from public or private resources or manually constructed.

3. EXPERIMENTS

In this section, we evaluate the performance of state-of-the-art (SOTA) methods in both English and
Chinese. The experimental results indicate that OmniEduBench remains a competitive benchmark.

3.1 EXPERIMENTAL SETUP

Baselines. We evaluate 11 mainstream large language models (LLMs) in total, including 3 cutting-
edge closed-source models and 8 open-source models, one of which is a newly released education-

oriented model. The closed-source models are GPT-40 2024), Gemini-2.5 Pro
 2025), and Claude-4 Sonnet 2025a). For the open-source models, we consider

two main factors. First, they are grouped by parameter size into small (8B), medium (14B/32B/36B),
and large (72B/235B/671B) scales. Second, they are categorized by functionality into: (a) general

BABIES RATE :                                  PES ARAL PR BIN WERT Hl, FIRE, AE EE?

The following statements are incorrect:                                                       Some students laughed and played while visiting the martyrs' cemetery, |
.,      me                   =                 aon                 feel very angry, how should | deal with it?

ALPE Eb da end 7° Ac te Se I a EF RR ik         ven

A. The amount of water produced by the combustion of toluene and glycerol            AL 43a GFE, Tb hie S Rit.

remains constant under the condition of equal amounts                                     A. Criticize harshly on the spot, ask him to apologize and write a review.

Be Ue Cabri CAE WS HS SCM yD FED A ei a FR YA BH iE EN Ci        B. PAE AAI UR bi Bh Jd TE A TH

B. After the elimination reaction of ethyl bromoide, the ethylene produced can be

B. Suspension of his extracurricular activities for one week as punishment.
directly tested with acidic potassium permanganate solution

C. PR BRAT : UTE BOTA Ht BER I FM 2 HE ARPES TS

CC. BESS IDR TEBE SR EP SER BOE AT EAR BEI,                             REPRE TG UM 2 RA EB AE AK?
C. Aldehydes can undergo silver mirror reaction when co-heated with silver               C. Talk to him after class: Do you know why we are here? Do you think such
ammonia solution under alkaline conditions

behavior is worthy of the martyrs? How do you want others to remember you?
Di BBP TAYE OL FT YEE PE ASP A ASF A I FR UN                      D. ib fle FURS OOUIN FEEDER A, GET A — fit Pe SEE.

D. Sodium carbonate solution dissolves phenol to produce sodium phenol
and sodium bicarbonate

BR: B

Answer: B

D. Let him serve as a docent on his next visit, responsible for introducing the
deeds of a martyr.

(a) Single-choice question type within the knowledge dimension             (b) Single-choice question type within the cultivation dimension

Figure 3: Example of (a) a single-choice question in the knowledge from a college chemist. (b) A
single-choice question in the cultivation. English translations are shown for better readability.

5


===== PAGE BREAK =====

Preprint

AT pn ERTL AR > AHR RAB ES 2 Be)          CMA: y? =4xK RAP, IPERS kA Ee 36 PP
Which of the following receptors are present on the BAT (brown adipose       A, BRR. §=(1) 4lAB|=10        g            HAA Br s

tissue) cell membrane? (Multiple Choice)                                         WE: 7EdWwWeART EPPA AAA      il       ~ SPAHR ie)

AL Ui BSE 8                                                                    It is known that the focus of the parabolic [: y?=4x is F, and the straight
A. Glucose receptors                                                                line with a slope of k crosses F [ at points A and B. (1) If | AB|=10, find
B. HL 36 Jia Se As                                                                  the abscissa of the midpoint of the line segment AB. (2) Prove: Among
B. Neurotransmitter receptors                                                         all the points on the parabolic I, the distance from the vertex to the

C. Pe HEL es                                                                focal point F is the smallest.

C. Secretin-stimulating receptors

FR RUS 52

D. Thyroid hormone receptors                                                   BR: (1) ABP BAA AA, (2) TA        (SE ie);
WEI. BAC                                                                             Answer: (1) The abscissa of the midpoint of AB is 4.       he distance

,                                                                                  from the vertex to the focal point is the smallest.

Answer: B; C

(a) Multiple-choice question within the knowledge dimension —_(b) Short-answer question within the knowledge dimension

Figure 4: Example of (a) a multiple-choice question in the knowledge from Biology. (b) A short-
answer question in the knowledge from Math. English translations are shown for better readability.

Table 3: Zero-shot average accuracy (%) across six categories in the knowledge. The highest accu-

racy is bold, and the second highest is underlined. More results are provided in the Appendix.
Model                  Parameters       Access       Creator       FD      HH SSEM_~ LP     MH     IIS | Average
Qwen3                       8B            Weights      Alibaba      53.02 38.53 36.58 30.17 36.71 37.75 | 43.86
Qwen3                      14B            Weights      Alibaba      36.32 36.78 35.12 27.29 36.82 35.67 | 35.62
MuduoLLM                   14B             Weights BNU & TAL | 28.20 40.82 32.99 36.15 39.11 3140) 33.68
QwQ                        32B            Weights      Alibaba      61.25 48.51 42.24 49.90 55.01 47.26 | 53.87
Seed-OSS                   36B            Weights ByteDance | 48.81 50.14 45.34 48.66 61.00 49.56 | 49.53
Qwen2.5                    72B            Weights      Alibaba       19.53. 30.95 20.57 13.26 23.86 20.90 | 22.76
Qwen3              235B (22B active) Weights      Alibaba      34.24 47.01 36.21 44.26 58.71 46.61 | 40.82
DeepSeek-V3.1  671B (37B active) Weights     DeepSeek     31.65 40.65 35.00 29.42 50.54 45.19     36.05
GPT-40                 Undisclosed         API         OpenAI      21.15 26.94 23.92 22.13 34.75 27.13 | 24.17
Claude-4 Sonnet        Undisclosed          API        Anthropic      41.49 44.29 35.36 27.56 34.86 42.34      40.35
Gemini-2.5 Pro         Undisclosed          API          Google        73.83 55.13 46.68 55.40 60.68 54.16 | 62.76

instruction-following models (Qwen2.5 (Yang et al.||2024), Qwen3 (
eral reasoning models (QwQ 2025c), Seed-OSS (Team}|2025b), DeepSeek-V3.1
2024b)); and (c) education-specific models (MuduoLLM (from BNU & TAL}|2025)).

Implementation details. In our experimental setup, we evaluate all large language models under
both zero-shot and few-shot settings, with few-shot examples (0-, 1-, 3-, and 5-shot) drawn from
a separately partitioned development set, distinct from the evaluation set. All open-source models
are run using their official code, while closed-source models are accessed via official APIs. We
consistently use Gemini-2.5 Pro as the LLM-assisted scoring model, unless otherwise specified.

 2025)); (b) gen-

3.2. MAIN RESULTS

We evaluated all baseline models on OmniEduBench, reporting both per-task category and overall
accuracy, as shown in Tables 3]and (4). Results show that in the knowledge dimension, Gemini-2.5
Pro achieves the highest accuracy at 62.78%, while in the cultivation dimension, the reasoning-
enhanced version of QWQ performs best with an accuracy of 70.27%. This performance highlights
the challenging nature and strong discriminative power of the constructed OmniEduBench.

In the knowledge dimension, it is evident that, except for Gemini-2.5 Pro, closed-source models
generally perform worse than open-source models on our OmniEduBench. For example, GPT-40
achieves an accuracy of 24.17%, far below that of Qwen3-8B. This may indicate that the GPT series
has relatively weak robustness when handling Chinese education exam-style questions. Meanwhile,
model architecture has a significant impact on performance, such as Seed-OOS outperforms the
Qwen family by more than 10%. In the cultivation dimension, models generally perform better than
in the knowledge dimension, which may be due to the fact that the cultivation tasks mainly consist
of multiple-choice questions, making them simpler compared to knowledge tasks with 11 common
exam question types. However, differences in performance between different model architectures
still exist. Overall, GPT-40 performs the worst in both dimensions, with accuracy largely concen-
trated around 59.57%, possibly because it has not been specifically optimized for this dimension.


===== PAGE BREAK =====

Preprint

Table 4: Zero-shot average accuracy (%) across six categories in the cultivation. The highest accu-

racy is bold, and the second highest is underlined. More results are provided in the Appendix.
Model                  Parameters       Access       Creator       TCS EMH SIS      CV      PD     TFS | Average
Qwen3                                    8B                   Weights          Alibaba          70.95 66.67 69.16 62.25 70.13 77.20 | 68.62
Qwen3                                   14B                  Weights          Alibaba          67.79 60.77 63.72 56.20 64.31 71.50] 63.60
MuduoLLM                 14B            Weights BNU& TAL | 64.42 60.77 63.45 66.14 67.51 64.77 | 63.96
QwQ                        32B            Weights      Alibaba      73.16 68.36 69.84 65.13 71.77 72.02 | 70.27
Seed-OSS                   36B            Weights ByteDance | 70.74 65.30 66.03 62.82 67.12 70.47 | 67.18
Qwen2.5                               72B                  Weights          Alibaba          67.89 64.38 65.62 59.51 65.57 67.88 | 65.34
Qwen3              235B (22B active) Weights      Alibaba      67.84 61.10 64.54 55.76 64.40 70.47 | 63.74
DeepSeek-V3.1 671B (37B active) Weights     DeepSeek     71.58 65.41 69.02 61.96 71.00 77.20     68.55
GPT-40                Undisclosed         API         OpenAl      61.63 59.57 59.24 55.33 57.71 65.80 | 59.57
Claude-4-sonnet           Undisclosed              API            Anthropic        71.95 70.05 70.92 64.55 69.25 71.50        70.03
Gemini-2.5-pro             Undisclosed              API              Google           72.26 66.07 70.79 65.71 70.32 67.36        69.14

Table 5: Average accuracy (%) across six categories in one-shot, three-shot, and five-shot settings
for the knowledge dimension. The highest accuracy is bold, and the second highest is underlined.

Model                    Parameters         Access       Creator        FD      HH SSEM_ LP      MH      IiS | Average
One-shot setting

Qwen3                                    8B                    Weights          Alibaba          52.80 4645 41.90 29.76 40.20 40.59        41.95

MuduoLLM                   14B              Weights BNU & TAL | 27.36 47.79 36.72 34.98 40.74 34.35      36.99

Qwen?2.5                               72B                   Weights          Alibaba          21.42 4043 27.04 20.96 28.10 22.65        26.77

Qwen3             235B (22B activate) Weights      Alibaba      37.72 60.79 44.03 45.77 59.59 54.05     50.12

DeepSeek-V3.1 671B (37B activate) Weights | DeepSeek 30.00 41.73 34.65 30.72 49.67 42.12 38.15

Three-shot setting

Qwen3                                    8B                    Weights          Alibaba          52.98 46.00 39.74 30.65 39.32 40.85        41.59
MuduoLLM                   14B              Weights BNU& TAL | 27.32 46.86 35.79 33.81 39.54 32.42      35.96
Qwen?2.5                               72B                   Weights          Alibaba          21.43 40.86 27.27 20.41 27.12 23.88        26.83
Qwen3             235B (22B activate) Weights      Alibaba      37.52 60.70 43.40 45.77 59.48 52.57     49.54

DeepSeek-V3.1 671B (37B activate) Weights | DeepSeek 29.09 41.42 34.02 28.59 48.80 42.06 37.33

Five-shot setting

Qwen3                                    8B                    Weights          Alibaba          56.86 46.70 39.44 30.65 38.24 42.23        42.35
MuduoLLM                   14B              Weights BNU& TAL | 26.93 46.57 36.28 35.74 39.11 34.57      36.53
Qwen2.5                      72B              Weights       Alibaba       21.46 41.19 26.96 20.82 26.03 28.56      27.50
Qwen3             235B (22B activate) Weights      Alibaba      37.40 60.41 44.13 45.77 58.61 55.58     50.32

DeepSeek-V3.1 671B (37B activate) Weights | DeepSeek 29.39 41.17 32.87 28.45 47.49 38.95 36.39

3.3. ANALYSIS AND FINDINGS

In this section, we further conduct extensive experiments at multiple levels, including few-shot ex-
amples, OmniEduBench HARD, and various LLM-assisted scoring methods.

Results in few-shot examples. In Table[5| we present in-context experimental results using differ-
ent numbers of shots. As the number of shots increases, model performance generally improves;
however, the overall gain is limited when considering the average results. We speculate that the drop
in accuracy for some models is due to the fact that they have not (or not appropriately) incorporated
few-shot examples during the instruction tuning stage. These findings suggest that while few-shot
prompting can be beneficial for certain models, its effectiveness strongly depends on the model’s
pretraining and instruction tuning strategies. Moreover, the limited average improvement indicates
that simply increasing the number of shots may not always lead to substantial gains, highlighting
the need for more sophisticated methods to integrate few-shot examples effectively.

Results on OmniEduBench HARD. In Figures [5] and (6| we present the average accuracy of each
model on OmniEduBench HARD. OmniEduBench HARD is a subset of OmniEduBench, consisting
of the bottom 26% of samples based on model performance, including approximately 1.552K culti-
vation samples and 7.620K knowledge samples, for a total of 9.172K examples. The experimental
results show that: (1) all 11 LLMs exhibit a significant performance drop on OmniEduBench HARD,
with even the best-performing model, Gemini, achieving less than 50% accuracy; (2) Qwen2.5-72
performs the worst, significantly lower than the other models, indicating limited capability in han-
dling difficult samples. These findings indicate that further research is needed to enhance LLMs’
ability to generalize and maintain high performance on hard subsets of educational benchmarks.

Results using different LLM-assisted scoring methods. In Table (6| we present the experimen-
tal results using different LLM-assisted scoring methods. The performance of the scoring model


===== PAGE BREAK =====

Preprint

0 FD 2 LP SHH 6 MH OC SSEM so 777 IIS): Average

401

mnt rll il  |

qwen?- 5-128 gpt4e  gwen” Aue   chande* sonnet nti   on    ate 8B g5-ADB        ae                      pe

93-23"           Qwa        eed oss se ani? 2.5"

Fieure 5: Zero-shot average accuracy (%) on the knowledge dimension of OmniEduBench HARD.

[2 tes @@ EMH Ma SIS) Mm cv [1 PD (47) TFS GG Average

wv

°o

5- 1728                                                                                         iM

indo"                    S38

gwen” ASB   oa -A0 N03 ae B sce?

4-8B    328   _a.sonnet   spied SPO

que                      eed OPP WERT QW Garde # Geri

Figure 6: Zero-shot average accuracy (%) on the cultivation dimension of OmniEduBench HARD.

directly affects the evaluation outcomes: higher-quality scoring models provide more accurate as-
sessments, leading to more precise measurements of the evaluated models’ capabilities. In this
study, we employed three scoring models of varying quality. Overall, GPT-40 performed relatively
poorly as a scoring model, failing to accurately evaluate the responses of LLMs. Consequently,
the overall effectiveness of LLM-assisted evaluation is reduced when GPT-40 is used, highlighting
the critical importance of selecting high-quality scoring models to ensure accurate and meaningful
assessments. These findings suggest that the choice of scoring model can substantially influence the
perceived performance of evaluated LLMs, and careful selection of scoring models is necessary.

4 RELATED WORK

In this section, we present a comprehensive survey of large language models (LLMs) and bench-
marks related to our constructed OmniEduBench, encompassing both English and Chinese datasets.

4.1 LARGE LANGUAGE MODELS

Recently large language models have advanced at an unprecedented pace. Leveraging increasingly
sophisticated architectures and ever-larger pretraining corpora, they have continuously pushed the
boundaries of performance in language understanding, reasoning, and generation tasks. Researchers
have explored various approaches to enhance LLMs’ capabilities. For example, Chain-of-Thought
prompting has been shown tobe
highly effective in guiding models to perform step-by-step oa for comple OE BOS a
In addition, instruction tuning (Dong et al.| {2025}  I.| {2024} [2025p  and
reinforcement learning from human feedback (RLHF) (   2017

have been widely adopted to align model outputs with human aS = preferences, am


===== PAGE BREAK =====

Preprint

Table 6: Zero-shot average accuracy (%) across six categories in the knowledge using different
LLM-assisted scoring methods. The highest accuracy is bold, and the second highest is underlined.

Model                            Parameters Access           Creator            FD           HH SSEM_ LP           MH           IIS | Average
Qwen3-A235B-assisted scoring method 2025
QwQ                32B      Weights    Alibaba    61.26   pg po dit0   55.66 48.36    56.39
Seed-OSS            36B      Weights ByteDance | 51.16 63.04 51.55 50.38 62.31 57.33    55.49
GPT-4o0          Undisclosed    API      OpenAI    23.59 36.77 28.79 23.92 35.73 31.40    28.96
Claude-4 Sonnet Undisclosed    API     Anthropic | 43.54 55.52 40.47 27.77 36.17 47.48    45.34
Gemini-2.5 Pro —_ Undisclosed          API              Google          75.01 65.67 52.95 56.29 61.44 61.49 | 67.41
Gemini-2.5 Pro-assisted scoring method
QwQ                32B      Weights    Alibaba    61.25 48:      AZ24-— 49-90 55.01 47.26    53.87
Seed-OSS            36B      Weights ByteDance | 48.81 50.14 45.34 48.66 61.00 49.56 | 49.53
GPT-40          Undisclosed    API      OpenAI    21.15 26.94 23.92 22.13 34.75 27.13    24.17
Claude-4 Sonnet Undisclosed    API     Anthropic | 41.49 44.29 35.36 27.56 34.86 42.34    40.35
Gemini-2.5 Pro —_ Undisclosed        API           Google        73.83 55.13 46.68 55.40 60.68 54.16 | 62.76
GPT-40-assisted scoring method  (Hurst et al.|
QwQ                32B      Weights    Alibaba    56.61 428       86 49.28 54.58 37.97    49.26
Seed-OSS            36B      Weights ByteDance | 45.07 47.89 41.94 4852 60.78 43.54] 46.61
GPT-4o0          Undisclosed    API      OpenAI    20.38 23.78 22.03 22.06 34.31 23.30    22.51
Claude-4 Sonnet Undisclosed    API     Anthropic | 40.43 41.70 31.89 27.90 35.08 34.79    38.48
Gemini-2.5 Pro —_ Undisclosed        API           Google        70.15 51.38 44.13 55.88 60.57 46.83 | 59.49

LLMs to generate responses that are more natural and reliable in open-ended dialogue and creative
tasks. Despite these remarkable advances, however, the question of how to comprehensively and
effectively evaluate the true capabilities of LLMs remains a critical and open challenge.

4.2 ENGLISH EDUCATION BENCHMARKS

Researchers have proposed a variety of benchmarks to evaluate the capabilities of LLMs, which
can be broadly categorized into three types: (1) task-specific evaluations, such as reading com-

prehension (SQUAD (Rajpurkar et al.||2016)), machine translation (Bojar et al.|/2014)), and sum-
(Hermann et al. 2013):

marization                       (2) general knowledge and advanced ability evaluations, for
example, the Massive Multitask Language Understanding (MMLU) benchmark
2021), which collects questions from real-world exams and textbooks to provide a diverse, multi-
domain test that effectively probes the breadth and depth of model knowledge. Similarly, the BIG-

bench benchmark comprises 204 diverse tasks; and (3) specialized ability
evaluations. In mathematical reasoning, benchmarks such as GSM8K and
MATH 2021) assess models’ ability to solve complex multi-step problems. In
code generation, HumanEval (Chen et al. and MBPP 2021) have become
standard benchmarks for measuring programming proficiency. Additionally, datasets such as MT-
bench have been introduced to evaluate performance in multi-turn, open-ended
dialogues. Despite the significant contributions of these datasets to advancing LLMs evaluation,
most of them remain heavily focused on English, with limited coverage of Chinese scenarios.

4.3 CHINESE EDUCATION BENCHMARKS

A series of comprehensive Chinese benchmarks have been proposed. For example, CLUE
[2020b}, as an early work, integrates multiple natural language understanding tasks and has become
an important reference for evaluating LLMs. Subsequently, benchmarks such as CMMLU
2023) and C-Eval collect multi-disciplinary, multi-task questions from Chi-
nese university exams, professional qualification tests, and textbooks, effectively assessing models’
general knowledge and their understanding. Beyond general capability evaluation, researchers have
also developed Chinese benchmarks targeting specific advanced skills. For example, in mathemat-
ical reasoning, CMATH Fearon models’ abilities to solve complex mathematical
problems. Meanwhile, EduBench (Xu et al.| constructs synthetic corpora for the education,
but its question types are relatively limited, making it difficult to fully capture models’ Chinese
potential. To address this critical gap, we propose OmniEduBench — a comprehensive Chinese ed-
ucation benchmark that uniquely combines knowledge and nurturing dimensions, providing a novel,
holistic framework for systematically evaluating LLMs’ potential as educational assistants.


===== PAGE BREAK =====

Preprint

5 CONCLUSIONS, DISCUSSIONS AND LIMITATIONS

In this paper, we present OmniEduBench, a comprehensive Chinese educational benchmark de-
signed to address the limitations of existing Chinese educational evaluation benchmarks. By mov-
ing beyond simple knowledge retrieval, the benchmark provides a holistic assessment of LLMs’
capabilities across two core dimensions: the knowledge and cultivation dimensions. We conducted
extensive experiments on 11 mainstream LLMs, revealing significant performance gaps. While
some models performed well on the knowledge dimension, their performance on cultivation tasks
dropped substantially, with even the best-performing models trailing human-level performance by
nearly 30%. These findings indicate that despite recent advancements in LLM technology, cur-
rent models still lack the deep reasoning and pedagogical skills necessary to function effectively
as educational assistants. We believe OmniEduBench will serve as an important tool for guiding
future research. Looking ahead, OmniEduBench plans to explore more complex question types in
the cultivation dimension and introduce multimodal educational scenarios, further enhancing the
benchmark’s role in evaluating and guiding the comprehensive capabilities of LLMs and MLLMs.

Ethics statement. Our constructed OmniEduBench educational benchmark is built from publicly
available educational resources as well as authorized private resources permitting open-source use,
strictly adhering to copyright and licensing requirements. All data have been systematically pro-
cessed to remove personally identifiable information (PII) and sensitive content, ensuring privacy
and security. The dataset is intended solely for research purposes, aiming to advance the develop-
ment and evaluation of large language models (LLMs) in educational scenarios.

Reprodicibility statement. To ensure reproducibility, we provide detailed descriptions of the
dataset construction process, annotation criteria, and experimental settings in both the main paper
and the Appendix. The proposed OmniEduBench education dataset, together with preprocessing
scripts, evaluation metrics, and model prompts, will be publicly released upon acceptance. All
experiments were conducted using standard LLM APIs or open-source checkpoints, with model
versions, hyperparameters, and evaluation protocols explicitly documented. This ensures that other
researchers can faithfully replicate our results and readily extend the benchmark in future studies.

REFERENCES

Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,
Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language
models. arXiv preprint arXiv:2108.07732, 2021.

Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su,
Tiezheng Ge, Bo Zheng, et al. Mt-bench-101: A fine-grained benchmark for evaluating large
language models in multi-turn dialogues. arXiv preprint arXiv:2402.14762, 2024.

Ondyej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Lev-
eling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia,
and Ale§ Tamchyna. Findings of the 2014 workshop on statistical machine translation. In Pro-
ceedings of the Ninth Workshop on Statistical Machine Translation, pp. 12-58, 2014.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared
Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large
language models trained on code. arXiv preprint arXiv:2107.03374, 2021.

Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and
Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge.
arXiv: 1803.05457v1, 2018.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John
Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168,
2021.

Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit
Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the

10


===== PAGE BREAK =====

Preprint

frontier with advanced reasoning, multimodality, long context, and next generation agentic capa-
bilities. arXiv preprint arXiv:2507.06261, 2025.

Xinpeng Dong, Min Zhang, Didi Zhu, Ye Jun Jian, Zhang Keli, Aimin Zhou, Fei Wu, and Kun
Kuang. Erict: Enhancing robustness by identifying concept tokens in zero-shot vision language
models. In Forty-second International Conference on Machine Learning, 2025.

MuduoLLM Contributors from BNU and TAL. Muduollm: A high-performance Ilm for intelligent
education solutions. https: //huggingface.co/ERC-ITEA/MuduoLLM, 2025.

Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu
Zhang, Shirong Ma, Xiao Bi, et al. Deepseek-r1 incentivizes reasoning in Ilms through reinforce-
ment learning. Nature, 645(8081):633-638, 2025.

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Ja-
cob Steinhardt. Measuring massive multitask language understanding. In International Confer-

ence on Learning Representations, 2021. URL|https://openreview.net/forum?id=
d7KB jmI3GmO

Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa
Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. Advances in neural
information processing systems, 28, 2015.

Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Train-
ing compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.

Hanglei Hu, Yingying Guo, Zhikang Chen, Sen Cui, Fei Wu, Kun Kuang, Min Zhang, and Bo Jiang.
Advancing personalized learning with neural collapse for long-tail challenge. In Forty-second
International Conference on Machine Learning, 2025.

Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu,
Chuancheng Ly, Yikai Zhang, Yao Fu, et al. C-eval: A multi-level multi-discipline chinese eval-
uation suite for foundation models. Advances in Neural Information Processing Systems, 36:
62991-63010, 2023.

Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Os-
trow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-40 system card. arXiv preprint
arXiv:2410.21276, 2024.

Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy
Baldwin. Cmmlu: Measuring massive multitask language understanding in chinese, 2023.

Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong
Ruan, Damai Dai, Daya Guo, et al. Deepseek-v2: A strong, economical, and efficient mixture-
of-experts language model. arXiv preprint arXiv:2405.04434, 2024a.

Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao,
Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint
arXiv:2412.19437, 2024b.

OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to fol-
low instructions with human feedback. Advances in neural information processing systems, 35:

27730-27744, 2022.

Bhrij Patel, Souradip Chakraborty, Wesley A Suttle, Mengdi Wang, Amrit Singh Bedi, and Di-
nesh Manocha. Aime: Ai system optimization via multiple llm evaluators. arXiv preprint
arXiv:2410.03131, 2024.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions
for machine comprehension of text. arXiv preprint arXiv: 1606.05250, 2016.

11


===== PAGE BREAK =====

Preprint

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv: 1707.06347, 2017.

Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam
Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adria Garriga-Alonso, et al. Beyond the
imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint
arXiv:2206.04615, 2022.

Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, and Kai
Yu. Scieval: A multi-level large language model evaluation benchmark for scientific research.
In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 19053-19061,
2024.

Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia,
Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for
science. arXiv preprint arXiv:2211.09085, 2022.

Anthropic Team. Claude-4 sonnet, 2025a. URL https://www.anthropic.com/news/
 [Online; accessed 23-May-2025].

ByteDance Seed Team.       Seed-oss open-source models.       https://github.com/
 2025b.

Qwen Team. Qwgq-32b: Embracing the power of reinforcement learning, March 2025c. URL
https://qwenlm.github.io/blog/qwq-32b/

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée
Lacroix, Baptiste Roziére, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and
efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.
GLUE: A multi-task benchmark and analysis platform for natural language understanding. In
International Conference on Learning Representations, ICLR, 2019.

Bin Wang, Chao Xu, Xiaomeng Zhao, Linke Ouyang, Fan Wu, Zhiyuan Zhao, Rui Xu, Kaiwen Liu,
Yuan Qu, Fukai Shang, et al. Mineru: An open-source solution for precise document content
extraction. arXiv preprint arXiv:2409. 18839, 2024.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny
Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in
neural information processing systems, 35:24824—24837, 2022.

Tianwen Wei, Jian Luan, Wei Liu, Shuang Dong, and Bin Wang. Cmath: Can your language model
pass chinese elementary school math test?, 2023.

Bin Xu, Yu Bai, Huashan Sun, Yiguan Lin, Siming Liu, Xinyue Liang, Yaolin Li, Yang Gao, and
Heyan Huang. Edubench: A comprehensive benchmarking dataset for evaluating large language
models in diverse educational scenarios. arXiv preprint arXiv:2505.16160, 2025.

Liang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao, Yudong Li, Yechen Xu, Kai Sun, Dian
Yu, Cong Yu, Yin Tian, Qianqian Dong, Weitang Liu, Bo Shi, Yiming Cui, Junyi Li, Jun Zeng,
Rongzhao Wang, Weijian Xie, Yanting Li, Yina Patterson, Zuoyu Tian, Yiwen Zhang, He Zhou,
Shaoweihua Liu, Zhe Zhao, Qipeng Zhao, Cong Yue, Xinrui Zhang, Zhengliang Yang, Kyle
Richardson, and Zhenzhong Lan. CLUE: A Chinese language understanding evaluation bench-
mark. In Proceedings of the 28th International Conference on Computational Linguistics, pp.
4762-4772. International Committee on Computational Linguistics, December 2020a.

Liang Xu, Xuanwei Zhang, Lu Li, Hai Hu, Chenjie Cao, Weitang Liu, Junyi Li, Yudong Li, Kai
Sun, Yechen Xu, Yiming Cui, Cong Yu, Qianqian Dong, Yin Tian, Dian Yu, Bo Shi, Jun Zeng,
Rongzhao Wang, Weijian Xie, Yanting Li, Yina Patterson, Zuoyu Tian, Yiwen Zhang, He Zhou,
Shaoweihua Liu, Qipeng Zhao, Cong Yue, Xinrui Zhang, Zhengliang Yang, and Zhenzhong Lan.
Clue: A chinese language understanding evaluation benchmark, 2020b.

12


===== PAGE BREAK =====

Preprint

Liang Xu, Anqi Li, Lei Zhu, Hang Xue, Changtai Zhu, Kangkang Zhao, Haonan He, Xuanwei
Zhang, Qiyue Kang, and Zhenzhong Lan. Superclue: A comprehensive chinese large language
model benchmark. arXiv preprint arXiv:2307.15020, 2023.

An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li,
Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin
Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang,
Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang,
Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan,
Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen?2.5 technical report. arXiv preprint
arXiv:2412.15115, 2024.

An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,
Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint
arXiv:2505.09388, 2025.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,
Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging Ilm-as-a-judge with mt-bench and
chatbot arena. Advances in neural information processing systems, 36:46595—46623, 2023.

Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied,
Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation
models. arXiv preprint arXiv:2304.06364, 2023.

A SUPPLEMENTARY MATERIAL

Use of LLMs. In this paper, LLMs were utilized in two primary ways: (1) as auxiliary tools for
data cleaning and preliminary quality checks under human supervision. (2) As evaluation targets
in benchmark experiments. To ensure data quality, no content directly generated by LLMs was
included in the released dataset. During manuscript preparation, LLMs were employed for minor
language polishing. All ideas, methodologies, and conclusions are original contributions of authors.

A.1 SUPPLEMENTARY STATISTICS OF OMNIEDUBENCH

In Table[7] we present the bilingual names and abbreviations of all subjects in the knowledge dimen-
sion. In Table [8] we present the bilingual names and abbreviations of all subjects in the cultivation
dimension. In Tables)9| and{I1] we present the detailed data distribution for all 61 subjects.

In Figures|8]and{7| more examples of various questions in the knowledge and cultivation dimensions.

13


===== PAGE BREAK =====

Preprint

Table 7: Bilingual names and abbreviations of all subject in the knowledge dimension.

Abbreviation English Name

MATH
CHEM
BIO
PHY
NSCI
PSTAT
PPHY
CS
BCHEM
OS
AMATH
CNET
LANG
GEO
HIST
IART
ILING
HSTUD
HFA
IARCH
HACL
HWP
POL
IMOR
MGMT
HRM
TAX
PSCI
MARX
ELOG
NJE
CLAW
CVLAW
LAW
TCM
WMED
NURS
IT
CEM
EDU
PSY

Mathematics

Chemistry

Biology

Physics

Nature & Science

Probability & Statistics

Plant Physiology

Computer Science
Biochemistry

Operating Systems

Advanced Mathematics
Computer Networks

Chinese Language
Geography

History

Introduction to Arts
Introduction to Linguistics
History Studies / Historiography
History of Foreign Art
Introduction to Archaeology
History of Ancient Chinese Literature
History of Western Philosophy
Politics

Ideology & Morality
Management

Human Resource Management
Taxation

Political Science

Marxist Theory

Economic Logic

National Judicial Exam
Criminal Law

Civil Law

Law / Jurisprudence
Traditional Chinese Medicine
Western Medicine

Nursing

Information Technology
Civil Engineering Materials
Education

Psychology

14

Chinese Name
My

WF

Dy

BBE

BENS

in A FI
ThE
SUIS
RIERA
fey eae
ThE BL PAZ
aM

Hose
ee
ZARIC
aa Sihie
Sh FB] SE 7K Tay BE
75 I
PT RC Ae
Poa
BY
‘a HEA

NTI GUNES
Bune

BURSA aan

Lai oo cr

a

EARLE BY
AAS
HE



===== PAGE BREAK =====

Preprint

Table 8: Bilingual names and abbreviations of all subject in the cultivation dimension

Chinese Name

Abbreviation English Meaning                                    i

Major Categories

TCS               Thinking & Cognitive Skills          FEES ABE
EMH              Emotional & Mental Health           RS DEER
SIS                 Social & Interpersonal Skills         tA SAME
CV                   Character & Values                        ah 5 (Ma

PD                    Personalized Development                  EM

TFS                 Teaching Feedback & Support        Bee Li 5M
Subcategories

IC                       Innovation & Creativity                    Op Ss aie 7

PSS                Problem-Solving Skills                 [F] eA IR AE

CT               Critical Thinking                     SALE AE

GRL                   Guided Reflective Learning               BPE AZ A) S|
MA                 Metacognitive Abilities                 FEU RNA TI

GKT                Guiding Knowledge Transfer         5| SANA IL Ss eT]
ER                   Emotional Regulation                   TA VAPE HET

EC                 Empathy & Compassion               A] UD SRR
SCSE             Self-Confidence & Self-Efficacy Hfab5 BRR HR
PR                       Psychological Resilience                   EH) | tH Sie
GM                 Growth Mindset                          Dg eiey ois

TC                   Teamwork & Collaboration            AB PM FETT
ECOM              Effective Communication               pees eZ]

SR            Social Responsibility             ALF it EK

RA                    Responsibility & Accountability  sifPR5fo4

IH                 Integrity & Honesty                   EBS wts

PLP                  Personalized Learning Paths           “MEL >) BRIE
IDL               Interest-Driven Learning              PERAK BN AF >]

HT                       Heuristic Teaching                              ALTA

CTF                Constructive & Timely Feedback Jrimiygixtt 5 RAY PE

Table 9: Statistics of OmniEduBench for K-12 in the knowledge dimension.

.                    .                   ERR Siew JAS PS S4R        Rit
English Name Chinese Name    Multiple Multiple — Fill-in-       Short | Composite
choice      answer the-blank -answer questions       Total
Chinese               iB                       350           8           1697         1261           51            3367
Mathematics            Be                          527               12              1865            1181              142              3727
Chemistry             (as                    274           716           799          477            14            1640
History                     Hy Be                           67               24                63              211                 5                 370
Geography           Ht Fe                   78           31           277         173           4            563
Moral Education 48 4h                14            30             34             56              4              138
Politics               BA                    260          241           281           64             12            858
Physics              pei                  82          15          178         46           16          337
Biology                    AE                               115              94               360              124                0                 693
Nature Science           IRSA               8                0               23              22                0                53
Technology           SEBO               18            2             14             1              1              36
Total                         iit                          1793            533             5591            3616             249           11.782K



===== PAGE BREAK =====

16

Table 10: Statistics of OmniEduBench for high, college, and professional schools in the knowledge dimension
Rael
Fill-in-
blank

English Name

Chinese Name

Lari al
Single
Choice

Eni
Multiple
choice

el RE
Term
explanation

Short
answer

Weal

Essay

RADA
Case
analysis

Fal

Calculation

7 ES

True/
False

Logical
reasoning

Traditional Chinese Medicine
Chinese Ancient Literary History
Human Resource Management
Jurisprudence

Criminal Law

History

Civil Engineering Materials

A Brief History of Foreign Art
Psychology

Nursing

Operating Systems

Politics

Pedagogy

Advanced Mathematics

Plant Physiology

Probability and Mathematical Statistics
Civil Law

judicial Practice

Biochemistry

Taxation

Management

Economics

Archaeology

Introduction to Archaeology
Western Medicine

Western Philosophy

Computer Science

Computer Networks
Introduction to Linguistics
Marxist Theory

CAE
1 OU

ian
X
Pra
i Baa

“LA LAMB
Sb ELSE 7K Tal SE
HE
PRs
PRET
Bue
AAS

Lieber

KI I-R

ap IM

tk

es

MRC SRT

FRIAS

me ese
Bue
us
PRE RAED
Bee
SABIE
GaSe
tt ALS
aa eitie
Set He

230
31
36
109
158
26
36
0
146
61
98
7
179
43
66
0
91
193
62

N

57
159
51
137
91
53

317
14
35

153

171

0
20
9

So

ooooocoococooconreo

So

121

i=)
Sr

24
20

0
27
38

0

3

6
21
37
35

5

0

0
45

0
134

2

0

0
70
30

0

0

0
45

0

0

0

0
22
31

0
19
19
0
1
17
57
15
0
0
0
0
39
3
33
0
0
0
23
0
0
0

26

ooo

26
13

0

owuoe

Aa

ooonooocooco

00
GRSe

So
+

io
ootroococoococqco
=

ecoooood

oooocoeo

257

nN
coo

ooooocoocoococoeo

cooocoomooooOoOtToocococS

4
—

ooo

So

ooooococoocoocoeoocqocoeoqocco

202

ove)
va)

ooooooc°eo

547
111
147
262
335
129
334
127
227
123
157
90
277
141
281
333
278
623
155
94
202
60
121
288
278
51
184
141
172
71

Total

iit

2141

1635

479

551

291

392

—

ooocooocoocoocoeoocooeoooqoco|»

374

260

6339

Preprint


===== PAGE BREAK =====

Preprint

Table 11: Statistics of OmniEduBench for 20 subjects in the cultivation dimension.

English Name                                    Chinese Name           Count

Emotional Regulation Skills                                             aA)                         325
Innovation & Creativity                                         OTS Be                    275
Heuristic Teaching                                                 RAAF                       434
Sense of Responsibility & Accountability                  mR i 4                    330
Problem-Solving Skills                                               a ea RRA He                         288
Team Collaboration Skills                                                 AR HIM ERE                             291
Empathy & Sympathy                                                     AFD SRT                        385
Self-Confidence & Self-Efficacy                                J fabS ARHER          358
Constructiveness & Timeliness of Feedback              Be A ES A PE      196
Integrity & Honesty                                                                                    371
Psychological Resilience & Anti-Frustration Ability                                        393
Personalized Learning Paths                                                                                  292
Reflective Learning                                                                                               224
Guiding Knowledge Transfer                                                                          384
Metacognitive Skills                                                                                338
Interest-Driven Learning                                                                                       320
Critical Thinking                                                                                          428
Growth Mindset                                                                                                   393
Social Responsibility                                                                                                                                           317
Effective Communication Skills                                                                     139

Total                                                                                                                                        6.481K

Sie + ARR ZLM ETA, See VET ete.   Bi Atha AS MEAT. MAT
SF? THER REAL BBD SI Al ela HE
ARAREARNA, HRERBEPRC.. BMY Soph aonae    SERA, mie RTM.
B. FARRER EILS, RETR IES MARTA, PRET.

C. RIE RRERRAN, (MITRE, ISIN SOLER.

D.RRHMZAGALR, MBAS TMK.

A student in my class exhibited aggressive behavior due to domestic violence, and other teachers recommended expelling him. |
think he needs help rather than punishment. How to convince a colleague? Please choose the answer that best reflects ‘empathy
and empathy’ and ‘problem-solving skills': A. This child is not bad by nature, he just uses aggression to protect himself. We should
analyze the reasons behind his behavior and develop a support plan.

B. Expelling him will only make him hate society more, so why not do a behavioral assessment with a psychologist and then
decide on an intervention plan.

C. Anyway, he is not my biological son, you decide, don't blame me for not reminding me when something happens.

D. | think he should go to a special school, ordinary classes can't control this kind of child at all.

Question within the cultivation dimension

RE AANA, IME, the  ag TAR. BAARILRARUFA REAR? TR
pee     BERYL Sw A A AEA AE TT IIS:
AUS S 7) Lia, FRETS.
BARES, WRICMEE PR.
C. RCRA AEB CREE, LMA KARR RMA CHNAKEA, KERMA
DK-BRBRNRTR, BARA Aisi, Le Nae
| always feel bored when reciting ancient poems, | can't remember them, and | don't know what they are used for. How can we
make traditional culture learning meaningful? Please choose the answer that best reflects "interest-driven learning" and
"knowledge transfer ability":

A. You have to copy it several times, practice makes perfect.

B. To take the exam, you must memorize it.

C. Try to use these verses in your diary or composition, such as ‘There will be times when the wind and waves will be long’ to
encourage yourself to face the pressure of the exam, and you will find that it has practical power.

D. Find a pop song you like, see if there are lyrics quoted from ancient poems, and compare the artistic conception.

Question within the cultivation dimension
Figure 7: Examples of different questions in the knowledge dimension and cultivation dimensions.

17


===== PAGE BREAK =====

Preprint

ARE: AmEIK

Definition:Dasheng School of Ci Poetry:

BR: WEARS, WARM RAAEB Mh Este, ILRI MUSA eae
YZ, (eT SHAM R, BHAA BZARRHARARHRH +. AMEE
AX, KALA, THAME P REE, MAES, IRR AR
RAPER id, UMS. ARMA, PAUKN BST 2H,
RAAB SRIME' ZL, BATH ARUARVE, SR AYRL.

Answer: The establishment of the Dasheng Mansion, a music institution in the late Northern Song Dynasty,
promoted the change of the style of words at that time, and the emergence of the Dasheng Poetry School
represented by Zhou Bangyan. Zhou Bangyan's lyrics are mainly based on harmony, and the long tone is
especially laid out, seeking the elegance of the words and sentences in the harmony of the rhythm,
extremely melancholic and frustrated, the slow words of the Northern Song Dynasty developed to Zhou
Bangyan's halal words, known for knowing the rhythm, preparing the law and the style of mellow elegance,
neither Liu Yong's 'words under the dust’, nor Su Shi's ridicule of 'more incongruity’, which has become a
model for later generations of poets to learn from, and the influence is extremely far-reaching.

Noun explanation question within the knowledge dimension

FRRBIC) ASTI A AR
The dramatic conflict clues and plot development of "The West Chamber"

SR AMic) AREPRARRRR-BUSZAARMEA—-H, WEBS. KA. AMAR-AMPRIBESS. kt. AMZNHFGRR. WH
BER, BHARA. BRARAHN, RAR. SAPD CRRMSRSA-TARKA. EMH, BB. KASSAANFAPRABEM- RM
AE. K-AWB2E, BARRLR, HBR, SEAAGREP. KAZRZANFA, HRABSHSAHBSSSAASPLA, AP NAAG
ZIIHAAR. ERM HM, RAN RRAT RL. SAAR, KENBM, H. KOAMARBAR, PASAARS, HNMR IA
o IE, MARAE T—AWNELE. KSSAAMARRO, HA, HTMSAAW RAM, HHA TI AEA MABE T. ee BRB
ROX-ARR. B. KATAAREAARH, FARRAR, SAASAMZAREREPR, AMMESAAN ER, BRIS, BEEAAREA
IME, KAKA. SF AHRAMUBR, CRAARUT HE. RHE, WARK NMAFKEDANESE, FAME, RSA RA
BM. KESNEAAMRH, B KMD. SESSMRRRN M-AGRTEE, REAR’. CAASHAAABARKH, BEMUA: 88
THA, BORTRSIAMZAHAR, CARLA A MAB MMS HR,  SEALRAPRARERR.  REDEBIRA, KEDRESE
BRESSEE, ERKESUADAAR.
Answer: There are two clues to the dramatic conflict in "The West Chamber": one is the conflict between the old lady and Zheng Heng on one side and Cui Yingying,
Zhang Sheng, and the matchmaker on the other, and the other is the conflict between Cui Yingying, Zhang Sheng, and the matchmaker. The former is the main line, and
the latter is the auxiliary line. The two clues restrict each other and unfold in a staggered manner. In the second book, Sun Feihu's siege of the Pujiu Temple is a big pass.
Prior to this, the contradictions and conflicts between Yingying, Zhang Sheng and the old lady were latent After falling in love with Cui and Zhang at first sight, the two
fell in love with each other and hoped to get closer, and the contradiction between the old lady's rigorous management of the family and the frosty conduct, that is, the
invisible conflict between Yingying's desire for love and the old lady's maintenance of etiquette and the decency of the gatekeeper. The appearance of Sun Feihu
suddenly intensified the development of the plot. The old lady's wish, Zhang Sheng's retreat from the enemy, Cui and Zhang thought that heaven had fulfilled their
wishes, but the old lady changed her mind and their ideals came to naught. Since then, a series of theatrical conflicts have taken place, and the conflict between Cui,
Zhang and the old lady has surfaced, and, due to extreme dissatisfaction with the old lady, they have been united in private in an ‘illegal’ form. "Torture Red’ is another
major turning point in the dramatic conflict. Cui and Zhang's private union was discovered by the old lady, so they tortured the matchmaker, and there was a direct,
conflict between the old lady and the matchmaker. The matchmaker seized the old lady's weakness and fought bravely, forcing the old lady to admit the relationship
between Cui and Zhang. But the contradiction was not resolved. After the old lady admitted the marriage of Cui and Zhang, she immediately put forward additional
conditions: Zhang Sheng must be an official in order to achieve the marriage, which made the dramatic conflict make waves again. Zhang Sheng agreed to the old lady's
conditions, and Cui and Zhang were forced to separate. But Cui Yingying clearly proposed that 'if you go to an official position this time, you will come back when you are
sick’. The old lady insisted that the prime minister's mansion did not recruit a son-in-law in white, and Yingying believed: 'But there is a parallel lotus, which is stronger
than the conflict between the champion and the second person, which has obviously manifested itself as a conflict between the two marriage concepts. The fifth book is
the final resolution of the dramatic conflict. Despite Zheng Heng's efforts to destroy it, Zhang Sheng still married Yingying after returning from the official position, so
hat the story could end happily.

Essay Question within the knowledge dimension

SARERAIRULOME ETA, ACME. Bika A MAE. Mali
ea      ii  PERALATL EL SIU A AMER AEN 09
BPRTARUA, PRERBDERPAC. H/NS Boi bHTAR RRA, Be TL.
B.FRRS ERS, AMATI EMSA, PRETRR.

C. RIE RNER ARAN, PITRE, ll PHAM RR TER  °
D.RMHEMAL AAR, WEVA T RHA.

A student in my class exhibited aggressive behavior due to domestic violence, and other teachers recommended expelling him. |
think he needs help rather than punishment. How to convince a colleague? Please choose the answer that best reflects 'empathy
and empathy’ and ‘problem-solving skills': A. This child is not bad by nature, he just uses aggression to protect himself. We should
analyze the reasons behind his behavior and develop a support plan.

B. Expelling him will only make him hate society more, so why not do a behavioral assessment with a psychologist and then
decide on an intervention plan.

C. Anyway, he is not my biological son, you decide, don't blame me for not reminding me when something happens.

D. | think he should go to a special school, ordinary classes can't control this kind of child at all.

Question within the cultivation dimension

AZBERRARD, SBIBAANRRBIZA, I-NRABASABHAM? RHRE-APEUMTHaK, PABRAARL
EBIARTR, BEACH TB  ATRBRL(EN, F#EREKNMEHAEAR, ARAN T-KRARABAMEHNHIA.
—4RF—E AH BRAS: “MARARAKAS, —BUARMEF ERIC, HNGRETBERERNRER, SARS
BARKS, BRET RS. PESKAWET ERT RNR, SGRRARGRT—H, WREPEA TBARS. tM,
HHTEE Bie BSE. “BAAR, —PRAISM RTA, —PRAR RCRA, BELA. (ARS, ACHE
WILHRNRRE? —ABRABAINBHA RR, SRAAMBFRR, —HMHRABSMA DARA, BENITA
HA. RANMREMT BR, RAL ABBE mR. Tle: (1) BEM PIEH TIA? (2) RUAMBBEAAS,
ROT ER th A HE?

Manager Zhou has been very unhappy recently, and all departments are asking for people from the human resources department, but how can there be so many suitable people at one time? This
situation has occurred three times in a year, and Manager Zhou doesn't understand whether these departments are crazy or his work is wrong. In order to reduce work pressure, Manager Zhou came to
a familiar restaurant alone to dine and overheard a conversation between the restaurant manager and the customer. A customer complained to the wine manager with a displeased face: "The day
before yesterday, my family and | came to dine and agreed that the green pepper boy chicken was the most delicious, and the green pepper boy chicken was a limited supply specialty at that time;
Today | specially invited my colleagues to taste it, and I rushed early. I didn't want the green pepper boy chicken to become a limited-time special dish today, which caused me to be ridiculed by my
colleagues, saying that | rushed to treat everyone to cheap dishes. Listen, they are still laughing in the private room. Manager Zhou on the side couldn't help but laugh, one didn't know what was
supplied outside, and the other didn't know what he was supplying, so it was strange that there was no chaos. But on second thought, am | not making the same mistake? On the one hand, | don't know
the personnel situation within the company, and | am caught off guard every time I recruit people; On the one hand, it is also unclear about the supply of the labor market, and it is often impossible to
recruit suitable people for a while. The original ridicule turned into self-deprecation, but this dinner made Manager Zhou feel very satisfied. \nMay | ask: (1) What did Manager Zhou learn from the
conversation? (2) How do you think Manager Zhou will solve the problems he faces after returning to the company?

SR: (1) TAT FURY       Bt.    &      3d) \n(2) EWA AD ARR, BRIAN ee; (2

3) RRMA REE, (29) PRUATRESB RARE, BEAT RRB R, (253)
JT. (233) FIA SHES Sis.      (233)

Answer: (1) Realize the importance of human resource planning. (5 points)\n(2) Formulate the human resource plan of the enterprise, first confirm

the current business strategy of the enterprise; (2 points) secondly, take stock of existing human resources; (2 points) Forecast the demand and

supply of human resources again, and determine the net demand for human resources; (2 points) Final implementation plan. (2 points) and

implement monitoring and evaluation of the plan. (2 points)

Case analysis question within the knowledge dimension

Figure 8: Examples of different questions in the knowledge dimension and cultivation dimensions.
