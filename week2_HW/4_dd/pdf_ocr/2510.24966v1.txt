arX1v:2510.24966v1 [cs.LG] 28 Oct 2025

Sequences of Logits Reveal the Low Rank Structure
of Language Models

Noah Golowich*                       Allen Liu*            Abhishek Shetty*
Microsoft Research                   UC Berkeley                MIT
noah. golowich@austin.utexas.edu        aliu42@berkeley.edu         shetty@mit.edu*

October 30, 2025

Abstract

A major problem in the study of large language models is to understand their inherent low-
dimensional structure. We introduce an approach to study the low-dimensional structure of
language models at a model-agnostic level: as sequential probabilistic models. We first empiri-
cally demonstrate that a wide range of modern language models exhibit low-rank structure: in
particular, matrices built from the model’s logits for varying sets of prompts and responses have
low approximate rank. We then show that this low-rank structure can be leveraged for genera-
tion — in particular, we can generate a response to a target prompt using a linear combination
of the model’s outputs on unrelated, or even nonsensical prompts.

On the theoretical front, we observe that studying the approximate rank of language models
in the sense discussed above yields a simple universal abstraction whose theoretical predictions
parallel our experiments. We then analyze the representation power of the abstraction and give
provable learning guarantees.

1 Introduction

Understanding the structure of language has been a long-standing goal in computer science, mo-
tivating various fundamental models (Shannon, 1951; Chomsky, 2002) such as Hidden Markov
Models (HMMs), finite state automata, and formal grammars (see (Jurafsky and Martin, 2025)).

Such questions have taken on a new light in recent years with the advent of Large Language
Models (LLMs); despite extensive study, though, their success at modeling language has largely
eluded a mathematically rigorous understanding. Such an understanding, while of interest in its
own right, is also crucial when it comes to evaluating the possible risks associated with LLMs.
Indeed, a sizeable literature has uncovered surprising ways to bypass the safety mechanisms of
LLMs (e.g., (Wei et al., 2023; Zou et al., 2023)) as well as ways they can become misaligned during
training (Razin et al., 2024; Betley et al., 2025). Understanding and defending these vulnerabilities
often requires mathematical insight into the model’s workings (Elhage et al., 2021; Olsson et al.,
2022; Turner et al., 2023; Arditi et al., 2024).

One of the principal ingredients we still lack when it comes to obtaining a more solid theoretical
foundation for studying LLMs are simple universal abstractions that are realistic enough to make

*Equal contribution.


===== PAGE BREAK =====

Ce ne en ee ee a a a                  =e Lingen; total=4.95
=== Lingen (single-token variant); total=14.41

2.0                         =— Short history (length 5); total=17.56
=== End of stage 1 ckpt; total=6.55

w
8

B

u

x
S.

KL Divergence
r
fo}

== Downsized by 16x

KL Divergence

=== Downsized by 8x
=== Downsized by 4x                                   0.5
io | === Downsized by 2x

== Downsized by 1x

102                         102                                        0        3        4

6            8           10          12          14
Rank                                                                 Token Position

(a) OLMo-7b low-rank approximation        (b) Performance of LINGEN with OLMo-1b

Figure 1: (a): Low-rank approximation error (measured by average KL divergence; see Definition 3.1) of
the extended logit matrix for OLMo-7b, and ranks 5-500. For fixed sets H,, the approximation errors for
the logit matrix Li4(H, F) behave according to a similar power law as to those of various sub-matrices with
{2,4,8, 16}-times fewer entries. Dashed line at top shows performance of a (suboptimal) rank-1 baseline.
(b): Performance of our generation procedure (LINGEN; star markers) which exploits the low-rank structure
of the extended logit matrix to generate from a given “target” prompt by only querying the language model
on nonsensical prompts unrelated to the target. We plot the KL divergence between LINGEN and the true
language model (OLMo-1b) at each token position, as well as various baselines (solid lines; see Section 3.3).

testable predictions about deployed LMs and are mathematically tractable enough to support prov-
able guarantees. Many such models have been proposed, ranging from simple types of transformers,
such as a single layer of attention heads (Sanford et al., 2024), to low-depth circuits (Merrill and
Sabharwal, 2023). While such frameworks have led to important insights, e.g., regarding expres-
sivity, most of them are limited in the types of models they can represent (e.g., transformers of
a certain depth (Merrill and Sabharwal, 2025)) or to certain types of tasks (e.g., RL fine-tuning
(Foster et al., 2025)). Further, they are often not very capable of making precise predictions about
the structure and behavior of modern LLMs.

In this paper, inspired by the folklore belief that language possesses intrinsic low-dimensional
structure, we propose an architecture-agnostic framework which allows us to study how information
in language is represented in low-dimensional subspaces. In particular, we study logit matrices
associated to any language model (Section 1.1). This approach treats the language model as a
sequential probabilistic map from sequences to sequences, sidestepping architecture-specific details.
A starting point for our work is the observation that the logit matrices associated to modern
autoregressive LLMs are well-approximated by low-rank matrices (Section 3.1). Additionally, this
low-rank structure can be exploited to exhibit several surprising phenomena, such as the ability
to generate from a given model by only querying it on unrelated or even nonsensical prompts
(Sections 3.2 and 3.3). Moreover, such a low-rank assumption yields a rich theoretical landscape,
from both a learning-theoretic and generative modeling standpoint (Section 4).

1.1 Structure Arises from The Extended Logit Matrix

Our goal is to study low-dimensional representations of language models: what aspects of the model
should such a representation capture? One natural requirement is that it allows us to compress
the past in order to generate the future. Perhaps the most fundamental and extensively-studied


===== PAGE BREAK =====

type of such compressions are those which are linear, in the following sense. For a sequence of
tokens h (e.g., a prompt), the compression consists of a vector ¢(h) € R@ so that the probability
of observing any possible completion f (e.g., a response) is proportional to exp((¢(h), w(f))), for
some embedding vector ~(f) € R¢? depending only on f.

We remark that a very similar setup has been considered in the context of LLMs, when the
sequence f consists of a single token: for essentially all modern LLMs, due to the structure of
the unembedding matrix, the probability of observing a single token y following a sequence h is
proportional to exp((¢(h), w(y))), for some embedding maps ¢,w. This property has been termed
the softmax bottleneck (Yang et al., 2017), and has had a broad array of implications, ranging
from expressivity and architecture (Press and Wolf, 2016; Kanai et al., 2018; Ganea et al., 2019;
Chang and McCallum, 2022; Godey et al., 2024) to security (Carlini et al., 2024; Finlayson et al.,
2024). However, this observation only allows us to reason about one token in the future. The
key insight of our work is that this low-dimensionality persists, even when we consider logits over
longer sequences of tokens.

To study this low-dimensionality over longer sequences, we introduce the extended logit matrix
as our main object of study. Its rows are indexed by all possible sequences of tokens, which we refer
to as histories. Roughly speaking, its columns are also indexed by all possible sequences, to be
interpreted as possible completions of a history; we refer to them as futures. In essence, the entry
of the extended logit matrix corresponding to a given (h, f) pair and a language model M is given
by log Pryy|f | A], i-e., the logarithm of the probability that M assigns to f given context h. This
choice is natural given the above discussion— indeed, the fact that log Pryy|f | h] ~ (é(h), w(f))
for some feature mappings ¢, ~ (as discussed above), is equivalent to the following statement:

The (extended) logit matrix is approximately low-rank for modern language models.

The main premise of this work is that the above statement is true for modern autoregressive
LLMs. While we cannot construct the entire extended logit matrix (which is exponentially large),
we can construct submatrices indexed by sets of histories # and futures *, and measure how the
low-rank structure of these submatrices (denoted Lay (H, F)) evolves with their size (Figure 1). As
is shown, this approximate low-rank structure is remarkably consistent as we scale up the number
of histories and futures. We emphasize both that this low-rank structure is atypical for a (random)
matrix of the same dimensions and that it is not a consequence of the low-rank structure of the
single-token logit matrix (see Appendix B). For the formal definition of the extended logit matrix,
which differs slightly from the simplified description here, we refer the reader to Section 2.

1.2. Our Contributions

In this paper, we conduct a thorough empirical study of the extended logit matrix across both a wide
range of LLMs and choices for the histories and futures, and complement our empirical findings with
theoretical results on expressivity and learnability. Our framework suggests a number of intriguing
open questions relating to interpretability, safety, and the theoretical underpinnings of LLMs.

Empirical Findings: low-rank structure. In Section 3.1, we study the extended logit matrices
defined by a wide range of language models, and sources of histories and futures. We observe that
the extended logit matrices are all well-approximated by low-rank matrices, and that the quality of


===== PAGE BREAK =====

the best low-rank approximation follows a power law which is strikingly consistent as the matrix is
scaled up. We also observe that the low-rank structure is not present at the beginning of training,
but rather emerges in the early stages of pre-training and further evolves throughout training.

Empirical Findings: consequences of low-rank structure. Given that the extended logit
matrix is typically well-approximated by a low-rank matrix, a natural follow-up question is to
understand the implied linear dependencies between the rows (i.e., histories) of the matrix. Such
linear dependences may be seen as generalizations of classical examples such as boy — girl © king —
queen (Mikolov et al., 2013). In Section 3.2, we verify that these linear dependencies are consistent
across the extended logit matrices of different LLMs. More surprisingly, they remain preserved even
if we significantly change the futures (i.e., columns) of the matrix, replacing the original futures
with nonsensical futures consisting of random sequences of tokens.

The fact that an LLM’s low-rank structure is preserved even when prompted with nonsencial
(random) sequences inspires us to investigate (in Section 3.3) the following setup for language
generation. Given any target prompt (i.e., history), using an appropriate extended logit matrix we
can approximately write it as a linear combination of unrelated, even nonsensical, histories. We
can then use this linear combination to generate a continuation to the target, by only querying the
model on the unrelated histories. We find that this procedure generates coherent continuations, and
its KL-divergence to the true model beats strong baselines, e.g., intermediate training checkpoints
(Figure 1b). Such a procedure has the potential to circumvent defenses, such as prompt filters
(Dong et al., 2024), established to ensure safety; we discuss further implications for AI safety in
Section 3.3.

Theoretical Framework. In addition, a key aspect of our framework is that it supports a simple
theoretical generative model that captures the low-rank structure that we find. Our empirical
observations inspire us to revisit the Input Switched Affine Network (ISAN) of Foerster et al.
(2017). ISANs were initially proposed as a simple, interpretable recurrent architecture, where a
low dimension hidden state is updated through a linear transformation that is allowed to depend
on the current sampled token, and were shown to achieve reasonable performance on language
modeling tasks. We show that ISANs capture precisely those distributions that have (exact) low
logit rank (Theorem 4.3).

To support the tractability and usefulness of ISAN as a theoretical model, we first study its
representation power, showing that it can express state space layers, the key component of a variety
of practically successful architectures broadly termed state space models (SSMs (Gu et al., 2021a;
Gu and Dao, 2023; Gu, 2025)). We also show that it can express algorithmic behaviors like copying
and noisy parity — the latter having implications for learnability. Since noisy parity is hard to
learn (Blum et al., 2003), efficiently learning an ISAN from samples is impossible in the worst case.
In light of this, we give a provably efficient learning algorithm with logit queries, a setting that
closely resembles practical model stealing setups for common APIs (Carlini et al., 2024).

1.3. Related Work

Low Rank Structure in Language Models There has been significant work understanding
the implications of the low rank structure in the next-token logit matrix. In particular, the output
embedding matrix constrains the vector of next-token logits to be in a space of dimension equal to
the hidden dimension (rather than the vocabulary size) and thus limits the family of conditional


===== PAGE BREAK =====

distributions that can be realized. This phenomenon, often called the softmaz bottleneck, was
formalized in Yang et al. (2017). The implications of this for language modeling are also studied in
Press and Wolf (2016); Kanai et al. (2018); Finlayson et al. (2023). Furthermore, recent works study
this softmax bottleneck from a different perspective, showing that it leaks information about models
even behind black-box APIs and results in vulnerabilities to model stealing attacks (Finlayson et al.,
2024; Carlini et al., 2024). However, these works all focus on just the next-token logit matrix,
whereas our framework allows us to reason about generating much longer sequences.

There have also been many works that observe low-rank structure in the weight matrices of
machine learning models, dating back to the early days of deep learning (Denil et al., 2013), and
this has been used to understand (Aghajanyan et al., 2020) and develop efficient methods for fine-
tuning large models e.g. LoRA and its variants (Zhang et al., 2023; Hu et al., 2022). While similar
in philosophy, our approach is very different because it is model-agnostic, does not involve looking
at the weights, but instead finds low-rank structure directly in the logits over sequences of tokens.

Linear structure in word embeddings has been observed in well-known works (Mikolov et al.,
2013; Goldberg and Levy, 2014; Levy and Goldberg, 2014) and also to some extent for sentence
embeddings (Zhu and De Melo, 2020; Bowman et al., 2016; Li et al., 2020). The linear representation
hypothesis is, informally, the idea that concepts are represented linearly as directions in some
representation space. Park et al. (2023) propose a formalization of this notion for language models,
showing that certain concepts for the next token, such as gender, can be represented linearly in
either the logit vector or the last hidden state. There have also been many works, that broadly
fall under mechanistic-interpretability, on probing and steering using the linear representations in
internal layers of the model e.g. Elhage et al. (2021); Meng et al. (2022); Hernandez et al. (2023);
Nanda et al. (2023); Turner et al. (2023); Todd et al. (2023); Hendel et al. (2023); Li et al. (2023);
Geva et al. (2022). Compared to these aforementioned works, our framework only uses the output
logits, but also allows us to reason about generating longer sequences of tokens (beyond just the
next token). It is an exciting direction to try to extract features and perform interventions through
the representations provided by our framework.

Related Theoretical Models Linear dynamical systems form the basis of control theory and
are also the key building block in modern state-space models. Given the vast literature in control
theory, we refer the reader to (Kailath, 1980; Chen, 1999) for a classical treatment and (Hazan
and Singh, 2025) for a modern treatment. A sequence of works over the past few years (Gu
et al., 2021a; Gu and Dao, 2023; Katharopoulos et al., 2020; Dao and Gu, 2024; Gupta et al.,
2022; Gu et al., 2021b) has led to the development of modern state-space models, introducing
additional twists such as selective gating (which is related to the token-dependent transitions we
use) and connections to linearized transformers. Further connections to linear dynamical systems
have led to new theoretically motivated architectures (Agarwal et al., 2024). We believe our work
presents a new perspective that is both theoretically and empirically justified, while maintaining
the conceptual simplicity of linear dynamical systems.

In terms of simpler, mathematically tractable generative models, the ISAN model is related
to weighted finite automata (Droste et al., 2009) and hidden Markov models (HMMs) (Rabiner
and Juang, 2003), except with the addition of a softmax nonlinearity to sample the next token.
This nonlinearity significantly improves its empirical performance on language modeling compared
to these earlier models (Foerster et al., 2017). There have been many earlier works studying the
learnability of these classical models such as Mossel and Roch (2005); Hsu et al. (2012); Balle et al.


===== PAGE BREAK =====

(2014). Query learning settings have been classically studied in the literature on learning automata
and formal languages (Angluin, 1987), and more recently received renewed interest in the context of
model stealing for large language models (Mahajan et al., 2023; Liu and Moitra, 2025). Compared
to these works which study classical models (e.g. HMMs), we believe the theoretical work here
takes another step closer to modern language models.

2 Setup and Notation

Our main object of study is autoregressive language models. We denote such models with the
letter M and the associated set of tokens with ©. For a token z € % and a context sequence
yit = (y1,---,yt) € X*, we will denote the corresponding probability distribution over the next
token by Pry: | yi], i-e-, Prag{z | y14] is the probability that the next token is z € ©. We let X*
and =< represent sequences of length ¢ and at most t, respectively, while ©* denotes the set of all
sequences of tokens. Note that these include the empty string, which we denote by Null. We use o
to denote the concatenation of two sequences, i.e., y oy’ denotes the concatenation of y and 7’.

To formally introduce the key object that we study, the extended logit matrix, we first define
the mean-centered logits; while empirically mean-centering doesn’t make a large difference, it will
be important for the theoretical generative model that we introduce later on (see Remark 1).

Definition 2.1 (Mean-Centered Logits). Given a sequence yi+ € ©’, we define the mean-centered
logits as Lyy[z|y14] = log Pry[z|y1] — By vex log Pru [z'|yiu] for z € Xd.

Given a model M and sets H,*F C &* consisting of sequences of tokens, the extended logit
matrix Ly (H,F) has rows indexed by H and columns indexed by F x 5}; accordingly, we will write
Lu (H, F) € R**(F*~), We refer to elements of H as histories and of F as futures. Formally:

Definition 2.2 (Extended Logit Matrix). Fiz a model M. Given subsets of histories H C &* and
futures F C &*, the associated extended logit matrix Lyy(H, F) € RHXx(Fx=) js defined for h € H
and (f,z)€ F xd by

Lu (H, F)nyy,z)) = Lulz | ho f].

In the remainder of this paper, we will refer to the extended logit matrix as simply the logit
matrix.

Relation to Simplified Definition in Section 1.1. Observe that if we set F = DS’, then
a row Ly({h},F) contains all of the information necessary to sample a continuation of h of up
to length T, token by token. From the information in this row, we can compute log Pr|f|h] for
any f € SS? — and if we ignore the normalizing constant and replace the mean-centered logits
with normalized logits, then we can compute log Pr[f|h] by simply summing up the corresponding
token-by-token conditional log probabilities in £yy({h}, F) (which is a linear transformation of the
matrix).


===== PAGE BREAK =====

Downsized by 16x
Downsized by 8x
Downsized by 4x
Downsized by 2x
Downsized by 1x

| =s— OB tokens
—e 1B tokens
—e 218 tokens
— 49B tokens
—e 126B tokens

—* 273B tokens
— 609B tokens

107                                                                              —e— 1280B tokens
—*— 2622B tokens
—e— main

10°                       10?                        10?                        10?                                                       101                                                         102

Singular Value Index                                               Rank

Normalized Singular Value
KL Divergence
5

Figure 2: OLMo-7b singular values; Power law _—_ Figure 3: Low-rank approximations (wrt. avg. KL di-
exponent a ® 0.536                   vergence) over Stage-1 pretraining of OLMo-1b.

3 Experiments

3.1 Low Rank Structure

In this section, we evaluate the extent to which the logit matrices associated to modern LLMs (Def-
inition 2.2) are actually low-rank. To do so, we computed logit matrices Lyy(H, F) corresponding
to various models M and sets H,F of sequences. The sets H,F were generated as follows starting
from a dataset D, according to some parameter n € N. We sampled 2n sequences from D and for
each of the 2n sequences included a random subsequence of it in either F or H (so that each of F,H.
ends up with n subsequences). We studied a broad range of datasets D and open-source models
M; precise details regarding the choices of D, M as well as H,F may be found in Appendix A.1.1.
For the figures shown in the main body of the paper, the dataset D was always taken to be the
wiki split of olmo-mix-1124 (OLMo et al., 2024).

We considered various values of n, ranging up to 10*. Note that the full logits matrix Ljy(H, F)
has n?- || entries, which is not feasible to store given that for many models (e.g., OLMo) |>| ~
10°. Thus, in our experiments we used instead a sub-matrix of Lyy(H,F), which we denote by
Luk(H, F), for some parameter k. The submatrix £y7,,(H,F) is obtained by selecting a subset of
the columns as follows: for each future f € F, we select the columns indexed by (f, z) where z is one
of the k most-likely tokens following f, i-e., for which Prjy[z | f] is largest. In our experiments we
took k = 50; in Appendix A.1.5 we also repeated some experiments for (a) k = 200 and (b) where
a random subset of k = 50 tokens was selected for each f, and observed similar results. At a high
level, Lizx(H,F) can be interpreted as containing information about the model’s predictions for
the most-likely next tokens for each possible context ho f (with h © H, f € F). We measured the
degree to which the resulting logit matrix Liy,(H,F) is well-approximated by a low-rank matrix
in two ways, discussed below. For ease of notation, we omit the parameter k (and ignore the
distinction between Ly (H,F) and Liy.~(H,F)) in our description of the experiments throughout
this section.


===== PAGE BREAK =====

Method 1: measuring the singular value decay. In Figure 2 (and Figure 6, which shows
additional LLMs), we display the singular values of the logits matrix Lyy(H, F) for various choices
of models M and various sizes n of the sets H,*#, when the dataset D was wiki. The singular
values decay approximately according to a power law, in that the ith singular value o; of the matrix
Lu (H,F) is approximately C'-i~° for some C,a > 0 (depending on M).

As shown in Figures 2 and 6, the exponent a for most logit matrices is slightly greater than
1/2; this holds as well for additional choices of the dataset D (see Figure 8 in the appendix).
Intriguingly, a = 1/2 represents a phase transition for low-rank approximation, in the following
sense (see Fact A.1 for a formal statement): if a > 1/2, then for any constant ¢, there is a constant
r, depending only on ¢€ so that a rank-r; matrix ¢-approximates the logit matrix. On the other
hand, if a < 1/2, then for sufficiently small constants ¢, a rank linear in the dimension is needed.

When it comes to approximating the logit matrices of language models, we are most interested
in the setting where H,¥ contain all possible histories and futures (or at least those that are
plausible in natural language), as then the logits matrix Ly(H,F) contains all the next-token
prediction information in the language model. In this setting, the logit matrix’s dimensions are
exponential in the length of the sequences, meaning that the case of power law decay with a < 1/2
requires that the rank be exponentially large to achieve approximation error less than some absolute
constant. This is in contrast to constant rank for the case a > 1/2 which we observe in Figures 2,
6 and 8.

While it is infeasible to compute the logits matrix when H,F are taken to be the sets of all
histories and futures, it is apparent in the figures that the exponent of the power law remains
essentially unchanged when we consider sub-matrices of Ljiy(H,F) (modulo some degeneration of
the power law at ranks that approach the size of #,#, which is expected). Thus, as H,F are
scaled up, we expect that the same power law holds.

Method 2: measuring the approximation via KL divergence. While measuring the decay
of singular values of a matrix is mathematically convenient (in the sense of, e.g., Fact A.1), it does
not directly yield a probabilistic interpretation of the fact that the logit matrix Lay(H,F) is close
to another (low-rank) matrix. To bridge this gap, we observe that any matrix L € R¥*(F*») (e.g,,
a logit matrix Lyy(H,F)) induces a collection of |H||F| “next token distributions” on © by taking
the softmax of the vector Ln,p := (Lnyp,z))zex, for each h, f. (Indeed, recall from Definition 2.2
that when L = Ly(H,F), we have Lp, ¢ = Ly|:| ho f].) In place of Frobenius norm, we propose
to measure the average KL divergence between these distributions:

Definition 3.1 (Average KL divergence). Fix sets H,F C &*, and consider matrices L,A €
RHx(Fx¥) We define Dev (L, A) to be the average KL divergence between the next-token distribu-
tions induced by the entries of Ly,p and Ap, for each (h, f) pair:

1
Dye (L, A) =                   Ss"     Dx. (softmax(L», ¢)||softmax(Ap, f)).

HIF croger

Thus, for a low-rank matrix A, the quantity Dgi®(Li(H,F), A) may be interpreted as the
ability of the low-rank matrix A to approximate the model M in KL divergence, when restricted
to contexts ho f forh EH, f € F. It is straightforward to show (see Fact A.2) that we may bound

Deb(Lu(H,F),A) < pgp: Lu (H,F) — Alle Combining this fact, Fact A.1, and the

observed decay in singular values in Figure 2, we should expect the average KL divergence between


===== PAGE BREAK =====

Lu (H,F) and a rank-r approximation to decay at least as fast as a power law (in r). This
prediction is confirmed in Figure la (see also Figures 7 and 9 in the appendix), where we also
observe consistency in the power law as the matrix size is increased, suggesting that such a low-
rank approximation holds when H,¥ are even exponentially large.

Evolution of approximation during the course of training. One intriguing exception to
the findings discussed above is that for a “baseline” model M corresponding to the “Step 0”
checkpoint of OLMo-1b (i.e., before any training steps), the logit matrix does not appear to exhibit
low-rank structure: see Figure 6d in the appendix, where we have a = 0.374. Further, while the
KL divergence to a low-rank approximation does decrease with the rank (Figure 7d), this decrease
does not seem to follow a consistent power law as the matrix’s size is increased, suggesting that for
larger sets H,F such a decrease may be diminished. These observations lead us to ask: At what
point in training does the logit matrix become (approximately) low rank?

To answer this question, we show, in Figure 3, the average KL divergence to a low-rank approx-
imation for the logit matrices at various pre-training checkpoints of OLMo-1b (given a fixed set of
H, F each of size approximately 4000). Early in training, the KL divergence to a low-rank approxi-
mation drops significantly, then rises slowly towards its final value. This observation suggests many
fascinating questions for follow-up work, foremost amongst them: How and why does low-rank
structure in the logit matrix emerge early in pre-training?

3.2 Shared Structure Across Models & Futures

Next, we proceed to discuss some consequences of the (approximate) low-rank structure of the logit
matrix Ly (H,F). One of the most basic facts about low-rank matrices is that they have nontrivial
(row) kernels, i.e., if a matrix A € R¥*(7*™) (say, an approximation of Ly;(H, F)) has rank much
less than |H|, then there is a large space of nonzero vectors v € R®™ for which v' - A = 0.

What does the existence of such vectors mean? Roughly speaking, such vectors v represent
“linear relationships” between different histories. Studying such linear relationships has been a
mainstay of NLP over the last decade, with a simple example being relationships between word
embeddings such as: boy — girl © king — queen (Mikolov et al., 2013). When it comes to logit
matrices, do such linear relationships manifest in ways that correspond to semantic relations between
histories? Towards answering this question, we first describe the following “sanity check”: for the
sets of histories H and futures F considered in Section 3.1, for each h € H we computed the h’ 4 h
in H minimizing the norm of the difference between the corresponding rows of the logit matrix,
namely Ly ({h}, F) — Lu ({h’},F). Some samples of resulting pairs are shown in Table 1, where
it can be seen that most of the pairs of histories share semantic similarities.

Systematic evaluation: description. We aim to more systematically evaluate whether vectors
v € R® describing linear relationships between rows (i.e., histories) of A ~ £y,(H, F) are inherent
to the histories themselves, as opposed to being spurious artifacts of either the model architecture
or the set of futures *. If indeed such vectors v were not spurious, then we would expect that: the
induced linear relationship given by such vectors transfers (a) across the choice of futures F and
(b) across models M. In other words (regarding (a)): given distinct sets of futures F,F’ (drawn
from different distributions), after choosing low-rank matrices A, A’ satisfying A + Ly(H,F) and
A’ = Ly (H, F’), the row kernels of A, A’ should be roughly equal. Equivalently, since the column
span of a matrix is the orthogonal complement of the matrix’s row kernel, we expect that the


===== PAGE BREAK =====

2.00
1.0                           —      .                                                  == Lingen; total=2.85
Rank: 1¢                          1.75                       === Lingen (single-token variant); total=10.79
Rank: 88                                                      === Short history (length 5); total=17.77
Y os                                     Rank: 166                         1.50                        == End of stage 1 ckpt; total=6.46
>                       — Rank: 244
<                        — Rank: 322                 g 1.25
Boe                           ==) Random (Rank: 10)           o
U                                                                  fon)
£                               ==» Random (Rank: 88)            g 1.00
o                              == Random (Rank: 166)          a
‘5 0.4                                                                    40.75
o    Ke               ==: Random (Rank: 244)       4
c      as                    —_
a     Ssl>te               Random (Rank: 322)           0.50
© 02   oss
SCOTS.                                                  0.25
~
N      yAR              ~=I>>      Ve
0.0    .                                           >=              0.00
0       50      100      150                                             0       2       4       6       8       10      12      14
Index                                                                     Token Position
Figure 4: Cos of principal angles btwn. column spaces            Figure 5: LINGEN with OLMo-1b.

of low-rank apx. of Ly (H,F) & Lu (H, Frese).

column spans of A, A’ should be roughly equal. Moreover (regarding (b)), a similar conclusion
should hold if A, A’ were low-rank approximations of Ly(H, F) and Lyy(H, F) for distinct models
M,M’.

Invariance to the choice of #: “nonsense” futures. To evaluate the above hypothesis
regarding differing sets F,F’ of futures: for a fixed model M (taken to be OLMo-1b), and sets
H,F as constructed in Section 3.1, we create a “nonsense” version F""*"%* of F by randomly
permuting all tokens amongst futures of F. Given a parameter r € N, we let A € R#*(**®)
(resp., Aronsens*) denote the best rank-r approximation to Lya;(H,F) (resp., Lac(H, Frorse"s*)) in
Frobenius norm. Are the column spaces of A, A°°™*"S* approximately equal? In Figure 4, we
answer this question by plotting, for various values ofthe rank r, the (cosines of the) principal
angles between the column spans of A, Ano"s*"5*.' A large fraction of the principal angles are close
to 0, i.e., have cosines close to 1 (and this fraction is much larger than for a pair of independent
uniformly random r-dimensional subspaces, which is shown in the dashed lines). Thus, linear
relationships between histories transfer to significantly different sets of futures.

The above experiment demonstrates that the model’s logits Lyy[- | ho fr"s*"**], for a history
h EH and a “nonsense” future fronsens® Ec Fnonsense | still yield significant information about ho f,
for real futures f € F. In particular, given a vector v representing some linear relationship between
histories in the sense that uv! - Lyg(H, FO") ~ 0, we typically have also v! - Lyy(H,F) ~& 0.
Developing a more thorough understanding of why this property emerges and how it depends on,
e.g., lengths of the futures in f, are intriguing directions for future work.

Invariance to the choice of M. Paralleling our conclusions from considering different sets of
futures, in Figure 11 in the apppendix, we observe a similar overlap between the column spaces of

‘Given two d-dimensional subspaces S,S’ C R”, the principal angles between S,S’ are a collection of d real
numbers 01,...,4a € [0,7/2] with 6; < --- < 0q which represent how much S,S’ overlap: if S = S’ then we have
6; = 0 for all i, and if S,S’ are orthogonal then we have 0; = 7/2 for all i. See (Golub and Van Loan, 2013, Section
12.4.3) for further discussion.

10


===== PAGE BREAK =====

low-rank approximations to Lyy(H,F) for various pairs (M, M’) of distinct models.

3.3. Exploiting Low Rank for Generation

Finally, we investigate one consequence of the fact that there are “linear relationships” between
histories as captured by vectors v € R®™ satisfying v' - £y(H,F) & 0, and moreover that these
relationships transfer to (potentially unrelated) sets of futures. We will show how to exploit these
relationships to approximately generate samples from a language model M conditioned on some
“target history”, by only querying M on unrelated sequences. In particular, consider some sequence
htarg € &*; typically, to generate continuations f = (21,..., 2m) of Atarg under M, having generated
214-1, we query Pryy[: | Atarg © 214-1] and sample z from it.

We ask: Can we generate a continuation of htarg as above without making any queries of the
form Pru: | Atarg ° 212-1]? To answer this question, suppose we are given a set of histories
H C &*, as well as a vector v = (vp)nexq € R™. One should interpret v as encoding a way to write
htarg aS a linear combination of h € H in the sense that for a generic set F of futures we have
Li (Mtarg, F) © v'-£Ly4(H,F). If this holds, then for “typical” sequences z1.;-1 we have generated,
we can hope that M’s next-token distribution, Liy[: | htarg ° 214-1] =Lu({Atarg}, {Z1:-1}), is close
tov! -Ly(H, {214-1}).

The above intuition suggests the following procedure to generate a sequence of tokens 21, Z2,....
For each t > 1, having generated z14-1, for each h € H, we compute the next-token logit vector
Ln = Lut: | ho ziti] € R~™ corresponding to h. Then we sample the next token z; from the linear
combination of the vectors L;, 4 as induced by 4, i.e., 2 ~ softmax (Shen Un Ln.t) , and repeat for
some number m of steps. We call this procedure LINGEN; see Algorithm 1 (in the appendix) for
a complete description, and Appendix C.4 for a proof that LINGEN generates approximately from
M’s distribution, under appropriate assumptions paralleling the intuitions discussed above. We
instantiate LINGEN in two different manners:

Option 1: In-distribution target. In our first experiment, the target htarg is “in-distribution”
for the set of histories H we use in LINGEN: in particular, the set H is taken to be subsequences of
wiki sequences and hiarg is taken to be a subsequence of a wiki sequence distinct from all of those
producing H. We considered 50 different choices of htarg, and for each generated 5 sequences of
length 15 according to LINGEN. The results are shown in Figure 5: we display, at each token index
t, the KL divergence between the distribution of the next token according to LINGEN (namely,
softmax (Shen Uh, * Ln,t)) and the model M (namely, Pry|- | ho 21:4-1]), averaged over all 250
generations. The “total” numbers reported in the figure represent the average KL divergence,
summed over all 15 positions. See Table 2 for some samples generated from LINGEN.

To compute the coefficients v for use by LINGEN, we regressed the rows of Lyy(H,F) onto
Li ({htarg}, F). Doing so requires knowledge of £yy(htarg, F) and thus requires us to query Prjy|- |
htarg 0 f| for each f € F. We expect that, however, by using our observations around model
transferability from Section 3.2, such a vector v can be computed instead using some other (perhaps
less powerful) model M’. We leave further investigation on this point to future work.

In Figure 5, we also display several baselines in which LINGEN is replaced either by (a) in orange,
a version of LINGEN where v is only computed by only using, for each history, the nezt-token logits
corresponding to the empty future (i.e., Lig(H, {0})), (b) in green, a version of M with restricted
context window; or (c) in red, an earlier training checkpoint of 1. We emphasize that (a) performs
well at generating the first token z, (as expected), but performs significantly worse at later tokens

11


===== PAGE BREAK =====

z, t > 1: this point emphasizes the importance of considering the extended logit matrix
Lu (H,F) as opposed to the single-token logit matrix which has been considered in prior
works (Yang et al., 2017). Altogether, LINGEN performs significantly better than all baselines. ?

Option 2: Out-of-distribution target (& futures). Next, we consider a more challenging
setting where the target htarg is unrelated to the set of histories H used in LINGEN. We took the
same 50 targets Atarg (origining from wiki) as described above, but now let the set of histories H be
the set H""5° obtained by randomly permuting all tokens amongst all elements of H. Moreover,
when computing v, we replace the set of futures F with Fr"s* (i.e., obtained by randomly
permuting all tokens). The results are shown in Figure 1b; while the KL divergences corresponding
to LINGEN are generally larger than in Figure 5, they are still smaller than all baselines, and the
generated sequences (‘Table 3) generally show clear evidence of incorporating information from htarg.
Thus, by using the low-rank structure of the logit matrix, we can generate starting from a
prompt by only querying the model at (nonsense) sequences unrelated to the prompt.
While we leave a more thorough investigation for future work, we believe that this approach suggests
novel ways to create jailbreaks for LLMs, an area that has seen extensive work recently (Wei et al.,
2023; Yi et al., 2024): for instance, querying the model on sequences in 1.°"*"5* as above may
allow us to circumvent input filters which aim to filter out harmful prompts; see Section 5 for
further discussion. In addition to implications for AI safety, we stress that the structure we observe
is fundamental and could lead to many other applications ranging from computational efficiency
at inference time to improved training procedures. We discuss this and other future directions in
Section 5.

4 Theoretical Results

In the following sections, we will develop theoretical foundations for understanding low logit rank.

Definition 4.1 (Logit Rank). A language model M has logit rank d if for any set of histories
H Cc d* and futures F C &*, the matrix Ly(H,F) has rank at most d.

We will show that low logit rank is equivalent to a simple generative model and then explore
the representation power of this model. Finally, we will show that given query access to a language
model with low logit rank, we can efficiently learn a description of a language model that approx-
imates it well in TV distance. These results taken together establish low logit rank as a natural
and tractable theoretical model for understanding language models.

4.1 A Low Rank Generative Model

Our first main result is demonstrating that the low logit rank condition can be captured by a simple
generative model. The idea is to consider a version of a linear dynamical system model but with
two important differences: (1) the dynamics are allowed to depend on the current sampled token
and the current timestep and (2) the observations are generated through a softmax nonlinearity.

In general, LINGEN improves at later tokens; we believe this holds since for such tokens, the distribution of the
next token depends more on the “more recent” previously generated tokens z1.:-1 (as opposed to htarg), and these
tokens are fed into the model to compute L;,,4. One exception is that LINGEN performs very well at the first token,
which may be since the single-token logit matrix of LLMs is low-rank (see Section 1).

12


===== PAGE BREAK =====

This model is an extension of the Input Switched Affine Network (ISAN) model studied by Foerster
et al. (2017) as an interpretable recurrent architecture (see Definition C.1), with the difference being

that our model allows the dynamics to change every timestep. We call our model a time-varying
ISAN.

Definition 4.2 (Time-varying ISAN). A time-varying ISAN of sequence length T is specified by
matrices Az, € R&4, B, € R**4 for z € ¥,t € [T] and an initial state x € R*. It defines a
distribution over sequences of tokens 2, z2,...,27 by: 2 is sampled from softmax(B,x4-1). Then,
the hidden state is updated as xy = Ax, t%t-1 3. We refer to d as the hidden dimension’ .

The (time-varying) ISAN captures the linear compression perspective since the hidden state x;
can be seen as compresion of the history {z;}{_, that is sufficient to generate the future {z; Vey 41°

As mentioned, a similar time-invariant model has been studied by Foerster et al. (2017) for its
interpretability, but our perspective differs because rather than viewing it as a separate architecture,
we treat it as a generative model that can approximate modern language models. In fact, the work
by Foerster et al. (2017) supports the usefulness of this model as a theoretically tractable surrogate,
as although far from state of the art, it can achieve relatively coherent language modeling at small
scales. We present a new motivation for studying this model by proving that the extended logit
matrix being low rank is equivalent to a language model being expressible as a time-varying ISAN.

Theorem 4.3 (Equivalence between Low Logit Rank and Time-Varying ISAN). Let M be a lan-
guage model over sequences of length T. Then M is expressible as a time-varying ISAN with hidden
dimension d if and only if the logit matrix Lyy(X*,OS7-*) has rank at most d for allt < T.

4.2 Representation Power

We show that ISANs can represent a variety of simple architectures and languages. Specifically,
we show that they can represent linear SSM layers, and can express copying and noisy parity. The
formal statements and proofs are deferred to Appendix C.2.

4.3. Provable Learning Guarantees

Given that ISANs can express noisy parity, and the standard cryptographic assumption on the
hardness of learning a noisy parity, it is computationally hard to learn an ISAN from samples
(see Theorem C.13). To circumvent this, we consider a logit query model, where the learner can
query the logits for the next token given any history. This model mirrors settings for practical
model stealing attacks e.g. (Carlini et al., 2024), for stealing the embedding dimension and output
embedding matrix for production language models and also connects to classical and modern works
on computational learning theory (Angluin, 1987; Mahajan et al., 2023; Liu and Moitra, 2025).
Our main result is the following theorem, which shows that given logit query access to a time-
varying ISAN, we can efficiently learn a time-varying ISAN that approximates it well in TV distance.

Theorem 4.4. Given logit query access to an unknown time-varying ISAN M over SS” with hidden
dimension at most d, there is an algorithm that uses poly(d,|5|,7,1/e) runtime and queries and
returns a description of a time-varying ISAN M’ such that E[Dry(M, M')] < e.

3Note that incorporating a bias term, i.e. v = Az,,4%1—-1 + bz,,4, is equivalent to adding an extra dimension that
is always 1. Thus, we will use the above form without the bias term.

‘For simplicity, we will not have an end-of-sequence token, but will treat the time-varying ISAN language model
as defining a distribution over sequences of a fixed length.

13


===== PAGE BREAK =====

5 Conclusions

Our paper develops a simple but mathematically justified framework built on the low-rank structure
of the extended logit matrix and demonstrates its empirical predictive power across different models,
data, and tokenizers. We believe this low-rank generative viewpoint will provide a general model-
agnostic foundation for understanding, probing, and steering language models. Furthermore, this
lens points to many promising future directions:

e Can we better understand how the singular value decay evolves during training (recall Fig-
ure 3) and can we use it as a diagnostic for training progress?

e The low rank structure allows us to represent each history as a vector (recall Section 1.1)
— can we extract concepts and features in this representation space? This would help build
towards a model-agnostic approach to interpretability.

e Can we use LINGEN or some variation of it to bypass safety guardrails and generate responses
to unsafe prompts? If so, does our framework suggest techniques for safeguarding against
such attacks?

e Can we extend our theoretical results to the case when the model is only approximately low-
rank as observed in practice? Further, it is an interesting open question to understand what
notions of approximation (e.g. total variation vs matrix norm) are theoretically feasible while
maintaining fidelity to practice.

Acknowledgements

AL was supported by a Miller Research Fellowship. We are grateful to Adam Block, Akshay
Krishnamurthy, and Ankur Moitra for helpful comments.

References

N. Agarwal, D. Suo, X. Chen, and E. Hazan. Spectral state space models, 2024. URL https:
//arxiv.org/abs/2312. 06837.

A. Aghajanyan, L. Zettlemoyer, and S. Gupta. Intrinsic dimensionality explains the effectiveness
of language model fine-tuning. arXiv preprint arXiv:2012.13255, 2020.

D. Angluin. Learning regular sets from queries and counterexamples. Information and computation,
75(2):87-106, 1987.

A. Arditi, O. Obeso, A. Syed, D. Paleka, N. Panickssery, W. Gurnee, and N. Nanda. Refusal in
language models is mediated by a single direction. Advances in Neural Information Processing
Systems, 37:136037—136083, 2024.

B. Balle, X. Carreras, F. M. Luque, and A. Quattoni. Spectral learning of weighted automata: A
forward-backward perspective. Machine learning, 96(1):33-63, 2014.

14


===== PAGE BREAK =====

J. Betley, D. Tan, N. Warncke, A. Sztyber-Betley, X. Bao, M. Soto, N. Labenz, and O. Evans.
Emergent misalignment: Narrow finetuning can produce broadly misaligned llms, 2025. URL
https: //arxiv.org/abs/2502.17424.

A. Blum, A. Kalai, and H. Wasserman. Noise-tolerant learning, the parity problem, and the
statistical query model. Journal of the ACM (JACM), 50(4):506-519, 2003.

S. Bowman, L. Vilnis, O. Vinyals, A. Dai, R. Jozefowicz, and S. Bengio. Generating sentences from
a continuous space. In Proceedings of the 20th SIGNLL conference on computational natural
language learning, pages 10-21, 2016.

N. Carlini, D. Paleka, K. D. Dvijotham, T. Steinke, J. Hayase, A. F. Cooper, K. Lee, M. Jagiel-
ski, M. Nasr, A. Conmy, et al. Stealing part of a production language model. arXiv preprint
arXiv:2403.06634, 2024.

H.-S. Chang and A. McCallum. Softmax bottleneck makes language models unable to represent
multi-mode word distributions. In Proceedings of the 60th Annual Meeting of the Association for
Computational Linguistics, volume 1, 2022.

C.-T. Chen. Linear system theory and design. Oxford university press, 1999.
N. Chomsky. On nature and language. Cambridge University Press, 2002.

T. Dao and A. Gu. Transformers are ssms: Generalized models and efficient algorithms through
structured state space duality. arXiv preprint arXiv:2405.21060, 2024.

M. Denil, B. Shakibi, L. Dinh, M. Ranzato, and N. De Freitas. Predicting parameters in deep
learning. Advances in neural information processing systems, 26, 2013.

Z. Dong, Z. Zhou, C. Yang, J. Shao, and Y. Qiao. Attacks, defenses and evaluations for llm
conversation safety: A survey, 2024. URL https://arxiv.org/abs/2402.09283.

M. Droste, W. Kuich, and H. Vogler. Handbook of weighted automata. Springer Science & Business
Media, 2009.

N. Elhage, N. Nanda, C. Olsson, T. Henighan, N. Joseph, B. Mann, A. Askell, Y. Bai, A. Chen,
T. Conerly, et al. A mathematical framework for transformer circuits. Transformer Circuits
Thread, 1(1):12, 2021.

A. G. et al. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407 . 21783.
G. T. et al. Gemma 3 technical report, 2025. URL https://arxiv.org/abs/2503.19786.

M. Finlayson, J. Hewitt, A. Koller, S. Swayamdipta, and A. Sabharwal. Closing the curious case
of neural text degeneration. arXiv preprint arXiv:2810.01698, 2023.

M. Finlayson, X. Ren, and 8. Swayamdipta. Logits of api-protected Ilms leak proprietary informa-
tion. arXiv preprint arXiv:2408.09539, 2024.

J. N. Foerster, J. Gilmer, J. Sohl-Dickstein, J. Chorowski, and D. Sussillo. Input switched affine net-
works: An rnn architecture designed for interpretability. In International conference on machine
learning, pages 1136-1145. PMLR, 2017.

15


===== PAGE BREAK =====

D. J. Foster, Z. Mhammedi, and D. Rohatgi. Is a good foundation necessary for efficient re-
inforcement learning? the computational role of the base model in exploration, 2025. URL
https: //arxiv.org/abs/2503.07453.

O. Ganea, S. Gelly, G. Bécigneul, and A. Severyn. Breaking the softmax bottleneck via learnable

monotonic pointwise non-linearities. In International Conference on Machine Learning, pages
2073-2082. PMLR, 2019.

M. Geva, A. Caciularu, K. R. Wang, and Y. Goldberg. Transformer feed-forward layers build
predictions by promoting concepts in the vocabulary space. arXiv preprint arXiv:2203.14680,
2022.

N. Godey, E. de la Clergerie, and B. Sagot. Why do small language models underperform? studying
language model saturation via the softmax bottleneck. arXiv preprint arXiv:2404.07647, 2024.

Y. Goldberg and O. Levy. word2vec explained: deriving mikolov et al.’s negative-sampling word-
embedding method. arXiv preprint arXiv:1402.8722, 2014.

G. H. Golub and C. F. Van Loan. Matrix computations. JHU press, 2013.

A. Gu. On the tradeoffs of state space models and transformers, 2025. URL https://goombalab.
github.io/blog/2025/tradeoffs/.

A. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv
preprint arXtv:2312.00752, 2023.

A. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective state spaces. In
First Conference on Language Modeling, 2024. URL https://openreview.net/forum? id=
tEYskwiVY2.

A. Gu, K. Goel, and C. Ré. Efficiently modeling long sequences with structured state spaces. arXiv
preprint arXtv:2111.00896, 2021a.

A. Gu, I. Johnson, K. Goel, K. Saab, T. Dao, A. Rudra, and C. Ré. Combining recurrent, convolu-
tional, and continuous-time models with linear state space layers. Advances in neural information
processing systems, 34:572-585, 2021b.

A. Gupta, A. Gu, and J. Berant. Diagonal state spaces are as effective as structured state spaces.
Advances in neural information processing systems, 35:22982—22994, 2022.

E. Hazan and K. Singh. Introduction to online control, 2025. URL https://arxiv.org/abs/
2211.09619.

R. Hendel, M. Geva, and A. Globerson. In-context learning creates task vectors. arXiv preprint
arXiv:2310.15916, 2023.

E. Hernandez, A. S. Sharma, T. Haklay, K. Meng, M. Wattenberg, J. Andreas, Y. Belinkov,
and D. Bau. Linearity of relation decoding in transformer language models. arXiv preprint
arXiv:2308.09124, 2023.

J. P. Hespanha. Linear systems theory. Princeton university press, 2018.

16


===== PAGE BREAK =====

D. Hsu, S. M. Kakade, and T. Zhang. A spectral algorithm for learning hidden markov models.
Journal of Computer and System Sciences, 78(5):1460—1480, 2012.

E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen, et al. Lora:
Low-rank adaptation of large language models. ICLR, 1(2):3, 2022.

S. Jelassi, D. Brandfonbrener, 5. M. Kakade, and E. Malach. Repeat after me: Transformers are
better than state space models at copying. In Forty-first International Conference on Machine
Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https:
//openreview.net/forum?id=duRRoGeoQT.

D. Jurafsky and J. H. Martin. Speech and Language Processing: An Introduction to Natural Lan-
guage Processing, Computational Linguistics, and Speech Recognition with Language Models. 3rd
edition, 2025. URL https: //web.stanford.edu/~jurafsky/slp3/. Online manuscript released
August 24, 2025.

T. Kailath. Linear systems, volume 156. Prentice-Hall Englewood Cliffs, NJ, 1980.

S. Kanai, Y. Fujiwara, Y. Yamanaka, and S. Adachi. Sigsoftmax: Reanalysis of the softmax
bottleneck. Advances in Neural Information Processing Systems, 31, 2018.

A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. Transformers are rnns: Fast autoregressive

transformers with linear attention. In International conference on machine learning, pages 5156—
5165. PMLR, 2020.

O. Levy and Y. Goldberg. Linguistic regularities in sparse and explicit word representations. In
Proceedings of the eighteenth conference on computational natural language learning, pages 171-
180, 2014.

B. Li, H. Zhou, J. He, M. Wang, Y. Yang, and L. Li. On the sentence embeddings from pre-trained
language models. arXiv preprint arXiv:2011.05864, 2020.

K. Li, A. K. Hopkins, D. Bau, F. Viégas, H. Pfister, and M. Wattenberg. Emergent world repre-
sentations: Exploring a sequence model trained on a synthetic task. ICLR, 2023.

A. Liu and A. Moitra. Model stealing for any low-rank language model. In Proceedings of the 57th
Annual ACM Symposium on Theory of Computing, pages 1755-1761, 2025.

G. Mahajan, S. Kakade, A. Krishnamurthy, and C. Zhang. Learning hidden markov models using
conditional samples. In The Thirty Sixth Annual Conference on Learning Theory, pages 2014—
2066. PMLR, 2023.

Kk. Meng, D. Bau, A. Andonian, and Y. Belinkov. Locating and editing factual associations in gpt.
Advances in neural information processing systems, 35:17359-17372, 2022.

W. Merrill and A. Sabharwal. The parallelism tradeoff: Limitations of log-precision transformers.
Transactions of the Association for Computational Linguistics, 11:531-545, 2023. doi: 10.1162/
tacl_a_00562. URL https: //aclanthology.org/2023.tacl-1.31/.

W. Merrill and A. Sabharwal. A little depth goes a long way: The expressive power of log-depth
transformers, 2025. URL https://arxiv.org/abs/2503.03961.

17


===== PAGE BREAK =====

T. Mikolov, W.-t. Yih, and G. Zweig. Linguistic regularities in continuous space word representa-
tions. In Proceedings of the 2013 conference of the north american chapter of the association for
computational linguistics: Human language technologies, pages 746-751, 2013.

E. Mossel and 8. Roch. Learning nonsingular phylogenies and hidden markov models. In Proceedings
of the thirty-seventh annual ACM symposium on Theory of computing, pages 366-375, 2005.

N. Nanda, A. Lee, and M. Wattenberg. Emergent linear representations in world models of self-
supervised sequence models. arXiv preprint arXiv:2309.00941, 2023.

T. OLMo, P. Walsh, L. Soldaini, D. Groeneveld, K. Lo, S. Arora, A. Bhagia, Y. Gu, S. Huang,
M. Jordan, N. Lambert, D. Schwenk, O. Tafjord, T. Anderson, D. Atkinson, F. Brahman,
C. Clark, P. Dasigi, N. Dziri, M. Guerquin, H. Ivison, P. W. Koh, J. Liu, 5. Malik, W. Mer-
rill, L. J. V. Miranda, J. Morrison, T. Murray, C. Nam, V. Pyatkin, A. Rangapur, M. Schmitz,
S. Skjonsberg, D. Wadden, C. Wilhelm, M. Wilson, L. Zettlemoyer, A. Farhadi, N. A. Smith,
and H. Hajishirzi. 2 OLMo 2 Furious, 2024. URL https: //arxiv.org/abs/2501.00656.

C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann, A. Askell, Y. Bai,
A. Chen, et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022.

K. Park, Y. J. Choe, and V. Veitch. The linear representation hypothesis and the geometry of large
language models. arXiv preprint arXiv:2811.03658, 2023.

K. Pietrzak. Cryptography from learning parity with noise. In International Conference on Current
Trends in Theory and Practice of Computer Science, pages 99-114. Springer, 2012.

O. Press and L. Wolf. Using the output embedding to improve language models. arXiv preprint
arXiv:1608.05859, 2016.

L. Rabiner and B. Juang. An introduction to hidden markov models. ieee assp magazine, 3(1):
4-16, 2003.

C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J.
Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of
Machine Learning Research, 21(140):1-67, 2020. URL http://jmlr.org/papers/v21/20-074.
html.

N. Razin, S. Malladi, A. Bhaskar, D. Chen, S. Arora, and B. Hanin. Unintentional unalignment:
Likelihood displacement in direct preference optimization. arXiv preprint arXiv:2410.08847,
2024.

M. Rudelson and R. Vershynin. Non-asymptotic theory of random matrices: extreme singular
values, 2010. URL https: //arxiv.org/abs/1003.2990.

C. Sanford, D. Hsu, and M. Telgarsky. One-layer transformers fail to solve the induction heads
task. CoRR, abs/2408.14332, 2024. URL https://doi.org/10.48550/arXiv. 2408. 14332.

C. E. Shannon. Prediction and entropy of printed english. Bell system technical journal, 30(1):
50-64, 1951.

18


===== PAGE BREAK =====

E. Todd, M. L. Li, A. S. Sharma, A. Mueller, B. C. Wallace, and D. Bau. Function vectors in large
language models. arXiv preprint arXiv:2810.152138, 2023.

A.M. Turner, L. Thiergart, G. Leech, D. Udell, J. J. Vazquez, U. Mini, and M. MacDiarmid. Activa-
tion addition: Steering language models without optimization. arXiv e-prints, pages ar Xiv—2308,
2023.

A. Wei, N. Haghtalab, and J. Steinhardt. Jailbroken: How does llm safety training fail? Advances
in Neural Information Processing Systems, 36:80079-80110, 2023.

Z. Yang, Z. Dai, R. Salakhutdinov, and W. W. Cohen. Breaking the softmax bottleneck: A high-
rank rnn language model. arXiv preprint arXiv:1711.08958, 2017.

S. Yi, Y. Liu, Z. Sun, T. Cong, X. He, J. Song, K. Xu, and Q. Li. Jailbreak attacks and defenses
against large language models: A survey, 2024. URL https://arxiv.org/abs/2407 .04295.

Q. Zhang, M. Chen, A. Bukharin, N. Karampatziakis, P. He, Y. Cheng, W. Chen, and
T. Zhao. Adalora: Adaptive budget allocation for parameter-efficient fine-tuning. arXiv preprint
arXiv:2303.10512, 2023.

X. Zhu and G. De Melo. Sentence analogies: Linguistic regularities in sentence embeddings. In
Proceedings of the 28th international conference on computational linguistics, pages 3389-3400,
2020.

A. Zou, Z. Wang, N. Carlini, M. Nasr, J. Z. Kolter, and M. Fredrikson. Universal and transferable
adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15048, 2023.

A Experimental Details

A.1 Additional experimental details from Section 3.1
A.1.1 Setup for experiments

Choice of datasets and models. We considered the following choices for the autoregressive
language model M and the dataset D:

1. The model M was chosen amongst OLMo-1b, Olmo-7b (OLMo et al., 2024), Llama-1b (et al.,
2024), Mamba-1.4b (Gu and Dao, 2024), Gemma-1b (et al., 2025). Moreover, as a baseline
comparison, we considered the checkpoint for OLMo-1b at time step 0 of training.

2. The dataset D was chosen amongst: the wiki, arxiv, starcoder subsets of the olmo-mix-1124
(OLMo et al., 2024) training dataset; the math subset of the dolmino-mix-1124 dataset (OLMo
et al., 2024); and the c4 dataset (Raffel et al., 2020).

Next, we detail the construction of the sets H,F from a dataset D. The datasets D we used
each consisted of a collection of many sequences, i.e., we have D = fy), yy yy for some large
N.

19


===== PAGE BREAK =====

Construction of histories. To generate a set H of histories of size n, we fixed parameters
fmin, max. We first chose a random subset S of n elements of D of length at least max (as
measured by the number of characters).° For each sequence y € S, we selected a uniformly random
contiguous subsequence of y of length €max. We then chose ¢ ~ Unif|¢min, max], and added sequence
yi to H (rounded to the nearest full word). We opted to measure length (and perform truncation)
with respect to characters (as opposed to tokens) since we wish to use the same set of histories
(and futures) for different models, which typically have different tokenizers.

Construction of futures. The set of futures F was generated identically to the set H of histories,
with the exception that in the context of the previous paragraph, ¥¢.\y| (where |y| denotes the length
of y) was added to F.

Computation of the exponent a. To compute the power law exponents a shown in Figures 2
and 6, we used least-squares regression, as follows. Given a matrix ZL € R"*™ (where n < m), we
let its singular values, arranged in decreasing order (excluding the largest) be o1,...,@,-1. We
performed a 2-dimensional linear regression with covariates (log(i/n),log((n — 7)/n)) and labels
log o;, for i € [n — 1]. Adding the covariate log((n — 7)/n) yielded a better fit as it accounts for the
precipitous drop in o; as i approaches the number of rows n (when the power law as described in
Section 3.1 ceases to describe the decay of the singular values o;). The resulting coefficient for the
log((n — i)/n) covariate was always very small, meaning that its effect was negligible for i <n.

Moreover, in the regression, the data point corresponding to index 7 was weighted by 1/7. We
chose this weighting (as opposed to, say, uniformly weighting all data points) to account for the
logarithmic scale of the covariates in terms of their dependence on 7.

A.1.2 Details for singular value plots

Recall that for an integer k, Cag~(H,F) denotes the sub-matrix obtained from Lyy(H,F) by
selecting, for each f € F, the columns (f,z) indexed by tokens z which are one of the k most
likely next tokens following f, with respect to M. Figures 2 and 6 show the singular values
for logit matrices Lyy50(H,F) for fixed sets H,¥F of histories and futures of size approximately
10000,° as generated according to the procedure from Appendix A.1.1. For this procedure we chose
€min = 50, max = 200. The figures also show the singular values for £Lyy50(Hi, Fi) (“Downsized by
ix”), where H; (resp., F;) contains the first |H|/Vi (resp., |F|/Vi) elements of H (resp., F), for
i € {2,4,8,16}. Thus the number of elements of £ay50(Hi, Fi) is roughly 1/7 that of Layso(H, F).
Singular values were normalized by the square root of the number of elements of each matrix.

Next, Figure 8 shows the singular values of Lyayso(H,F) when M was OLMo-1b and H,F
were obtained via the procedure described in Appendix A.1.1 when the dataset D was taken to be
arxiv, starcoder,math,c4. We remark that for these plots the sets H,* were taken to be of size
only 4000.

>For some of the larger datasets, we used a buffer size of at least 10° to stream the dataset, so technically the set
of histories was not a uniformly random subset.

®°In particular, we generated sets H,F of size exactly 10000 and then filtered out 0-length futures, of which there
were approximately 60.

20


===== PAGE BREAK =====

A.1.3 Details for KL divergence plots

Figures la, 7 and 9 show the average KL errors between the logit matrices considered in Figures 2,
6 and 8, respectively, to low-rank approximations. Low-rank approximations were computed using
an approximate singular value decomposition (via torch.svd_lowrank) and then zeroing out all
singular values apart from the largest r ones. (Note that, for an exact SVD, this procedure gives
the closest rank-r approximation in Frobenius norm.) Since we are considering the submatrices
L£_r50(H, F), these KL divergences are taken with respect to distributions restricted to the 50 most
likely tokens per future.

The dashed line at the top of the figures represents the following rank-1 baseline: for each future
f, we simply use the next-token distribution for f, i-e., Pra,|- | f], to approximate all entries (i.e.,
rows) corresponding to f. We remark that this approximation is in general not the optimal rank-1
approximation to the logit matrix (with respect to Frobenius norm).

A.1.4 Supporting theoretical results

Below we collect a couple of standard facts which we use in our discussion of the experiments. For
completeness, we provide their proofs (which are standard).

Fact A.1 (Phase transition in singular value decay). Suppose L € R"*™ with n < m, and its
singular values 01,...,0n satisfy oj x C-71-%, for some constants C,a > 0. Then:

1. Ifa > 1/2, then for anyr EN, there is a matrix A of rank r for which |\|L — Al|p < ||L||r - O(r2~°).

(Here || - ||r represents the Frobenius norm.)

2. Ifa < 1/2, then any matriz A of rank r < s7i=aay Satisfies ||L — Alp > Q(||LI|r).
In the above statements, the O(-),Q(-) hide absolute constants.

Proof. The fact is standard, but we provide the proof anyways. Let us suppose that Co, C, > 0 are
so that Cp -i-% < 0; < Cy -2-% for all 7.

Case 1: a > 1/2. First suppose that a > 1/2. We may write the SVD of Las )>;_, oi Uiv; , where
{uj} and {v;} are orthonormal bases of R” and R™, respectively. Note that ||L||p = \/>7_, 0? >

Co-2(y/s44).
Fix any r EN, and let A= )0j_, ojujv) . Then ||L — Allp = \/S0i.4,07 < C1 O (   =:

It follows that
IZ= Alle. 5 (@ | ri) |
|||         Co

where the O(-) hides dependence on an absolute constant, as desired.

Case 2: a < 1/2. It is a standard fact that for any r, the best rank-r approximation to a matrix
L is given by truncating its SVD at rank-r, i.e., for any rank-r matrix A, we have that

;
|Z — Alle > ]L— So ojusvt
i=1

F


===== PAGE BREAK =====

to                        =—— Downsized by 16x              to!                        == Downsized by 16x
v                           == Downsized by 8x             ()                           == Downsized by 8x
>}                                                                 >}
g                           =—— Downsized by 4x             g                           =—— Downsized by 4x
10°                    —— Downsized by 2x           ar                    —— Downsized by 2x
3S                           == Downsized by 1x            3S                           =——= Downsized by 1x
a                                                                 a
&                                                                 &
w                                                                 w
TD 107                                                                T 107
N                                                                 N
£                                                                 £
8 10-2                                                                8 107?

10°           10!           10?           103                          10°           10?           10?           103
Singular Value Index                                       Singular Value Index
(a) OLMo-1b; a & 0.561                        (b) Gemma-1b; a ¥ 0.565

10                        =—— Downsized by 16x                                        =—— Downsized by 16x
¥                           =—— Downsized by 8x             ¥                           =—— Downsized by 8x
g                           —— Downsized by 4x             g                           —— Downsized by 4x
s  10°                        =—— Downsized by 2x             aot                        =—— Downsized by 2x
3                           =——= Downsized by 1x            3                           =——= Downsized by 1x
[=                                                                 [=
no...                                                             1%)
oi                                                                   Be]
co)                                                                 co)
N                                                                 N
rd                                                                 rd
E                                                                 E
610?                                                                   iS)
2                                                                 2 10

10°           10!           10?           103                          10°           10!           10?           103
Singular Value Index                                       Singular Value Index
(c) Llama-1b; a = 0.573                  (d) OLMo-1b (ckpt 0); a = 0.374
lot                      =—— Downsized by 16x

—— Downsized by 8x
=—— = Downsized by 4x

gz

=—— Downsized by 2x
=—— Downsized by 1x

107

Normalized Singular Value

5

10°             107             107             10?
Singular Value Index

(e) Mamba-1.4b; a & 0.563

Figure 6: Singular values for £Lyz,50(H, F) for various models M; see Appendix A.1.2.

22


===== PAGE BREAK =====

eee ee ee ee ee ee         ee ee ee eee ee ee ee ee
4x107
wo 3x107                                                              wo
iS)                                                                         iS)
=                                                                         =
co)                                                                         co)
Pox102                                                                   2
co)                                                                         co)
2                                                                         2
a        == Downsized by 16x                                 a ao | == Downsized by 16x
x        === Downsized by 8x                                  x      === Downsized by 8x
io? | === Downsized by 4x                                          === Downsized by 4x
== Downsized by 2x                                          == Downsized by 2x
=== Downsized by 1x                                          === Downsized by 1x
10!                       10?                                             102                       10?
Rank                                                 Rank
(a) OLMo-1b                                                   (b) Gemma-1b
oe eee ee eee es      4x 10-2 —— ee eee
3x 10-7
7)                                                                     7)
1S)                                                                     1S)
=                                                                     =
co)                                                                     co)
>                                                                    >
co)                                                                     co)
2                                                                    2      1
a      === Downsized by 16x                               a 2x20") =e Downsized by 16x
10-?
x      == Downsized by 8x                                x        == Downsized by 8x
=== Downsized by 4x                                          ==—= Downsized by 4x
=== Downsized by 2x                                          ==—= Downsized by 2x
=== Downsized by 1x                                          === Downsized by 1x
10?                       10?

10°                         10?

Rank                                                 Rank

(c) Llama-1b                            (d) OLMo-1b (ckpt 0)

Downsized by 16x
Downsized by 8x

=——
=—
107 + === Downsized by 4x
—
—

KL Divergence

Downsized by 2x
Downsized by 1x

10°                                    10?

Rank

(e) Mamba-1.4b

Figure 7: Average KL divergence to a low-rank approximation for various models M; see Appendix A.1.3.

23


===== PAGE BREAK =====

Normalized Singular Value

Normalized Singular Value

10°

10°

10-2

10

10°

1077

10-7

10-3

Downsized by 16x
Downsized by 8x
Downsized by 4x
Downsized by 2x
Downsized by 1x

10°             10!             10?             103
Singular Value Index

(a) arxiv;

a & 0.598

Downsized by 16x
Downsized by 8x
Downsized by 4x
Downsized by 2x
Downsized by 1x

10°             107             10?             107
Singular Value Index

(c) math;
a = 0.590

Normalized Singular Value

Normalized Singular Value

to                                 Downsized by 16x
Downsized by 8x
Downsized by 4x
Downsized by 2x

Downsized by 1x

10°

107°

10-2

10°             10!             10?             103
Singular Value Index

(b) ¢4;
a © 0.539

10%                                 Downsized by 16x
Downsized by 8x
Downsized by 4x
Downsized by 2x

Downsized by 1x

10°

10°             107             10?             107
Singular Value Index

(d) starcoder;
a & 0.589

Figure 8: Singular values of the logit matrix Fyyz50(H,*) when H,F are chosen from various datasets as
detailed in Appendix A.1.1. The model M was OLMo-1b.

24


===== PAGE BREAK =====

Figure 9:
Figure 8.

KL Divergence

HVT

10-7

KL Divergence

HVT tl

10-2

Average KL errors to a low-rank approximation, for the same logit matrices as described in

Downsized by 16x
Downsized by 8x
Downsized by 4x
Downsized by 2x
Downsized by 1x

102                                                     102

Rank

(a) arxiv

Downsized by 16x
Downsized by 8x
Downsized by 4x
Downsized by 2x
Downsized by 1x

10°                                                     10?

Rank

(c) math

4x 10-2

3x10

6x 10-2

o
UV
Cc
o
2)
o
>
[a)
x
o
UV
Cc
o
a
2
oO
2
[a)
x

25

2x10

S

HTT

Downsized by 16x
Downsized by 8x
Downsized by 4x
Downsized by 2x
Downsized by 1x

102                                                      10?

Rank

(b) c4

Downsized by 16x
Downsized by 8x
Downsized by 4x
Downsized by 2x
Downsized by 1x

HTT

10°                                                      107

Rank

(d) starcoder



===== PAGE BREAK =====

But |Lllp < Ci: o(  te). meaning that for r < si77taay, we have [ile > O(Co/C1), as

desired.

Fact A.2. Consider sets H,F of sizes n,m respectively. Then for any matrices A, A’ € REX(FX®) |

it holds that

1
SS" Dx. (softmax(Ap, f)||softmax(A), -)) < 5l4- Al ||%,.
hEH, fEeF

Proof. It follows immediately from the definition of average KL divergence that it suffices to prove
the following fact: if v,v’ € R¢, then letting p = softmax(v), p! = softmax(v’) (so that p,p’ € A%),
then we have Dx. (pl||p’) < ||v — v’||3. To prove this fact, let us define F(v) = log yw e”. Thena
direct computation yields

Du (pllp') = F(v) — F(v') — (VF(v), v= v).                             (1)

Next, for any x € R®, for q = softmax(x) we have V?F (a) = diag(q)—qq' ~ Iq, ie., F is 1-gradient
Lipschitz. But this yields that the right-hand side of Equation (1) is bounded above by $-||v—v'||3,
as desired.

A.1.5  Ablations for the parameter k

Recall that our experiments exhibiting approximate low-rank structure of the logit matrix actually
apply to the sub-matrices Lyy,(H, F) described in Section 3.1, for k = 50. In Figure 10, we present
two modifications to this approach (all with OLMo-1b): in Figures 10a and 10b we measure the
low-rank approximability of Liv200(H,F), for sets H,F. Due to memory limitations, we took
|H|,|F| to be of size approximately 2000 in Figure 10a and to be of size approximately 5000 for
Figure 10b.

Next, in Figures 10c and 10d, we measure the low-rank approximability of a submatrix £ u50(H, F)
obtained from Ly (H,F) by sampling randomly 50 tokens per future and selecting the correspond-
ing columns of Lyy(H,F). For these figures we used H, F of sizes approximately 4000.

All of the aforementioned ablations show similar evidence of low-rank structure similarly to the
logit matrices Lyy50(H, F). We remark that, in general, as the size of H, F are increased, the fitted
power law exponent a describing the decay of the singular values tends to decrease slightly, perhaps
because the effect of the slight “hump” for the first few singular values is diminished. This may
explain why the exponents a in Figures 10a and 10c are slightly smaller than in Figure 6a.

A.2 Subspace comparison

Figure 4 displays the principal angles between low-rank approximations of £Liy.50(H, F) and Liy50(H, Fre")
where the model M was OLMo-1b, H,F were generated from wiki as described in Appendix A.1.1,
with |H| = 10000 and where F, F"°"*"5* were each of size approximately 5000." Low-rank approx-
imations were computed by truncating (approximate) singular value decompositions, as described
in Appendix A.1.3. The dashed lines show the principal angles between a pair of independent
uniformly random subspaces of the indicated dimension. It is evident that the column spaces of the
logit matrices have significantly more overlap than what would be expected from random subspaces.

"We started with sets F, F"°""®* of size 5000 and then removed a small number of 0-length futures.

26


===== PAGE BREAK =====

102               Downsized by 16x          eee ee ee
fo)                           —— Downsized by 8x
=}
ot                           =—— = Downsized by 4x
>                                         .
& 10°                    =—— Downsized by 2x           ¥
3                           —— Downsized by 1x             5
a
<                                                                 2
a                                                                 3
@ 10                                                             a      === Downsized by 16x
i                                                              x      =e Downsized by 8x
E                                                                     == Downsized by 4x
2 oe                                                             1077 === Downsized by 2x
=== Downsized by 1x
10°            10!            10?            103                              102                      10?
Singular Value Index                                                                Rank
(a) k = 200 sing. vals; a = 0.543                (b) k = 200; Avg. KL Divergence
Downsized by 16x          BHHEHEHEEEHEEEEEeeee
fo)                           =——= Downsized by 8x
3 10°
at                           =—— = Downsized by 4x
>                                         .
s                           =—— Downsized by 2x             v
3S                           —— Downsized by 1x             c
Fo}                                                                 co)
<                                                                 >
in 107+                                                                   g
g                                                              a      === Downsized by 16x
3                                                              < 107} == Downsized by 8x
E  4                                                                  ===—= Downsized by 4x
© 10
2                                                                     === Downsized by 2x
=== Downsized by 1x
10°           10?           10?           107                                  10?                      10?
Singular Value Index                                                                Rank
(c) k = 50 w/ random tokens, sing. vals;        (d) k = 50 w/ random tokens; Avg. KL
a & 0.527                                             Divergence

Figure 10: ((a), (b)): Low-rank approximability of Laz200(H, H). (e), (d): Low-rank approximability of
Lus0(H, F); see Appendix A.1.5.

27


===== PAGE BREAK =====

10

Rank: 10                                v0                            Rank: 10

Rank: 88                                                                             Rank: 88
oe                                                                               oe                                 Rank: 166                                    oe                                 Rank: 166
Rank: 244                                                                                 Rank: 244
Rank: 322                                                                              Rank: 322                                                                              Rank: 322

Random (Rank: 10)
Random (Rank: 88)
Random (Rank: 166)
Random (Rank: 244)
» Random (Rank: 322)

» Random (Rank: 10)
» Random (Rank: 88)
* Random (Rank: 166)
 Random (Rank: 244)
» Random (Rank: 322)

Random (Rank: 10)
Random (Rank: 88)
Random (Rank: 166)
Random (Rank: 244)
» Random (Rank: 322)

Cosine of Principal Angle

Cosine of Principal Angle
Cosine of Principal Angle

t
ty
ty
ty
eal
ul
t
ai
4
ul
ul
t
L
t
im
t
ty
it
a
t
ul
t
I
L

0.0                                          00

200         250         300                                                         0           50          100         150         200         250         300                                                         °           50          100         150         200         250         300

(a) Gemma-1B vs Mamba-1.4B           (b) Gemma-1B vs Olmo-7B             (c) LlaMA-1B vs Gemma-1B

10                                                     *                   — Rank: 10                         10                   — Rank: 10
—— Rank: 88                                           —— Rank: 88
oe                                                                    oe                        —— Rank: 166                              oe                        —— Rank: 166
2                                                      2                    — Rank: 244                       2                    —— Rank: 244
=                       Rank: 322                      =                    — Rank: 322                      =                    —— Rank: 322
3S                                                    3S                                                    3S
Bos                    » Random (Rank: 10)                B06                  ==» Random (Rank: 10)                Bos                   =» Random (Rank: 10)
ro                                                    ro                                                    ro
£                     » Random (Rank: 88)                ©                   ==» Random (Rank: 88)                ©                    =» Random (Rank: 88)
a                        » Random (Rank: 166)                  So                     ==» Random (Rank: 166)                  Soa                        » Random (Rank: 166)
°                    » Random (Rank: 244)              °                  — =» Random (Rank: 244)              °                    » Random (Rank: 244)
5                    » Random (Rank: 322)                  ==» Random (Rank: 322)                    » Random (Rank: 322)
So2                                    \       \                       S02                                                                          So2                                    \       \
0.0                     _—e   —-                  0.0                                                   0.0                     a   7>s~
ao «280 «WO                      r    so o~CSOSSCDSSCSSCSC                      3    so «wo~*C«SOSC«OS*«SCSC«
Index                                                 Index

=
cs
ey
Ss
P
un
w
<
n
Ss
2
8
ion
a
=
iw
w
S
cs
ey
Ss
>
un
w
<
n
c
3
?
un
w
eo
eh
so}
+
&
S
s
2
Ss
is
un
w
<
n
g
3
?
“I
w

Rank: 10                              0                      — Rank: 10                              v0                           Rank: 10
Rank: 88                                           — Rank: 88                                               Rank: 88
oe                      Rank: 166                        oe                  — Rank: 166                        oe                      Rank: 166
Rank: 244                                        — Rank: 244                                            Rank: 244
Rank: 322                                           —— Rank: 322                                              Rank: 322

» Random (Rank: 10)                         0.6

» Random (Rank: 88)

» Random (Rank: 166)
» Random (Rank: 244)
» Random (Rank: 322)

—=—- Random (Rank: 10)
=» Random (Rank: 88)
—-—» Random (Rank: 166)
—-—: Random (Rank: 244)
==» Random (Rank: 322)

» Random (Rank: 10)
=» Random (Rank: 88)
» Random (Rank: 166)
» Random (Rank: 244)
» Random (Rank: 322)

04

Cosine of Principal Angle

Cosine of Principal Angle
Cosine of Principal Angle

Me
ty
iy
iy
iy
bev
L
t
t
ul
l=
a
t
tt
iy
te
t
t
t
L
oat

00

(g) Olmo-1B vs Gemma-1B              (h) Olmo-1B vs LlaMA-1B             (i) Olmo-1B vs Mamba-1.4B

v0                   —— Rank: 10                          20                   —— Rank: 10                          v0                   —— Rank: 10
—— Rank: 88                                          —— Rank: 88                                          —— Rank: 88
oa                      — Rank: 166                           Boa                      — Rank: 166                           oa                      — Rank: 166
2                   — Rank: 244                     2                   — Rank: 244                      2                   — Rank: 244
=                   —— Rank: 322                      =                   —— Rank: 322                      =                   —— Rank: 322
Bos                   ==» Random (Rank: 10)                 Bos                   ==» Random (Rank: 10)                 Bos                   ==» Random (Rank: 10)
2                    © Random (Rank: 88)               E                  ==» Random (Rank: 88)               E                   =» Random (Rank: 88)
e                        » Random (Rank: 166)                  e                     ==» Random (Rank: 166)                  e                        » Random (Rank: 166)
S                    » Random (Rank: 244)              °                  ==» Random (Rank: 244)              °                    » Random (Rank: 244)
5                    » Random (Rank: 322)                  ==» Random (Rank: 322)                    » Random (Rank: 322)
Boa                                                                          Baz                                                                          Boa
00                                                   00                                                   00
3    so~C«o~*~C«OS*C«SSCOSC                      3    so aot                      3    so~C«wo”~*C«OS*~«SCtSSC
Index                                                 Index                                                 Index

(j) Olmo-1B vs Olmo-1B (ckpt 0)            (k) Olmo-1B vs Olmo-7B           (1) Olmo-1B (ckpt 0) vs Gemma-1B

Rank: 10
Rank: 88
oe                              Rank: 166                                +                        — Rank: 10                                 r0                             Rank: 10
2                       Rank: 244                                           —— Rank: 88                                               Rank: 88
=                                Rank: 322                               wos                        — Rank: 166                               © og                              Rank: 166
Bos                         Random (Rank: 10)                   2                       — Rank: 244                          2                           Rank: 244
E                       Random (Rank: 88)                S                    — Rank: 322                      S                       Rank: 322
ia                       © Random (Rank: 166)                Bos                   ==» Random (Rank: 10)                 Bos                     “Random (Rank: 10)
see                   © Random (Rank: 244)              E                  ==» Random (Rank: 88)               E                   =» Random (Rank: 88)
5                                » Random (Rank: 322)                       Sos                          ==» Random (Rank: 166)                       oom                              » Random (Rank: 166)
G02                                                °                   —-— Random (Rank: 244)               °                     + Random (Rank: 244)
5                     © Random (Rank: 322)               5                     © Random (Rank: 322)
Boz                                                                 Boa
00                                                                                                                             |
Index                                00                                                   00                       >=     =
g    so asoSCSOSSCDSSCOSSCSC                      3    sowo~S~CSOSSC«SSC*«iSCSC
Index                                                 Index

(m) Olmo-1B (ckpt 0) vs Mamba-
1.4B

n) Olmo-1B (ckpt 0) vs Olmo-7B         (0) Olmo-7B vs Mamba-1.4B

Figure 11: Principal angles between column spacesjef logit matrices across all pairs of models; see Ap-
pendix A.2.


===== PAGE BREAK =====

History

Closest history

Investigations personnel in 1927. The
name, derived from the eighth letter of the
Greek alphabet, appears to have been first
used on a 1946 Argentine government chart
following surveys

on Honest Jon’s capitalised on this with
Theo Parrish, Rashad & Spinn, Ricardo

Shiroro Airfield is an airstrip serving the
village of Shiroro and the Shiroro
Hydroelectric Power Station in the Niger
State of Nigeria. The runway is

to numerous publications, and supported
the development of a range of new PES
schemes including the BioCarbon Fund at
the World Bank and the Mexican PES

Thereupon, Gangaram slays out Janardhan
Seth, perceiving the dirty deed angered
Rambabu reaches Janardhan Seth’s

process of removing government-controlled
entry and price restrictions on airlines
affecting, in particular, the carriers

On Spitfires. Scotland was in range of Nazi
Germany’s long-range bombers and
reconnaissance aircraft. The Luftwaffe’s

U-boat built for the Nazi Germany’s
“Kriegsmarine” for service during World
War II.

European works, classical Chinese works
and Nom works into Quoc Ngu - modern

appointed Assistant Chaplain at Geelong
Grammar School from 1959 to 1961, vicar
of Romsey and Sunbury with Lancefield
from 1961 to 1964

(BIT) was created on 9 December 1898, in
response to the Bombay plague epidemic of
1896. It was created

for Nelly Furtado, N-Dubz, Nina Sky, Lil’
Flip, Noel Gourdin, Starboy Nathan,
Shawnna, Jimmy Cozier, Alison Hinds,
Jazmine Sullivan, I 20, Cory Lee, Chris
Webby, City

Canadian Forces Base located immediately
south of the town of Chatham, New
Brunswick, Canada. Parts are now
operating as Miramichi Municipal Airport
since 1974 with a partial runway

as a mitigation bank providing ecosystem
services to the public in the form of
Environmental

stealing the idol’s jewellery and his wife is
accused of being a witch. They are killed
by a mysterious person after which
Chandni’s father Yash Narayana Vashisht
becomes the Mahant. Dev escapes

stated model. It maintains close ties to
other Green parties

minor improvements that increased their
anti-aircraft capabilities. Their crew
numbered 228 officers and enlisted men.
The

floatplane of the 1910s produced by
Flugzeugbau Friedrichshafen.

supported the creation of the Empire of
China and the 1917 Manchu

the Young Women’s Christian Association
of New Zealand. She was a member of its
Dunedin board from 1930 until 1944,
Dunedin

Table 1: For 10 values of h € H (left column) we display the history h’ € H,h’ 4 h minimizing

the distance between the respective rows of the

gits matrix, ie., ||La({h}, F) — Lu ({h’}, F)le.

Examples were not cherry-picked (they correspond to the first 10 examples in the set H described

in Section 3.1, as obtained from wiki).


===== PAGE BREAK =====

In a similar manner, Figure 11 shows the principal angles between low-rank approximations
of Linso(H,F) and Lay 50(H,F) for each pair (M,M"‘) of models described in Appendix A.1.1.
For these matrices, we used the same sets H,F deriving from the wiki dataset as were used in
Appendices A.1.2 and A.1.3. Note that all pairs of models (M, M’) show significant overlap in the
column spaces of their logit matrices, with the exception of when either M or M’ is the untrained
variant of OLMo-1b. This is unsurprising, given that the latter cannot be expected to capture any
semantic information in sequences. Interestingly, there is still a bit more overlap than would be
expected from random subspaces. We believe this may be because of the nature of the transformer
architecture: for instance, if two sequences are very close (e.g., differ in a single token), then it is
reasonable to expect that the next-token distributions induced by even a transformer with random
weights are somewhat close, due to the nature of attention. We leave further investigation of this
point to future work.

A.3 Linear generation

The experiments using LINGEN in Section 3.3 make use of OLMo-1b and generate for a total of 15
tokens. Below we provide details pertaining to how the coefficient vector v was chosen for each of
Option 1 and Option 2 described in Section 3.3:

Option 1. For this case, we chose 40000 histories and 10000 futures from the wiki dataset, as
described in Appendix A.1.1. We chose the lengths min = 8, max = 30, but now measured the
lengths with respect to tokens (not characters), since we only intend to use the resulting histories
and futures for a single model. As in the previous sections, because the token space © is extremely
large, we cannot perform the regression described in Section 3.3 on the entire logit matrix Lyy(H, F),
so we instead used the submatrix £Lyy50(H, F) as described in Section 3.1.

Option 2. For this case, we generated sets H,F of histories and futures as in Option 2, but
then randomly permuted all tokens amongst all elements of each of # and ¥, which yielded sets
Hronsense | Fnonsense ‘The remaining details of the experiment are identical to Option 1.

Baselines. In Figures 1b and 5 we compare the performance of LINGEN to the following baselines:

e Single-token version of Lingen: this may be viewed as using LINGEN but where the
matrix Ly 50(H,F) was replaced with £Lyy(H, {Null}), i-e., the full logit matrix where there
is a single future, namely the empty future.

e Generation from a short context: we generate from OLMo-1b using a context window
of 5 tokens (i.e., we mask out all tokens more than 5 units in the past).

e Intermediate training checkpoint: we generate from OLMo-1b using the checkpoint
“stagel-step1907359-tokens4001B” which corresponds to the end of Stage-1 pretraining.

B Comparison to Natural Baselines

In this section, we briefly discuss comparison of the low rankness of the extended logit matrix to
natural baselines. First, let us consider a random model for comparison. Recall that the extended

30


===== PAGE BREAK =====

Lingen

Third Sea Lord was given the command
upon taking leave from the Admiralty. He
hoisted his flag in “Bacchon” (1874), which
was in the Mediterranean under Rear
Admiral

in 2007 in Rajasthan, India. Krishna Bhatt
has been twice the recipient of the Award
for literary Excellence and has been to
Ghana, Switzerland,

“and Katy Gibson in “Gig?” had the
world’s best-known actors and pop singers
Peter O’To

2018 including Jago Wali Raat and her first
devotional song Tor Dita Lalan Nu. For the
song God Save India, she uses traditional
instruments like Bhajan

KSTQ (93.5 FM) is a radio station licensed
to Stuart, Oklahoma, United States. The
station is owned by KFBL Radio and is
currently simulcasting KXLP

Lingen with single-token logit matrix

Third Sea Lord was given the command
upon taking leave from the Admiralty. He
hoisted his flag in “Bacchusa in 1881 and
1892 compilation also showed that the

in 2007 in Rajasthan, India. Krishna Bhatt
has been twice the winner of the Golden
Mapuhal Trophy at the General Bob badi of

“and Katy Gibson in “Gig? and “Diva”
were produced, and they were thus of the

2018 including Jago Wali Raat and her first
devotional song Tor Dita Lalan Nu.
Rebecca and Mr. Costa confess to each
other that they were falling in

KSTQ (98.5 FM) is a radio station licensed
to Stuart, Oklahoma, United States. The
station is locally called “The Rock,” is
currently operated by Axion and E

Table 2: Sample generations from LINGEN (Option 1) and single-token baseline. Prompts are
shown in gray and italicized, and continuations are shown in black. Samples were not cherry-
picked, i.e., the first generation from each prompt was selected. The generations from LINGEN
(left) all read well and make clear use of context in the prompt; in contrast, while generations from
LINGEN with the single-token logit matrix (right) typically are reasonable for the first token or few,

they become derailed soon thereafter.

Algorithm 1 LINGEN Algorithm

: function LinGenjy(H, v,m)
for t=1,2,...,m do

Compute logits, = v! - Lig(H, {214-1}).

: Input: Language model M, histories H C b*, vector v € R”.
: Input: Parameter m (number of new tokens to generate)

> (Lur(H, {21:1-1}) is the logits matrix where

the set of futures is the singleton sequence 21-1.)

end for
Return: the sequence (z1,..
end function

Sample next token z ~ softmax(logits,)

31


===== PAGE BREAK =====

Lingen generations with #°°"%*"*¢

Third Sea Lord was given the command upon taking leave from the Admiralty. He hoisted
his flag in “Bacchill” confusing manoeuvre that involved making the windward course.
Third Sea Lord was given the command upon taking leave from the Admiralty. He hoisted
his flag in “Bacch” for the first time. After Sapp didn’t take part in

Third Sea Lord was given the command upon taking leave from the Admiralty. He hoisted
his flag in “Bacchel” and joined with her and the USS Vixen, a

in 2007 in Rajasthan, India. Krishna Bhatt has been twice the National Head of Dairy
Labelling 2004 and 2014. He

in 2007 in Rajasthan, India. Krishna Bhatt has been twice the national shortlisting for the
International Youth Olympic Festival (TYOOF)

in 2007 in Rajasthan, India. Krishna Bhatt has been twice the author of a book entitled
‘Chaerbato Jaataa Aap

* and Katy Gibson in “Gigi. Consists of: -lrb- lead -rrb-
* and Katy Gibson in “Gigi, for which she received a nomination at the 2020 Jogja Music

* and Katy Gibson in “Gig. Music career. In addition to her television work, Bryant has
released several

2018 including Jago Wali Raat and her first devotional song Tor Dita Lalan Nu Ghareba
Hae. A new format was created by exposing a new

2018 including Jago Wali Raat and her first devotional song Tor Dita Lalan Nu, Ranthi
Narayan Thakkar are known for their voice and

2018 including Jago Wali Raat and her first devotional song Tor Dita Lalan Nu Ram”.
She made her film debut opposite Satish Kekar in Raj

KSTQ (93.5 FM) is a radio station licensed to Stuart, Oklahoma, United States. The
station is a 24/7 transmitter reports that broadcasts to Tulsa, Page,

KSTQ (93.5 FM) is a radio station licensed to Stuart, Oklahoma, United States. The
station is locally owned by Quad Cities Radio and is maintained by Educational Insights.
In

KSTQ (93.5 FM) is a radio station licensed to Stuart, Oklahoma, United States. The
station is a reading of a Victim News radio station licensed to Tulsa, Oklahoma broadcasts

Table 3: Sample generations from LINGEN and single-token baseline with H""*"** (i.e., Option
2), for the same 5 target prompts as in Table 2. Prompts are shown in gray and italicized, while
LINGEN’s generations are in black. While not all of the generations are natural continuations of
the target prompt, most of the generations show clear use of information from the prompt.

32


===== PAGE BREAK =====

logit matrix Lp(H,F) is a |H| x |F| - |X| matrix. A matrix of the same dimension with i.i.d.
Gaussian entries, denoted by G (i.e., Gi; ~ N(0,1)), has a well-understood asymptotic spectrum
given by the Marchenko-Pastur law. Further, non-asymptotic bounds on the singular values of G
are also known (See e.g. (Rudelson and Vershynin, 2010)). In particular, the results suggest that
the singular values of G are all of the order O(,/n) where n = max{|H|,|F|-|5|} with even the
deviation of the smallest singular value from ,/n being of the order O(n!/®). This indicates that
the singular values of G do not decay rapidly and are all of the same order and hence G is not
approximated well by a low-rank matrix.

More, interestingly in order to observe power law decays in the singular values, as noted empir-
ically in Section 3.1, one needs to consider matrices with dependent entries. An interesting future
direction suggested by our work is to potentially find interesting random matrix models that can
be both used to explain the power law decay in the singular values of the extended logit matrix,
while also helping us understand the underlying structure in natural language.

A second natural baseline to compare the low-rankness of the extended logit matrix that arise
purely from the low-rank structure of the single step logit matrix. To flesh this out consider, the
matrix L = Ly(H,F) for H = dS! and F = Nulland set L = Ly (H, F) for H = S"/? and F = d!/?,
The choice of t/2 was made for simplicity, and the argument can be easily extended to other splits
of t. Now, from the argument regarding the low-rankness of the single step logit matrix, we have
that rank(L) < d but for the sake of argument let us assume that d = 1. This indicates that the
rows of LZ are all multiples of each other, again for simplicity assume that it is a constant multiple
of e; the first standard basis vector. Understanding the struture of L tells us that the rows of L
are concatenations of the rows of LZ corresponding to splits of the history into two halves. But,
since the coefficient corresponding to each row of L was arbitary, the rank of L corresponds to the
rank of an arbitrary matrix of dimension ||'/? x |X|‘/? which is exponentially large in t. Thus,
the low-rankness of the single step logit matrix does not imply any non-trivial upper bound on the
rank of the extended logit matrix. In particular, this can be seen as a strong argument for the
claim that low-rankedness of the extended logit matrix is an extremely surprising phenomena that
points towards the presence of interesting structure in natural language and models thereof.

C Deferred Proofs of Theoretical Results

As a reference, we first state the definition of an Input Switched Affine Network (ISAN) by Foerster
et al. (2017). Note that the difference compared to a time-varying ISAN in Definition 4.2 is that
the transitions depend only on the current token but not on the timestep.

Definition C.1 (Input Switched Affine Network (ISAN)). An Input Switched Affine Network
(ISAN) is defined by matrices A, € R?%* for each z € XS and a matric B € R™*4 and an initial
state x9 € R¢. It defines a distribution over sequences of tokens at each timestep t = 1,2,...:

e Sampling: token z is sampled from softmax(Bat_1).
e State Update: the hidden state is updated as x, = Az,Xt-1.-

First, we show an equivalence between an ISAN and a time-varying ISAN up to a factor of T
in the hidden dimension.

33


===== PAGE BREAK =====

Fact C.2 (Reducing a Time Varying ISAN to an ISAN). For any time-varying ISAN with hidden
dimension d that generates a distribution over sequences of tokens of length T, there exists an ISAN
with hidden dimension Td that generates the same distribution.

Proof. Let the initial state of the time-varying ISAN be zo € R@ and its transition and observation
matrices be A, € R&®?, B, € R**? for z € S,t € [T]. To express this as a (time-invariant) ISAN,
we build a Td-dimensional hidden state consisting of T’ separate d-dimensional blocks. We can
then aggregate the transition and observation matrices across the different timesteps by defining
for each token z € &,

0     O -::-     0     0
Az: 0            0 0
0 0 ... Arr 0

B={|B, By ...Br].

We let the initial state be (xo, 0,...,0) ie. zo on the first d coordinates and 0 everywhere else. It is
immediate by construction that this ISAN is equivalent to the time-varying ISAN because at each
timestep t, the hidden state x, of the original time-varying ISAN is simply stored in the t + 1st
d-dimensional block and is moved to the next block after a transition (the state becomes identically
0 after the last transition). Thus, this ISAN generates the exact same distribution over sequences
of length T, as desired.

C.1 Equivalence Between Time-varying ISAN and Log Logit Rank

This subsection is devoted to proving Theorem 4.3. We first show one direction, that low logit rank
implies expressibility as a time-varying ISAN.

Lemma C.3 (Low Logit Rank Implies Time-Varying ISAN). Let M be a language model such
that for each t € [T], the matrix Ly (%*, UST") (recall Definition 2.2) has rank d. Then there is
a time-varying ISAN of hidden dimension d that generates the exact same distribution as M on
sequences of length T.

Proof. For each length t = 0,1,...,2—1, let S® Cc D* be a (multi)set of sequences with |S| = d
such that the rows of Ly(S, S57) span the row space of £Lyy(', NS7~*). In other words, this
is saying that the rows indexed by S™ span all of the rows indexed by sequences of length t. Note
that such a set exists by the rank assumption — for t = 0 there is only one row, and we allow for
a multiset so if |=“| < d then we can arbitrarily choose to duplicate some of the elements so that
|S| = d exactly.

For each timestep t € [7] and token z € © we can construct a matrix A,, € RY such that

Lu (S@) 02,08?) = A, Ly (SM, EST)                              (2)

where on the LHS, S-) o z denotes the set obtained by appending the token z to each sequence
in S¢-)). The above is possible because each of these are now length t sequences in X* and we
assumed that the rows of Ly(S, 5S?) span all of these. Also, let B; € R™*? be the matrix
B, = Lu (S“, {Null})' where here the set of futures consists of only the empty string.

34


===== PAGE BREAK =====

Now we claim that the ISAN defined by A,+, B; for z € &,t € [T] and initial state x = e1
(where e is the first standard basis vector) exactly samples from the distribution M over sequences
of length T’. We first show by induction that if z1., is the sequence sampled so far after ¢ timesteps,
then the hidden state satisfies

wv) Lu (SST) = Lu ({zin}, US?)                             (3)

fort = 0,1,...,7. Note that there is only one empty string so the base case for t = 0 is obvious.
Now given the inductive hypothesis for t, we immediately have that for any choice of token 241,

a} Ly (S © 2441; yst-tly = Lu (f{er041}, ysttl)

just from Definition 2.2, since the above is just selecting a subset of the columns of the equality in
(3). Now since 2441 = Al pit, combining with (2) gives

tila (S, Est?) = Laur ({e1241}, 557 1)

completing the induction. Next, it remains to show that given (3) for all t, that each token is
sampled from the same conditional distribution as M given the prefix 21. To see why this is the
case, (3) implies that

a} B) =a} Lu(S“,{Null}) = Laur ({212}, {Null})

since this is just obtained by selecting a subset of the columns in (3). Note that Lag ({z1:4}, {Null})
is exactly the mean-centered next token logits conditioned on z1.4, and so for both the underlying
distribution M and the ISAN we defined, conditioned on the prefix z}.,, the next token is sampled
according to softmax(B;2,). Thus, the ISAN generates the same distribution as MW, completing the
proof.

Now we prove the reverse direction that a time-varying ISAN expresses a distribution with low
logit rank.

Fact C.4 (Time-Varying ISAN has Low Logit Rank). Let M be a time-varying ISAN with hidden
dimension d that generates some distribution over sequences of tokens of length T. Then for any
t € [T], the matrix Ly (d!, OST") (recall Definition 2.2) has rank at most d.

Proof. For any sequence h € %*, let xp be the hidden state of the ISAN at timestep t, after
outputting the sequence of tokens h. Now consider any future f € DS?~*. Let the tokens of f be
Zt41,---;2t+k- Then the vector of mean-centered logits for the next token conditioned on ho f,
given by {Lyy(z|ho f)}zem (recall Definition 2.1), can be obtained by taking the vector

BA zy ttk uc Azis1t+10h

and then subtracting the mean from all of the entries. In other words, these logits are a linear
function of x, and there is a matrix Ay € R»*4 such that A f&h gives exactly the mean-centered
logits {Lyz(z|ho f)}zex. This implies, by Definition 2.2, that we can write

U hy
Ly (',ES7™) = | no} (Az,

AL.)

1

39


===== PAGE BREAK =====

where the rows of the first matrix are indexed by all possible histories h € X! and the second matrix
is a block matrix with blocks indexed by futures f € SS’~*. The above factorization implies that
Ly (dt, OS?) has rank at most d, as desired.

Remark 1 (Importance of Mean-centered Logits). Note that the mean-centering of the logits is
important for Fact C.4 to be true. Otherwise, if we used, say, normalized logits, then these logits
would no longer be a purely linear function of the hidden state x, due to subtracting the normalizing
constant which is not a linear function of the entries (whereas the mean is).

Proof of Theorem 4.3. Theorem 4.3 now follows immediately from combining Lemma C.3 and
Fact C.4.

C.2 Time-varying ISAN Representation Power

In this section, we analyze the representation power of the time-varying ISAN, showing that it
can represent a variety of simple architectures and languages. First, we show that it can represent
linear state space layers, the core building block in modern state space models (Gu et al., 2021a;
Gu and Dao, 2023). We begin by formally defining the class of state space models that we consider.

Definition C.5 (Selective State Space layer). A selective state space layer maps a sequence of
inputs u,,...,ur © R? to a sequence of outputs y1,...,yr © R% and has a hidden state with
dimension d. The initial state is x9 € R®@ and the state space layer operates according to the
following recurrence:

xy = A(uz)a4-1 + Bug) ur
Yt = C(ut)ar-1 + D(ut)ur

where the matrices A(u;) € R¢*4, B(uz) € R&%?, C(uz) € RY4, D(uz) € R&%*? may depend arbitrarily
on the input uz at timestep t.

Remark 2 (On Variants and Stacking of SSM Layers). This is a general form that encompasses
most definitions of a selective state space layer — implementations that are used in practice often
enforce more structure on the matrices, especially A. Earlier, simpler variants of the SSM used
time-invariant layers where A,B,C, D are all fixed, independent of T.

State space layers are often stacked together — although with the above form, a stack of ¢
state space layers can be represented as a single layer with dimension éd as long as the transitions
matrices A(ur), B(ut),C (ur), D( ut) depend only on the external input (but not on the recursively
obtained intermediate inputs to layers in the middle, see Hespanha (2018)).

Now it is not difficult to see that an ISAN can represent a language model consisting of a
selective state space layer with softmax readout and no additional nonlinearities (where the tokens
are embedded and then fed in as the inputs u;) as defined below.

Definition C.6 (Linear-in-state SSM). A linear-in-state SSM defines a distribution over sequences
in DS? as follows. There is a selective state space layer with input, output and hidden state dimen-
sions p,q,d. The output tokens are obtained by sampling x ~ softmar(Uy,) where U € R»™% is
the readout matrix and the neat input is obtained as uz, = Ve(%) where e(z) € R®™ denotes the
one-hot encoding of % and V € R®*™ is the embedding matrix. The initial input u, is some fixed
vector.

36


===== PAGE BREAK =====

We now prove the following fact:

Fact C.7 (Representing Linear-in-state SSMs). A Linear-in-state SSM with input, output, and
hidden dimension p,q,d can be expressed as an ISAN with hidden dimension d+2q+1°.

Proof of Fact C.7. We construct an ISAN that at each timestep stores hy, = (x4, C(ut)x¢_-1, D(ut) uz, 1)
as its hidden state. Note that the dimension of this is d+2q+1. Now by definition, the next token z
is sampled from softmax(U(C(uz)a¢-1+ D(uz)uz)) and the vector inside the softmax is clearly a lin-
ear function of the hidden state. Next, once we fix z, the next input uz+1 is fixed and we have #441 =
A(uty1)¢4¢ + B(ue41)ut41, and thus the next hidden state hy = (x¢41, C(ue41)@2, D(ur41)ut41, 1) is
a linear function of the previous one once uz, is fixed. This implies that the entire linear-in-state
SSM can be expressed as an ISAN, completing the proof.

Next, we will also show that the ISAN model can express basic languages and algorithmic
behaviors. We show that it can solve the task of copying, i.e. generating sequences of the form
aoa where a is a random bitstring. This task is a standard benchmark for testing the ability of
sequence models to capture long-range dependencies and has been used to separate the power of
different architectures (Jelassi et al., 2024).

First, we formalize the task of copying as being able to sample from the following distribution.

Definition C.8 (Copying). Define the n-bit “copying” distribution as the distribution over length
2n sequences of the form aoa where a € {0,1}”" is uniformly random.

Now we prove that ISANs can express copying.

Fact C.9 (Representing Copying). There is a time-varying ISAN with hidden dimension n+1 that
generates a distribution arbitrarily close in TV distance to the n-bit copying distribution.

Proof. Initialize the hidden state x9 to n zeros and the last bit is 1. Now for the first n steps,
the output matrices By,...,B, are all 0 (so the output is uniformly random) and the transition
matrices simply store the first n bits generated in the first n bits of the state while maintaining
that the last bit is 1. Now for the next n steps, the transition matrices are identity and the output
matrices Bn41,..., Bon are

0... 0... 0 E/2

Pri=lo 2 Oo ... 0 0

where the last column is always the same and the column with C is the jth column and C can
be chosen arbitrarily large. Then if the jth bit of the state is 0, then the output will be 0 with
exp(C'/2)/(1 + exp(C/2)) probability and if the jth bit of the state is 1, then the output will be 1
with exp(C/2)/(1 + exp(C/2)) probability. Since C' can be chosen arbitrarily large, this ISAN can
generate a distribution that is arbitrarily close in TV distance to the copying distribution.

Finally, we show that the ISAN model can generate from a noisy parity distribution. Noisy
parity is a fundamental problem that has been extensively studied in computational complexity,
learning theory, and cryptography. In fact, the hardness of learning a noisy parity from samples is
a standard cryptographic assumption (Blum et al., 2003; Pietrzak, 2012). First, we formally define
a noisy parity distribution:

’This result can also be applied to a stack of such layers, as long as we only allow the transitions to depend on
the external input, but not the intermediate values.

37


===== PAGE BREAK =====

Definition C.10 (Noisy Parity). A noisy parity distribution over {0,1}"*1 is defined by a sequence
y € {0,1}”" and probability p € (0,1/2) and is obtained by drawing z € {0,1}" uniformly at random
and outputting (z,b) with b € {0,1} defined as

b=   (y,z) mod 2 with probability 1 — p
— \1e (y,z) mod 2 with probability p

Now we prove that ISANs can express any noisy parity.

Fact C.11 (Representing Noisy Parity). For any noisy parity distribution over {0,1}"*1 , there is
a time-varying ISAN with hidden dimension 2 that exactly generates it.

Proof. We claim that for M being a noisy parity distribution, matrices Ly, ({0, 1}*, {0, 1}S"t1~*)
have rank at most 2 for all0 <t<n-+1. To see this, we simply observe that each of these matrices
has at most two distinct rows — the only information in the first t bits that matters is the parity
of the inner product with y restricted to these first t bits. Now we apply Theorem 4.3 and this
completes the proof.

The fact that ISANs can represent noisy parities, combined with the following standard crypto-
graphic assumption, implies that it is computationally hard to learn an ISAN from samples, which
we state in Theorem C.13.

Assumption C.12. [Hardness of Learning a Noisy Parity] There exists no polynomial time algo-
rithm that given access to samples from a noisy parity distribution with hidden vector y can recover
a vector y that is non-trivially correlated with y (say |y ® y| < n/4).

Theorem C.13. Under Assumption C.12, there is no polynomial time algorithm that given ac-
cess to samples from a time-varying ISAN can learn a distribution that is close in total variation
distance.

C.3 Learning an ISAN from Logit Queries

In this subsection, we present the details of our algorithm for learning a time-varying ISAN from
logit queries. First, we formalize the query model.

Definition C.14 (Logit Query). The learner can specify any prefix h € &* and obtain the mean-
centered logits Ly[z|h] for z € &.

Observe that logit queries can simulate samples from the language model. We do this by
repeatedly getting the next token logits and then taking softmax to sample the next token and
then repeat.

Fact C.15. Given logit query access to M, given any history h € &*, we can sample a future f of
any length from the distribution Prys[f|h].

Definition C.16 (Slicing Notation). For anyt < T, we will use the notation M|: t] to denote the
distribution of length t prefixes for sequences drawn from M.

38


===== PAGE BREAK =====

Now we present and analyze the algorithm for learning from logit queries. At a high level, the
algorithm tries to construct “spanning” sets of histories and futures H,, 7; for t = 0,1,...,77—1.
Ideally, we would be able to ensure that the matrix Lyy(Ht, F;) has rank equal to Ljy(D!, NS7~*)
which would mean that H:,7; in some sense cover all of the dimensions. Unfortunately, this
guarantee is not possible because the rank of Lyjy(H',0S7~*) could arise from some very rare
sequences that will be impossible for us to find algorithmically. We can instead guarantee a weaker
notion where we cover all dimensions that are not too rare. The algorithm, described by the
subroutine in Algorithm 3, iteratively constructs the sets H:,7; by adding new sequences that
increase the rank of Lyy(Hz, F;) until no such sequences can be found. The key to the analysis is
defining the right notion of coverage so as to ensure that the algorithm terminates in polynomial
time (see Lemma C.17). We then complete the proof by showing that with this guarantee, we can
construct an ISAN that is € close to the true distribution M in TV distance.

Algorithm 2 Learning from Logit Queries for (time-varying) ISAN

1: Input: logit query access to time-varying ISAN M
2: Initialize H, = {y*} for an arbitrary y € © for t =0,1,...,7—1 (where y° = Null)
3: Initialize F, = {Null} for t =0,1,...,T-1
4: Set N = 4T/elog(dT/e)
5: Set {Hz}e, {Fi}: < CompleteSpany,({Hz}:, {Fi}, N)
6: Set initial state to be 7 € R@ as xo = (1,0,...,0)
7: for t € [T—1],z € Udo
8:     Solve for Aut € RIMIxIHt—-1! such that
—T

Lui (Hi-1 ° 2, Ft) = Azt  Lui (Ht, Ft) .

9: end for

10: for t € [T] do

li: Set By = Lag (Hy-1, {Null})7

12: end for

13: Pad matrices Az1, Bi to appropriate sizes (d x d,|| x d respectively) by adding rows and
columns of zeros

14: Output: ISAN with parameters Zo, Axt, B,

Lemma C.17. Algorithm 3 terminates within dT iterations of the while loop and with 1—dT?(1—
e/(2T))% probability, we have the following property: for any t € [T — 1], if we draw h ~ MI; t],
then with 1 — €/(2T) probability over the randomness of h,

Rank(£Lyy (He, F;)) = Rank(Ly (Hit U Hi-1 oMU {h}, Fi UxXo Fi41))                (4)

where Hyi_; 0 denotes the set of all possible sequences obtained by concatenating some element of
Hi_1 with some token in X.

Proof. Note that by induction, we always have |H;| — Rank(Lys(He, Fr)) € {0,1} and |F;| —
Rank(Ly¢ (Hi, F4)) € {0,1} (the off-by-one is just because the initial matrix could have rank 0).
Note that since M is generated by a time-varying ISAN, by Fact C.4, the rank is always at most
d. Also in each iteration of the while loop before termination, the sizes of H,,7; increase by 1

39


===== PAGE BREAK =====

Algorithm 3 Complete Span

1: Input: logit query access to time-varying ISAN M

2: Input: Sets H;, 7; fort =0,1,...,7%-1

3: Input: Parameter NV

4: function CompleteSpanj;({H:i}:, {Fi}, NV)

5      Initialize Incomplete = True

6     while Incomplete do

7:        Set Incomplete = False

8    for t € [T — 1] do

9             Construct S; by taking N i.i.d. samples from MM: t]

10:               if Rank(L yy, (Hz, Fi)) < Rank(L yy (Ht UHi-10 LU St, FU No Fi41)) then
11:                 Set Incomplete = True
12:                 Find elements h € Hi-1 0 MU & and f € No F441 such that

Rank(Lar(He U {h}, FU {f})) = Rank(Lar(He, F)) +1

13:                  Set Ht <— Hy; U {h} and Fy «+ FU {f}
14:            end if

15:         end for

16:     end while

17:    Return: {H:}:, {Fi}:
18: end function

for at least one t. Thus, the algorithm can execute at most dT iterations of the while loop before
termination.

Next, to prove the second part of the lemma, whenever the desired property doesn’t hold for
some t € [T — 1], then with probability at least 1 — (1 — €/(2T)), the check in the while loop will
fail and the flag Incomplete will be set to True. Thus, union bounding over every time we check the
rank condition, the overall failure probability is at most dT?(1— €/(2T))%.

Now we can complete the proof of Theorem 4.4.

Proof of Theorem 4.4. First, note that the ISAN we construct in Algorithm 2 is well defined because
whenever Algorithm 3 terminates, we must have

Rank(L yy (Hz, F4)) = Rank(L yy (Hz U Ht  1 oO xX, Ft UxXo Ft } 1))

and thus the matrix Ast must exist because the rows of Lay (Hz, F;) must span the rows of Lay (Hy_10
z, F;). Also note that all of the operations in Algorithm 2 can be implemented in poly (d, |=], 7, 1/e)
time and queries.

Now assuming that the property in Lemma C.17 holds for the output of the CompleteSpan step,
we claim that we have Dpy(M, M’‘) < 0.5e. For each t, let the good set G, C X! be the subset of
histories h for which (4) holds.

Now sample token by token according to the ISAN we learn with parameters Zo, Azt, By.  At
timestep t = 0, by construction, we have

fo. Lu (Ho, Fo) = Lu ({Null}, Fo)

40


===== PAGE BREAK =====

and our current string is Null. Now assume that at timestep t, we have sampled the t tokens z1.4
and the current hidden state 7; satisfies

&' Lu (Hi, Fr) = Laur (21:t, Fr) -

Then since Null € F;, we get that softmax(B,2;) exactly samples the next token with the correct
probabilities. Let the next token be z+41. If z14 € G, then the above equality also implies

By! Lu (He, 41 © Fear) = La (21st, 2041 © Fy)
which is the same as saying

Lar (2atsi, Fez) = 0! Lu (Ht 0 2441, Fest)

-
=f Az lu (Hep, F141)

= fi! Lu (Hey, F041) -

In other words, by induction, as long as all subsequences remain in the good sets Gj,...,Gr7,
then every token is sampled according to the correct conditional distribution, matching M. By
Lemma C.17 and the definition of the good sets, the probability we ever sample outside of a good
set is at most 0.5¢. The way we set N ensures the conclusion of Lemma C.17 holds with probability
at least 1 — 0.5€ and thus the overall expected TV distance between M’ and M is at most e€, as
desired.

C.4 Generalization Bounds

In this section, we show a generalization bound for when the LinGen algorithm can guarantee high
accuracy. We assume that we have fixed the language model M, we have a target history ho, and
we will approximate ho as a linear combination of “basis” histories H = {hi,...,hn}. We then
generate k tokens according to LinGen (Algorithm 1).

Lemma C.18 allows us to relate the linear regression error when we draw futures from an
arbitrary distribution to the KL divergence between the LinGen distribution and the ground truth
distribution for continuations of ho, as long as we have the matrix ordering inequality in (5). The
main point is that (5) and (6) (the regression error) can be tested empirically from samples because
of matrix concentration (see Fact C.20). Lemma C.18 then shows that if these quantities are
sufficiently small, then we can bound the KL divergence between the true distribution and the one
obtained by generating according to LinGen.

While our theoretical bounds are not tight enough to directly apply, we still believe this is a
worthwhile conceptual point that generalization can be bounded in terms of some testable quan-
titative “subspace overlap” that is related to the subspace overlap for different distributions of
futures F,F’ discussed in Section 3.2. We also discuss below how we expect tighter bounds to hold
in practice due to better concentration than the “worst-case” theoretical analysis.

Lemma C.18. Define the distributions P, and P2 as follows:

e P; is obtained by uniformly drawing t € {0,1,...,k —1} and then sampling uniformly at
random from X*

e P2 is obtained by uniformly drawing t € {0,1,...,k—1} and then sampling a continuation to
ho of length t, according to M

Al


===== PAGE BREAK =====

Let Ho = HU {ho}. Assume that for some parameters a, 7,

Epps [Cu (Ho, {f})L1(Ho, (f})" |
5 aE yp, [Lar (Hos (F})La0(Po. (F})"] + Yat

Then for any vector v such that

Ef ~P

|car(tto} 7) eT EwtH rp | | <a,                   (6)

if we define
© PrinGen 1s the distribution obtained by sampling from LinGen(v, H, k)
© Prruth 18 the distribution of length k continuations of ho according to M

then

Dxx(Prruth||PrinGen) < aka + (1+ |lvll?).

Proof. Let u be the n + 1-dimensional vector u := (1,v). Taking (5) and taking the inner product
of both sides with wu! and then applying (6), we have

Ef Pz

[eau 0}, CF) - oT Cant, IP | Sad 4401+ UelP),              (7

Now assume that LinGen has sampled a sequence f = 2122... 2 so far. Then by the definition of
LinGen we can bound

Dk [Parti of]

PY LinGen(v,H,k) 7)  < Max log Pr [z|ho ° f] -    log PY inGen( (v,H,k)  [z  lf]
<2 max |Larlz|ho ° f] a LinGen(v,H,k) [z|f]|            (8)
< 2||Car({ho}, {F}) — oT Lau, (F})|

where the arguments on the LHS are distributions over the token space © given by the next-token
probabilities in the two different sampling procedures. Now using the KL chain rule, we get

Dx (Peruth||Princen) = KE sap,

Dx [Petit of]

PYLinGen( v,H,k) [- ii)

< 2ky/od +4(1 + [ull?)

where the last step follows by combining (7), (8) and Cauchy-Schwarz. This completes the proof.

Remark 3 (Improving the Bound by a Factor of ||). The above proof can be “lossy”, costing an
extra factor of JE] when translating from KL to L? distance in (8). This factor can be saved,
provided that the errors in the logits are roughly uniformly distributed over the token space — this
is what we observe in practice.

42


===== PAGE BREAK =====

We can also flip the arguments of the KL divergence to match the quantity we measured
empirically in Section 3.3, provided that the logits of the true language model are bounded.

Corollary C.19. Assume that |Liy[z|h]| < C for any history h € &* and token z © X. Then under
the same conditions as Lemma C.18, we also have

2   0.25
Dix(Princenl|Piratn) < 2(1 + b(log || + 2C))Vk (aA +4(1 + [ull?))

Proof. Note that the assumption implies
— log Pryy[z|h] < log || + 2C

for any h € &*,z € U. Now we can simply combine Fact C.21 and Lemma C.18 to get

Dx (PtinGen||Piruth) < (1 + k(log || + 2C))\/2D xx (Pirutnl| PLinGen)
9, \ 0.25
< 2(1 + k(log|2| + 2C)) Vk (aA +9(1 + lvl?)

as desired.

Finally, we also state concentration bounds for being able to test (5) and (6) from samples.

Fact C.20. Let H C &* with |H| =n and let P be any distribution over sequences in X&*. Assume
that the language model M satisfies the bound |Lyy4|-|h]| < C for any history h € &*. Then with
1—6 probability, if we let F be a subset of S > |S|?C*n?(1/e)? log(1/d) samples drawn from P,
then

| peat Fe (OF) — Bra [Lawl EH (19)"]

and consequently, for any v € R",

2
le" eure] sete.
Proof. For the first inequality, note that

Lu (H,F)Lu(H,F)' = So Lau (H, {f})Lu(H, {f})"
LF

Also note ||£Ly.(H, {f})|]| < C./n|| by assumption, so Matrix Bernstein immediately gives the
first inequality. The second inequality follows directly from the first by taking the inner product of
the LHS with vv!.

Remark 4. As before, we expect much faster concentration in practice e.g. we used a worst case
bound on the individual summands when we applied matrix Bernstein, but a much tighter bound,
that saves the dependence on ||, seems to hold empirically because the columns of Ly (H,{f}) are
generally not too correlated with each other.

Fact C.21. Let P,Q be two discrete probability distributions on s elements given by P = {pi,...,ps}
and Q = {q1,.--,qs}. Assume that —log p; > C for alli € [s]. Then

Dx (Q||P) < 1+ C)V2DxKL(P||Q).

43


===== PAGE BREAK =====

Proof. We can write

Dx (Q\|P) = d— ai(log qi — log pi)

i€[s

< S > g(log qi — log pi)
1€|s],4i>Di

< So (G—pi)(log qi + 1 — log pj)
1€|s],4i>Di

<(1+C) / lai — il

< (1+ C)V2Dx1(P||Q).

Note that for the second inequality we used the convexity of xlog x and the final step follows from
Pinsker’s inequality.

44
