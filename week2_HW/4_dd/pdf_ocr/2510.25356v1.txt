arX1v:2510.25356v1 [cs.CL] 29 Oct 2025

Abhishek Purushothama*!

Not ready for the bench: LLM legal interpretation is unstable and
out of step with human judgments

Junghyun Min*!

Brandon Waldon!2 Nathan Schneider!

'Georgetown University
2University of South Carolina
{ap2089, jm3743, bw686, nathan. schneider }@georgetown. edu

Abstract

Legal interpretation frequently involves
assessing how a legal text, as understood
by an ‘ordinary’ speaker of the language,
applies to the set of facts characterizing a
legal dispute in the U.S. judicial system.
Recent scholarship has proposed that legal
practitioners add large language models
(LLMs) to their interpretive toolkit. This work
offers an empirical argument against LLM
interpretation as recently practiced by legal
scholars and federal judges. Our investigation
in English shows that models do not provide
stable interpretive judgments: varying the
question format can lead the model to wildly
different conclusions. Moreover, the models
show weak to moderate correlation with
human judgment, with large variance across
model and question variant, suggesting that
it is dangerous to give much credence to the
conclusions produced by generative AI. We
publicly release our code to facilitate future
work.!

1 Introduction

Legal decisions often come down to the interpre-
tation of written text (e.g., in a statute or contract).
Often, legal interpretation is straightforward. At
times, however, the text is imprecise or ambiguous,
leading to disputes about how to apply a written
provision to a particular set of circumstances. In
the U.S. legal system, judges often place consid-
erable weight on what they deem the ‘ordinary
meaning’ (meaning as would be understood by
ordinary speakers of American English) of a le-
gal term (Slocum, 2015).7 One recent case (Snell

“Equal contribution

"https ://github.com/bwaldon/11ms-legal-interp

“Tn this paper, we are officially agnostic as to whether
‘ordinary meaning’ is a useful legal analytical construct (or
even a coherent linguistic concept). Our focus is on whether
LLMs are useful for pursuing interpretative methodologies
that are purportedly grounded in ‘ordinary meaning.’

John is a contractor with insurance that cov-
ers property loss, damage, or personal injury
claims that arise due to his ‘landscaping’ work.

John is employed by a family, the Smiths,
to install an in-ground trampoline in the family’s

backyard. A few years after John completes
the project, the Smiths successfully sue John
for injuries that their daughter sustained while
playing on the trampoline. John files a claim
with his insurance company to recover losses
incurred from the lawsuit.

Considering just how “landscaping” would be
understood by ordinary speakers of English, is
John covered by the insurance—yes or no?
Figure 1: A legal interpretation scenario represented as
QA task with binary questions. The example is based
on the case Snell v. United Specialty Insurance Co. and

constructed in the style of our task.

v. United Specialty Insurance Co.) required inter-
pretation of the term ‘landscaping’ concerning an
in-ground trampoline (see Figure 1). A second
case (U.S. v. Deleon) tured on the interpretation of
the phrase ‘physically restrained’ and whether the
phrase describes indirectly restricting movement
by threatening someone with a gun.

How can a judge ascertain ‘ordinary meaning’ of
a legal text? Often, the judge will deploy armchair
intuition buttressed by hypotheticals and dictionar-
ies (Krishnakumar, 2024). On occasion, judges
reference corpora (Solan and Gales, 2017; Gries
and Slocum, 2017) and surveys (Tobia, 2024).

Enter LLMs. The advent of LLMs has produced
considerable excitement in the legal field, and tex-
tual interpretation is no exception. Hoffman and
Arbel (2024) opined that LLMs are good resources
for ascertaining ordinary meaning. Federal judi-
cial opinions (Newsom, 2024a,b) have included
“direct queries’ to LLMs on the ordinary meaning


===== PAGE BREAK =====

of ‘landscaping’ and ‘physically restrained’? on
the grounds that these models supposedly capture
and reflect patterns of ordinary language use.

These developments have been met with critical
scholarship assessing the benefits, risks, and impact
of these technologies from the standpoint of both
legal theory and practice. Waldon et al. (2025) and
Choi (2025) demonstrated that LLMs’ interpretive
judgments are highly sensitive to how queries about
the meaning are phrased and suggest that LLMs are
not reliable tools for legal interpretation; however,
the studies were limited in scope, testing only a few
models and interpretive questions.*

In this work, we investigate the stability of LLM-
based legal interpretation methods, with a focus
on LLMs’ ability to reflect ‘ordinary meaning’ as
understood by human native speakers. In light of
recent research (§2), our hypotheses are as follows:

1. LLM judgments are highly sensitive to subtle
manipulations in how ordinary meaning queries are
posed to the model.

2. LLM judgments are poorly correlated to hu-
man judgment.

To test our hypotheses, we create queries based
on a previously developed set of 138 scenarios
that assess linguistic interpretation in a variety of
hypothetical insurance contract disputes (Waldon
et al., 2023), and query 14 open-weight models
(and one closed-weight model) across 9 systematic
question variants ($3). In line with our hypothe-
ses, we find in §4 that LLM judgments are incon-
sistent across model family and size, as well as
across subtle choices in question phrasing. The out-
puts of some instruction-tuned models and some
model scales are correlated to human judgment
only in some question variants; moreover, neither
decoded tokens nor probability distributions offer
a reliable source of human-like ‘ordinary meaning’
judgment.

Though LLMs possess undeniably fluent gen-
eration capabilities, our results add to a growing
chorus of skepticism about LLMs as tools for legal
interpretation (§5).

3In Snell and Deleon, J udge Kevin Newsom posed the two
following direct queries, respectively: “What is the ordinary
meaning of landscaping?” and “What is the ordinary meaning
of physically restrained?”. See Waldon et al. (2025) for a
detailed discussion of the direct query method.

4Waldon et al. (2025) utilized two scenarios, and Choi
(2025) utilized five.

2 Background

In the current era of generative AI enthusiasm,
some scholars have expressed optimism that LLMs
will revolutionize legal interpretation. Multiple
variations of this idea have been put forward.

One argument is that LLMs need not be perfectly
accurate, but as long as they are ‘good enough’,
they will produce interpretive outcomes that are
more consistent and more affordable to the masses
than the legal status quo (Hoffman and Arbel, 2024,
writing about contract interpretation). This, of
course, presumes that LLM judgments are con-
sistent. Hoffman and Arbel offer a small-scale
case study of robustness with 5 scenarios and 20
machine-generated paraphrases of each prompt.
This study was expanded by Choi (2025), who gen-
erated 2000 paraphrases for each of the 5 scenarios,
and found models to be unreliable with prompt vari-
ation, in ways that differ across models, training
methods, and output processing methods. While
this result is suggestive, we will take the study of
prompt variation a step further, with more rigor-
ous testing that covers more scenarios and uses
tightly controlled prompt variants (instead of LLM
paraphrasing) to ensure meaning preservation.

A second line of reasoning holds that for difficult
interpretive problems, for investigating ordinary
meaning in English, LLMs are potentially better
than humans because they have been trained on
so much ordinary English data (we can call this
the ‘omniscience’ argument). Judge Kevin New-
som in his opinion in the Snell case gives voice to
this idea, citing the diversity of ordinary usage that
these models might draw upon: “models train on
a mind-bogglingly enormous amount of raw data
...as understand LLM design, those data run the
gamut from the highest-minded to the lowest, from
Hemmingway [sic] novels and Ph.D. dissertations
to gossip rags and comment threads. Because they
cast their nets so widely, LLMs can provide use-
ful statistical predictions about how, in the main,
ordinary people ordinarily use words and phrases
in ordinary life” (Newsom, 2024a). He appears to
tone down the omniscience argument in a subse-
quent case, saying LLMs “may well serve a valu-
able auxiliary role as we aim to triangulate ordinary
meaning,’ complementing “traditional interpretive
tools [such as] dictionaries” (Newsom, 2024b).

Is it plausible that a careful judge, armed with
established tools of ordinary meaning analysis and
a bevy of law clerks, would arrive at a better-


===== PAGE BREAK =====

informed result because an LLM was consulted?
Newsom’s rationale is predicated on certain as-
sumptions about LLMs that have been called into
question (Waldon et al., 2025). In particular, his
comment about the vast training data behind LLM
chatbots suggests he believes systems are capable
of conducting metalinguistic reasoning about the
language in their training data in order to synthe-
size an interpretive conclusion. But recent analyses
have put in doubt the proposition that LLMs are
capable of deep metalinguistic reflection (Behzad
et al., 2023; Thrush et al., 2024; Cheng and Amiri,
2025). Instead, it seems likely that LLMs are good
at imitating or summarizing metalinguistic text in
the training data—be it dictionary definitions, text-
books, or online language forum discussions (Be-
hzad et al., 2023; Waldon et al., 2025). Thus, to the
extent that an LLM produces plausible-sounding re-
sponses to an interpretive prompt, it likely draws on
what humans say about language (Almeman et al.,
2024), rather than what they do as language users.
Still, if LLMs could accurately synthesize ordinary
people’s opinions about meaning, then they might
be a reliable and cost-effective tool for ordinary
meaning analysis, as hypothesized by Hoffman and
Arbel (2024).

It is crucial, then, to test whether LLMs’ interpre-
tive judgments are reliably correlated to human in-
terpretations. By some measures, LLM probability
models exhibit human-like sensitivity to grammati-
cal phenomena in English sentences: these include
linguistic or syntactic acceptability (Gauthier et al.,
2020; Warstadt et al., 2019, 2020), and semantic
plausibility (Kauf et al., 2023, 2024). However,
there is increasing behavioral evidence of differ-
ences between LLMs and humans when it comes to
learning and processing language (Oh et al., 2024;
Aoyama and Wilcox, 2025; McCoy and Griffiths,
2025) and answering questions (Srikanth et al.,
2025). To test whether LLMs and humans arrive at
similar interpretive conclusions, we use the human
judgments from Waldon et al.’s (2023) to evaluate
LLMs’ interpretive judgments.

3 Legal Interpretation with LLMs

Consider the landscaping example in Figure 1. A
term within the contractor’s insurance, ‘landscap-
ing, must be interpreted to determine whether the
insurance covers the described scenario. The task
explicitly demands a judgment as to how ordinary
speakers would understand the contract language

in context. This judgment may not cohere with
one’s beliefs regarding the ‘correct’ interpretation
of the provision, or regarding how a judge will
actually resolve the legal dispute at hand. Rather,
our investigation is perspectivist in that we are not
assessing model behavior against a single, fixed
‘ground truth’ linguistic interpretation or judicial
outcome (Frenda et al., 2025). Instead, our assess-
ment metrics assume that ‘ordinary interpretation’
is both highly varied and to some extent subjec-
tive. Our first metric—robustness to variation—
evaluates the stability of model judgments across
multiple prompt formulations. Our second metric—
human correlation (see §4.3)—evaluates the extent
to which model judgments cohere with the intu-
itions of a relevant human population.

In the remainder of this section, we describe a
study designed to investigate LLMs’ legal inter-
pretation capabilities as assessed against these two
metrics.

Vague Contracts. Our study adapts materials
originally developed by Waldon et al. (2023) for
a human study of legal interpretation and consists
of 138 items based on real-world insurance con-
tracts. Table 1 provides an example item. Each
item names a category of insurance coverage (e.g.,
Wind Damage) and provides a definition of that
category. The item then describes a policyholder’s
loss, which may or may not be covered by the
named category.

Waldon et al. (2023) analyzed human responses
from 1,338 U.S.-based native English speakers re-
cruited via Prolific. The participants were shown
a vignette with the scenario and insurance text,
and were asked to answer questions. The first of
the three questions elicited interpretive judgments,
YES, NO, or CAN’ TDECIDE. A sample vignette
(Figure 4) can be found in §A.

Interpretation as binary QA. Each query ends
with a cue designed to induce single-token judg-
ment that addresses the binary question. To mea-
sure model response, we examine first-token con-
tinuation probabilities. Since the first token prob-
abilities may not be fully representative of model
judgment, as highlighted by Wang et al. (2024), we
review output text (decoded with temperature set to
zero) to check for such inconsistency between the
first token probabilities and the model’s intended
judgment. Additionally, we consider several types
of yes and no tokens across casing as indicators
of coverage judgment, and operationalize their re-


===== PAGE BREAK =====

Query element

Example (‘Vehicle Damage’)

Insurance text

Steve’s car insurance policy includes coverage for “Vehicle Damage,” defined as “loss or

damage to the policy holder’s 1) car; or 2) car accessories (while in or on the car)”

Scenario

One day, Steve is involved in a minor accident. His GPS navigation system, which was in

the car at the time, was damaged. Steve files a claim with his insurance company for the

damage.
Framing for ‘ordinary meaning’
Question
Cue                                           Final answer is:

Considering just how “accessory” would be understood by ordinary speakers of English,
is Ken covered by the insurance—yes or no?

Table 1: Elements of the interpretive queries. ‘Insurance text’ and ‘Event’ are sourced from the study item, and the
‘Framing’ and ‘Question’ are added to solicit interpretive judgment. The “Cue* was found with exploratory tests to

induce high rate of first token judgment.

spective sums as judgment probabilities, which we
further describe in 8E.1.

Formulation of prompt variants. To investigate
whether LLMs’ interpretive judgments are robust
to minor changes in prompt design, we constructed
a template of 9 question variants and applied that
template to each of the 138 items. An example
paradigm is presented in Table 2.

Some variants reflect LLM behavior that is al-
ready attested to be challenging and unlike hu-
mans. Garcia-Ferrero et al. (2023) and Truong
et al. (2023) show, for example, that LLMs find
natural language negation words challenging and
lack a deep understanding of the phenomenon.

Moreover, Sharma et al. (2024) and Hong et al.
(2025) demonstrate that in some contexts, LLM
outputs are modulated by prompts that overtly so-
licit agreement (e.g., Do you agree that... ?). Our
study builds upon these previous findings in a novel
domain of legal interpretation; we also investi-
gate variants that, to our knowledge, remain under-
studied (e.g., overt solicitation of disagreement) or
that combine multiple previously-studied phenom-
ena (e.g., negation plus agreement solicitation).

For most variants, an affirmative yes response
would correspond to the COVERED judgment, but
in some variations (e.g., the Negation variant in
Table 2), the COVERED judgment would be ex-
pressed with a no token. For readability, we report
and discuss probabilities corresponding to Cov-
ERED and NOTCOVERED judgments, rather than
discussing yes and no token probabilities.

Models. We collect judgments from base and
instruction-tuned open-weight LLMs of vary-
ing size up to 70B parameters and GPT-42>
They include GPT-2 (Radford et al., 2019),
Llama (Grattafiori et al., 2024), OLMo (Groeneveld

>This is a closed weight model, whose judgment was ob-
tained via API. See §E.2 for details.

et al., 2024), Mistral (Karamcheti et al., 2021),
and Gemma (Team et al., 2024). The full list is in
Table 3, and implementation details SE. Given our
evaluation data’s release in 2023 we note that data
contamination is a possibility, which we discuss in
SB.

4 Results and Analysis

From these models, we collect both categorical
(COVERED or NOTCOVERED) and distributional
(probabilistic) judgments over tokens that represent
the COVERED judgment (yes, Yes, YES for Yes/No
variant), tokens that represent the NOTCOVERED
judgment (no, No, NO for Yes/No variant), and the
residual other tokens. We use these metrics to an-
alyze how robust the models are with respect to
model family, size, and question variation. Addi-
tionally, we examine correlation (or lack thereof)
between human and LLM judgment.

4.1 Analysis of judgments for Yes/No prompts

We begin by focusing on what is arguably the most
basic question variant: Yes/No. The categorical
and distributional for all models with the Yes/No
question variant are reported in Table 4.

Some models can be stopped clocks. We ob-
serve that many models repeat the same categorical
judgment no matter the scenario, or are highly bi-
ased towards one judgment: 6 of the 15 models
tested provide the same response in all question
variant conditions for at least 127 (#92%) of the
138 tested scenarios. In these cases, it is doubtful
that the model judgment reflects substantive en-
gagement with the provided prompt. These models,
rather, are only as useful as the proverbial stopped
clock that correctly tells the time twice a day.

We also observe that models allocate judgments
over different probability ranges from each other
and instruction tuning has inconsistent effects,


===== PAGE BREAK =====

Variant                 Question

Considering just how the word ‘landscaping’ would be understood by ordinary speakers of English,
Yes/No                  is John covered by the insurance—yes or no?
No/Yes                  is John covered by the insurance—no or yes?
Negation               is John not covered by the insurance—yes or no? Final answer is:
Agreement             do you agree with the statement, “John is covered by the insurance” —yes or no?:
AgrWithNeg           do you agree with the following statement: “John is not covered by the insurance”— yes or no?
Disagreement        do you disagree with the following statement: John is covered by the insurance”— yes or no?
DisagrWithNeg      do you disagree with the following statement: John is not covered by the insurance”— yes or no?
Options                is John covered by the insurance? Options: A. John is covered. B. John is not covered.
OptionsFlipped is John covered by the insurance? Options: A. John is not covered. B. John is covered.

Table 2: Systematic variation of the question (Table 1) in the interpretive queries. Further discussion of these

variants is available in §D.

Family      Models
Llama-3     1B, 1B-Inst (3.2)

3B, 3B-Inst (3.2)

8B, 8B-Inst (3.1)

70B (3.1), 70B-Inst (3.3)
GPT          GPT-2-medium, GPT-4>
OLMo-2 7B, 7B-Inst
Ministral 8B-Inst
Gemma     7b, 7b-it

Table 3: We query a total of 15 models across 5 families.
None are considered reasoning or thinking models.

% LLM judgments p(Covered) > p(NotCovered)

43%
EES 20%

EL 3.4%
m4%

Yes/No

OptionsFlipped

TN 32%

as 83%
‘EYEE 45%

No/Yes  ii7%      45%

rs 87%

a 14%

a 93%

LL 67%

mm 8%

ET 39%

Options
Llama-70B

Hi  Base
Hi  Instruct

Negation
DisagrWithNeg

Disagreement

Question variations

AgrWithNeg [lt ss se 67%

Agreement ams 23%          “ne

0%             30%            60%            90%
Proportion

Figure 2: Llama-70B model responses across question
variants, each of which results in a large shift away from
values in either directions given the Yes/No variant, in-
dicated with the dotted lines.

which also vary by the models and sizes. Detailed
discussion is available in §C.

4.2 Robustness to question variation

For a model to be considered reliable at answering
interpretive questions, it should be robust to varia-
tion in how the question is phrased. As described
in §3, we measure model responses while varying
the phrasing of the question in the prompt and leav-
ing the content unchanged, and analyze how the
variation affects the models’ categorical and distri-
butional judgments. We report three major findings
in this section of our study.

Given the nine question variants, for each item
and model, one of the categorical judgments

Categorical           Distributional
Counts                     Spread

Model                        Coy. NorTCov. Min        Max
Llama-70B                  28            110          0.21        0.48

+Inst                         59              79           0.02        0.85
Llama-8B                      80              58           0.14        0.37

+Inst                         0           138          0.11        0.74
Llama-3B                    127               11           0.09        0.52

+Inst                       53             85          0.16       0.69
Llama-1B                  138               0          0.06       0.29

+Inst                     138               0          0.15        0.59
OLMo-2-7B                 70              68           0.19        0.56

+Inst                         53               85           0.00        0.99
Ministral-8B-Inst        75             63          0.21        0.59
gemma-7b                     39              99           0.19        0.49

+it                         131               7          0.00        1.00
GPT-4                           77               61           0.00         1.00
GPT-2-medium        5       133      0.13    0.31
Human Majority          84             54

Table 4: Count and probability range for each model’s
COVERED and NOTCOVERED judgments in response
to Yes/No questions. Both distribution and the effect of
instruction tuning vary significantly across models. The
human majority is mentioned for reference.

(COVERED, NOTCOVERED) will be the majority
judgment and the other the minority judgment. The
strength of the majority can vary—from 5 of 9 vari-
ants (an indication of brittleness) to 9 of 9 variants
(unanimity, indicating robustness). A frequency
table for majority judgments collated by items for
each model is shown in Table 5.

Ubiquitous inconsistency across question vari-
ants. We observe inconsistency across the 9 ques-
tion variants in each LLM we study. As illustrated
in Table 5, in 2,061 of 2,070 model responses
(138 scenarios for each of the 15 models), both
judgments can be found across the question vari-
ants with each model; only in 9 item-model com-
binations is the judgment fully consistent across
question variants. Model judgments are sensitive
even to simple variations like reversing the order
of the provided answer choices, as seen in Fig-


===== PAGE BREAK =====

Majority Count           5        6        7        8 9

Model                                   Frequency for count
Llama-70B          33 68 36    1 0O
+Inst                               25 33 78          2 O
Llama-8B                          40 52 46          0 O
+Inst                                  6 39 59 31 3
Llama-3B                          95 40          3          0 O
+Inst                               75 48 15          0 O
Llama-1B           12 57 69    0 O
+Inst                             129          9          0          0 O
Ministral-8B-Inst    24 65 30 18 #1
OLMo-2-7B                        46 65 20            7 O
+Inst                               51 57 30          0 O
gemma-7b                         44 58 29          7 =O
+it                                       9 31 79 19 O
GPT-4                                    4          9 57 63 5
GPT-2-medium              50-83          5          0 O

Table 5: Number of items by number of question vari-
ants that yielded the majority judgment for the model.
For example, there were 33 items for which Llama-70B
produced one judgment for 5 variants, and the other
judgment for 4 variants. Each judgment is a binary
choice between COVERED and NOTCOVERED.

ure 2 for Llama-7@B models, where the swap from
Agreement to AgrWithNegation produces a 64%
absolute shift in the base model, and a 46% ab-
solute shift in the instruction-tuned model, with
respect to the rate of categorical COVERED judg-
ments. This ubiquity of inconsistency exposes
a generalization failure on the part of the model.
Worse, it invites users invested in a particular out-
come to engage in “prompt shopping,” varying the
prompt until the desired response is produced (Wal-
don et al., 2025).

Some questions are more likely to elicit mi-
nority judgments. Four variants Disagreement,
AgreementWithNegation, DisagrWithNegation
and Options elicit +67% of minority responses.
Even in the absence of these four variants leads
to unanimity only in +33% of cases, although up
from less than 1% of model responses and reduc-
ing the predominance of inconsistency. We provide
additional details on question variants that induce
minority judgments in §D.1, and details of their
effect on distributional judgments in §D.2.

The lack of robustness supports our first hypothe-
sis, and should be considered a limitation of LLMs
for use in legal interpretation, since such ubiqui-
tous inconsistency shows models are brittle when
used for legal interpretation.

4.3 Correlation to human judgment

Finally, we compare LLMs’ responses and distri-
butional judgments to human judgment data from

Waldon et al. (2023) to test our hypothesis that
model responses are poorly correlated to human
judgments. In Waldon et al.’s (2023) study, each
human participant was asked for a response of yes,
no, or cantdecide (where yes and no correspond to
COVERED and NOTCOVERED judgments, respec-
tively).°

The dataset contains 138 items, and a total of
4,140 judgments from 1460 participants, for an
average of 30 judgments per scenario.

For our correlation analysis, we operationalize
human judgment as a rate—the % COVERED of all
human responses on the same item. We operational-
ize model judgment as a difference in model prob-
abilities: A = p(COVERED) — p(NOTCOVERED).
We consider this linear linking hypothesis, which
accounts for the probability judgments for both
COVERED and NOTCOVERED with the decision
boundary at 0. With this hypothesis (further dis-
cussed in §F), a perfectly correlated model would
represent human judgment with %COVERED =
0.5A+0.5.

Only some LLMs are moderately correlated to
human judgment some of the time. Our results
show that only larger instruction-tuned models’
judgment show some correlation to human judg-
ment for some specific question variants. This is
despite evidence for scaling—models do show bet-
ter fit to human data and stronger performance in
predicting human consensus as the number of pa-
rameters (and likely the amount of training data) in-
creases. In particular, only instruction-tuned mod-
els with 70 billion parameters or more report an R?
value greater than 0.5 , as shown in Table 6. Cor-
relation between Llama-70B-Inst response and
human judgment across question variants is illus-
trated in Figure 3, where one variant yields a nega-
tive correlation (m < 0) with a R? value of 0.18. The
significant variation in R* values across question
variants suggests that the models’ responses are not,
in general, representative of ordinary meaning or
human-like judgment, but rather prompt design and
question form. This highlights a second limitation
to LLMs in legal interpretation, despite achieving
up to moderate correlation to human judgment in
select model-question variant pairs.

Furthermore, even when there is correlation be-
tween token probabilities and human judgment,

®Participants were also asked to rate the confidence of
their response on a Likert scale from 0 to 4 (can be seen
in Figure 4). We do not scale human judgments with the
confidence response.


===== PAGE BREAK =====

Human and LLM judgment by Question Variation

Agreement                AgrWithNeg               Disagreement

1.00                      1.2                   * 1.25                   .

0.75                              “ & 1.00   A          A

a                    0.8                     0.75                 A

0.50                                                 %                050
G 0.25                      4 -                  0.25
® 0.00         .     A       0.0                     0.00
g         -04 0.0    0.4           -0.6-0.4-0.2 0.0 0.2      -0.75-0.50-0.250.00 0.25
[e)
R         DisagrWithNeg                Negation                   No/Yes
& 0.75                a, (10 A             A 0.75
€ 0.50 R*=0                      05       R=               A 9.50                                      Base
® 0.25                       :
2 O00             AA  0.0               “oO                     Instruct
3      -0.25 0.00 0.25 0.50         -04 00 04 08
c
=            Options                OptionsFlipped
=]

49.

A

D

-0.5 0.0

.                         0.5
p(Covered) — p(NotCovered)

Figure 3: Llama-70B model probabilities versus human consensus across question variants. Dotted lines and the
corresponding R? are best best-fit lines between human and instruction-tuned LLM. The Yes/No question variant,
highlighted in red, represents the highest R? value in our study.

the output tokens can be overwhelmingly biased.
Llama-7QB-Inst’s responses to the AgrWithNeg
variant shown in Figure 3 illustrates that despite
a (weak) correlation of R? = 0.26 when examining
gradations of probabilities, the actual responses
are overwhelmingly NOTCOVERED, where A < 0,
further raising caution to LLMs’ use for legal inter-
pretation.

Even the best-correlated LLMs are unreliable
predictors of human judgment. As another
measure of correlation, we consider an oracle
(threshold) classifier that predicts the binary human
judgment as a function of the LLM’s probability-
difference A. The threshold is set based on the
human data, where the best-fit line crosses 0.5 in
human judgment. Given a probability difference
value exceeding the threshold, the classifier pre-
dicts the COVERED judgment; otherwise, it pre-
dicts NOTCOVERED. Intuitively, this tells us how
accurate the LLM would be at giving probabilities
consistent with the human majority judgment, if
only we knew how to interpret the model-specific
probability scale. Binary classification accuracies
appear in the last column of Table 6.

The model-variant pair best correlated to hu-
man judgment, GPT-4 responses to Y/N (R? = 0.60),
yields a linear model that accurately predicts hu-
man consensus 83% of the time. Even if this may
initially appear satisfactory, we contend that even
1 in 6 binary classification error rate in an inter-

pretative tool as considered in Hoffman and Arbel
(2024); Newsom (2024a,b) defeats its purpose. For
comparison, a majority-class classifier always vot-
ing COVERED would be correct 61% of the time.

On some question variants, model responses are
sometimes negatively correlated to human judg-
ment when the question includes negation, which
suggests that LLMs’ attested difficulties with nega-
tion (Garcia-Ferrero et al., 2023; Varshney et al.,
2025) persist in legal interpretation. We suspect
this may be attributable to LLMs providing answers
to a different question than was asked and discuss
the phenomenon in §D.3.

On the most optimistic interpretation of the
results—that the largest models with certain
prompt types achieve nontrivial correlation, which
might be improved with further engineering to the
point of reliability—we underscore two additional
points of caution: (a) we have not ruled out the
possibility of data contamination (§B), and (b) our
experimental data is specific to insurance contract
scenarios. Thus, there is a long road ahead for any
efforts to develop trustworthy interpretive LLMs
for practical use.

5 Related Work

Legal AI, legal language, and LLM robustness are
popular areas of research. Nonetheless, there is
limited scholarly work on the utility of LLMs for
novel interpretation questions in the law, and most


===== PAGE BREAK =====

Model                   Variant          R? Ace. Thresh.
Llama-70B             Options F. 0.25 0.69        -0.17
+Inst                  Y/N             0.52 0.79        —0.28
Llama-8B               Agr.             0.04 0.59        —0.02
+Inst                  Options        0.20 0.67        —0.36
Llama-3B                Disagr.          0.03 0.40           0.11
+Inst                  Y/N             0.06 0.59        —0.25
Llama-1B               N/Y             0.01 0.38          0.13
+Inst                  Neg.             0.01 0.38          0.34
Ministral-8B-Inst Y/N                 0.27 0.69          -0.11
OLMo-7B                    N/Y                  0.07 0.64           —0.10
+Inst                  Y/N             0.18 0.64        —0.63
gemma-7b                   Y/N                  0.09 0.59           —0.10
+it                             Neg.                 0.06 0.60           —0.16
GPT-4                    Y/N             0.60 0.83        —0.25
GPT-2-medium          Options           0.01 0.62           -0.19
Majority class baseline                    0.61

Table 6: For each model, we report the question variant
with the highest correlation to human judgments along
with the R?. We also report the accuracy and threshold
of an oracle classifier predicting the human majority
judgment from the LLM probability difference A. We
offer the always-COVERED accuracy as a majority class
baseline.

of it is based on conceptual arguments and case
studies rather than controlled experimentation.

LLM synopsis of legal interpretations. Luo
et al. (2025) instruct LLMs to consider a target le-
gal term (say ‘landscaping’) and generate, based on
legal documents from previous cases, an interpre-
tive explanation (along with conditions governing
the applicability of the interpretation). This task
concerns the extraction of a previously articulated
interpretation, rather than novel conclusions about
meaning. They find the system performance com-
parable to that of human experts for the task, but do
not evaluate models for robustness across prompts.

Instability of LLM legal judgments. Blair-
Stanek and Van Durme (2025) investigate LLM
instability in the context of a legal judgment pre-
diction task, where the models are prompted with
a synopsis of the case and asked to predict which
party should prevail. This task demands a full ac-
counting of all the issues in a particular case, rather
than a focused interpretive question. There is no
indication that the cases were selected with empha-
sis on language interpretation. Investigating closed
commercial models with a reasoning step, they find
significant instability, i.e., the answer will not be
consistent even within the same model and prompt.

LLMs as arbiters of ordinary meaning. Our
study asks whether LLMs can reliably answer
novel questions about ordinary meaning. Several
voices of legal scholarship have expressed skepti-

cism. Lee and Egbert (2024) contrast LLMs with le-
gal corpus linguistics, arguing that LLM interpreta-
tions are less replicable, transparent, and generaliz-
able. Waldon et al. (2025) argue that the way LLMs
are designed makes them vulnerable to misinterpre-
tation and misuse. In a similar vein, Grimmelmann
et al. (2025) argue that LLMs are unproven for le-
gal interpretation due to a reliability gap (LLMs
are not always consistent and reproducible), and
an epistemic gap (interpretive conclusions in text
produced by the model are not necessarily accurate
measurements of ordinary meaning as understood
by humans). Our robustness and correlation evalu-
ations are ways of quantifying the respective gaps.

In the existing literature, experiential claims
about LLM behavior have been supported with
ad hoc case studies—with one exception: Choi
(2025) conducted a set of experiments to test LLMs’
sensitivity to prompt variation. While our goal and
conclusions are similar to Choi’s, there are impor-
tant methodological differences: (i) Choi utilizes 5
contract scenarios from Hoffman and Arbel (2024)
while we use 138 insurance policy scenarios (from
Waldon et al., 2023); and (ii) we perform controlled,
systematic variation of the questions for our studies
rather than LLM-generated synthetic variations a
la Choi. Our similar conclusion, based on entirely
different methodology and non-synthetic data, un-
derscores the need for caution when contemplating
applying LLMs to legal interpretation.

6 Conclusion

Given the excitement about and increasing critical
legal scholarship on the use of LLMs for legal inter-
pretation, we conducted a systematic study of this
capability with a focus on direct LLM judgments
regarding the ‘ordinary meaning’ of legal language,
formulated as binary-choice QA. We examined the
LLM judgments for both robustness to prompt vari-
ation and correlation to human judgments, finding:

¢ Some LLMs can be stopped clocks, with a
strong tendency to provide the same judgment
for most input, regardless of the scenario.

* Models show a ubiquitous lack of consistency
across question variants.

¢ Correlation with human judgment is at
best moderate, and is strongest in larger,
instruction-tuned models.

Our work complements a growing body of le-

gal scholarship on LLMs for legal interpretation,
adding to growing skepticism about LLMs as in-


===== PAGE BREAK =====

terpreters of ordinary meaning. Our experimental
coverage of LLMs is, of course, not exhaustive; per-
haps there is a model or approach whose suitability
for this task could be demonstrated in rigorous ex-
periments. But the evidence thus far spectacularly
fails to meet the burden of proof.

Limitations

Legal interpretation and ‘ordinary meaning’ are
complex topics of theory and practice in the legal
field. Our work only looks at a specific aspect of
LLM usage, posing direct queries for ascertaining
ordinary meaning with a binary-choice QA task.
This does not represent other mechanisms of using
LMs for legal interpretation, such as producing ar-
guments for and against an interpretation (Waldon
et al., 2025), or eliciting examples (Almeman et al.,
2024).

We use data from a previous study of consensus
in legal interpretation. The authors of that study
make no claims as to the overall representativeness
of their experimental stimuli to questions that come
up in day-to-day contractual interpretation. They
also explicitly discuss the role of researcher sub-
jectivity in the constructing the stimuli. We do not
report any representativeness or coverage informa-
tion for the data. Hence, despite the diversity of
scenarios compared to other related works, it is
currently unclear how representative it is of LLM
use for legal interpretation in practice.

Our evaluation utilizes a small sample of 138
unique items. This is a small dataset for testing
the generalization of language model judgments
for the task.

We attempt to use question variants in a con-
trolled manner to investigate how such variation
affects judgment. However, the interplay between
question variants and LLM judgment may not be
easily disambiguated. LLMs’ ability to understand
and follow task instructions cannot be guaranteed,
especially without targeted post-training (Tamkin
et al., 2023).

Our models are not chose based on their attested
performance on natural language benchmarks. Ad-
ditionally, none of the models evaluated are consid-
ered “reasoning” or “thinking” models.

While we attempt to include models with vary-
ing size, architecture, and training recipe, we do not
present our findings to be comprehensive or conclu-
sive. Larger and more recent models with ‘reason-
ing’, ‘retrieval’ and/or search methods may be able

to provide stable, reliable sources of human-like
legal interpretation. We also do not use “chain-of-
thought” or other prompting or in-context learning
methods meant to elicit or induce intermediate or
reasoning steps. These have been shown to im-
prove model performance in many tasks. Hence,
our results may represent a lower estimation of
model ability.

We use first token probability as the model
response; this has advantages both in comput-
ing resources and analysis, but provides a lim-
ited representation of model output (Hu and Levy,
2023). Additionally, the first token probability has
some known drawbacks (Wang et al., 2024), espe-
cially for instruction-tuned models, which we take
additional steps to mitigate. Alternatively, non-
instruction-tuned models are less likely to provide
appropriate responses to our QA formulation.

Due to the complexity of responding to negation
and interpreting it from the first token, we check
the text to ascertain the correct polarity (COVERED
or NOTCOVERED) to use for the judgments. How-
ever, this does not guarantee that all scenarios get
captured under the polarity judgment. For distri-
butional judgment, we collate the probabilities to
only three categories, in which the tail of the prob-
ability distribution is reduced and represented as
‘Other’. This approximation reduces the precision
and validity of the distance metric. Our correlation
analysis uses a specific transformation of model
responses and compares it to the COVERED pro-
portion. We did not perform targeted validation
for establishing our linking hypothesis in connect-
ing human responses and LLM responses for legal
interpretation.

We did not perform checks for data contamina-
tion (Sainz et al., 2023) in the language models we
have used. Data contamination of the published ma-
terials and reference policies used in the previous
study (Waldon et al., 2023) could have influenced
the models we studied. We catalog the cutoff and
release dates in §B.

Ethical considerations

Data sourcing. We create task data and utilize
anonymized crowd-sourced human survey data
from a previous work (Waldon et al., 2023) which
was made available publicly. We did not collect
any new human judgment data.

Highlighting scope for abuse. We highlight
drawbacks of using LLMs to obtain legal interpreta-


===== PAGE BREAK =====

tion and report the effects of variation across input
on LLM judgment. However, outside of ‘prompt
shopping’ which is a risk of LLM use we discuss,
we do not propose or provide any adversarial meth-
ods that can be adapted to abuse and exploit LLMs.

Responsible research statement

We utilized ChatGPT (OpenAI et al., 2024),
GitHub Copilot, and AI Assistant in Pycharm dur-
ing the implementation of our experiments.

Acknowledgments

This research was supported in part by NSF award
IIS-2144881. The experimental portion of this
work was made possible by the Georgetown Uni-
versity High Performance Computing cluster, Cal-
cul Québec, and the Digital Research Alliance of
Canada. We thank Kevin Tobia, Ethan Wilcox,
and Amir Zeldes, members of the NERT lab, and
anonymous reviewers for helpful discussions and
feedback that informed this work.

References

Fatemah Yousef Almeman, Steven Schockaert, and Luis
Espinosa Anke. 2024. WordNet under scrutiny: Dic-
tionary examples in the era of large language mod-
els. In Proceedings of the 2024 Joint International
Conference on Computational Linguistics, Language
Resources and Evaluation (LREC-COLING 2024),
pages 17683-17695, Torino, Italia. ELRA and ICCL.

Tatsuya Aoyama and Ethan Wilcox. 2025. Language
models grow less humanlike beyond phase transition.
In Proceedings of the 63rd Annual Meeting of the
Association for Computational Linguistics (Volume 1:
Long Papers), pages 24938-24958, Vienna, Austria.
Association for Computational Linguistics.

Shabnam Behzad, Keisuke Sakaguchi, Nathan Schnei-
der, and Amir Zeldes. 2023. ELQA: A corpus of
metalinguistic questions and answers about English.
In Proceedings of the 61st Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 2031-2047, Toronto, Canada.
Association for Computational Linguistics.

Andrew Blair-Stanek and Benjamin Van Durme. 2025.
LLMs Provide Unstable Answers to Legal Questions.
Preprint, arXiv:2502.05 196.

Jiali Cheng and Hadi Amiri. 2025. Linguistic blind
spots of large language models. In Proceedings of the
Workshop on Cognitive Modeling and Computational
Linguistics, pages 1-17, Albuquerque, New Mexico,
USA. Association for Computational Linguistics.

Jonathan H. Choi. 2025. Off-the-Shelf Large Language
Models Are Unreliable Judges. Preprint, Social Sci-
ence Research Network:5188865. August 2025 ver-
sion.

Simona Frenda, Gavin Abercrombie, Valerio Basile,
Alessandro Pedrani, Raffaella Panizzon, Alessan-
dra Teresa Cignarella, Cristina Marco, and Davide
Bernardi. 2025. Perspectivist approaches to natural
language processing: A survey. Language Resources
and Evaluation, 59(2):1719-1746.

Iker Garcfa-Ferrero, Begofia Altuna, Javier Alvez, Itziar
Gonzalez-Dios, and German Rigau. 2023. This is
not a dataset: A large negation benchmark to chal-
lenge large language models. In Proceedings of the
2023 Conference on Empirical Methods in Natural
Language Processing, pages 8596-8615, Singapore.
Association for Computational Linguistics.

Jon Gauthier, Jennifer Hu, Ethan Wilcox, Peng Qian,
and Roger Levy. 2020. SyntaxGym: An online plat-
form for targeted evaluation of language models. In
Proceedings of the 58th Annual Meeting of the Associ-
ation for Computational Linguistics: System Demon-
strations, pages 70-76, Online. Association for Com-
putational Linguistics.

Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,
Abhinav Pandey, Abhishek Kadian, Ahmad Al-
Dahle, Aiesha Letman, Akhil Mathur, Alan Schel-
ten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh
Goyal, Anthony Hartshorn, Aobo Yang, Archi Mi-
tra, Archie Sravankumar, Artem Korenev, Arthur
Hinsvark, and 542 others. 2024. The llama 3 herd of
models. Preprint, arXiv:2407.21783.

Stefan Gries and Brian Slocum. 2017. Ordinary Mean-
ing and Corpus Linguistics. BYU Law Review,
2017(6):1417-1471.

James Grimmelmann, Benjamin Sobel, and David Stein.
2025. Generative misinterpretation. Harvard Jour-
nal on Legislation, 63.

Dirk Groeneveld, Iz Beltagy, Evan Walsh, Akshita
Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya
Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang,
Shane Arora, David Atkinson, Russell Authur, Khy-
athi Chandu, Arman Cohan, Jennifer Dumas, Yanai
Elazar, Yuling Gu, Jack Hessel, and 24 others. 2024.
OLMo: Accelerating the science of language mod-
els. In Proceedings of the 62nd Annual Meeting of
the Association for Computational Linguistics (Vol-
ume I: Long Papers), pages 15789-15809, Bangkok,
Thailand. Association for Computational Linguistics.

David A Hoffman and Yonathan Arbel. 2024. Genera-
tive interpretation. New York University Law Review,
page 451.

Jiseung Hong, Grace Byun, Seungone Kim, Kai Shu,
and Jinho D. Choi. 2025. Measuring Sycophancy of
Language Models in Multi-turn Dialogues. Preprint,
arXiv:2505.23840.


===== PAGE BREAK =====

Jennifer Hu and Roger Levy. 2023. Prompting is not
a substitute for probability measurements in large
language models. In Proceedings of the 2023 Con-
ference on Empirical Methods in Natural Language
Processing, pages 5040-5060, Singapore. Associa-
tion for Computational Linguistics.

Siddharth Karamcheti, Laurel Orr, Jason Bolton, Tianyi
Zhang, Karan Goel, Avanika Narayan, Rishi Bom-
masani, Deepak Narayanan, Tatsunori Hashimoto,
Dan Jurafsky, and 1 others. 2021. Mistral—a journey
towards reproducible language model training.

Carina Kauf, Emmanuele Chersoni, Alessandro Lenci,
Evelina Fedorenko, and Anna A Ivanova. 2024. Log
probabilities are a reliable estimate of semantic plau-
sibility in base and instruction-tuned language mod-
els. In Proceedings of the 7th BlackboxNLP. Work-
shop: Analyzing and Interpreting Neural Networks
for NLP, pages 263-277, Miami, Florida, US. Asso-
ciation for Computational Linguistics.

Carina Kauf, Anna A Ivanova, Giulia Rambelli, Em-
manuele Chersoni, Jingyuan Selena She, Zawad
Chowdhury, Evelina Fedorenko, and Alessandro
Lenci. 2023. Event knowledge in large language
models: the gap between the impossible and the un-
likely. Cognitive Science, 47(11):e13386.

Anita S Krishnakumar. 2024. Textualism in practice.
Duke Law Journal, 74(3):573-679.

S. Kullback and R. A. Leibler. 1951. On Information
and Sufficiency. The Annals of Mathematical Statis-
tics, 22(1):79-86.

Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying
Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gon-
zalez, Hao Zhang, and Jon Stoica. 2023. Efficient
Memory Management for Large Language Model
Serving with PagedAttention. In Proceedings of the
29th Symposium on Operating Systems Principles,
SOSP °23, pages 611-626, New York, NY, USA.
Association for Computing Machinery.

Thomas R. Lee and Jesse Egbert. 2024. Artificial
Meaning? Preprint, Social Science Research Net-
work:4973483.

Kangcheng Luo, Quzhe Huang, Cong Jiang, and Yan-
song Feng. 2025. Automating legal interpretation
with LLMs: Retrieval, generation, and evaluation.
In Proceedings of the 63rd Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 4015-4047, Vienna, Austria.
Association for Computational Linguistics.

R. Thomas McCoy and Thomas L. Griffiths. 2025. Mod-
eling rapid language learning by distilling Bayesian
priors into artificial neural networks. Nature Commu-
nications, 16(1):4676.

Kevin Newsom. 2024a. Concurring opinion in Snell v.
United Specialty Insurance Co.

Kevin Newsom. 2024b. Concurring opinion in United
States v. Deleon.

11

Byung-Doh Oh, Shisen Yue, and William Schuler. 2024.
Frequency explains the inverse correlation of large
language models’ size, training data amount, and
surprisal’s fit to reading times. In Proceedings of
the 18th Conference of the European Chapter of the
Association for Computational Linguistics (Volume 1:
Long Papers), pages 2644-2663, St. Julian’s, Malta.
Association for Computational Linguistics.

OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,
Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-
man, Diogo Almeida, Janko Altenschmidt, Sam Alt-
man, Shyamal Anadkat, Red Avila, Igor Babuschkin,
Suchir Balaji, Valerie Balcom, Paul Baltescu, Haim-
ing Bao, Mohammad Bavarian, Jeff Belgum, and
262 others. 2024. GPT-4 technical report. Preprint,
arXiv:2303.08774.

Ferdinand Osterreicher and Igor Vajda. 2003. A new
class of metric divergences on probability spaces and
its applicability in statistics. Annals of the Institute
of Statistical Mathematics, 55(3):639-653.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, and | others. 2019.
Language models are unsupervised multitask learn-
ers.

Oscar Sainz, Jon Campos, Iker Garcia-Ferrero, Julen
Etxaniz, Oier Lopez de Lacalle, and Eneko Agirre.
2023. NLP evaluation in trouble: On the need to mea-
sure LLM data contamination for each benchmark.
In Findings of the Association for Computational
Linguistics: EMNLP 2023, pages 10776-10787, Sin-
gapore. Association for Computational Linguistics.

Mrinank Sharma, Meg Tong, Tomasz Korbak, David
Duvenaud, Amanda Askell, Samuel R. Bowman,
Esin DURMUS, Zac Hatfield-Dodds, Scott R John-
ston, Shauna M Kravec, Timothy Maxwell, Sam Mc-
Candlish, Kamal Ndousse, Oliver Rausch, Nicholas
Schiefer, Da Yan, Miranda Zhang, and Ethan Perez.
2024. Towards understanding sycophancy in lan-
guage models. In The Twelfth International Confer-
ence on Learning Representations.

Brian G. Slocum. 2015. Ordinary Meaning: A Theory
of the Most Fundamental Principle of Legal Interpre-
tation. University of Chicago Press.

Lawrence M Solan and Tammy Gales. 2017. Corpus
linguistics as a tool in legal interpretation. Brigham
Young University Law Review, 2017(6):1311-1357.

Neha Srikanth, Rachel Rudinger, and Jordan Lee
Boyd-Graber. 2025. No questions are stupid, but
some are poorly posed: Understanding poorly-posed
information-seeking questions. In Proceedings of the
63rd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
3182-3199, Vienna, Austria. Association for Compu-
tational Linguistics.

Alex Tamkin, Kunal Handa, Avash Shrestha, and Noah
Goodman. 2023. Task ambiguity in humans and
language models. In The Eleventh International Con-
ference on Learning Representations.


===== PAGE BREAK =====

Gemma Team, Thomas Mesnard, Cassidy Hardin,
Robert Dadashi, Surya Bhupatiraju, Shreya Pathak,
Laurent Sifre, Morgane Riviére, Mihir Sanjay
Kale, Juliette Love, Pouya Tafti, Léonard Hussenot,
Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam
Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros,
Ambrose Slone, and 89 others. 2024. Gemma: Open
models based on gemini research and technology.
Preprint, arXiv:2403.08295.

Tristan Thrush, Jared Moore, Miguel Monares, Christo-
pher Potts, and Douwe Kiela. 2024. I am a strange
dataset: Metalinguistic tests for language models. In
Proceedings of the 62nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 8888-8907, Bangkok, Thailand.
Association for Computational Linguistics.

Kevin Tobia. 2024. New Methods in Statutory Inter-
pretation: Surveys, Corpus Linguistics, ChatGPT.
Preprint, Social Science Research Network:4932610.

Thinh Hung Truong, Timothy Baldwin, Karin Verspoor,
and Trevor Cohn. 2023. Language models are not
naysayers: an analysis of language models on nega-
tion benchmarks. In Proceedings of the 12th Joint
Conference on Lexical and Computational Seman-
tics (*SEM 2023), pages 101-114, Toronto, Canada.
Association for Computational Linguistics.

Neeraj Varshney, Satyam Raj, Venkatesh Mishra, Ag-
neet Chatterjee, Amir Saeidi, Ritika Sarkar, and
Chitta Baral. 2025. Investigating and addressing hal-
lucinations of LLMs in tasks involving negation. In
Proceedings of the 5th Workshop on Trustworthy NLP
(TrustNLP 2025), pages 580-598, Albuquerque, New
Mexico. Association for Computational Linguistics.

Brandon Waldon, Madigan Brodsky, Megan Ma, and
Judith Degen. 2023. Predicting consensus in legal
document interpretation. Proceedings of the Annual
Meeting of the Cognitive Science Society, 45(45).

Brandon Waldon, Nathan Schneider, Ethan Wilcox,
Amir Zeldes, and Kevin Tobia. 2025. Large lan-
guage models for legal interpretation? Don’t take
their word for it. Georgetown Law Journal, 114(1).

Xinpeng Wang, Chengzhi Hu, Bolei Ma, Paul Rottger,
and Barbara Plank. 2024. Look at the text:
Instruction-tuned language models are more robust
multiple choice selectors than you think. In First
Conference on Language Modeling.

Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-
hananey, Wei Peng, Sheng-Fu Wang, and Samuel R.
Bowman. 2020. BLiMP: The benchmark of linguis-
tic minimal pairs for English. Transactions of the
Association for Computational Linguistics, 8:377—
392.

Alex Warstadt, Amanpreet Singh, and Samuel R. Bow-
man. 2019. Neural network acceptability judgments.
Transactions of the Association for Computational
Linguistics, 7:625-641.

12

A Additional details on vague contracts

Waldon et al. (2023) collected human judgments
for interpretive questions about insurance contract
vignettes. Here we detail the structure and topics
of these vignettes. Figure 4 illustrates the kind of
text seen by human subjects in their study:

Steve’s car insurance policy includes cover-
age for “Vehicle Damage,” defined as “loss
or damage to the policy holder’s 1) car; or
2) car accessories (while in or on the car).”

One day, Steve is involved in a mi-
nor accident. His GPS navigation
system, which was in the car at the
time, was damaged. Steve files a
claim with his insurance company
for the damage.

1. Do you think that the claim is covered
under Vehicle Damage as it appears in the
policy? [YES / NO/ CAN’ T DECIDE]

2. You are one of 100 people who have vol-
unteered to answer these questions. How
many of the 100 do you think will agree
with your answer to question (1)?

3. How confident are you in your answer
to question (1)? [(Not at all / Slightly /
Moderately / Very / Totally) confident]

Figure 4: An example vignette from the questionnaire
provided to the participants by Waldon et al. (2023).
The vignette corresponds to one of the 138 items. Since
our study focuses on interpretative judgment, question 1
is of interest to us, and responses make up the human
judgments used in correlation analysis in §4.3.

Each vignette consists of a hypothetical contract
provision with a term and definition, coupled with
a hypothetical scenario meant to test the interpre-
tation of that definition (Table 1). The provisions
are drawn from a range of insurance types (Ta-
ble 7). The term at issue, or locus of uncertainty,
depends on the scenario; the full list of these terms
appears in Table 8. We note that these vignettes
were artificially constructed, but are meant to imi-
tate real-world insurance contract scenarios.

B_ Data contamination

Since our source data is from 2023, it is possible
that it was part of the training for one or more


===== PAGE BREAK =====

Insurance Types

Emergency Damages

Escape of Oil

Escape of Water

Fire

Flooding

Garden Plants

General Damages

Ground Heave

Hail Damage

Hot Work

House Removal

Identity Theft

Loss and Accidental Damage
Loss or Damage to a Goods Carrying Vehicle
Malicious Acts or Vandalism
Personal Accident

Personal Accidents

Public Liability Property Damages
Storm Damage

Trace and Access

Vehicle Damage

Vehicle Fire

Vehicle Glass

Vehicle Theft

Wind Damage

Table 7: The unique insurance types from the vague
contracts data. These refer to insurance types that were
used to design the various definitions (Insurance text
in Table 1). Each such insurance text has exactly three
scenarios with the same locus of uncertainty.

models in our selected suite of LLMs, especially
the larger models (which show the best correlation).

Table 9 catalogs the known cutoff dates for for
the models. Any model with the exception of GPT-
2 may have been partially or wholly exposed to the
dataset.

C_ Additional analysis of judgments to
Yes/No

Different distributions for different models.
We observe that different models allocate their dis-
tributional judgments in different ranges. For exam-
ple, GPT-4 allocates probabilities through the full
range (0.00,0.99], while Ministral-8B-Inst al-
locates it in a narrower range (0.19,0.58]. How-
ever, these variations across models do not repre-
sent the quality of models’ interpretive judgments.
Rather than basing analysis on the absolute values
of the probabilities, we look at the distributions
separately for each model to determine whether a
given model provides a useful signal for interpreta-
tion.

Instruction tuning allows a wider range of
judgment probabilities while introducing un-
predictable bias. Across the board, instruction-

13

Loci of Uncertainty

accessory
accidental

audio equipment

broken glass

causative “from”

cause

connected with business
connected with occupation
custody or control

damage

deliberate

family or employee

fire damage

first responder

flammable or combustable materials
flow of water

glass

ground heave

hard surface

heating installation
internal source

key theft

leaking

malicous people or vandals
naturally occuring fire
necessary and reasonable
outside the building
perceived emergency
permanent or total loss
political disturbance
professional movers

rapid build-up

reasonable steps

regular working conditions
requires / uses / produces
sudden/unforseen
temporarily removed

third party

tracking device

traveling in

wear and tear

Table 8: Loci of uncertainty in contract provisions.
(Note that these are descriptions in the dataset; the actual
text prompts in the Waldon et al.’s (2023) study did not
contain metalinguistic terms like causative.)

tuned models utilize a wider range of judgment
probabilities than their base counterparts, with
instruction-tuned OLMo, gemma, and GPT-4 mod-
els utilizing the entire space in [0, 1] as shown in
Table 4. However, the other changes introduced
by instruction tuning are less consistent— the mag-
nitude and direction of change in both categori-
cal and distributional judgment varies by model
and question variant. For example, instruction tun-
ing on Llama-8B leads to a predominance of NOT-
COVERED judgments, while instruction tuning of
Llama-7@B leads to predominance of COVERED
judgment, when prompted using the Yes/No ques-
tion variant. However, Figure 3 shows that instruc-


===== PAGE BREAK =====

Model           Reported Cutoff Date                      Data available before cutoff?
GPT-4         Dec 1, 2023                             Yes

GPT-2            Unknown (Released November 2019) No

Llama-3           December 2023                                      Yes

OLMo-2           December 2023                                      Yes

Ministral-8B Unknown (Released October 2024)       Maybe

Gemma-3        August 2024                                       Yes

Table 9: Reported cutoff date for training data or if unknown, model release date for each model. It is possible that
our source evaluation data was included in the training dataset for 14 out of 15 models we use for our study.

tion tuning the 70B model yields more COVERED
judgment with the Disagreement question vari-
ant, and more NOTCOVERED judgment with the
Negation question variant.

D Question variants

Our question variants in Table 2 contain styles
and phenomena we expect to be common.
These include ordering of choices (No/Yes,
OptionsF lipped), negation, and asking for agree-
ment or disagreement. Two variants, AgrWithNeg
and DisagwithNeg, contain multiple such phenom-
ena, compounding the challenge. These are also
the variants that are predominant in minority judg-
ments.

D.1 Minority Judgments

Some question variants are more likely to induce
minority judgment, disagreeing with the majority
of judgments induced by the same model given the
same query. As shown in Table 10, we find that
the Disagreement variant yields the minority judg-
ments most frequently, while the Yes/No variant
yields it the least frequently.

Variant                              Count Proportion
Disagr.            1256         0.21
Agr. w/ Neg.       1045         0.17
Disagr. w/ Neg.     918         0.15
Options                              809                     0.14
Agr.                                     501                     0.08
Options F.                     493                   0.08
Negation           489         0.08
N/Y                                           275                         0.05
Y/N                                     188                     0.03
Total                                      4975

Table 10: The number of minority judgments for each
question variant, and the percentage proportion in mi-
nority judgments. The counts are sorted vertically in
descending order. An equal proportion would lead to a
0.09 proportion for each variant.

Majority Count           3             4                 5
Model                              Frequency for count
Llama-3-70B                  22          21               95
+Inst                              24          93              21
Llama-3-8B        33    43      62
+Inst                              13          98              27
Llama-3-3B        11    113     14
+Inst                              57          48              33
Llama-3-1B        4    25     109
+Inst                              13         125              0
Ministral-8B-Inst    16    67      55
OLMo-2-7B        71    26     41
OLMo-2-7B-Inst    36    77      25
Gemma-7b                          45            50                43
+it                                          9             47                 82
GPT-2-medium             67          71                0
GPT-4                                 5            41               92
All                                            426 945              699

Table 11: Number of items by number of question vari-
ants that yielded the majority judgment for the model.
For example, there were 5 items for which GPT-4 pro-
duced one judgment for 3 variants, and the opposite
judgment for 2 variants. Each judgment is a binary
choice between COVERED and NOTCOVERED. This
is a replication of Table 5 but without the four most
minority response inducing variants Disagreement,
AgreementWithNegation, DisagrWithNegation and
Options in the mix Table 10.

D.2 Some question variants have a stronger
distributional effect than others.

To further quantify the effect of question variation,
we operationalize the effects of each question vari-
ant as a distribution distance between that judgment
for the default Yes/No question and the question
variants. We measure distribution distance with
the Jensen-Shannon distance metric (JSD; Oster-
reicher and Vajda, 2003), which is based on KL
divergence (Kullback and Leibler, 1951), but is
symmetric and provides a distance between 0.0
(identical) and 1.0 (maximally different). The ques-
tion variant that yields the most distant distribution
for each model are listed in Table 12, where aver-


===== PAGE BREAK =====

Model               Variant             Mean     Std
Llama-3-70B        Options              0.09 0.04
+Inst                            Negation                        0.34 0.17
Llama-3-8B           Options F.             0.15 0.04
+Inst             Disagr. w/ Neg.     0.18 0.06
Llama-3-3B         Agr. w/ Neg.        0.19 0.06
+Inst                 Options F.             0.22 0.07
Llama-3-1B                   Options                           0.28 0.05
+Inst                 Options F.             0.32 0.04
OLMo-2-7B        Disagr. w/ Neg.     0.37 0.05
+Inst              Agr. w/ Neg.        0.48 0.23
Ministral-8B-Inst Options                           0.16 0.04
Gemma-7B         Disagr. w/ Neg.     0.08 0.03
+it                 Options              0.78 0.04
GPT-2-medium     Disagr. w/ Neg.     0.17 0.02
GPT-4              Disagr. w/ Neg.     0.56 0.30

Table 12: The question variant for each model with
the largest Jensen-Shannon distance from the Yes/No
question. Higher distance indicates greater difference
between distributions.

age distance is obtained by calculating JSD at the
item level, and then aggregated by question vari-
ant for each model. e.g. The overloaded variant
Disagreement with negation most frequently (5
of the 15 models) leads to the biggest change in the
distributional judgments in the most models, includ-
ing up to 0.56 for GPT-4. These metrics also show
similar pattern as the minority responses, however
provide a probably based numerical overview.

D.3 Responses in cases of negation

We noted a mismatch between the expected polar-
ity of the answer, and the greedily decoded text
output for many models when it involved negation.
The models tended to provide judgment on the pos-
itive coverage statement rather than the negated
statement. However, our polarity indication at the
variant level can be inconsistent with polarity by
specific items and models.

E_ Implementation and Compute

E.1 Load-and-infer pipeline.

We use vl1m (Kwon et al., 2023) to implement our
inference pipeline and use the model implemen-
tations available on https: //huggingface.co/
models.

All our inference was completed on Tesla L4
GPUs with 24GB of memory, with the exception of
Llama-7QB, which were run on 4xA100, each with
40GB of memory. Our inference configuration will
be available as part of our public code.

Judgment as a sum of probability. In §3, we
consider LLM judgment as a sum of token proba-
bilities that correspond to each judgment. That is,

15

when answering yes to the question would indicate
the COVERED judgment, we consider p(COVERED)
= p(Yes) + p(yes) + p(YES).

Random Seed and Temperature Because a sig-
nificant portion of our study works with token prob-
abilities, we set temperature to 0, and hence there
is no randomness in our inference pipeline.

E.2. OpenAI GPT-4 Inference with APIs

We used OpenAI API platform for inference with
GPT-4. We use temperature=@ to get the highest
determinism. We use the GPT-4 model with the
model identifier gpt-4-0613.

F Linking hypothesis

In §4.3, we assume that our respective operational-
izations of human and LLM judgment, namely the
proportion of covered judgments %COVERED in
human responses and the probability difference be-
tween LLM covered and not covered judgments A,
have a linear relationship. While the assumption is
difficult to justify, we provide our attempt.

By definition, token probability p(w |C) = f(C)
in autoregressive language modeling (Radford
et al., 2019) represents the proportion of cases
where the next token w occurs given a model f and
number of environments with context C. In our im-
plementation, we consider this as a computational
analogue of querying a human population and cal-
culating the proportion of which that respond with
one judgment, or, the proportion of human judg-
ments in human responses. This is the basis of our
assumption that judgment probability as sum of to-
ken probabilities p(>COVERED) = p(Yes) + p(yes) +
p(YES) and proportion of covered judgments have
a linear relationship.

However, due to residual token probabilities that
linger in LLM probability distributions, we are
unable to represent LLM judgment with a single
token probability, as a low p(COVERED) does not
indicate a high pDU(NOTCOVERED). We thus take the
difference of the two judgments, A = p(COVERED)
— p(NOTCOVERED) to represent LLM judgment
that has a linear relationship with human majority
judgment. This allows for clear analysis, as the
expectation value of a judgment E(A) = 0, and A>
0 yields the COVERED judgment and A < 0 the
NOTCOVERED judgment.

Based on our linking assumption, consider the
ideal case where our two variables are perfectly
correlated to each other with R? = 1. In such case


===== PAGE BREAK =====

where A ¢€ [-1,1] and p(COVERED) € [0,1], we
predict that proportion of human covered judg-
ments is 0 when probability difference is —1 since
p(COVERED) =0, p(NOTCOVERED) = 1. It fol-
lows that proportion of human covered judgments
is 1 when probability difference is 1; and 0.5 when
probability difference 0. The best fit line would
then have m = 0.5, b= 0.5. Here, all of the variance
across human judgment is explained by A.

16


===== PAGE BREAK =====

Human and LLM judgment by Question Variation

Agreement              AgrWithNeg             eee

1.00                  1.00                  1.00

0.75             <-   0.75          e        0.75

0.50         “sen ze            0.50 § SoRe= e575.          0.50
> 0.25                                      0.25 fe oe” og                 0.25           sepa
8 0.00             0.00   oe        0.00 3
@   -1.0 -05 00 05 1.0  -1.0 -05 00 05 1.0 -1.0 -05 00
[o}
S        DisagrWithNeg              nope 9 the                No/Yes
& 1.00                  1.00                  1.00      °
& 0.75            0.75            0.75     é .-*=   LLM
= 0.50      2.80, 237      0.50                    0.50 [| o Re= 0565 ©
© 0.25         m*_® 025       Pw     0.25              © GPT-4
5 0.00  og ‘S06  “ere 0.00               9.00
3    -1.0 -05 00 05 1.0 -1.0 os  0.0           -1.0 -05 00 05 1.0
Cc
2          Options              OptionsFlipped              YesiNo |
5 1.00       ve          1.00            ®        1.00                ge
I 0.75         7    0.75   ee © a Ke 075

0.50     _ R= coz.      0.50     = Re= 0553       0.50      ain o00

0.25            "8 0.25              0.25

0.00                  90%                  0.00

-1.0 -05 00 05       -1.0 -05 00 05 1.0    -1.0 -05 00 05 1.0
p(Covered) - p(NotCovered)

Figure 5: GPT-4 judgment probabilities versus human consensus across question variants. Dotted lines are best
best-fit lines between human and instruction-tuned LLM.

17
