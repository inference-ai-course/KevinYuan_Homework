2510.25536v2 [cs.CL] 30 Oct 2025

e
e

arX1Vv

©. TwinVoice: A Multi-dimensional Benchmark

Twinvoice

Towards Digital Twins via LLM Persona Simulation

Bangde Du!*, Minghao Guo”*, Songming He’, Ziyi Ye*', Xi Zhu”, Weihang Su', Shuqi Zhu', Yujia Zhou!,
Yongfeng Zhang”, Qingyao Ai‘, Yiqun Liu!

‘Tsinghua University, 7Rutgers University, 3Fudan University
*Equal Contribution, ‘Corresponding authors

Large Language Models (LLMs) are exhibiting emergent human-like abilities and are increasingly
envisioned as the foundation for simulating an individual’s communication style, behavioral tendencies,
and personality traits. However, current evaluations of LLM-based persona simulation remain limited:
most rely on synthetic dialogues, lack systematic frameworks, and lack analysis of the capability
requirement. To address these limitations, we introduce TwinVoice, a comprehensive benchmark
for assessing persona simulation across diverse real-world contexts. TwinVoice encompasses three
dimensions: Social Persona (public social interactions), Interpersonal Persona (private dialogues), and
Narrative Persona (role-based expression). It further decomposes the evaluation of LLM performance
into six fundamental capabilities, including opinion consistency, memory recall, logical reasoning,
lexical fidelity, persona tone, and syntactic style. Experimental results reveal that while advanced
models achieve moderate accuracy in persona simulation, they still fall short of capabilities such as
syntactic style and memory recall. Consequently, the average performance achieved by LLMs remains
considerably below the human baseline.

 Project Page: https://twinvoice.github.io

€) Code: https://github.com/TwinVoice/TwinBench

© Leaderboard: https: //huggingface.co/spaces/bangdedadi/TwinVoice-Leaderboard

S Dataset: https: //huggingface.co/datasets/bangdedadi/TwinVoice

1. Introduction

Large Language Models (LLMs) are rapidly evolving from basic text generators into human-like agents [8, 47, 9].
Existing studies have shown that the most advanced LLMs are capable of producing text indistinguishable
from human writing [19, 20, 18]. Consequently, the research focus is shifting toward a highly specific
challenge: Can we construct “digital twins” of specific individuals that are indistinguishable from themselves?
To address this challenge, existing studies have relied on LLM-based persona simulation, which replicates a
person’s unique style of talking, behavior, and personality [39, 33] based on their data. LLM-based persona
simulation is supposed to unlock a series of applications, including highly personalized assistants [29, 25],
social simulations [24, 37, 38], healthcare [3], and marketing [16]. Despite growing interest in creating digital
twins with LLM-based persona simulation, its current ability remains unexplored due to the lack of systematic
evaluation [45, 53].

To address this issue, current evaluations have attempted to test LLM’s ability in imitating and predicting
human behaviors. For example, BehaviorChain [27] evaluates continuous persona-based behavior by requiring
models to iteratively predict the next action given persona profile and history, with performance degrading as
chains lengthen. Human Simulacra [48] and PersoBench [1] assess human-likeness and personalized response

Authors’ email addresses: Bangde Du(dbd23@mails.tsinghua.edu.cn), Minghao Guo(minghao.guo@rutgers.edu), Songming
He(superansonhe@gmail.com), Ziyi Ye(zyye@fudan.edu.cn), Xi Zhu(xi.zhu@rutgers.edu), Weihang Su(swh22@mails.tsinghua.edu.
cn), Shuqi Zhu(zsq19991106@gmail.com), Yujia Zhou(zhouyujia@mail.tsinghua.edu.cn), Yongfeng Zhang(yongfeng.zhang@rutgers.
edu), Qingyao Ai(aiqy@tsinghua.edu.cn), Yiqun Liu(yiqunliu@tsinghua.edu.cn).


===== PAGE BREAK =====

Persona      Self Voice                Persona Simulation                Twin Voice

Memory
Recall

Evaluation by LLM-as-a-Judge

Stimulus: “Updated news:

Logical                           Memor
Lakers beats Warriors!”                                                                        he

Stimulus: “NBA is popular                                                                                                                     Reasoning                Recall

in US."

Self: "I really love                          :                             =
watching NBA, especially           Kegftasll 3)           Opinion @:
Lakers’, Join us! ”                        Reasoning ~                 Consistency                                                                                                                        0.560

Digital Twin:
Yeah! Lakers            Damn! T hope
is my favorite          it was not
team!          “Mts truth,         3

‘p.508
Lexical                      ‘| Opinion
» | Consistency
/\¢

Mindset Coherence

A             Persona
h a |        Simulation S L

(Stimulus: "Hey man! What do
you think of cryptocurrency?"
Digital Twin:

Stimulus: "Hey man! What )
do you think of
cryptocurrency?"

Self: "Hey bro don't
touch it! it is fakel u will
lose everything"

Believe me.               Congratulatio
bro it isa              ns, Sir, you
‘rapl           +fts | deserve it

—j/ Syntactic
Style

2

51.8% 51.4%
46.4%

40   |    38.9% 38.3% 37.3%. 35.8%

(Stimulus: "The door is locked
| from inside. What happened?”

>»   Digital Twin:
Clearly, that             Maybe magic
        window was               opened the
      the only exit +/I"s | door.
3}

Figure 1 The conceptual framework of TwinVoice: (Left) The evaluation is structured across three core dimensions that
represent distinct aspects of identity expression: Social Persona (public facing), Interpersonal Persona (private
interaction), and Narrative Persona (fictional scenarios). The LLMs are prompted with a person’s historical context
to simulate their behavior. The LLM’s ability for persona simulation is categorized into six fundamental capabilities.
(Right) Experimental results averaged over three dimensions are presented.

Narrative
Persona

ad

Stimulus: Sherlock is logical. )
“There's a footprint”

Syntactic
Style

Self: “The wet marks and
footprints are everywhere.
| Show a recent exit.”

quality, while other studies probe persona-driven decision making, counterfactual adherence, and large-scale
dynamic profiling [49, 23, 17]. However, those evaluation benchmarks face limitations in both their scope and
granularity. On the one hand, the predominant reliance on synthetic dialogues [41, 46] prevents benchmarks
from capturing the rich expression of human identity across diverse real-world contexts (Scope Limitation).
On the other hand, current benchmarks are often evaluated based on an LLM’s accuracy in predicting human
behavior, leaving a critical gap in understanding the fundamental capabilities—such as memory, reasoning,
and lexical fidelity—that a model must possess for authentic simulation (Granularity Limitation).

To bridge the gap between the vision of digital twins and the current capabilities of persona simulation, we
introduce TwinVoice, a comprehensive benchmark designed for realistic and fine-grained persona evaluation (see
Table 1 for a comparison with prior persona-simulation benchmarks; “persona size” denotes the number of
distinct, independent personas per benchmark). To address the scope limitation, TwinVoice is grounded in
both real-world and synthetic data across three complementary dimensions in persona simulation (see Figure 1):
Social Persona, Interpersonal Persona, Narrative Persona. The Social Persona dimension leverages real-world
social media data to evaluate a public-facing identity, while the Interpersonal Persona dimension utilizes
multi-session dialogue data to assess a more private, relational self. While these two dimensions are grounded
in authentic digital footprints, the Narrative Persona is designed to complement such data with fictional
scenarios to test behaviors and narrative consistency in more diverse contexts. Addressing the granularity
limitations of holistic accuracy evaluations, we shift from end-to-end scoring to capability-level assessment.
Building on psycholinguistic evidence that language conveys both what people say and how they say it [34],
we group persona fidelity into Mindset Coherence and Linguistic Expression. Mindset Coherence assesses the
logical and factual consistency of the content, including Opinion Consistency [51], Memory Recall [11], and
Logical Reasoning [21]. Linguistic Expression evaluates the language’s stylistic form, encompassing Lexical
Fidelity [30, 22], Persona Tone [7], and Syntactic Style [6]. Digital twins face two practical interaction modes:
constrained selection among plausible replies and open-ended generation under realistic stimuli. Accordingly,
TwinVoice instantiates both modes in its tasks to balance objectivity and ecological validity. We later evaluate
the tasks with a multiple-choice protocol and a judge-based protocol, and include a human consistency check
(see Sections 5.3 and 5.4).

We evaluate a series of state-of-the-art LLMs on TwinVoice under both paradigms and reveal several key
insights into current capabilities and limitations in persona simulation. In the discriminative task, GPT-
3.5-Turbo achieves an accuracy of 47.5%, while more advanced models show considerable gains, reaching
71.2% and 76.2% for GPT-5 and Claude-Sonnet-4, respectively [2]. In the generative tasks, GPT-5 [32] leads
with 48.5% accuracy in generative-ranking tasks and a score of 2.13 in the generative-scoring tasks. The
average performance across different models demonstrates that the dataset can significantly differentiate
the advancement of LLMs. However, the persona simulation performance achieved by LLMs still falls short
of human baselines. On the subset under the discriminative task, the human majority vote accuracy is


===== PAGE BREAK =====

Table 1 A comparison of TwinVoice with Prior LLM Persona-Simulation Benchmarks.

Persona Real-World Multiple Multi-Paradigm Human Fine-Grained Multilingual
Benchmark

Size       Sourcing     Dimensions      Evaluation      Baseline Capabilities      Coverage
Human Simulacra [48]       11            @)             @)                @)              ~            &)               @®)
BehaviorChain [27|        1,001                           ®)                @)              «@)            ®)               @)
PersonaEval [52]           130                                           ()             @)                            @)
PERSONAMEM [17]       20          ®)                                           ®)                           ®)
TwinVoice             4,553            Y)                                     8                Vv)                                   Vv)

66.0%, while GPT-5 reaches 60.0%, indicating clear headroom. The performance also varies across different
dimensions and capabilities. For dimensions, we observe that the average performance is highest under the
Narrative persona, while Social and Interpersonal lag. This finding suggests that simulating human behavior
within Social and Interpersonal Personas presents a greater challenge, likely due to the demands of dynamic
interaction and long-term consistency inherent to these dimensions. Across capabilities, models perform best
on Lexical Fidelity and Opinion Consistency and worst on Persona Tone and Memory Recall, indicating an
imbalance in their persona simulation abilities. These patterns will guide subsequent research and upgrades
to LLM persona simulation.

Contributions of this work are threefold: (1) We introduce TwinVoice, a comprehensive benchmark for
evaluating LLM-based persona simulation across multiple real-world scenarios with systematic competency
decomposition; (2) We develop novel evaluation methodologies combining discriminative assessment with
LLM-as-a-Judge for generative tasks; and (3) We provide extensive empirical analysis showing the limitations
of the most advanced LLMs in person simulation and offer crucial insights for advancing personalized AI
systems.

2 Related work

2.1 Personalized Agents and Digital Twins

The construction of digital twins, virtual replicas of specific individuals, is an emerging challenge in AI [39, 33].
Originating in engineering as counterparts to physical systems [14], the concept now extends to AI agents that
capture a person’s communication style, preferences, and personality. Recent efforts have operationalized this
vision across diverse domains. Examples include reviving anime characters [24], simulating agent societies
from novels [37, 28], and evaluating impersonation of writing styles and memories [42]. Applications have been
explored in healthcare [3], marketing [16, 43], and through industry systems like SecondMe [40] for lifelong
personal modeling. While these human-centered digital twins promise highly personalized chatbots [29, 25]
and ubiquitous computing applications [13], prior research has often focused narrowly on style imitation,
overlooking the broader competencies required for authentic persona simulation.

2.2 Datasets, Benchmarks, and Evaluation for Persona Simulation

Progress in this area depends on high-quality datasets and benchmarks. Recent resources have begun to fill this
gap, offering diverse evaluation protocols. Benchmarks have been developed from large-scale surveys of human
traits [45, 10], persona-based behavior chains [27], psychology-guided agent evaluations [48], persona-driven
decision-making tasks [1, 49], and multi-party dialogue role identification [53]. More recent work explores
challenging settings like counterfactual simulation [23] and dynamic user profiling [17].

Despite this growing landscape, evaluations remain fragmented and often rely on synthetic data, limiting their
ecological validity. This highlights the need for a unified framework to advance digital twin research rigorously.
Our TwinVoice benchmark addresses these limitations by leveraging real-world social media, conversational,
and fictional data to provide authentic and systematic evaluation across multiple persona dimensions.


===== PAGE BREAK =====

3 Task Formulation

Input                                                                       Evaluations

Stimulus:                                  Discriminative                    Generative-Ranking                Generative-Scoring

Did you watch the            IL Lak         fe     HN                Yeplll Lake    intl!                     Yep!!! Lak     m0
.                                       Yesll! Lakers are my favoritel!!                       epit Lakers winth                              ep! Lakers win!!!
finals between                                     v                               v

Lakers and MIA
Post              yesterday?

eS                                                              Damn! MIA lose...           x                 Oh no! MIA lose!           x                 Yepl!! MIA winsll!

No, I don't watch basketball | 3€              No, I don't like basketball       x                  No. How about you?

I prepared for my final >_<      x                T am busy with my final         x             Lakers finally wins the game.

Ground truth
voice:                                                                       ¥ Digital Twin generates “twin              ¥ Digital Twin generates “twin

¥ Digital Twin directly picks   a          voice”                           ez           voice"                           2)
Yesll! Lakers are          the exact “self voice"      oa    v LLM-as-a-Judge picks the 2       v LLM-as-a-Judge rates the    i“
Profile          my favoritell!                  @ exact selr voice       self        most similar one.            twin        similarity (opinion, logic, style) twin

Figure 2 TwinVoice experiment evaluation overview: Top: The LLMs are prompted with a specific persona’s history and
tasked with a stimulus. Bottom: Three evaluation protocols: Discriminative: the model chooses among A—D, one of
which is the ground truth persona behavior. Generative-Ranking: the model writes and an LLM-as-a-Judge selects the
best candidate, yielding Acc.(Gen). Generative-Scoring: the model writes and the Judge rates similarity on opinion,
logic, and style, yielding Score(Gen).

3.1. Problem Definition

TwinVoice evaluates LLMs’ ability to simulate human personas through a unified task paradigm that captures
the essence of digital twin functionality. Formally, we define the persona simulation task as follows:

Given a persona’s historical data H = {hj,h2,...,hn} and a current stimulus s, the history is instantiated
per dimension (Social, Interpersonal, or Narrative) as social posts, multi-session conversations, or narrative
materials, respectively. The objective is to generate a response r that maximally approximates the ground
truth response r* that the original persona would produce in stimulus s, which can be formulated as an
optimization problem:

r* = arg max P(r|H, 8, Opersona);                                    (1)

where Opersona represents the latent persona characteristics learned from historical data H. The evaluation

objective is to assess how well an LLM M can approximate this optimal response:

Score = fsim(M(H, s), 7°),                                                      (2)

where fsim denotes a similarity function that measures persona consistency across multiple dimensions.

TwinVoice instantiates this general framework across three dimensions, each defined by its history source and
interaction stimulus:

Persona Dimensions

Social Persona. In this dimension, 1 consists of a user’s historical social media posts H8°%#! =
(social) 1 (social)     (social)           9                            99

{hj      hs      pooo gap     }\, and the stimulus s represents a new post requiring a response. The

challenge lies in maintaining stylistic consistency and opinion alignment in public discourse.

Interpersonal Persona.    Here, H comprises multi-session conversational history H™ =
int     int        int               int              9         9         9       9

niin er) nvr er) nin er) where each nin °") represents a dialogue session. The stimulus s is

a new utterance from a conversation partner, requiring the model to generate contextually appropriate
responses while maintaining conversational authenticity and memory-grounded consistency.

Narrative Persona. In this dimension, 7 encompasses character background information and behavioral
records Hnare = {plrarr) pimarra)— pimarra)y here each Ai" denotes either background

information or a prior action. The stimulus s describes a narrative scenario requiring character reaction,
testing the model’s ability to maintain role-based expression fidelity.



===== PAGE BREAK =====

Across all three settings, we adopt a capability-centric evaluation rather than a single holistic score. The
decomposition and scoring criteria are detailed in Section 4.2.

3.2 Evaluation Methodologies

To balance objectivity and ecological validity, we pair a discriminative multiple-choice evaluation (objective,
low-variance accuracy under controlled distractors) with a generative evaluation (open-ended persona fidelity
via LLM-as-a-Judge in ranking and scoring).

3.2.1. Discriminative Evaluation

The discriminative evaluation transforms the generation task into a multiple-choice selection problem. For each
test instance (s,r*), we construct a candidate set C = {r*,1r1,r2,r3} where r* is the ground truth response
and {r1, 72,73} are distractors. The evaluated LLM must select the most persona-consistent response from
the shuffled candidate set.

The construction of distractors varies across dimensions to ensure realistic evaluation scenarios:

Distractor Construction

Social Persona: Distractors are sampled from authentic responses by other users to similar posts,
preserving topical relevance while introducing stylistic and opinion variations.

Interpersonal Persona: Distractors are selected from real conversational responses in similar contexts,
maintaining conversational appropriateness while differing in personal characteristics.

Narrative Persona: Distractors are generated using advanced LLMs with alternative character interpre-
tations, ensuring narrative coherence while diverging from the target persona’s behavioral patterns.

Discriminative evaluation provides direct accuracy measurements:

N
1
Accuracy = NW S- 1[M(Hi, s:) = rj],                                      (3)
i=1

where N is the total number of test instances and 1[-] is the indicator function.

3.2.2 Generative Evaluation

While discriminative evaluation offers clear interpretability, real-world digital twin applications require open-
ended generation capabilities. Our generative evaluation employs LLM-as-a-Judge [15, 50] protocols to assess
response quality along multiple dimensions.

We implement two distinct judging approaches:

Scoring-based Evaluation. The judge model rates generated responses against ground truth using structured
evaluation criteria. Given a stimulus s, generated response rgen, and ground truth r*, the judge assigns
a score on a 1-5 scale based on three key dimensions: opinion consistency, logical coherence, and stylistic
fidelity. The scoring rubric emphasizes faithful persona replication, with higher scores awarded to responses
that demonstrate comprehensive alignment across all dimensions.

Ranking-based Evaluation. The judge identifies the most persona-consistent response from a candidate set
containing the generated response and the same distractors used in discriminative evaluation. This approach
mirrors discriminative evaluation while leveraging the judge’s nuanced understanding of persona consistency.

The generative evaluation score is computed as:

N
1
Scoregen = W S- Judge(Tgen,i; rz, Si),                                          (4)

i=l

where Judge(-) represents either the scoring or ranking function implemented by GPT-5.


===== PAGE BREAK =====

4 Benchmark Construction

4.1 Data Pre-processing

Social Persona. We constructed this dataset from the PChatbot Chinese microblog corpus [36]. To mitigate
noise and ensure each evaluation instance is meaningful, we started with 8,045 samples and applied our PCCD
(Persona-Clarity and Choice-Distinctiveness) framework. We filtered for users with rich histories (average
reply length of more than 10 characters; Type-Token Ratio not in the bottom 20th percentile) and for tasks
with unambiguous choices (response option cosine similarity less than 0.95). We then ranked the remaining
samples by a persona-choice alignment score, calculated as the similarity to the true response minus the
similarity to the most similar distractor, to select the final 2,000 high-quality instances.

Interpersonal Persona. We used the Pushshift Telegram corpus [4] to evaluate memory-grounded consistency.
Our curation process followed a multi-stage filtering funnel to distill a high-quality message set from 438,975
raw messages. We first selected high-activity users (active in three or more channels with 500 or more total
messages and 100 or more per channel). We then processed their messages by removing short utterances of
fewer than 5 tokens, retaining only the top 10% most informative instances by TF-IDF, and applying semantic
deduplication (similarity threshold of 0.90), resulting in 6,150 messages. From these, we extracted 2,500
multilingual tasks (including several languages like EN, RU, ES, PT), using GPT-5 to generate challenging
distractors to ensure the task tests deep persona understanding rather than superficial cue matching. We
also incorporated users’ cross-channel chat history as memory to test for consistency across different social
contexts.

Narrative Persona. We selected eight novels from the Project Gutenberg corpus [35] to test the model’s
ability to mimic the speaking styles of the given characters. From these novels, we extracted 1,187 speech
segments covering more than 50 characters. To obtain these data, we first segmented each novel into short,
indexed chunks, and from each chunk we extracted at most one utterance together with its context. We
then matched the speakers to the list of main characters, whose profiles contained their personality traits,
goals, motivations, and utterance histories. Once we finished collecting these speeches, each accompanied
by the relevant profile and context, we constructed our test dataset, which included both multiple-choice
questions and open-ended generative tasks. For the former, we paired each extracted utterance with three
distractor options created based on the personalities of the other main characters. For the latter, we provided
the context to the model and let it generate the most plausible utterance under the given circumstances.

4.2 Capability Decomposition

Guided by psycholinguistic evidence that language simultaneously conveys what people say (content) and
how they say it (style) [34], we coarsely group persona fidelity into two complementary dimensions: mindset
coherence and linguistic expression. This view is consistent with stable individual differences in language
documented across psychology and linguistics and their computational operationalizations [12, 5, 44, 31, 26].
We then instantiate these dimensions with six fundamental capabilities: mindset coherence comprises Opinion
Consistency [51], Memory Recall [11], and Logical Reasoning [21], whereas linguistic expression comprises
Lexical Fidelity [30, 22], Persona Tone [7], and Syntactic Style [6].

Annotation follows a prompt-aligned rubric: for each instance, annotators choose exactly one primary
capability and independently assess all six capabilities as true or false under strict criteria. Capabilities are
non-orthogonal by design, so multiple capabilities can be true while a single primary label captures the best-fit
signal. Full instructions, criteria, and prompt excerpts appear in Appendix B, with seed examples and the
JSON output format for reproducibility.


===== PAGE BREAK =====

Table 2 Dataset statistics across three dimensions. Each instance corresponds to a unique persona (#Users =
#Instances). Avg = average; Gen = generative; Disc = discriminative. The instruction template is counted into Token
counts.

Dimension             Instances Avg history turns Avg prompt tokens (Disc) Avg prompt tokens (Gen)

Social Persona             2000            15.0                    1371.1                        1215.2
Interpersonal Persona 2500                       30.0                                        1163.5                                              1139.4
Narrative Persona            1187                15.7                             934.3                                  910.7

Table 3 Benchmark results for Digital Twin models: We evaluate models using three distinct metrics: Acc. (%) is the
accuracy on the discriminative task. Acc. (Gen) (%) is the accuracy where a generative model’s output is evaluated
via multiple choice questions by a Judge. Score (Gen) is a pairwise comparison score against the ground truth for
generative outputs by a Judge. Higher values indicate better performance. The best result and the second best result
are in Bold and underlined, respectively.

Social Persona           Interpersonal Persona         Narrative Persona                 Average
Model / Tasks            Acc.       Acc.       Score | Acc.       Acc.       Score | Acc.       Acc.       Score | Acc.       Acc.       Score
(%) (Gen)(%) (Gen) | (%) (Gen)(%) (Gen) | (%) (Gen)(%) (Gen) | (%) (Gen)(%) (Gen)

GPT-3.5-Turbo | 34.9     26.0      2.57 | 41.2     40.1      1.53 | 66.3     46.2      1.98 | 47.5     37.4      2.03
Qwen2.5-14B       36.2      30.1       2.56 | 49.6      42.0       1.56 | 60.5      44.6       1.68 | 48.8      38.9       1.93
GPT-4o-mini       35.3      26.9       2.61 | 39.2      41.3       1.50 | 63.1      46.5       1.91 | 45.9      38.2       2.01
LLM GPT-OSS-20B      39.1      24.1       2.39 | 63.3      46.0       1.47 | 43.9      48.0       1.77 | 48.8      39.4       1.88
DeepSeek-V3       42.6      34.1       2.77 | 70.0      52.7       1.51 | 81.0      48.6       1.90 | 64.5      45.1       2.06
GPT-5-Chat        46.9      38.7       2.73 | 77.4      54.0       1.63 | 89.4      52.9       2.03 | 71.2      48.5       2.13
Claude-Sonnet-4 | 53.9           37.5            2.67 | 84.4           52.9             1.67 | 90.2           53.4            2.02 | 76.2           47.9            2.12

5 Experiments

5.1 Experimental Setup

We evaluate digital twin fidelity across Social, Interpersonal, and Narrative personas with a discriminative
multiple-choice task and a generative evaluation that consists of generative-ranking and generative-scoring.
The generative-scoring task in evaluation uses a GPT-5 as a judge to score the generated content from 1 to 5.
Dataset scale and average prompt length are summarized in Table 2.

5.2 Overall Results and Key Findings

The overall results across the three dimensions and both evaluations are reported in Table 3, performance on
each capability is shown in Figure 3, and text similarity metrics are presented in Table 5. Strong models,
notably GPT-5-Chat and Claude-Sonnet-4, lead across tasks. The generative evaluation remains harder than
the discriminative formulation: for the same model, Acc. (Gen) and Score (Gen) are consistently lower than
discriminative accuracy. Models are relatively stronger on Lexical Fidelity and Opinion Consistency and
weaker on Persona Tone and Memory Recall. Agreement between the judge and humans appears in Table 4.
Given that humans are imperfect simulators in this task, we treat human accuracy as a practical reference
point rather than a strict upper bound. BLEU-1, METEOR, and BERT-Score provide complementary
evidence in Table 5. Overall, the results point to remaining gaps in persona tone realization and in recalling
and using persona-specific details during generation.

On average, Claude-Sonnet-4 achieves the best discriminative accuracy, while GPT-5-Chat achieves the best
aggregate generative metrics. Among open-source models, DeepSeck-V3 leads overall and attains the best
Score (Gen) on the Social Persona dimension (2.77). The Narrative Persona yields the highest scores; Social
and Interpersonal are lower. Within the Interpersonal dimension, Acc. (Gen) and Score (Gen) produce close
but not identical orderings for top models, consistent with the distinction between ranking and absolute
scoring protocols.


===== PAGE BREAK =====

Social Persona            Interpersonal Persona            Narrative Persona      © Mean across dimensions ---- Chance level (25%)

70%              Opinion Consistency                                        Memory Recall                                          Logical Reasoning
S 60%}
8 50%                                               5

40%                       ol                                              al                                         s
€ 30%
[)     L    _    _    _    |                      L    _    _    |    _    |    |    |        L    _    |    _    |    |    |    |
™ 20%

70%                Lexical al                                         Persona Tone                                          Syntactic Style
S 60%              -                                                                  _
8 50%                 i                         ms

>
& 40%                                                       ol
al
€ 30%
[)     L    _    _    _                   |        L    _    _    |    HELE    |    |        L    _    |    _    |    |    |    |
30%                                                            ‘        i                                        i
& as   NX? ,                           ak   oy       N°,             Ve)                 & as   N°,         ce)    V2)

woo tino os see i:   SO              eo . Do” soe a    oye  ees      goo Sao Do” - see a     VO’ as

Far HY geek gael    3                Fgrd™ HY peek Qaeh       stg            Fad HY geek gel       og!

Figure 3 Performance across six capabilities. Each panel shows one capability. For each model, bars give scores on
the three dimensions—Social, Interpersonal, and Narrative. Purple diamonds indicate the mean across the three
dimensions for that model. The y-axis is the average over the three evaluation protocols: discriminative, generative
ranking, and generative scoring. The gray dashed line denotes chance level (25%).

5.3 Capability-wise Analysis

We analyze performance at the capability level within our framework and present the results in Figure 3,
aggregating discriminative accuracy with the two judge protocols (ranking and scoring).

From Figure 3, we have the following observations. First, the performance ranking of models across different
capabilities closely matches their ranking based on average performance. This indicates that stronger systems
improve relatively uniformly rather than only in a narrow skill. Second, on average, models score highest on
Lexical Fidelity and Opinion Consistency, and lowest on Persona Tone and Memory Recall. This suggests
that current systems capture surface wording and stable stances better than persona-consistent tone and
long-horizon recall. Third, individual models show distinct comparative advantages; for example, DeepSeek-V3
approaches GPT-5 on Lexical Fidelity despite trailing on others.

5.4 Generative Evaluation

5.4.1. Judge-based scoring and ranking evaluation

We assess the generative evaluation with two judge-based protocols introduced earlier, scoring (from 1 to 5)
and ranking, and we aggregate their outcomes as Acc.(Gen) and Score(Gen). We instantiate the judge with
GPT-5. Full results appear in Table 3, with prompt templates and rubrics in Appendix A.

Key results are as follows: GPT-5-Chat attains the strongest aggregate generative performance (Acc.(Gen)
48.5%, Score(Gen) 2.13), closely followed by Claude-Sonnet-4 (47.9%, 2.12). DeepSeek-V3 is competitive and
achieves the best Score(Gen) on the Social Persona dimension (2.77), despite trailing the leaders on other
dimensions. Compared with discriminative evaluation, generative performance is systematically lower across
models, underscoring the added difficulty of free-form persona simulation and the substantial headroom for
improvement.

5.4.2 Reliability of the Judge and Human Study

We validate the LLM-as-a-Judge methodology with a human study. Three expert annotators evaluated a
stratified sample of 50 items per judging mode (ranking and scoring), following our instruction set (Appendix E).
Annotators worked independently and were blinded to each other’s labels.


===== PAGE BREAK =====

Table 4 Agreement of the judge against human annotations and inter-annotator reliability.

Task                           Agreement judge vs. human ___Inter-annotator reliability
Ranking (four choice)             0.646"                      0.673"
Scoring (one to five)                0.591?                       0.605?

Symbols: & is Cohen kappa for categorical labels and p is Spearman correlation for ordinal scores. Sample size is 50.
Table 5 Text similarity metrics for Digital Twin models. We evaluate the generative outputs against the ground truth
using three distinct metrics. BLEU-1 ¢ measures unigram precision. METEOR ¢ considers precision, recall, and synonymy.
BERT-Score ¢ measures semantic similarity using contextual embeddings. Higher values are better for all metrics. Bold
numbers denote the best result and underlined numbers denote the second best in each column.

Social Persona               Interpersonal Persona              Narrative Persona                      Average

Model / Tasks            pieu-: meteor PERT | prey. meteor BERT peur meteor PERT | peu: meteor BERT
+          +       Score      +          +       Score      +          +       Score      +          +       Score

t                              t                              t                              t
GPT-3.5-Turbo     16.03      15.50     62.96     24.76      22.52     81.54     12.06      12.86     84.10     17.62      16.96     76.20
Qwen2.5-14B        17.68      15.38     63.25     26.09      23.76     81.57     11.67      11.92     83.99     18.48      17.02     76.27
GPT-4o0-mini        15.94      15.19     62.89     23.48      21.38     81.26     12.50      13.34     84.13     17.31      16.64     76.09
LLM GPT-OSS-20B      14.55      12.87     61.90     20.67      19.20     81.17     10.81      10.59     84.36     15.34      14.22     75.81
DeepSeek-V3        16.85      15.49     63.25     26.86      25.21     82.65     11.11      11.58     84.12     18.27      17.43     76.67
GPT-5-Chat        18.67      14.09     63.26     27.18      25.30     82.67     11.54      11.59     84.27     19.13      16.99     76.73
Claude-Sonnet-4    18.68      18.14     64.19     25.22      23.45     82.14     12.38      13.12     84.37    18.76      18.24     76.90

Agreement between the judge and humans is reported in Table 4 and is comparable to human inter-annotator
reliability: for ranking, Cohen’s « is 0.646 (judge vs. human) versus 0.673 (human—human); for scoring,
Spearman’s p is 0.591 (judge vs. human) versus 0.605 (human-human). These results indicate that the judge
is reliable, while the human inter-annotator agreement supports the quality and consistency of our annotation
protocol.

5.4.3 Text Similarity Metrics

To provide an objective reference, we also evaluate free-form generations with standard text similarity metrics:
BLEU-1, METEOR, and BERT-Score. Results are presented in Table 5. Averaged over the three dimensions,
Claude-Sonnet-4 attains the best BERT-Score (76.90) and METEOR (18.24), while GPT-5-Chat achieves the
best BLEU-1 (19.13). The resulting model ranking is broadly consistent with our judge-based evaluation.
These metrics serve as complementary evidence, as they emphasize lexical and local semantic overlap rather
than opinion alignment, reasoning, or persona tone.

5.5 Human vs. Model Performance

We benchmark human performance on the Social Persona discriminative task. Three expert annotators labeled
a stratified set of 50 items following our guidelines (Appendix E). The task involves long contexts and implicit
cues, which makes it challenging even for humans. Therefore, we treat human accuracy as a reference rather
than a strict upper bound.

Table 6 compares models to human baselines. GPT-5-Chat reaches 0.60 accuracy, below the human mean of
0.64 and the majority-vote aggregate of 0.66. Agreement with humans is high but short of human—human
reliability: Cohen’s « is 0.634 for model vs. human and 0.690 for inter-annotator agreement.

These results indicate that state-of-the-art models approach human reliability on this discriminative formulation
yet still trail aggregated human judgments, leaving measurable headroom.

Summary of Findings. Across three persona dimensions and two task formulations, strong models (GPT-5-Chat,
Claude-Sonnet-4) lead consistently, yet generative evaluation remains notably harder than multiple-choice
selection. Capability analysis pinpoints style control and memory recall as primary bottlenecks, while lexical
fidelity and opinion consistency are comparatively robust. Dimension-wise, performance is generally higher on
Narrative than on Social and Interpersonal.


===== PAGE BREAK =====

Table 6 Discriminative evaluation against a reference standard

Task           |              Accuracy              |          Agreement («)
| GPT-5 Humanmean Human vote | Model vs human _Inter-annotator
Discriminative | 0.60     0.64      0.66 |    0.634       0.690

Human mean is the average across individual annotators. Majority vote accuracy evaluates the
aggregated vote by annotators. Agreement uses Cohen kappa Kk. Sample size is 50.

Across tasks, results exhibit substantial variance between models without evident ceiling effects. There remains
clear headroom in three areas: maintaining persona coherence over extended contexts and across sessions,
producing a persona-consistent tone, and recalling and using persona-specific facts during generation.

6 Conclusions and Discussions

This paper addressed the evaluation of LLM-based persona simulation by introducing TwinVoice. Built on
real-world and fictional data from three dimensions, TwinVoice aims at testing LLMs’ ability in persona
simulation by decomposing it into six capabilities of mindset coherence and linguistic expression. Our
extensive evaluation of state-of-the-art models reveals a crucial gap: while leading models like GPT-5-Chat
and Claude-Sonnet-4 show improved accuracy over their predecessors, their performance still falls significantly
short of human-level capabilities. We also find that LLMs are adept at mimicking surface-level linguistic
styles, they consistently fail to maintain long-term consistency, particularly in memory recall and opinion
stability. By establishing the first fine-grained baselines in this domain, TwinVoice not only exposes the key
limitations of current models but also provides a clear roadmap towards personalized AI and digital twins
built with LLMs.

Rationale for Three Dimensions. TwinVoice is constructed based on Social, Interpersonal, and Narrative
personas to balance realism, coverage, and privacy. Social and Interpersonal tracks are built on real interaction
traces because evaluating digital twins requires performance in authentic public and private contexts; synthetic
or model-generated corpora alone underestimate the difficulty of sustaining identity over long horizons. For
Narrative persona, full real-world narrative streams are hard to obtain and raise privacy concerns; we therefore
use curated fiction to probe role-consistent expression under controlled, ethically tractable settings.

Evaluation Design. Digital twins must go beyond constrained selection to produce persona-consistent lan-
guage under open prompts. We therefore pair a discriminative multiple-choice protocol (with carefully
constructed, topically plausible distractors) with a generative protocol that assesses free-form responses
using two LLM-as-a-Judge variants (ranking and scoring) along opinion consistency, logical/factual fidelity,
and stylistic similarity. Judge reliability is supported by a human study with three expert annotators:
GPT-5-as-a-Judge reaches agreement close to human inter-annotator levels (ranking « ~ 0.646 vs. 0.673;
scoring p0.591 vs. 0.605).

Usability, Reproducibility, and Robustness. We release precise task definitions, prompts, and data paths so
researchers can plug in fine-tuning, RAG, long-term memory, or multi-agent controllers on the same inputs.
For generation, we fix temperature=0.0 and publish decoding settings, seeds, and candidate-construction
scripts; we log model build identifiers where available and release raw outputs to mitigate closed-API drift.
Social Persona derives from PChatbot; to reduce leakage we enforce semantic distinctiveness in choice sets and
apply persona-choice alignment filters, and we plan annual refreshes to retire suspect items. With parallelism
set to 10, end-to-end evaluation per model per dimension completes within 2 hours on our setup.

Coverage and Limitations. TwinVoice currently spans three dimensions and five languages: Social (Chinese),
Interpersonal (English, Spanish, Portuguese, Russian), and Narrative (English). Despite this breadth,
language balance within each dimension remains imperfect, and phenomena such as code-switching and
dialectal variation are underrepresented. Future releases will expand per-dimension language coverage and
diversify domains where consented and de-identified data are available.

Maintenance and Outlook. We will maintain TwinVoice with annual updates to address potential contamination,

10


===== PAGE BREAK =====

accommodate new model behaviors, and extend language and domain coverage. Planned upgrades include
longer-horizon tasks that jointly stress memory and opinion stability, adversarial tone/stance confounders
for robustness, and, where ethically permissible, additional dimensions and task types. All releases will be
versioned, with code and results publicly available for reproducibility.

11


===== PAGE BREAK =====

Ethics Statement

We follow standard ethical guidelines for dataset usage, evaluation, and model deployment. All datasets
used in this paper are publicly available under their original licenses, and we removed personally identifiable
information (PII) where applicable. No human subjects experiments were conducted beyond voluntary
annotation; annotators (if any) received fair compensation and provided informed consent. We prohibit the
misuse of our benchmark and models for profiling or harmful decision-making about individuals. Third-party
models/APIs used in our experiments comply with their terms of service. Upon acceptance, we will release
our code, prompts, and evaluation scripts with a research license and a model card detailing limitations and
appropriate use.

Reproducibility Statement

We enable independent re-implementation of our evaluation by disclosing all essential ingredients in the paper
and appendices:

e Prompts & Protocols: Full templates for the discriminative MCQ task, generative persona imitation,
and LLM-as-a-Judge (ranking and scoring), together with the 1-5 scoring rubric aligned with opinion,
logic/facts, and style.

e Data Construction Recipes: Step-by-step textual recipes for all three dimensions, including sources and
filtering thresholds (e.g., average reply length > 10, bottom-20% TTR removal, option cosine similarity
< 0.95 for Social; token-length cleaning < 5, TF-IDF top-10% selection, and semantic deduplication at
0.90 for Interpersonal), and the rules used to form distractors.

e Dataset Statistics: Per-dimension instance counts and summary statistics as reported in the main text.
e Evaluation Definitions: Exact metrics and equations (e.g., Accuracy and Scoreégen) used throughout.

e Model Usage: The list of model families evaluated and our access window (06/2025-09/2025). We set
the decoding temperature to 0 (temperature=0); all other generation hyperparameters (e.g., top__p,
max_ tokens, presence/frequency penalties) used provider defaults.

All experiments are inference-only (no supervised training). With these disclosed materials, readers can
re-implement the pipeline and obtain comparable results under the same inputs and judging criteria.

12


===== PAGE BREAK =====

References

[1]

[2|

[3]

[4]

[5]
[6]

[7|
[8]

[9

[10]

[11]
[12]

[13]

[14]

[15]

[16]

[17|

[18]

[19]

[20]

[21]
[22|

Saleh Afzoon, Usman Naseem, Amin Beheshti, and Zahra Jamali. Persobench: Benchmarking personalized
response generation in large language models. arXiv preprint arXiv:2410.03198, 2024.

Anthropic. Introducing claude 4. URL: https://www.anthropic.com/news/claude-4, May 2025. Official an-
nouncement of the Claude 4 model family, including Opus 4 and Sonnet 4.

Barbara Rita Barricelli, Elena Casiraghi, Jessica Gliozzo, Alessandro Petrini, and Stefano Valtolina. Human
digital twin for fitness management. IEEE Access, 8:26637—26664, 2020. doi: 10.1109/ACCESS.2020.2971576.

Jason Baumgartner, Savvas Zannettou, Megan Squire, and Jeremy Blackburn. The pushshift telegram dataset,
2020. https://arxiv.org/abs/2001.08438.

Douglas Biber. Variation across speech and writing. Cambridge university press, 1991.

Douglas Biber. Dimensions of register variation: A cross-linguistic comparison. Cambridge University Press,
1995.

Penelope Brown. Politeness: Some universals in language usage, volume 4. Cambridge university press, 1987.

Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee,
Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with
gpt-4. arXiv preprint arXiv:2808.12712, 2023.

Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang
Wang, Yidong Wang, et al. A survey on evaluation of large language models. ACM transactions on intelligent
systems and technology, 15(3):1—-45, 2024.

Runjin Chen, Andy Arditi, Henry Sleight, Owain Evans, and Jack Lindsey. Persona vectors: Monitoring and
controlling character traits in language models. arXiv preprint (Anthtropic technical report), 2025.

Herbert H Clark and Susan E Brennan. Grounding in communication. 1991.

Paul T Costa and Robert R McCrae. Normal personality assessment in clinical practice: The neo personality
inventory. Psychological assessment, 4(1):5, 1992.

Ethan Fast, William McGrath, Pranav Rajpurkar, and Michael S Bernstein. Augur: Mining human behaviors
from fiction to power interactive systems. In Proceedings of the 2016 CHI Conference on Human Factors in
Computing Systems, pages 237-247, 2016.

Michael Grieves and John Vickers. Digital twin: Mitigating unpredictable, undesirable emergent behavior in
complex systems. 2017.

Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie
Ma, Honghao Liu, et al. A survey on Ilm-as-a-judge. arXiv preprint arXiv:2411.15594, 2024.

Jacob Hornik and Matti Rachamim. Ai-enabled consumer digital twins as a platform for research aimed at
enhancing customer experience. Management Review Quarterly, 05 2025. doi: 10.1007/s11301-025-00527-3.

Bowen Jiang, Zhuoqun Hao, Young-Min Cho, Bryan Li, Yuan Yuan, Sihao Chen, Lyle Ungar, Camillo J. Taylor,
and Dan Roth. Know me, respond to me: Benchmarking llms for dynamic user profiling and personalized responses
at scale, 2025. https://arxiv.org/abs/2504.14225.

Cameron Jones and Ben Bergen. Does gpt-4 pass the turing test? In Proceedings of the 2024 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language Technologies
(Volume 1: Long Papers), pages 5183-5210, 2024.

Cameron R Jones and Benjamin K Bergen. Large language models pass the turing test. arXiv preprint
arXiv:2503. 23674, 2025.

Cameron Robert Jones, Ishika Rathi, Sydney Taylor, and Benjamin K Bergen. People cannot distinguish gpt-4
from a human in a turing test. In Proceedings of the 2025 ACM Conference on Fairness, Accountability, and
Transparency, pages 1615-1639, 2025.

Daniel Kahneman. Thinking, fast and slow. macmillan, 2011.

Moshe Koppel, Jonathan Schler, and Shlomo Argamon. Computational methods in authorship attribution. Journal
of the American Society for information Science and Technology, 60(1):9-26, 2009.

13


===== PAGE BREAK =====

[23]

[24]

[25]

[26]

[27|

[28]

[29]

[30]

[31]

[32|

[33]

[34]

[35]

[36]

[37|

[38]

[39]

[40]

[41]

[42|

[43]

Sai Adith Senthil Kumar, Hao Yan, Saipavan Perepa, Murong Yue, and Ziyu Yao. Can Ilms simulate personas
with reversed performance? a benchmark for counterfactual instruction following, 2025. https://arxiv.org/abs/
2504.06460.

Cheng Li, Ziang Leng, Chenxi Yan, Junyi Shen, Hao Wang, Weishi Mi, Yaying Fei, Xiaoyang Feng, Song Yan,
Haosheng Wang, Linkang Zhan, Yaokai Jia, Pingyu Wu, and Haozhen Sun. Chatharuhi: Reviving anime character
in reality via large language model, 2023. https://arxiv.org/abs/2308.09597.

Hao Li, Chenghao Yang, An Zhang, Yang Deng, Xiang Wang, and Tat-Seng Chua. Hello again! Ilm-powered
personalized agent for long-term dialogue, 2025. https://arxiv.org/abs/2406.05925.

Jiwei Li, Michel Galley, Chris Brockett, Georgios P Spithourakis, Jianfeng Gao, and Bill Dolan. A persona-based
neural conversation model. arXiv preprint arXiv:1603.06155, 2016.

Rui Li, Heming Xia, Xinfeng Yuan, Qingxiu Dong, Lei Sha, Wenjie Li, and Zhifang Sui. How far are llms from
being our digital twins? a benchmark for persona-based behavior chain simulation, 2025. https://arxiv.org/abs/
2502.14642.

Shuhang Lin, Wenyue Hua, Lingyao Li, Che-Jui Chang, Lizhou Fan, Jianchao Ji, Hang Hua, Mingyu Jin, Jiebo
Luo, and Yongfeng Zhang. Battleagent: Multi-modal dynamic emulation on historical battles to complement
historical analysis. arXiv preprint arXiv:2404.15532, 2024.

Xiao Ma, Swaroop Mishra, Ariel Liu, Sophie Su, Jilin Chen, Chinmay Kulkarni, Heng-Tze Cheng, Quoc Le,
and Ed Chi. Beyond chatbots: Explorellm for structured thoughts and personalized model responses, 2023.
https://arxiv.org/abs/2312.00763.

Matthias R Mehl, Samuel D Gosling, and James W Pennebaker. Personality in its natural habitat: manifestations
and implicit folk theories of personality in daily life. Journal of personality and social psychology, 90(5):862, 2006.

Yair Neuman. Computational personality analysis: Introduction, practical applications and novel directions.
Springer, 2016.
OpenAI. Introducing gpt-5. URL: https://openai.com/index/introducing-gpt-5/, August 2025. Official an-

nouncement of the GPT-5 model, a unified system with built-in reasoning capabilities.

Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein.
Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium
on user interface software and technology, pages 1-22, 2023.

James W Pennebaker, Matthias R Mehl, and Kate G Niederhoffer. Psychological aspects of natural language use:
Our words, our selves. Annual review of psychology, 54(1):547-577, 2003.

Project Gutenberg. Project gutenberg. https://www.gutenberg.org, 1971-.

Hongjin Qian, Xiaohe Li, Hanxun Zhong, Yu Guo, Yueyuan Ma, Yutao Zhu, Zhanliang Liu, Zhicheng Dou, and
Ji-Rong Wen. Pchatbot: a large-scale dataset for personalized chatbot. In Proceedings of the 44th international
ACM SIGIR conference on research and development in information retrieval, pages 2470-2477, 2021.

Yiting Ran, Xintao Wang, Tian Qiu, Jiaqing Liang, Yanghua Xiao, and Deqing Yang. Bookworld: From novels to
interactive agent societies for creative story generation, 2025. https://arxiv.org/abs/2504.14538.

Giulio Rossetti, Massimo Stella, Rémy Cazabet, Katherine Abramski, Erica Cau, Salvatore Citraro, Andrea Failla,
Riccardo Improta, Virginia Morini, and Valentina Pansanella. Y social: an llm-powered social media digital twin,
2024. https://arxiv.org/abs/2408.00818.

Murray Shanahan, Kyle McDonell, and Laria Reynolds. Role play with large language models. Nature, 623(7987):
493-498, 2023.

Jingbo Shang, Zai Zheng, Jiale Wei, Xiang Ying, Felix Tao, and Mindverse Team. Ai-native memory: A pathway
from llms towards agi, 2024. https://arxiv.org/abs/2406.18312.

Tianhao Shen, Sun Li, Quan Tu, and Deyi Xiong. Roleeval: A bilingual role evaluation benchmark for large
language models. arXiv preprint arXiv:2812. 16182, 2023.

Quan Shi, Carlos E. Jimenez, Stephen Dong, Brian Seo, Caden Yao, Adam Kelch, and Karthik Narasimhan.
Impersona: Evaluating individual level lm impersonation, 2025. https://arxiv.org/abs/2504.04332.

Yunxiao Shi, Wujiang Xu, Zeqi Zhang, Xing Zi, Qiang Wu, and Min Xu. Personax: A recommendation agent
oriented user modeling framework for long behavior sequence. arXiv preprint arXiv:2508.02398, 2025.

14


===== PAGE BREAK =====

[44]

[45]

[46]

[47|

[48]

[49]

[50]

[51]
[52|

[53]

Efstathios Stamatatos. A survey of modern authorship attribution methods. Journal of the American Society for
information Science and Technology, 60(3):538-556, 2009.

Olivier Toubia, George Z. Gui, Tianyi Peng, Daniel J. Merlau, Ang Li, and Haozhe Chen. Twin-2k-500: A
dataset for building digital twins of over 2,000 people based on their answers to over 500 questions, 2025.
https://arxiv.org/abs/2505.17479.

Quan Tu, Shilong Fan, Zihang Tian, and Rui Yan. Charactereval: A chinese benchmark for role-playing
conversational agent evaluation. arXiv preprint arXiv:2401.01275, 2024.

Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten
Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint
arXiv:2206.07682, 2022.

Qiujie Xie, Qiming Feng, Tianqi Zhang, Qingqiu Li, Linyi Yang, Yuejie Zhang, Rui Feng, Liang He, Shang Gao,
and Yue Zhang. Human simulacra: Benchmarking the personification of large language models. In The Thirteenth

International Conference on Learning Representations, 2025. https://openreview.net/forum?id=BCP5nAHXqgs.

Rui Xu, Xintao Wang, Jiangjie Chen, Siyu Yuan, Xinfeng Yuan, Jiaqing Liang, Zulong Chen, Xiaoqing Dong,
and Yanghua Xiao. Character is destiny: Can role-playing language agents make persona-driven decisions?, 2024.
https://arxiv.org/abs/2404.12138.

Ziyi Ye, Xiangsheng Li, Qiuchi Li, Qingyao Ai, Yujia Zhou, Wei Shen, Dong Yan, and Yiqun Liu. Learning
Ilm-as-a-judge for preference alignment. In The Thirteenth International Conference on Learning Representations,
2025.

John Zaller. The nature and origins of mass opinion. Cambridge university press, 1992.

Jialing Zhang, Lingfeng Zhou, Jin Gao, Mohan Jiang, and Dequan Wang. Personaeval: Benchmarking Ilms on
role-playing evaluation tasks.

Lingfeng Zhou, Jialing Zhang, Jin Gao, Mohan Jiang, and Dequan Wang. Personaeval: Are llm evaluators human
enough to judge role-play’, 2025. https://arxiv.org/abs/2508.10014.

15


===== PAGE BREAK =====

Contents

1 Introduction                                                                                               1
2 Related work                                                                                              3
2.1 Personalized Agents and Digital Twins... 2.2... ee ee ee    3
2.2 Datasets, Benchmarks, and Evaluation for Persona Simulation. .................    3

3 Task Formulation                                                                                         4
3.1 Problem Definition... 2... 2.     4
3.2 Evaluation Methodologies .... 2... 2.2    5
3.2.1 Discriminative Evaluation... 2... ee    5

3.2.2 Generative Evaluation... 2... ee    5

4 Benchmark Construction                                                                               6
4.1 Data Pre-processing ... 2... 2.     6
4.2 Capability Decomposition .. 2... . ee     6

5 Experiments                                                                                               7
5.1 Experimental Setup 2... 2. 2.     7
5.2 Overall Results and Key Findings... 2... 0...    7
5.3. Capability-wise Analysis... 2... ee    8
5.4 Generative Evaluation ... 2...    8
5.4.1 Judge-based scoring and ranking evaluation... . 2... 0.0.00... 0.2.0.0 .0000.    8

5.4.2 Reliability of the Judge and Human Study. .....................00.    8

5.4.3 Text Similarity Metrics 2.2... . ee    9

5.5 Human vs. Model Performance ..... 2... 2.20.00 ee ee    9

6 Conclusions and Discussions                                                                10
A Evaluation Protocols and Full Prompts                                                     18
A.1 Scope and Alignment with Competencies .........0.0 0.0000 eee ee   18
A.2 Unifying Instructions and Placeholders.. 2... ee ee   18
A.3 Discriminative Evaluation (Multiple-Choice Selection) ...............002. 220040.   18
A.4 Generative Evaluation: Persona Imitation (Free-form Generation)... ...........0..   21
A.5 LLM-as-a-Judge: Ranking-Based Evaluation... 2... 20.0.0... 2 eee eee eee eee   22
A.6 LLM-as-a-Judge: Scoring-Based Evaluation .. 1... 2... 0.0 0. ee ee ee   23
A.7 Implementation Note: Judge Model ......... 2.2.0.0. 00 ee ee ee ee   25

B Capability Annotation Prompts and Labeling Protocol                                    26
C Capability Distinguishing Case Studies                                                  29
C.1 Opinion Consistency .. 2...   29
C.2 Memory Recall .. 2... ee    29
C.3 Logical Reasoning .. 2...    29
C.4 Lexical Fidelity .. 2... ee   30
C.5 Persona Tone... 2...   30
C.6 Syntactic Style .. 2...   30

D Radar Charts across Three Dimensions                                                  31
D.1 Social Persona (Dimension 1)... 2...   32
D.2 Interpersonal Persona (Dimension 2)... 2... ee   33
D.3 Narrative Persona (Dimension 3) .. 2... ee   34

E Human Annotation Guidelines                                                              35
E.1 Task Background and Objectives... 2... 2. ee   30


===== PAGE BREAK =====

E.2 Discriminative Task Annotation... 2... 0. ee   35
E.2.1 Task Description... 2... ee   35
E.2.2. LLM Prompt (Use the Same Evaluation Standard) .................004   35
E.2.3 Evaluation Criteria... 2... ee   35
E.2.4 Additional Human Guidance .........0.0.00 0002 ee ee   35
E.2.5 Annotation Method .. 1... 20... 00.000   36

E.3 Generative Ranking Task Annotation. 2... . ee   36
E.3.1 Task Description... 2... 2. ee   36
E.3.2. LLM Prompt (Use the Same Evaluation Standard) .................004   36
E.3.3 Evaluation Criteria... 2... ee   36
E.3.4 Additional Human Guidance .........0.0 00.0000 pe   36
E.3.5 Annotation Method ... 2... 2.0.0.0. 00 ee   36

E.4 Generative Scoring Task Annotation... 2... ee ee   36
E.4.1 Task Description... 2... ee   36
E.4.2 LLM Prompt (Use the Same Evaluation Standard) .................004   37
E.4.3 Scoring Rubric (1-5 Seale)...   37
E.4.4 Additional Human Guidance .........0.000 0002 ee   37
E.4.5 Annotation Method ... 2... 2.0.0.0. 00 ee   37

E.5 General Guidelines and Notes... 2... ee   37
E.5.1 Quality Assurance 2... 2... ee   37
E.5.2 Language Considerations .. 1... 0...   38

Use of Large Language Models                                                            39

F.1 Scope of Use 2... ee   39

F.2 Models and Access . 2... ee   39

F.3. Human Oversight... 2...   39

F.4 Reproducibility... 2... ee   39

F.5 Data Privacy and Safety... 2... ee   39

F.6 Limitations . 2... ee   39

17


===== PAGE BREAK =====

A_ Evaluation Protocols and Full Prompts

This appendix details our evaluation protocols and the full instruction templates used across multiple data
forms, including public social interactions, interpersonal messaging, and narrative dialogue. We adopt a unified
instruction design and provide template variants for different data shapes when needed. Unless otherwise
noted, the LLM-as-a-Judge component is instantiated with GPT-5.

A.1_ Scope and Alignment with Competencies

Our evaluation comprises (1) discriminative multiple-choice selection and (2) generative evaluation, including
persona imitation (free-form generation) and LLM-as-a-Judge assessment via ranking and scoring. The judge
scoring rubric is organized along three pillars—Opinion Consistency, Logical & Factual Fidelity, and Stylistic
Similarity—which align with the six fundamental capabilities defined in the main text. We offer equivalent
template variants per evaluation mode to fit different data shapes; metrics and scoring criteria remain identical
across variants.

A.2 Unifying Instructions and Placeholders

We use a single instruction family per evaluation mode. Differences are limited to how inputs are presented.
We standardize placeholders as follows:

e {history}: persona-establishing prior content by the same user or character.

e {context}: the situation /post /message/scene the user or character is responding to (replacing earlier
{anchor} or {anchor_post}).

e {ground_truth_reply} or {groundtruth_response}: the human-written reply.

e {lmut_reply} or {generated_content}: the model-generated reply to be evaluated.

A.3  Discriminative Evaluation (Multiple-Choice Selection)

Canonical template (General)

Discriminative Selection Prompt (General)

Your task is to act as a specific social media user, becoming their digital twin.
Note: All provided text (history, context, choices) is in the original language of the data. You must
analyze the user’s style directly within that language.

Based on the user’s reply history, think and respond with their mindset, tone, and style.

Your reply history:
(Note: ‘‘Context’’ is another user’s post/message, and ‘‘UserReply”’ is your own reply.)
{history}

Now, you see a new context message:
“‘{context}”’

Below are 4 candidate replies. Which one is most likely something you would say?

A. {a}

B. {b}

C. {c}

D. {d}

Please respond in the following JSON format. In the ‘
”) to explain your choice.

‘reasoning”’ field, use the first-person perspective (‘‘I

18


===== PAGE BREAK =====

“son
{{
"predicted_comment": "A",
"reasoning": "Explain, from my perspective as the user, why I would choose this option."

}}

cee

Alternative template (Dimension 2: Interpersonal Messaging).

Discriminative Selection Prompt (Messaging Variant)

You are given a user’s reply history and 4 candidate replies to a context message. Only one of the replies
was actually written by this user. The other three were written by different users replying to the
same context message.

Your task is to choose the most likely reply written by the same user, based on writing style, tone, and
expression habits. Focus on how the user typically speaks, their phrasing, and how they respond
emotionally or humorously.

User’s Historical Conversations:
{history}

Current Context Message:
“‘{context}”’

Candidate Replies:
A. {a}
B. {b}
Cc. {c}
D. {d}

Please respond in the following JSON format:
“son
{{
"predicted_comment": "A",
"reasoning": "Explain why this option best matches the user’s style."

}}

cee

Distractor Generation for Discriminative Data (Dimension 3: Narrative).

Distractor Writer Prompt (Narrative Variant)

You are a precise persona-grounded writer.

Given one TARGET speaker (whose original utterance is the correct answer) and THREE OTHER
characters, write EXACTLY THREE distractor lines that those other characters would plausibly say
in this context.

Return ONLY this JSON:

{{
"distractors":[
{{"text":"...", "by":"<OtherCharacterName>"}},
{{"text":"...", "by":"<OtherCharacterName>"}},
{{"text":"...", "by":"<OtherCharacterName>"}}

19


===== PAGE BREAK =====

]
}}

Context (narration BEFORE anyone speaks):
wu {context_text}" wu

TARGET (do NOT imitate in distractors):
- name: {target_name}

- traits: {t_traits}

- goals: {t_goals}

- details: {t_ details}

- history: {t_history}

THREE OTHER characters (write one distractor for each; must sound like them):
1) name: {ol1_name}
traits: {ol_ traits}
goals: {01_ goals}
details: {o01_ details}
history: {o01_history}
2) name: {o2_name}
traits: {02_ traits}
goals: {02_ goals}
details: {02_ details}
history: {02_history}
3) name: {03_name}
traits: {03_ traits}
goals: {03_ goals}
details: {03_ details}
history: {03_history}

Rules (STRICT):

- Context fit: Each distractor must be logically possible GIVEN the context (time/place/people/danger
level). Do NOT introduce facts that contradict the context (e.g., saying ‘‘it’s calm’’ when the scene is

a chase or fire).

- Persona fit: Each distractor must match the specified OTHER character’s traits/goals/details AND be
consistent with their history. Do NOT copy, paraphrase, or stylistically mimic the TARGET.

- History use: Use the OTHER character’s UtteranceHistory to guide tone, stance, formality, and typical
verbs; NEVER copy any sentence from history verbatim. Avoid the TARGET’s pet phrases or
signature moves.

- Style \& length: Keep 1 short line per distractor, in the book’s tone/era (no modern slang/emojis).
Prefer 8--28 words; comparable length to a typical line in this book. Natural punctuation (commas/
semicolons/em dashes) is OK.

- Voice: No stage directions, no ‘‘X said,’’ no speaker names in the line. The content should read as the
spoken line itself.

- Uniqueness: The three distractors must be meaningfully different in stance/wording; no near-duplicates.

- Safety checks:

* If any distractor contradicts the context, resembles the TARGET’s voice, copies history verbatim, or
breaks style/length constraints, REWRITE it.
* Output EXACTLY three items; no extra keys or commentary.

Output ONLY the JSON object described above.

Notes.

e Placeholders are standardized: {history}, {context}, and option texts {a}, {b}, {c}, {d}. In narrative
data, the distractor writer prompt is used to construct options and is not itself a judging template.

20


===== PAGE BREAK =====

A.4 Generative Evaluation: Persona Imitation (Free-form Generation)

Canonical template (General, text-only output)

Generative Persona Imitation Prompt (General)

You are acting as a digital twin of a specific social media user.

Your task is to analyze the user’s posting history to understand their personality, tone, vocabulary, and
style.

All provided text (history, context) is in the original language of the data. You must analyze and respond
in that language.

Here is the user’s posting history:
(Note: ‘‘Context’’ is a post/message by someone else, and ‘‘UserReply”’ is the user’s own reply to it.)

{history _text}

Now, you must imitate this user’s persona perfectly and write a new reply to the following message.
Respond ONLY with the text of the reply. Do not add any extra explanations, greetings, or surrounding
text.

Message to reply to:
“‘{context}”

Variant (Dimension 2: Messaging, JSON output).
LMUT Prompt (Messaging Variant, JSON Output)

You are acting as a digital twin of a specific messaging app user.

Your task is to analyze the user’s messaging history to understand their personality, tone, vocabulary, and
style.

Different provided text (history, context, message) may use different language. You must analyze and
respond in the same language as the provided text.

Here is the user’s messaging history:
(Note: ‘‘Context”’ is a message by someone else, and ‘‘UserReply”’ is the user’s own reply to it.)

{history _text}

Now, you must imitate this user’s persona perfectly and write a new reply to the following message.

Please include your response in the following JSON format:

{{"generated_content": "your reply text here"}}

You may include thinking process or other content, but make sure to include the JSON format with the
generated_content field.

Message to reply to:
“‘{context}”

Variant (Dimension 3: Narrative, single-line JSON).

21


===== PAGE BREAK =====

Digital Twin Line Generation (Narrative Variant)

You are the digital twin of the TARGET speaker in a literary dialogue dataset.

Your job: write ONE new reply that this TARGET would plausibly say in the exact scene below,
matching their historical voice and habits.

### Inputs

- TARGET speaker: {speaker}

- Scene context (preceding narration \& situation, NOT the speaker’s own words):
"""{context}"""

- (Optional) TARGET’s conversation history snippets (style anchors):

{history __block}

### Hard requirements (STRICT)

1) Language \& Era: Match the book’s tone/era (no modern slang/emojis). If the scene reads like 19th-
century prose, mimic that diction.

2) Persona Fit: Keep the TARGET’s typical formality, sentence length, cadence, favorite turns of phrase (
use hints from history if provided).

3) Scene Consistency: The line must be logically possible given the context. Do NOT introduce new facts/
characters/locations. No meta-commentary.

4) Length \& Shape: One spoken line only (no stage directions, no speaker tag). Prefer 8--28 words unless
the scene clearly calls for a very short assent/command.

5) No Copying: Do NOT copy any exact sentence from the dataset. Paraphrase in the TARGET’s voice.

6) Output format: Return ONLY a JSON object:

{{

"generated_content": "<the single line>"

}}

Now produce the JSON with your single-line reply.

Notes.
e Use {context} as the reply trigger across all variants. The narrative variant mandates a single-line

JSON output.

A.5 LLM-as-a-Judge: Ranking-Based Evaluation

Canonical template (JSON + concise reasoning).
Judge Ranking Prompt (General)

You are an expert evaluator of writing style. Your task is to compare several candidate replies against a
known ‘‘Reference Reply’’ written by a specific user.

Your goal is to identify which candidate is the most similar to the reference in terms of **style, tone,
vocabulary, sentiment, and topic**.

This is the Reference Reply (the ground truth written by the user):

{ground_truth_reply}

These are the **Candidate Replies**:
{candidate_replies_text}

22


===== PAGE BREAK =====

Now, determine which single candidate is the closest match to the Reference Reply.
You MUST respond ONLY with a JSON object in the following format. Do not include any other text.
The reasoning should be concise, limited to 2--3 sentences.

“son

{{
"choice": "The letter of the best matching candidate (e.g., A’, ’B’, °C’, or ’D’)",
"reasoning": "A brief explanation for your choice, focusing on the stylistic similarities."

}}

cee

Letter-only MAP Prompt (Dimension 3: Narrative).
MAP Prompt (Narrative Variant, Letter Only)

You are a strict classifier. Output ONLY a single letter (A/B/C/D).
Choose the option that best matches the style, tone, vocabulary, and stance of the Generated Reply.

[Options]
A. {A}
B. {B}
Cc. {C}
D. {D}

[Generated Reply]
{pred}

Output exactly one letter: A, B, C, or D.

Notes.

e Ranking supports two outputs: a JSON object with brief reasoning (general) and a letter-only output
(narrative variant).

A.6 LLM-as-a-Judge: Scoring-Based Evaluation

Canonical template (applies as-is).

Judge Scoring Prompt (All Variants)

You are a meticulous and objective evaluator for a digital twin benchmark. Your task is to assess how well
a ‘“‘“Generated Reply’’ replicates a ‘‘Ground Truth Reply’’ for a given interaction.

The ‘‘Ground Truth Reply”’ is the single, undisputed gold standard. Your entire evaluation must be based
on comparing the ‘‘Generated Reply’’ against it.

The evaluation rests on three key pillars:

1. **Opinion Consistency**: Does the ‘‘Generated Reply’’ express the exact same core opinion, stance,
and sentiment as the ‘‘Ground Truth’’?

2. **Logical \& Factual Fidelity**: Is the ‘‘Generated Reply’’ based on the same reasoning and facts as
the ‘“‘Ground Truth’’? It must not introduce new, unsupported information or follow a different
logical path.

3. **Stylistic Similarity**: How closely does the ‘‘Generated Reply’’ match the ‘Ground Truth”’ in terms
of writing style?

23


===== PAGE BREAK =====

* *«*Lexical**: Use of similar vocabulary, slang, or emojis.
* **Tone**: Capturing the same tone (e.g., humorous, sarcastic, empathetic, proud).
* **Syntactic**: Similarity in sentence structure, length, and degree of formality.

SCORING RUBRIC (1--5 Scale):

- **5: Perfect Replication**: The ‘‘Generated Reply’ is a perfect match across all three pillars (Opinion,
Logic/Factual, Style). It feels like a natural, alternative expression from the same user. A perfect
substitute for the ground truth.

**4: High Fidelityx*: The Opinion and Logic/Factual pillars are perfectly matched. There are only
minor, subtle differences in the Style pillar (e.g., a missing catchphrase, a slightly more formal tone),
but the reply is still an excellent imitation.

- **3: Core Alignment, Detail Loss**: The core Opinion is consistent, but there’s a noticeable loss of
detail in the Logic or Style pillars. For example, the tone is flattened, or unique phrasing is lost. The
reply captures the ‘‘what’’ but not the ‘‘how’’. It feels more like a summary than a replication.

- **2: Partial Relevance, Major Deviation**: There is a major failure in at least one of the three pillars.
For instance, the opinion is distorted (e.g., strong support becomes neutral), the logic is completely
different, or the style is entirely mismatched.

- **1: Irrelevant or Contradictory**: The ‘‘Generated Reply’’ has almost nothing in common with the “

Ground Truth’’ or expresses a contradictory opinion. A total failure of replication.

YOUR TASK:

You will be provided with the context message, the ground truth reply, and the generated reply. User-
generated content may be in different languages, but your analysis and final JSON output must be in
English. You MUST respond ONLY with a JSON object in the following format. Do not include any
other text or explanations.

“son
{{
"analysis": {{
"opinion_consistency": {{
"is_ consistent": true,
"justification": "A brief justification for the consistency of the opinion."
3},
"logical_factual_fidelity": {{
"is_ faithful": true,
"justification": "A brief justification for the fidelity of the logic and facts."
3},
"stylistic_similarity": {{
"similarity_level": "High/Medium/Low",
"justification": "A brief justification for the level of stylistic similarity."
}}
3},
"final_score": "An integer score from 1 to 5",
"final_justification": "A concise, overall justification for the final score, synthesizing the three pillars."
}}

Now, evaluate the following case:

Context Message:
{context}

Ground Truth Reply:

24


===== PAGE BREAK =====

“‘{ground_truth_reply}‘

Generated Reply to Evaluate:
“‘{Imut_reply}‘

Notes.

e Inputs are standardized as {history}, {context}, {ground_truth_reply} (or {groundtruth _response}),
and {Imut_reply} (or {generated _content}).

A.7 Implementation Note: Judge Model

We instantiate the LLM-as-a-Judge with GPT-5 for both ranking- and scoring-based evaluation, unless
otherwise specified. Ranking includes a letter-only variant for narrative data.

25


===== PAGE BREAK =====

B Capability Annotation Prompts and Labeling Protocol

We annotate each example to identify which capability a model must primarily exercise to replicate a
user’s reply, while also recording the presence of all six capabilities. Each annotation unit contains three
elements: {history} (persona-establishing prior content), {context} (the situation the user is replying to),
and {groundtruth_response} (the user’s actual reply). An expert LLM performs the annotation to ensure
consistency and structured outputs (we use GPT-5 with temperature set to 0).

Canonical Annotation Prompt.
Capability Annotation Prompt (Canonical)

\# ROLE AND GOAL

You are an expert linguistic and persona analyst. Your task is to analyze user data to identify the core
capabilities a generative model would need to successfully create a ‘“‘digital twin’’ of the user. You
will be given a user’s conversational history, a new context they are replying to, and their actual
response (‘‘groundtruth’’).

\# INPUT DATA STRUCTURE

You will receive a JSON object for each annotation task with the following structure:
- “context’’: The situation, post, or utterance the user is responding to.

- ‘‘groundtruth\_response’”’: The user’s actual, human-written response to the
- “‘history’’: A list of the user’s past posts and replies, which establishes their persona.

‘

‘context’’.

\# CORE TASK: CAPABILITY ANNOTATION

Your task is twofold.

Part 1 is mandatory: You must first identify the single ‘‘primary\_capability’’. This is the one capability
that serves as the best-fit or most representative label for the example, even if the signal is weak.

‘

This choice is required for every single data point.

Part 2 is for detail: After identifying the primary capability, you will then perform a standard evaluation
for all six capabilities, marking ‘‘true’’ or ‘‘false’’ for each based on the strict criteria. This allows for
multiple capabilities to be ‘‘true’’.

\# CAPABILITY DEFINITIONS AND ANNOTATION CRITERIA
Evaluate each capability independently based on the refined criteria below.

C1: Opinion Consistency

- Core Question: Does this response require explicitly reaffirming a specific, previously-stated opinion?

- Label ‘“‘true’”’ if: The ‘“‘groundtruth\_response’’ expresses a clear opinion (e.g., support for a team,
dislike for a policy) that directly and unambiguously repeats or reinforces an opinion explicitly stated
in the ‘‘history’’.

- Do not label ‘‘true’’ for new opinions on new topics, even if they seem plausible for the user, or for
generic positive/negative sentiment that isn’t tied to a specific, recurring viewpoint.

primary\_capability’’ if: The core purpose of the response is to state a known, consistent

“ce

- Choose as
opinion.

C2: Memory\_ Recall

- Core Question: Does the response rely on shared context or information from the history that an outside
reader would not fully understand?

- Label ‘“‘true’”’ if: The ‘‘groundtruth\_response’’ makes an explicit or implicit reference to a past event,
personal information, or a previously established piece of context from the “‘history’’.

- Do not label ‘‘true’’: If the response is entirely self-contained and can be perfectly understood by anyone
just by reading the ‘‘context’’.

- Choose as ‘‘primary\_ capability” if: The response would be confusing or lose its meaning without
knowledge of the user’s history. This is often a good default choice for very short, context-dependent

replies.

26


===== PAGE BREAK =====

C3: Logical Reasoning
- Core Question: Does this response provide a justification or explanation for a claim?

20 6      20 66

- Label ‘“‘true”’ if: The ‘“‘groundtruth\_response’’ contains a rationale (e.g., using ‘‘because,    since,    so

,’ or implying a cause-and-effect relationship), AND the user’s ‘‘history’”’ shows a pattern of them
providing reasons for their opinions.
- Do not label ‘‘true’’: If the response is a simple, unsupported statement of fact or feeling.

- Choose as “‘primary\_capability’’ if: The response structure is clearly ‘‘Claim + Justification’.

C4: Lexical\_ Fidelity

- Core Question: Does this response use a creative, personal, and repeated signature word or phrase?

- Label ‘“‘true’”’ if: The ‘‘groundtruth\_response’’ uses a specific word, phrase, or emoji pattern that is
both repeated in the “‘history’”’ and idiosyncratic (not common slang).

- Do not label ‘‘true’’: For common slang or any single-use clever phrase.

‘

- Choose as “‘primary\_capability’’ if: The most noticeable feature of the response is the use of a

signature word/phrase.

C5: Persona\_ Tone

- Core Question: Does the response use a specific, non-literal tone (like sarcasm or deep irony) that is a
core part of the user’s persona?

- Label ‘“‘true’’ only if: The history shows a recurring pattern of a specific, non-literal tone AND the
response is a clear instance of that same tone.

- Do not label ‘‘true’’: If the two strict conditions are not both met.

‘

- Choose as ‘‘primary\_capability’’ if: The meaning of the response is inverted or altered by a clear,

persona-defining tone (e.g., obvious sarcasm).

C6: Syntactic\_ Style

- Core Question: Does this response use a distinctive, repeated structural pattern?

- Label ‘“‘true’”’ only if: The response uses a clear, repeated, and non-standard stylistic pattern (e.g.,
habitual use of sentence fragments, a unique punctuation signature).

- Do not label ‘“‘true’’: For common conversational variations.

- Choose as ‘‘primary\_ capability” if: The response is very simple and its most defining characteristic is a
structural quirk (e.g., it’s just a single, fragmented phrase, which is a common pattern for the user).
This can be a fallback for otherwise simple responses.

\# INSTRUCTIONS \& OUTPUT FORMAT
1. Step 1: Determine the ‘‘primary\_capability’’ (Mandatory Choice).

- First, analyze all the data.

- To ensure a fair evaluation and eliminate any potential ordering bias, you must give equal and
independent consideration to all six capabilities, regardless of their order, before selecting the
primary\_ capability.

- Then, you MUST choose exactly one capability from the list (C1--C6) that you consider the best fit.

- Use the ‘“‘Choose as primary\_ capability if...’’ guidelines to help you decide. If no signal is strong,
choose the one that is the most plausible or least incorrect. For very generic replies, ‘‘Memory\
_Recall’” or ‘‘Syntactic\_Style”’ are often good candidates.

- This choice is not optional.

2. Step 2: Evaluate All Capabilities (Detailed Annotation).
- Now, go through each of the six capabilities (C1 to C6) one by one, including the one you chose as
primary.
- For each one, decide if the
‘‘false’’.

“

groundtruth\_response’’ meets the strict definition and assign ‘‘true’’ or
- Provide a brief, one-sentence justification for every capability you mark as “‘true’’.

3. Step 3: Format the Output.
- Your final output must be a single, valid JSON object with the exact two-level structure shown
below.
- The “‘primary\_capability”’ field MUST contain the string name of your choice from Step 1. It
cannot be null or empty.

27


===== PAGE BREAK =====

- The ‘‘all\_evaluations’’ field MUST contain the detailed boolean labels from Step 2.

“son

{
"primary _capability": "Name_Of_The_Single_ Best_Fit_ Capability",
"all_ evaluations": {

"Opinion_Consistency": { "label": false, "reasoning": "" },
"Memory_Recall": { "label": false, "reasoning": "" },
"Logical_ Reasoning": { "label": false, "reasoning": "" },
"Lexical_Fidelity": { "label": false, "reasoning": "" },
"Persona_Tone": { "label": false, "reasoning": "" },
"Syntactic_Style": { "label": false, "reasoning": "" }
}
}

cee

Inputs for the prompt. We pass a single JSON object per example with three keys: history, context, and
groundtruth_response. No length truncation is applied.

28


===== PAGE BREAK =====

C Capability Distinguishing Case Studies

This section presents case studies that illustrate how our six capabilities appear in practice. The examples
are drawn from our public social persona corpus (dimension 1). For readability we show faithful translations
and only the key slices. If any discrepancy arises, the original Chinese dataset is authoritative. Explanatory
remarks appear outside the boxes. Inside each box, ------ marks omitted portions of longer cases.

C.1 Opinion Consistency

The user maintains a specific stance across contexts, namely choosing shows based on a favorite actor and
praising acting skill. The new reply preserves this granular stance rather than defaulting to generic positivity.

Case 1: Opinion Consistency (user 527222)

Context. ------ “Tonight is the finale. Xiang Qian returns to the seaside house where he once lived in hard
times, surely full of feelings. Seeing Alisa in this moment is so beautiful, hope they both have a good
life.” -.

Key History. ------ “I watched this show for Huang Zitao, I think his acting is great.” ------

Ground Truth Reply. ------ “I watched it for Liu Tao, her acting is really getting better and better.” ------

Why this shows Opinion Consistency: The historical pattern is watch for a specific actor and praise that acting.
The ground truth reply mirrors the same stance toward another named actor, preserving topic granularity
and evaluative angle.

C.2 Memory Recall

The reply uses a nickname that is not introduced in the immediate context, presupposing shared knowledge
from prior interactions. Understanding the line fully requires recalling who that nickname refers to.

Case 2: Memory Recall (user 205470)

Context. ------ “Met a teacher who is a high level LEGO player, buys LEGO by the sack.” ------
Key History. ------ “When Dan jie builds LEGO she looks like a serious kid, always supporting Dan jie.”
Ground Truth Reply. ------ “When she plays LEGO her eyes light up, still that adorable Wang Sansui.” ------

Why this shows Memory Recall: The affectionate nickname Wang Sansui is not grounded in the current
context and relies on earlier persona knowledge to resolve the reference.

C.3 Logical Reasoning

The user’s pattern is Observation then Deduction. In history, a physical observation supports an inference.
The reply replicates this approach by citing scene features to argue against an assumption.

Case 3: Logical Reasoning (user 369593)

Context. ------ “Do an ice drifting video. If it is not minus twenty or thirty degrees, do not show off.” .-----
Key History. ------ “There is no snow on the roof opposite, which shows the heat inside that house is
considerable.” ------

Ground Truth Reply. ------ “This river channel is quite narrow and there is a road next to it, so it probably
did not fall in from drifting on the ice.” ------

Why this shows Logical Reasoning: The reply marshals concrete observations (narrow channel, road nearby)
to support a causal judgment, matching the user’s habit of evidence based inference.

29


===== PAGE BREAK =====

C.4 Lexical Fidelity

A personal catchphrase recurs across contexts. The reply deploys the same idiosyncratic exclamation seen in
history, signaling a learned lexical signature.

Case 4: Lexical Fidelity (user 45899)

Context. ------ “Emirates Bling777 plane is encrusted with Swarovski crystals, the joy of the rich is beyond
imagination.” ------

Key History. ------ “OMG, for this kind of dog, give me a dozen and it is not too many.” ------

Ground Truth Reply. ------ “OMG, this, this, it is full of diamonds?! Maybe one will drop off for me.” ------

Why this shows Lexical Fidelity: The same colloquial exclamation equivalent to OMG appears in both history
and reply, demonstrating consistent, user specific lexical choice.

C.5 Persona Tone

The user favors playful hyperbole and adoring expressions that are nonliteral. The reply echoes that tone
with a different bodily metaphor, preserving the same stylistic stance.

Case 5: Persona Tone (user 270844)

Context. ------ “Group stage, Hai Lu’s acting is on point, those long legs are eye catching. Did not expect
such solid dance foundation, the high kicks are captivating.” ------

Key History. ------ “Listening made my ears pregnant, you all should listen, it is super good. Hope my
male god keeps getting better. Could you be my boyfriend, so shy.” ------

Ground Truth Reply. ------ “Hai Lu, your long legs had me staring at them the whole time, haha, my nose
is about to bleed.” ------

Why this shows Persona Tone: Both history and reply use exuberant, nonliteral bodily metaphors (ears
pregnant, nosebleed) as playful, adoring exaggerations that define the user’s persona.

C.6 Syntactic Style

Beyond words and tone, the user’s structure features stacked, breathless exclamations with intensifiers. The
reply reproduces that sentence shape.

Case 6: Syntactic Style (user 108194)

Context. ------ “Sci fi fans, gather up. The film The Wandering Earth is set for Lunar New Year, a concept
poster has been released.” ------

Key History. ------ “Wow wow wow, I am truly so excited inside, really looking forward to it, hahaha.” ------
Ground Truth Reply. ------ “Wow wow wow, look closely, this poster design really has such a vibe, you
could call it outstanding. This kind of movie theme is especially attractive, must support.” ------

Why this shows Syntactic Style: The reply stacks short, exclamatory clauses with intensifiers and colloquial
particles, recreating the user’s distinctive, breathless rhythm observed in history.

30


===== PAGE BREAK =====

D Radar Charts across Three Dimensions

We present capability-wise radar charts for the three persona dimensions: Dimension 1 (Social Persona),
Dimension 2 (Interpersonal Persona), and Dimension 3 (Narrative Persona). For each dimension, we report
four evaluation configurations: (i) Combined Average (aggregated across protocols), (ii) Discriminative
(multiple-choice selection), (iii) Generative Ranking (LLM-as-a-Judge; Acc.(Gen)), and (iv) Generative Scoring
(LLM-as-a-Judge; Score(Gen), 1-5). Each radar covers six capabilities: Opinion Consistency, Memory Recall,
Logical Reasoning, Lexical Fidelity, Persona Tone, and Syntactic Style.

31


===== PAGE BREAK =====

D.1 Social Persona (Dimension 1)

Lexical

[0.396,                                                                                 0.446]

[0.278, 0.

26)

—o— GPT-5-Chat
-- Claude-sonnet-4

—s— GPT-40-mini
~-©- DeepSeek-V3

—— Qwen2.5-14b

—>— GPT-OSS-20B
-~<- GPT-3.5-turbo

|
Lexical Fidel

0                    inion Consistency
[0.450, 0.768                    ‘TO.

0, 0.512]

—o— GPT-5-Chat
-+- Claude-sonnet-4

—s— GPT-40-mini

—7— Qwen2.5-14b
-~<- GPT-3.5-turbo

—>— GPT-OSS-20B
© DeepSeek-V3

(a) Dimension 1 (Social Persona): Combined Average
radar over six capabilities (all labeled capabilities). Ag-
gregates across discriminative and generative protocols;
higher is better along each spoke.

Logical Reasoning ~ Memory Recall
0.355]        [0.26:

—o— GPT-5-Chat
-- Claude-sonnet-4

—s— GPT-40-mini
--©- DeepSeek-V3

—— Qwen2.5-14b
-~<- GPT-3.5-turbo

—>— GPT-OSS-20B

(c) Dimension 1 (Social Persona): Generative Ranking
radar (LLM-as-a-Judge, Acc.(Gen)) across six capabili-
ties (all labeled capabilities). Reflects relative imitation
quality; higher is better.

32

(b) Dimension 1 (Social Persona): Discriminative evalua-
tion radar (accuracy-based) across six capabilities (all la-
beled capabilities). Shows multiple-choice persona match-
ing performance; higher is better.

~~ Memory Recall
31

Logical Reasoning

Lexical

lonsistency
[0.398.

0.479]

—o— GPT-5-Chat
-4- Claude-sonnet-4

—s— GPT-40-mini
© DeepSeek-V3

—— Qwen2.5-14b
-~<- GPT-3.5-turbo

—>— GPT-OSS-20B

(d) Dimension 1 (Social Persona): Generative Scoring
radar (LLM-as-a-Judge, Score(Gen), 1—5) across six ca-
pabilities (all labeled capabilities). Captures absolute
similarity to the ground truth; higher is better.


===== PAGE BREAK =====

D.2_ Interpersonal Persona (Dimension 2)

Lexical
[0.358

onsistency

, 0.566]

—o— GPT-5-Chat
-- Claude-sonnet-4

—s— GPT-40-mini
~-©- DeepSeek-V3

—— Qwen2.5-14b

—>— GPT-OSS-20B
-~<- GPT-3.5-turbo

Lexical

i  ny  lonsistency
[0.446,

10.44%, 0.913]
/
10.263, 0.679] _

—o— GPT-5-Chat
-+- Claude-sonnet-4

—s— GPT-40-mini
© DeepSeek-V3

—— Qwen2.5-14b

—>— GPT-OSS-20B
-~<- GPT-3.5-turbo

(a) Dimension 2 (Interpersonal Persona): Combined Av-
erage radar over six capabilities (all labeled capabilities).
Aggregates across discriminative and generative protocols;
higher is better along each spoke.

Lexical

lonsistency
[0.469,

, 0.568]

—o— GPT-5-Chat
-+- Claude-sonnet-4

—s— GPT-40-mini

—— Qwen2.5-14b
-~<- GPT-3.5-turbo

—>— GPT-OSS-20B
--©- DeepSeek-V3

(b) Dimension 2 (Interpersonal Persona): Discriminative
evaluation radar (accuracy-based) across six capabilities
(all labeled capabilities). Shows multiple-choice persona
matching performance; higher is better.

Lexical Fi
[0.109,

—o— GPT-5-Chat
-++- Claude-sonnet-4

—s— GPT-40-mini

—— Qwen2.5-14b

-~<- GPT-3.5-turbo

—>— GPT-OSS-20B
© DeepSeek-V3

(c) Dimension 2 (Interpersonal Persona): Generative
Ranking radar (LLM-as-a-Judge, Acc.(Gen)) across six
capabilities (all labeled capabilities). Reflects relative
imitation quality; higher is better.

33

(d) Dimension 2 (Interpersonal Persona): Generative
Scoring radar (LLM-as-a-Judge, Score(Gen), 1-5) across
six capabilities (all labeled capabilities). Captures abso-
lute similarity to the ground truth; higher is better.


===== PAGE BREAK =====

D.3. Narrative Persona (Dimension 3)

Lexical F
(0.385, 0.572                             0.4:  , 0.623]

i   ion)   lonsistency

—o— GPT-5-Chat      —s— GPT-40-mini —— Qwen2.5-14b —>— GPT-OSS-20B
—+- Claude-sonnet-4 -.©-- DeepSeek-V3 -~<- GPT-3.5-turbo

Lexica
(0.457,

—o— GPT-5-Chat      —s— GPT-40-mini —*— Qwen2.5-14b -—>— GPT-OSS-20B
--+- Claude-sonnet-4 .-©-- DeepSeek-V3_ -~<- GPT-3.5-turbo

(a) Dimension 3 (Narrative Persona): Combined Average
radar over six capabilities (all labeled capabilities). Ag-
gregates across discriminative and generative protocols;
higher is better along each spoke.

Lexical Fidetity                  oh Consistency
[0.370, 0.543] (“SS  Z  ie

37, 0.444]

[0.400,

—o— GPT-5-Chat      —s— GPT-40-mini —— Qwen2.5-14b  —>— GPT-OSS-20B
—1+- Claude-sonnet-4 --©-- DeepSeek-V3 -~<- GPT-3.5-turbo

(b) Dimension 3 (Narrative Persona): Discriminative
evaluation radar (accuracy-based) across six capabilities
(all labeled capabilities). Shows multiple-choice persona
matching performance; higher is better.

Persol
(0.177,

—o— GPT-5-Chat      —s— GPT-40-mini —— Qwen2.5-14b  —>— GPT-OSS-20B
-a+- Claude-sonnet-4 .-©-- DeepSeek-V3 -~<- GPT-3.5-turbo

(c) Dimension 3 (Narrative Persona): Generative Ranking
radar (LLM-as-a-Judge, Acc.(Gen)) across six capabili-
ties (all labeled capabilities). Reflects relative imitation
quality; higher is better.

34

(d) Dimension 3 (Narrative Persona): Generative Scor-
ing radar (LLM-as-a-Judge, Score(Gen), 1-5) across six
capabilities (all labeled capabilities). Captures absolute
similarity to the ground truth; higher is better.


===== PAGE BREAK =====

E Human Annotation Guidelines

E.1 Task Background and Objectives

This study aims to evaluate the performance of Large Language Models (LLMs) as judges in digital twin
tasks. To validate the reliability of model judgments, we need human annotators to independently annotate
selected data to establish a trustworthy benchmark.

The annotation task consists of three subtasks corresponding to different evaluation modes: discriminative
tasks, generative ranking tasks, and generative scoring tasks. Each annotator will annotate the same 50 data
samples to ensure consistency and comparability in evaluation.

Important Note: All provided content (anchor posts, reply history, choices) is in Chinese. You should analyze
and understand the content within the Chinese language context, but your reasoning and annotations should
be provided in English when specified.

E.2 Discriminative Task Annotation

E.2.1 Task Description

In the discriminative task, you need to act as a specific social media user, becoming their digital twin. Based
on the given conversation history and anchor post, select the most appropriate reply from four candidates
that best matches the user’s personal style and language habits.

E.2.2 LLM Prompt (Use the Same Evaluation Standard)
The LLM uses the following prompt for this task. Please follow the same reasoning approach:

Your task is to act as a specific social media user, becoming their digital twin. Note: All provided
teat (history, post, choices) is in Chinese. You must analyze the user’s style directly within the
Chinese language contest.

Based on the user’s reply history, think and respond with their mindset, tone, and style.

Your reply history: (Note: "AnchorPost" is another user’s post, and "UserReply" is your own
reply.)

Now, you see a new post: [anchor post]

Below are 4 candidate replies. Which one is most likely something you would say?

Please respond by explaining your choice from the user’s perspective using "I".

E.2.3 Evaluation Criteria

Style Consistency: Does the reply maintain consistency with the user’s language style demonstrated in
conversation history?

Tone Matching: Does the reply’s tone (formal/informal, humorous/serious, etc.) match the user’s
characteristics?

Vocabulary Usage: Are the vocabulary choices and expressions consistent with the user’s habits?

Logical Coherence: Is the reply content logically related to the anchor post and historical context?

E.2.4 Additional Human Guidance
e Carefully read through the entire conversation history to understand the user’s communication patterns
e Pay attention to recurring phrases, greeting patterns, and emotional expressions

e Consider the user’s typical response length and level of detail

35


===== PAGE BREAK =====

e Think from the user’s perspective: "If I were this user, which response would I most likely choose?"

E.2.5 Annotation Method

Please fill in the option number (0, 1, 2, or 3) that you consider most appropriate in the human _ choice field,
corresponding to the index position in the choices array.

E.3 Generative Ranking Task Annotation

E.3.1 Task Description

In the generative ranking task, you need to identify which candidate reply is most similar to a reference reply
in terms of style, tone, vocabulary, sentiment, and topic.

E.3.2 LLM Prompt (Use the Same Evaluation Standard)
The LLM uses the following prompt for this task:

You are an expert evaluator of writing style. Your task is to compare several candidate replies
against a known "Reference Reply" written by a specific user.

Your goal is to identify which candidate is the most similar to the reference in terms of style, tone,
vocabulary, sentiment, and topic.

Now, determine which single candidate is the closest match to the Reference Reply. The reasoning
should be concise, limited to 2-3 sentences, focusing on the stylistic similarities.

E.3.3 Evaluation Criteria

Style Similarity: Lexical choices, sentence structure, formality level

Tone Matching: Emotional tone, attitude, and mood

Vocabulary Consistency: Use of similar words, phrases, or expressions

Sentiment Alignment: Overall emotional orientation and sentiment

Topic Relevance: Relevance and approach to the main topic

E.3.4 Additional Human Guidance
e Focus on stylistic elements rather than factual content
e Look for subtle language patterns and preferences

Consider both what is said and how it is said

Compare the "voice" and "personality" reflected in each candidate

E.3.5 Annotation Method

Please fill in the letter (A, B, C, or D) of the option you consider best matching in the human _choice field.

E.4 Generative Scoring Task Annotation

E.4.1. Task Description

In the generative scoring task, you need to assess how well a generated reply replicates a ground truth reply,
providing a score from 1-5 based on comprehensive evaluation criteria.

36


===== PAGE BREAK =====

E.4.2 LLM Prompt (Use the Same Evaluation Standard)
The LLM uses the following detailed evaluation framework:

You are a meticulous and objective evaluator for a digital twin benchmark. Your task is to assess
how well a ’Generated Reply’ replicates a Ground Truth Reply’ for a given social media post.

The evaluation rests on three key pillars:

1. Opinion Consistency: Does the Generated Reply express the exact same core opinion, stance,
and sentiment as the Ground Truth?

2. Logical & Factual Fidelity: Is the Generated Reply based on the same reasoning and facts as
the Ground Truth?

3. Stylistic Similarity: How closely does the Generated Reply match the Ground Truth in terms
of lexical, tone, and syntactic elements?

E.4.3 Scoring Rubric (1-5 Scale)

5 - Perfect Replication: Perfect match across all three pillars. Feels like a natural, alternative expression
from the same user.

4 - High Fidelity: Opinion and Logic/Factual pillars are perfectly matched. Only minor, subtle differences
in Style.

3 - Core Alignment, Detail Loss: Core opinion is consistent, but noticeable loss of detail in Logic or Style
pillars.

2 - Partial Relevance, Major Deviation: Major failure in at least one of the three pillars.

1 - Irrelevant or Contradictory: Almost nothing in common with the Ground Truth or expresses contradic-
tory opinion.

E.4.4 Additional Human Guidance

e First identify the core opinion/stance in the ground truth reply

Check if the generated reply maintains the same logical flow and reasoning
e Evaluate stylistic elements: word choice, sentence length, formality, emotional tone

e Consider the reply as a whole - would it serve as an acceptable substitute?

Be objective and consistent across all annotations

E.4.5 Annotation Method

Please fill in your score (1, 2, 3, 4, or 5) in the human_score field.

E.5 General Guidelines and Notes

E.5.1 Quality Assurance

Read all conversation history carefully to understand the user’s communication patterns

Maintain objectivity and consistency throughout the annotation process
e Avoid letting personal preferences influence your judgment
e Each data sample should be annotated independently

e When facing difficult decisions, choose the relatively best option

Double-check for missing annotations or format errors after completion

37


===== PAGE BREAK =====

E.5.2 Language Considerations
e All content is in Chinese - analyze within the Chinese language context
e Pay attention to Chinese-specific expressions, internet slang, and cultural references
e Consider Chinese punctuation and writing conventions

e Understand the social media context and communication norms

38


===== PAGE BREAK =====

F Use of Large Language Models

F.1 Scope of Use

LLMs assisted with (i) prompt drafting and refinement, (ii) minor code refactoring suggestions, (iii) generating
synthetic evaluation items (e.g., distractor options and candidate responses), and (iv) light copy-editing of
non-technical prose. LLMs did not originate novel claims, conduct final analyses, or decide conclusions; all
substantive results are author-verified.

F.2 Models and Access

We used the following LLMs via API/local inference: GPT-5-Chat (OpenAI), Claude-Sonnet-4 (Anthropic),
DeepSeek-V3 (DeepSeek), GPT-40-mini (OpenAI), GPT-3.5-Turbo (OpenAI), GPT-OSS-20B (OpenAI), Qwen2.5-
14B (Alibaba / Qwen Team). Access window: 06/2025-09/2025.

F.3. Human Oversight
All LLM outputs were screened by the authors; items entering quantitative evaluation were validated via
deterministic scripts or double review.

F.4 Reproducibility

We include the full evaluation prompts and protocols, the 1-5 scoring rubric, the textual recipes for constructing
multiple-choice questions, the data filtering thresholds per dimension, dataset sizes/statistics, and the evaluation
equations and metrics. These disclosures are sufficient to re-implement our evaluation.

F.5 Data Privacy and Safety

Only public data were processed; no PII or sensitive user data were sent to third-party services. We complied
with provider Terms of Service and applied toxicity/safety filters where applicable.

F.6 Limitations

LLM outputs may reflect training-data biases or hallucinations. We mitigated these via rule-based validators
and manual review; residual errors may remain.

39
