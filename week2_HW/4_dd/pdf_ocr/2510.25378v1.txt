arX1v:2510.25378vl1 [cs.CL] 29 Oct 2025

HALLUCINATIONS IN BIBLIOGRAPHIC RECOMMENDATION:

CITATION FREQUENCY AS A PROXY
FOR TRAINING DATA REDUNDANCY

A PREPRINT

© Junichiro Niimi* »

Meijo University
°RIKEN AIP

ABSTRACT
Large language models (LLMs) have been increasingly applied to a wide range of tasks, from natural
language understanding to code generation. While they have also been used to assist in bibliographic
recommendation, the hallucination of non-existent papers remains a major issue. Building on prior
studies, this study hypothesizes that an LLM’s ability to correctly produce bibliographic informa-
tion depends on whether the underlying knowledge is generated or memorized, with highly cited
papers (i.e., more frequently appear in the training corpus) showing lower hallucination rates. We
therefore assume citation count as a proxy for training data redundancy (i.e., the frequency with
which a given bibliographic record is repeatedly represented in the pretraining corpus) and inves-
tigate how citation frequency affects hallucinated references in LLM outputs. Using GPT-4.1, we
generated and manually verified 100 bibliographic records across twenty computer-science domains,
and measured factual consistency via cosine similarity between generated and authentic metadata.
The results revealed that (i) hallucination rates vary across research domains, (ii) citation count is
strongly correlated with factual accuracy, and (iii) bibliographic information becomes almost ver-
batimly memorized beyond approximately 1,000 citations. These findings suggest that highly cited
papers are nearly verbatimly retained in the model, indicating a threshold where generalization shifts

into memorization.

Keywords Large language model - Natural language processing - Hallucination - Information retrieval - Recommen-

dation system

1 Introduction

Large language models (LLMs) have achieved remarkable
fluency across a wide range of domains [1]. However,
they are also known to generate hallucinations that are
nonsensical or unfaithful to the provided source content
 3]. In particular, the generation of non-existent aca-
demic references or legal precedents has been widely rec-
ognized as a critical issue [4]. For example, in the field of
marketing, where Recency—Frequency—Monetary (RFM)
analysis [[5] 6) [7] is commonly employed as a customer
relationship management (CRM) [8], when prompted to
“Please suggest recent academic papers on RFM analysis
with Author (Year) Title, Journal, Vol, No, pp style,” the
model (GPT-4.1) produced the following response:

Chitturi, P., Raghunathan, B., Scian-
dra, R., & Sikora, J. (2010). “RFM
and CLV: Using Customer Data for Im-
proved Decision Making.” Journal of

Direct, Data, and Digital Marketing
Practice, 12(1), 1-10.

Although the output follows the correct bibliographic for-
mat, the paper itself does not exist. Each component imi-
tates genuine studies, such as the author names (e.g., Chit-
turi and Raghunathan [9]), journal name (Journal of Di-
rect, Data, and Digital Marketing Practice ([10}), and the
paper title (e.g., “RFM and CLV” [I1]}), but the numeri-
cal details are fictitious, suggesting that multiple authentic
entries were probabilistically merged into a coherent yet
fabricated citation.

These fabricated yet plausible references suggest that hal-
lucinations in bibliographic recommendation may not oc-
cur arbitrarily, but rather reflect how knowledge is rep-
resented within the model. For example, the probabil-
ity of reproducing training data has been shown to cor-
relate with its frequency of appearance [12]. This study
therefore focuses on bibliographic recommendation using
LLMs and empirically examines how factual correctness


===== PAGE BREAK =====

Hallucinations in Bibliographic Recommendation

A PREPRINT

varies with domain popularity (number of papers in the
field) and citation prominence (citation count of the gen-
erated reference). Our findings suggest that hallucinations
arise not randomly but systematically from imbalanced
knowledge distributions within the representation space.

2 Related Study

Hallucination in LLMs has been examined from diverse
perspectives {15}. OpenAI’s analysis
argued that reinforcement learning with human feedback
(RLHF) may inherently encourage hallucination,
as current LLMs are penalized for responding “I don’t
know” (IDK) and instead rewarded for producing statis-
tically plausible continuations. This alignment objective
can thus promote confident but unreliable statements.

Conversely, security-oriented studies have highlighted the
opposite tendency: information repeated multiple times
during pretraining is more likely to be memorized and re-
produced verbatim [18] {19} [20] [12]. This view aligns with
recent theoretical accounts positioning LLMs as proba-
bilistic pattern recognizers that approximate data distri-
butions rather than explicitly “understanding” knowledge
[21] [22]. From this perspective, hallucination and expo-
sure (i.e., training data leakage) represent opposite
outcomes of the same probabilistic learning dynamics,
where the frequency of exposure governs whether infor-
mation is faithfully recalled or spuriously synthesized.

In the context of citation recommendation [26],
this implies that frequently cited papers which appear
across numerous publications and other web sources are
more likely to be verbatimly recalled by LLMs, whereas
sparsely represented works tend to be fabricated. This
study hypothesizes that hallucination in bibliographic rec-
ommendation is systematically related to the training data
redundancy (1.e., the frequency with which a given biblio-
graphic record is repeatedly represented in the pretraining
corpus). Highly cited papers are expected to be more ro-
bustly represented, leading to lower hallucination rates,
while limited-redundancy papers are more prone to plau-
sible but non-existent references.

3 Experiments

3.1 Methodology

Bibliographic records were generated using GPT-4.1 ac-
cessed via API (knowledge cutoff: June 2024). To mini-
mize domain-specific citation bias, twenty topics were se-
lected within the field of computer science (e.g., trans-
former [27], diffusion model [28], retrieval-augmented
generation [29]}). For each topic, the model was prompted
to recommend five academic papers in JSON format
(Fig. 3] see Appendix A), yielding a total of 100 samples.

Each output record was manually validated using Google
Scholar. Existence of the paper was confirmed primarily
by its title; minor coincidences in author or journal names

were not considered sufficient. Each record was scored
as completely correct (score = 2), partially hallucinated
(score = 1; when some metadata such as author names,
journal, or year were inaccurate), or completely halluci-
nated (score = Q). Citation counts were retrieved from
Google Scholar (as of October 2025). Semantic similarity
between generated and authentic bibliographic metadata
was computed using Sentence-BERT (all-MiniLM-
L6-v2) embeddings with cosine similarity, and the rela-
tionship between citation frequency and similarity was an-
alyzed.

3.2 Results

Experiment 1. We first compared low- and high-
citation groups divided at the median citation count
(Man = 818). A one-tailed t-test revealed that the high-
citation group achieved significantly higher factual scores
than the low-citation group (¢(98) = —5.12, p < .001;
Mhigh = 1.245, Mow = 0.725).

Experiment 2. Figure |1] shows the average factual
scores across research domains. The scores differ
markedly among domains: Domains related to image
processing, such as Vision Transformer (ViT) and
Diffusion Model [28], achieved notably higher accuracy,
whereas recent LLM-oriented techniques, such as RAG
 and LoRA [32], exhibited substantially lower scores.
This discrepancy likely reflects the popularity of the re-
search domain, which is related with the data redundancy.
Consequently, hallucinations occurred more frequently
in underrepresented domains such as LoRA and Graph
Transformer [33].

Experiment 3. Using only valid records (score > 0;
nm = 81), we analyzed the relationship between log-
transformed citation counts and cosine similarity (Fig. [2).
A strong positive correlation (r = 0.75, p < .001) in-
dicates a clear log-linear relationship, which reflects the
memorization scaling law [12]. In this context, we may
interpret our finding as a form of a redundancy scaling
law, where the probability of factual recall significantly
increases as log(citation) rises.

Experiment 4. Notably, Fig. [2| also indicates the sat-
uration near 1.0 when log(citation) exceeded 7 (approxi-
mately 1,000 citations), suggesting that highly cited pa-
pers are almost verbatimly retained within the model.
In that regime, frequently cited papers are not merely
represented through probabilistic token associations but
are instead recalled almost verbatim. To quantify this
non-linear pattern, we conducted logistic regression us-
ing min—max-normalized cosine similarity as the depen-
dent variable. The model revealed a significant positive
relationship between log(citation) and normalized simi-
larity (6, = 0.523, p = 0.003) with a low intercept
(80 = —2.360, p = 0.020). The estimated inflection
point (—6 9/8, ~ 5) corresponds to roughly 100 citations,


===== PAGE BREAK =====

Hallucinations in Bibliographic Recommendation

A PREPRINT

2.5

2.0 4

1.54

ui

1

fo}

0

ui

Average Score (0-2 scale)

aT

w
oe                                                                                Ne
Ns        eo       oe                        i

Ss     eS        Sos oe
RIENCE   so
os ge s 6 ge ao   PP PS
DH ee? aod?      ek
Se”     oh ge?                  &   oo  eo
PO KS              x

e
Pe

KP wo
&    ee  w oo oF
se ve a   & eo   eo
&  om   R   @
Ss   ss ro   wv Oy   sO
RY   ord   a ot   we
2   wr os   sr xe   res
a”   ROS
i   WL
RNY BS)

Figure 1: Average factual score by domain. Error bars indicate the 95% confidence interval. Accuracy varies by

domain familiarity.

marking the transition from generalization to memoriza-
tion.

These four experiments collectively support our initial hy-
pothesis that citation count acts as a proxy for the training
data redundancy. The positive and non-linear relation-
ship between log(citation) and cosine similarity indicates
that hallucination is not random but structurally linked to
uneven knowledge distributions within the model’s repre-
sentation space.

4 Conclusion

This study empirically examined how citation frequency
functions as a proxy for hallucination in bibliographic rec-
ommendation by LLMs. The model was instructed to
output JSON-formatted results without explanations, ef-
fectively disabling IDK responses. In line with previous
study [15], such output constraints encourage the model
to produce plausible yet non-existent entries.

The key findings are as follows: (i) hallucination rates
vary across research domains, (ii) citation count is
strongly correlated with factual accuracy, and (iii) bibli-
ographic information becomes almost verbatimly mem-
orized beyond approximately 1,000 citations. In other
words, while LLMs can faithfully reproduce informa-
tion about highly cited papers, they struggle in domains
with shorter publication histories or limited redundancy in
the training corpus. Contrary to the discoverability phe-
nomenon where memorization emerges only when
sufficient context is given, our results suggest the opposite
direction: highly redundant knowledge can be recalled
even with minimal prompting. Although this study fo-
cused on GPT-4.1 within the computer science domain,
future research should extend the analysis to other models,
disciplines, and multilingual contexts to assess the gener-
ality of this threshold behavior.

References

[1] Jan Ole Krugmann and Jochen Hartmann. Sentiment
analysis in the age of generative ai. Customer Needs
and Solutions, 11(1):3, 2024.

[2] Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn,
and Yarin Gal. Detecting hallucinations in large
language models using semantic entropy. Nature,
630(8017):625-630, 2024.

[3] Ziwei Xu, Sanjay Jain, and Mohan Kankanhalli.
Hallucination is inevitable: An innate limita-
tion of large language models. arXiv preprint
arXiv:2401.11817, 2024.

[4] Lei Huang, Weijiang Yu, Weitao Ma, Weihong
Zhong, Zhangyin Feng, Haotian Wang, Qianglong
Chen, Weihua Peng, Xiaocheng Feng, Bing Qin,
et al. A survey on hallucination in large language
models: Principles, taxonomy, challenges, and open
questions. ACM Transactions on Information Sys-
tems, 43(2):1-55, 2025.

[5] Connie L Bauer. A direct mail customer purchase
model. Journal of Direct Marketing, 2(3):16—24,
1988.

[6] Jan Roelf Bult and Tom Wansbeek. Optimal selec-
tion for direct mail. Marketing Science, 14(4):378-
394, 1995.

[7] Sunil Gupta and Donald R Lehmann. Customer life-
time value and firm valuation. Journal of Relation-
ship Marketing, 5(2-3):87-110, 2006.

[8] Jacob Jacoby and Robert W Chestnut.

alty: Measurement and management.
&amp; Sons Incorporated, 1978.

[9] Ravindra Chitturi, Rajagopal Raghunathan, and Vi-
jay Mahajan. Form versus function: How the in-
tensities of specific emotions evoked in functional

Brand loy-
John Wiley


===== PAGE BREAK =====

Hallucinations in Bibliographic Recommendation                            A PREPRINT

Domains
Transformer
Tabular transformer
Time-series transformer
Diffusion model
Retrieval-augmented generation
Graph transformer
In-context learning
Low-rank adaptation
Vision transformer
Mixture-of-experts
Knowledge distillation
Bayesian optimization
Generative adversarial networks
Vision-language models
Reinforcement learning
Multimodal deep learning
Large language models
Federated learning
Self-supervised learning
Few-shot learning

1.04                                      goot, “oe
@          e

op
%

0.94

0.84

cosine similarity

0.74

0.674

log(citations)
Figure 2: Relationship between citation frequency and generation fidelity. Each dot represents a factual citation
generated by the model, colored by research domain. The dashed line indicates the fitted linear regression (95% CI

in gray). The correlation (r = 0.75, p < .001) demonstrates a strong log-linear relationship between citations and
factual accuracy, with saturation near log(citation) ~ 7.

versus hedonic trade-offs mediate product prefer- [15] Adam Tauman Kalai, Ofir Nachum, Santosh S Vem-

ences. Journal of marketing research, 44(4):702-        pala, and Edwin Zhang. Why language models hal-
714, 2007.                                                              lucinate. arXiv preprint arXiv:2509.04664, 2025.
[10] Efthymios Constantinides and Stefan J Fountain. [16] Paul F Christiano, Jan Leike, Tom Brown, Miljan
Web 2.0: Conceptual foundations and marketing is-        Martic, Shane Legg, and Dario Amodei. Deep re-
sues. Journal of direct, data and digital marketing        inforcement learning from human preferences. Ad-
practice, 9(3):23 1-244, 2008.                                          vances in neural information processing systems, 30,
[11] Peter S Fader, Bruce GS Hardie, and Ka Lok       2017.
Lee. Rfm and clv: Using iso-value curves for cus- [17] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu,
tomer base analysis. Journal of marketing research,        Tom B Brown, Alec Radford, Dario Amodei, Paul
42(4):415—430, 2005.                                  Christiano, and Geoffrey Irving. Fine-tuning lan-
[12] Nicholas Carlini, Daphne Ippolito, Matthew Jagiel-     guage models from human preferences. arXiv
ski, Katherine Lee, Florian Tramer, and Chiyuan      preprint arXiv:1909.08593, 2019.
Zhang. Quantifying memorization across neural lan- [18] Katherine Lee, Daphne Ippolito, Andrew Nys-
guage models. In The Eleventh International Con-         trom, Chiyuan Zhang, Douglas Eck, Chris Callison-
ference on Learning Representations, 2022.                Burch, and Nicholas Carlini. Deduplicating train-
[13] Hoang Anh Dang, Vu Tran, and Le-Minh Nguyen.         ing data makes language models better. In Proceed-
Survey and analysis of hallucinations in large lan-       ings of the 60th Annual Meeting of the Association
guage models: attribution to prompting strategies or        for Computational Linguistics (Volume 1: Long Pa-
model behavior. Frontiers in Artificial Intelligence,         pers), pages 8424-8445, 2022.
8:1622292, 2025.                           [19] Nikhil Kandpal, Eric Wallace, and Colin Raffel.
[14] Joseph Spracklen, Raveen Wijewickrama,       Deduplicating training data mitigates privacy risks
AHM Nazmus Sakib, Anindya Maiti, and Bi-       in language models. In International Conference

mal Viswanath. We have a package for you! a       on Machine Learning, pages 10697-10707. PMLR,
comprehensive analysis of package hallucinations      2022.

by code generating {LLMs}. In 34th USENIX [20] Nicholas Carlini, Florian Tramer, Eric Wallace,
Security Symposium (USENIX Security 25), pages      Matthew Jagielski, Ariel Herbert-Voss, Katherine
3687-3706, 2025.                                                  Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar


===== PAGE BREAK =====

Hallucinations in Bibliographic Recommendation                            A PREPRINT

[21

[26

[27

[28

[29

[30

[31

sy

=

—

“4

—“

]

sy

Erlingsson, et al. Extracting training data from large
language models. In 30th USENIX security sym-
posium (USENIX Security 21), pages 2633-2650,
2021.

Suvir Mirchandani, Fei Xia, Pete Florence, Brian
Ichter, Danny Driess, Montserrat Gonzalez Arenas,
Kanishka Rao, Dorsa Sadigh, and Andy Zeng. Large
language models as general pattern machines. In
Conference on Robot Learning, pages 2498-2518.
PMLR, 2023.

Pablo Contreras Kallens and Morten H Christiansen.
Distributional semantics: Meaning through culture
and interaction. Topics in cognitive science, 2024.

Nicholas Carlini, Chang Liu,  Ulfar  Erlingsson,
Jernej Kos, and Dawn Song. The secret sharer: Eval-
uating and testing unintended memorization in neu-
ral networks. In 28th USENIX security symposium
(USENIX security 19), pages 267-284, 2019.

Chanwoo Jeong, Sion Jang, Eunjeong Park, and
Sungchul Choi. A context-aware citation recom-
mendation model with bert and graph convolutional
networks. Scientometrics, 124(3):1907-1922, 2020.

Zitong Zhang, Braja Gopal Patra, Ashraf Yaseen, Jie
Zhu, Rachit Sabharwal, Kirk Roberts, Tru Cao, and
Hulin Wu. Scholarly recommendation systems: a
literature survey. Knowledge and Information Sys-
tems, 65(11):4433-4478, 2023.

Jie Zhu, Braja G Patra, and Ashraf Yaseen. Rec-
ommender system of scholarly papers using public
datasets. AMIA summits on translational science
proceedings, 2021:672, 2021.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and [lia Polosukhin. Attention is all you
need. Advances in neural information processing
systems, 30, 2017.

Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denois-
ing diffusion probabilistic models. Advances in neu-
ral information processing systems, 33:6840-6851,
2020.

Patrick Lewis, Ethan Perez, Aleksandra Piktus,
Fabio Petroni, Vladimir Karpukhin, Naman Goyal,
Heinrich Kiittler, Mike Lewis, Wen-tau Yih, Tim
Rocktaschel, et al. Retrieval-augmented generation
for knowledge-intensive nlp tasks. Advances in neu-
ral information processing systems, 33:9459-9474,
2020.

Nils Reimers and Iryna Gurevych. Sentence-bert:
Sentence embeddings using siamese bert-networks.
In Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP), page 3982.
Association for Computational Linguistics, 2019.

Alexey Dosovitskiy, Lucas Beyer, Alexander
Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,

Thomas Unterthiner, Mostafa Dehghani, Matthias
Minderer, Georg Heigold, Sylvain Gelly, et al. An
image is worth 16x16 words. International Con-
ference on Learning Representations (ICLR 2021),
2021.

[32] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu,
Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,
et al. Lora: Low-rank adaptation of large language
models. In International Conference on Learning
Representations, 2022.

[33] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin
Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-
Yan Liu. Do transformers really perform badly for
graph representation? Advances in neural informa-
tion processing systems, 34:28877—28888, 2021.

[34] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,
Zheng Zhang, Stephen Lin, and Baining Guo. Swin
transformer: Hierarchical vision transformer using
shifted windows. In Proceedings of the IEEE/CVF
international conference on computer vision, pages

10012-10022, 2021.

[35] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junx-
ian He, Liangke Gui, Graham Neubig, Jonathan
May, and Luke Zettlemoyer. Mega: Moving av-
erage equipped gated attention. arXiv preprint
arXiv:2209. 10655, 2022.

A Model Settings

Actual prompt to generate bibliographic information is
shown in Fig. To strictly generate structured records,
we include JSON schema and ”no additional explanation”
descriptions.

B_ Generated Records

Some records generated with GPT-4.1 are shown in Table
 For the partially hallucinated records, the tendency for
the first author, year and paper title to be strictly repro-
duced, while other information is reproduced in a plausi-
ble manner.


===== PAGE BREAK =====

Hallucinations in Bibliographic Recommendation                            A PREPRINT

### Instruction:
You are an academic assistant that outputs structured bibliographic data in JSON format.
Please suggest 5 recent academic papers related to "{domain}".

Each paper should be represented as a JSON object following this schema:

{
"author": "Author name(s) in APA style, e.g., ’Smith, J. & Tanaka, K.’",
"year": 2023,
"title": "Title of the paper",
"journal": "Name of the academic journal",
"volume " :   " 12" >
"number " :   ug >
"pages": "123--145"
}

Output must be a single valid JSON array of objects and contain **no additional explanation**.
If you are unsure about any field, please leave it as an empty string ("").

Figure 3: Prompt to generate bibliographic information

Table 1: Examples of generated bibliographic records. Hallucinated parts are shown in bold.

Class        Domain      Cos                                          Results
Completely Vision         0.995 Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., & Guo, B. (2021).
correct       transformer          Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.

Proceedings of the IEEE/CVF International Conference on Computer Vision,
10012-10022.[34]

Partially      Transformer 0.798 Ma, X., Xu, J., Sun, Y., He, Y., & Lin, J. (2022) Mega: Moving average
halluci-                          equipped gated attention. International Conference on Machine Learning,
nated                                  162, 15369-15384.

Completely Tabular       n.a.    Kossen, J., Probst, P., Schirrmeister, R. T. & Bischl, B. (2023) Self-Attention
halluci-        transformer             for Raw Numerical Tabular Data. IEEE Transactions on Neural Networks and
nated                                             Learning Systems.
