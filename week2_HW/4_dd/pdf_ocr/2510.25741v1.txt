2510.25741v1 [cs.CL] 29 Oct 2025

arXiv

ht! ByteDance | Seed

Scaling Latent Reasoning via Looped Language Models

Rui-Jie Zhu*!:?:', Zixuan Wang*!*, Kai Hua*!, Tianyu Zhang**’°, Ziniu Li*', Haoran Que*!°, Boyi Wei*®,
Zixin Wen*!:’, Fan Yin*!, He Xing*!!, Lu Li’, Jiajun Shi, Kaijing Ma!, Shanda Li!’, Taylor Kergan?”’,
Andrew Smith?’’, Xingwei Qu'!°, Mude Hui”, Bohong Wu’, Qiyang Min!, Hongzhi Huang’, Xun Zhou’,
Wei Ye®, Jiaheng Liu'’, Jian Yang'', Yunfeng Shi'', Chenghua Lin!°, Enduo Zhao'', Tianle Cai’,
Ge Zhang*!'', Wenhao Huang!', Yoshua Bengio*’’, Jason Eshraghian?*

1ByteDance Seed, 2UC Santa Cruz, ?Princeton University, Mila - Quebec Al Institute, 5University of Montreal, “Peking
University, 7Carnegie Mellon University, ®University of Pennsylvania, ?Conscium, !°University of Manchester, ''M-A-P

*Core Contributors, ‘Corresponding authors

Abstract

Modern LLMs are trained to “think” primarily via explicit text generation, such as chain-of-thought
(CoT), which defers reasoning to post-training and under-leverages pre-training data. We present
and open-source Ouro, named after the recursive Ouroboros, a family of pre-trained Looped
Language Models (LoopLM) that instead build reasoning into the pre-training phase through
(i) iterative computation in latent space, (ii) an entropy-regularized objective for learned depth
allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and 2.6B models enjoy superior performance
that match the results of up to 12B SOTA LLMs across a wide range of benchmarks. Through
controlled experiments, we show this advantage stems not from increased knowledge capacity, but
from superior knowledge manipulation capabilities. We also show that LoopLM yields reasoning
traces more aligned with final outputs than explicit CoT. We hope our results show the potential of
LoopLM as a novel scaling direction in the reasoning era.

Correspondence: ridger@ucsc.edu, zhangge.eli@bytedance.com, huang.wenhao@bytedance.com, jsn@ucsc.edu
Project Page & Base / Reasoning Models: http: //ouro-1lm.github.io

Input
Embedding

Hellaswag                 om,                             ‘      <                                             Hellaswag

Wino-
grande

x
Eo

en

MATHS0O                           HumanEval+                                                                            MATHS00                           HumanEval+
Exit Gate                                                                                         = Ouro 1.4B                                                                                                                    Ouro 2.6B

;                                    Gemma31B = Qwen31.7B ===Gemma34B =—=Qwen3 4B                   Gemma3 4B —— Quwen3 4B    ===Gemma312B ——-=Qwen3 8B
f

Figure 1 Ouro Looped Language Model performance. (Left) The parameter-shared looped architecture. (Middle &
Right) Radar plots comparing the Ouro 1.4B and 2.6B models, both with 4 recurrent steps (red), against individual
transformer baselines. Our models demonstrate strong performance comparable to or exceeding much larger baselines.


===== PAGE BREAK =====

1 Introduction

The advancement of Large Language Models (LLMs) has historically relied on scaling up model size as
the primary driver, accompanied by increases in data and compute [1-4]. However, deploying models with
hundreds of billions of parameters requires extensive infrastructure, increasing latency and cost while limiting
accessibility. These factors make parameter efficiency critical: achieving better model capability within a
fixed parameter budget. Such models not only mitigate overfitting on finite datasets with fewer trainable
parameters, but also enable more practical deployment with lighter infrastructure. To achieve such parameter
efficiency, two main avenues have been explored. The first expands the training corpus regardless of model
size [5], though data scarcity increasingly limits this path. The second leverages inference-time compute
through Chain-of-Thought (CoT) reasoning [6], allowing models to spend more compute on complex problems
via extended token generation.

We explore a third pathway based on architectural innovation: achieving dynamic computation within
a fixed parameter budget. This is accomplished by recursively reapplying shared parameters, where a
group of weight-tied layers are iteratively reused during the forward pass. We call this the Looped Language
Model (LoopLM). The LoopLM paradigm has gained wide attention and shown strong promise recently,
originating with the seminal Universal Transformer [7]. Plenty of works on looped transformers [8-13],
recursive transformers [14], and latent reasoning frameworks [15, 16] have demonstrated the benefits of deeper
computational processing on equivalent training data, indicating the potential benefits at a larger scale. These
approaches share a common principle: through iterative reuse of weight-shared layers, computational scaling
is decoupled into the depth of computation and the number of parameters involved.

The LoopLM design yields several advantages. First, LoopLM allows adaptive computation via a learned early
exit mechanism: simple inputs may terminate after fewer recurrent steps, while complex ones naturally allocate
more iterations, enabling flexible computational depth without increasing the parameter count. Moreover,
unlike inference-time methods like CoT, LoopLM achieves scalability by deepening its internal computational
graph rather than extending the output sequence, thereby avoiding excessive context growth. Finally, LoopLM
potentially improves capacity per parameter and achieves stronger performance than standard transformers of
comparable size when trained on the same data. While the prior studies have demonstrated the benefits of
LoopLM at modest scales, there is little evidence that Looped Language Models can perform on par with
non-looped architecture at practically meaningful scales. To this end, we ask the following question:

Does LoopLM exhibit more favorable scaling behavior (in loss, efficiency, safety, or capability growth),
compared to existing non-recursive transformer models?

In this work, we aim to answer this question affirmatively. We carefully investigate the LoopLM scaling
trajectory to understand its point of saturation, illustrating that LoopLM represents a fundamentally more
efficient path towards higher performance. While prior works have shown promise on modest scales (e.g.,
hundreds of billions of tokens), we study whether these benefits persist or even amplify when scaled to the
multi-trillion token regimes of training that define SOTA foundation models. In particular, we study the
mechanisms of LoopLM by further asking the following questions:

1. Does the recursive application of weight-shared layers enhance the LoopLM’s capabilities in a manner
analogous to increasing the number of unshared layers?

2. Are the performance gains LoopLM monotonic in the number of loops? Are there more factors involved
in the use of the adaptive computation of LoopLM that are different with prior empirical results in
smaller scale experiments?

To resolve these uncertainties, we move beyond small-scale proofs of concept and undertake a comprehensive
empirical investigation. Our results are presented below.
Our Contribution

In this work, we attempt to address the above research questions through a multi-faceted approach. We
scale up the pretraining of LoopLM to a total of 7.7T tokens and thoroughly investigate the scaling behavior
of LoopLM in various aspects. To achieve adaptive computation with LoopLM, we develop novel training


===== PAGE BREAK =====

AIME24                             AIME25                        Olympiadbench                      BeyondAIME
100                                  100                                  100                                   50
1) passe@l                        1  pass@1
86.7                  --
C= pass@10     r-1               C= pass@10     81.3
sof 7 1A             tt                   80        76.7             rc                   80                         75:25                  40        39.0             38.0
WA                rt                     VA 3 5,             1 |      73.3           n. 551          73.18          72.00
VA          ria      ry                                              34.0
1a                                          Yves      63.3           it                                                                      31.0           30.0
“7A      rm            I                                                                        -
o ©                                            o ©         VA        ry             rile ©             56.44           56.44          o 2°
i                                           i            VA                        i} e                                            i
3°                                    3°                              1  I   3°                                     i}
u                             u                     43.3 |    u                             u
”                              ”                      ryt  t  * ©                            9 50
40                         40       33.3       1h
=]          ry                                                          15.0
ty          ty
1
20                                   20          A         i          20                                   10                       9.0
0                                    0                                    0                                    0

2  \
ae

Sf 2

a                           arc                         1    OS OP UN
Fe SP               eo  oP EF OE at  &       eo  &  SS SS       er PF SFE SF
oF FF ow           & oF Xf     eo ms . é ee     FX FF MK KS
Fe                    io               oe       x
rod                       eo                      rod                       eo
HLE                  SuperGPQA                 GPQA
7                      70                      70
6    5.58                60                      60           59.10
5.21      5.21      5.14        53.68   51.89                52.69   54.54     51.01            Models
48.00
5                      50 147.37             46.60, 50445 45                       ZZ) Ouro-1.48
4.13       4.22                                                                Wz Ouro-2.6B
a4                                    @ 40                                    @ 40
35.92                                                 =
8                             5                             A          34.00       33.16              1) Qwen3-1.7B
uv                      uv                      uv                                    =
H3                                 W 30                      26.50       W 30                                            BB) Qwen3-4B
2.22                                                                   9) Qwen3-8B
2                        20                       20                             G9): Deepseek-1.5B
H):- Deepseek-7B
1                      10                      10
0                       0                       0
SS PCY YS KP       PP PP SF FP       SS © YS LK PK
Xo or SS      Yo oe & Se       XK or SS
SS  SS  a       2          SS  SS  x       oF          ‘3  ‘3  a       eo
Fe eX F&F gf &      er Xr SF SST YK      FX FF SY KX
oS               fo       XR oe                    Rk  eo
ew 8                   & ¢                   es 9

Figure 2 Performance on advanced reasoning benchmarks. Ouro-Thinking models compared with strong baselines
such as Qwen3 and DeepSeek-Distill. Ouro-1.4B-Thinking R4 is competitive with 4B models, and Ouro-2.6B-Thinking
R4 matches or exceeds 8B models across multiple math and science datasets.

objectives that enable computationally efficient recurrent computation while maintaining peak performance.
We further conducted several controlled experiments on synthetic tasks to understand the mechanism of
LoopLM superiority. Specifically, our contributions are:

e Exceptional parameter efficiency at scale.

By pre-training on 7.7 trillion tokens, we demonstrate
that 1.4B and 2.6B parameter LoopLMs match the performance of 4B and 8B standard transformers
respectively across nearly all benchmarks, achieving 2-3x parameter efficiency improvements critical
for deployment under resource constraints. As illustrated in Figure 1 and Figure 2, our Ouro and
Ouro-Thinking models demonstrate exceptional parameter efficiency across a suite of benchmarks, with
our 1.4B and 2.6B parameter models matching the performance of larger non-recurrent LLMs.

Entropy regularized adaptive computation. We develop an entropy regularized training objective with a
uniform prior over exit steps that enables unbiased exploration of all recurrent depths. Moreover, we
add a focused training stage for adaptive gates training, specifically to optimize the tradeoff between
computation efficiency and loop performance, allowing learned adaptive allocation of recurrent steps
based on input difficulty.

Mechanistic understanding via synthetic tasks. Using controlled experiments inspired by the physics
of language models framework, we show that recurrence does not increase raw knowledge storage
(approximately 2 bits per parameter for both looped and non-looped models) but dramatically enhances
knowledge manipulation capabilities on tasks requiring fact composition and multi-hop reasoning.

Improved safety and faithfulness. LoopLM architectures reduce harmfulness on HEx-PHI [17], with
safety improving as recurrent steps increase, including extrapolated steps. In contrast to CoT, our
iterative latent updates yield a causally faithful reasoning process rather than post hoc rationalization.

Our study of LoopLM establishes the number of recursion as a third scaling axis beyond model size and data,
and we publicly release the Ouro model family (1.4B and 2.6B parameters) to demonstrate the benefits of
LoopLM at scale.


===== PAGE BREAK =====

2 Related Works

We define notation and confine background only to what is needed for presenting our method. A standard
L-layer Transformer is a composition of layer functions. It processes an input sequence of hidden states
H©) € RN*¢ through a series of unique layers, parameterized by 6),..., Oz:

H™) = TransformerLayery _(... TransformerLayerg, (TransformerLayerg, (H ())) )

In contrast, the Universal Transformer [7] replaces L distinct layers with a recurrent application of a single
Transformer block repeatedly for T’ updates:

HY = TransformerLayer,(H‘~)),   fort =1,...,T, with H©) as input

The core ideas of this architecture have resurfaced in recent literature, with recurrent-depth structures used
to improve the efficiency and reasoning capabilities of modern LLMs. For example, Geiping et al. [15] adopts
a “recurrent depth” to scale test-time computation in latent space. Similarly, Saunshi et al. [8] demonstrates
that “looped transformers” can match the performance of much deeper non-looped models on reasoning tasks,
formally connecting looping to the generation of latent thoughts. The approach is refined by converting
standard models into “Relaxed Recursive Transformers” with a common base block while injecting unique
LoRA adapters across recursive steps [14]. Similar concepts have emerged under different terms, such as
“pondering” in continuous space [16] and “inner thinking” for adaptive computation [18]. More advanced
variants, such as Mixture-of-Recursions [19] combine recursive parameter efficiency with adaptive, token-level
routing.

Across all these works, from the original Universal Transformer to its modern descendants, this emerging
line of architectures can be understood in two complementary ways. From one perspective, it behaves like
a deep Transformer where the weights of all layers are tied. From another, iteration functions as latent
reasoning, where the hidden states H™,...,H form a latent chain of thought that progressively refines
the representation to solve a task. Taken together, these results suggest that models can improve their ability
to reason by reusing computation internally without having to increase parameter count, shifting scale to
substance.

Perspective 1: Parameter Sharing for Model Efficiency. This view treats LoopLM as parameter
sharing: one or more Transformer blocks, or even submodules (e.g., attention, FFN), are reused across the
depth of the model, reducing parameters without changing the computation. The most prominent example in
the modern transformer era is ALBERT [20], which combines parameter re-use with embedding factorization
to drastically reduce the total parameter count. Prior to the widespread adoption of LLMs, parameter sharing
was explored extensively in machine translation [21]; Takase et al. [22] systematically studied sharing strategies
to balance compression and accuracy. Interest in parameter reuse dropped as models grew larger, but it has
resurged to shrink the memory footprint of LLMs. For example, Megrez2 [23] reuses experts across layers in a
standard Mixture-of-Experts (MoE) model, and shows a viable path forward for edge LLM deployment with
limited memory.

Perspective 2: Latent Reasoning and Iterative Refinement. Here, the LoopLM’s iteration is viewed
as latent reasoning where each step is a non-verbal “thought” that refines the model’s internal representation.
Empirically, increasing the number of recurrent steps improves performance on complex reaasoning tasks [8, 15].
Some models make this process explicit by feeding hidden states back into the input. Coconut inserts a
“continuous thought” token, which is derived from the previous steps’s last-layer hidden state, so the model
can “ponder” in a continuous latent space [24]. CoTFormer interleaves activations back into the input before
reapplying this augmented sequence to the shared layers [25]. These explicit feedback loops contrast with
implicit LoopLM variants, where the entire thought process is contained within the evolution of hidden states
from H“-) to H/, Thus, both Perspective 1 (model compression) and Perspective 2 (latent reasoning)
leverage shared-parameter iteration to improve parameter efficiency, and are being explored for enhance
reasoning and efficient sequence-length expansion (e.g., PHD-Transformer [26]).


===== PAGE BREAK =====

input                                              Training                      input                                                                   Inference
Embedding                                                                                                             Embedding

|       R=1

+
L.                                                                                                                    1                                  Pa                                Pm
1                                                                                                                                          CDF, =p, + p>                 CDF n= Px to" + Pm

Tinax                                                                                                                                                                      CDF, > threshold
t
L= Som £24 — 8 HD, 1 PT)
t=1

Early Exit
Entropy Regularization

Ain
Expected Task Loss

Figure 3 Overview of Looped Language Model (LoopLM) architecture. Left (Training): During training, the
model applies a stack of N shared-weight layers for n recurrent steps (R = 1 to R =n). At each recurrent step 7,
an exit gate predicts the probability p; of exiting, and a language modeling head £; computes the task loss. The
training objective combines the expected task loss across all steps with an entropy regularization term H(pi,...,pn)
to encourage exploration of different computational depths. Right (Inference): At inference time, the model can exit
early based on the cumulative distribution function (CDF) computed from exit probabilities. When CDF; = an Dk
exceeds a threshold, the model terminates at step i, enabling adaptive computation that allocates more steps to harder
inputs while maintaining efficiency on simpler ones. The dashed line indicates potential future steps that may be
skipped through early exit.

3 Learning Adaptive Latent Reasoning with LoopLM

In this section, we shall formally define the LoopLM architecture based on (causal) transformers and explain
how we train LoopLMs that are able to perform latent reasoning with adaptive computation. Figure 3
illustrates our architecture during both training and inference phases. Our objective is to let the model decide
how many recurrent steps to use for each token and each example, spending less compute on easy inputs and
more compute on hard inputs, without sacrificing accuracy when many steps are available.

3.1 LoopLM Architecture

Let Trg(-) : R“*4 > R™”*4 denote a causal transformer layer equipped with parameter 0, with hidden
dimension d and input length M. Moreover we let Imhead(-) : R¢ > R'Y! denote the language-modeling
head where V is the vocabulary of tokens, and emb(-) : R!”! — R@ denote the embedding layer. A typical
non-looped causal language model is defined by stacking L layers as follows:

F(-) :=Imhead o H’ oemb(-:), where H“(-) := Trg, o--+0 Tro, (-) is the hidden layers.

Let t € N,t < Tax be the number of loop steps, which we also call the number of recurrent steps or recurrent
depth, we define the looped language model F™ by:

F((.) = Imhead o H" 0 H 0---0 H¥ o emb(-).                     (1)
—————$ SS
t times
Apparantly F) = F is the non-loop model. Given a sequence of tokens £1.47 = (11, 22,...,0.) of length M,

a LoopLM model F') acts as a sequence to sequence model and produces M tokens yy... = (Y1, Y2,--- YM)
that is consistent with the causal dependency condition that yy... = F () (t1m) and for any m < M. The


===== PAGE BREAK =====

Algorithm 1 Q-exit Early Stopping Criterion

Require: Input x; maximum steps Tyyax; threshold q € [0, 1];
1: cdf —0;  surve 1                                                                 > suru = Wad — ;)
2: for t = 1 to Ty,ax do
3:     A, < o(Gate(F™ (x)))

4      qg(t|z) — Az: surv

5      cdf + cdf + pt                                                                             > CDF(t | x)
6      if cdf > q then

7:         return ¢                                                                                            > texit (2)
8      suru & suru: (1— Az)

9: return Tinax                                                        > fallback if the threshold is never reached

next-token-prediction objective is simply the cross-entropy loss on all yj, 1 <i < M,

LY = | So =log Pr(F (214) = v2:i41)|, for t € [1, Tinaxl      (2)
1<i<M-1

where Pr(F“ (a1.;) = 2441) is computed by taking a softmax over the Imhead output. The loss £“ measures
the precision of a t-step LoopLM. Prior literature such as [8, 12] have shown that scaling up the loop count
t is beneficial for reasoning tasks in smaller scale. However, increasing the loop count t costs computation,
and not all language tokens are reasoning-heavy and need those computation to be predicted correctly. In
fact, many tokens in the next-token-prediction tasks are simple or can be learned with high certainty [27, 28].
Thus it is crucial to spend the computation budget on the right tokens for pareto-optimal allocation between
performance and efficiency. This is achieved by the gating mechanism we shall describe below.

3.2 Adaptive Computation via Gating Mechanism

The early exit gate at loop step t < Tmax is defined by \4(a) = o(Gate(F(a))) for an input sequence x
through a learned linear projection followed by sigmoid activation o. After that, LoopLM uses the gates
{Az (2) }4€[1, Tax] $0 compute a distribution gg(-|x) € A+ over {1,...,Tinax}' to decide whether to continue
the recursive computation or stop to yield the output. Following [29], we adopt a deterministic Q-exit criterion
based on the cumulative distribution function (CDF), described in Algorithm 1. At each step t, we compute
the following:

CDF (tl2) =} aolila) = Ce) TT 5(@))

and qg(i|x) represents the probability of exiting exactly at step i given input «. The product term W240 —2j;)

ensures that we can only stop at step 7 if we did not exit at any earlier step 7 <7. For input x, we exit early
when the CDF exceeds a set threshold q € [0, 1):

texit(a) = min{t : CDF(t|x) > q}
The threshold g controls the compute-accuracy tradeoff, where lower values encourage earlier exits and higher
values allow deeper computation. The gating function A; shall be learned in the two-stage training:

e Stage I: during pre-training, the gates A;(-) are learned by optimizing an entropy-regularized objective.
In this stage, the exit distribution aims to widen the coverage of the optimal exit step.

e Stage Il: we focus on training only the gates in this stage. The objective exploits the pattern learned in
stage I and sharpens the distribution by choosing the optimal exit step with a soft cross-entropy loss.

We shall immediately explain the two stages below.

1A¢ — {x € [0,1]? | ve, x; = 1} is the probability simplex.


===== PAGE BREAK =====

3.3. Stage I: Learning An Entropy-Regularized Objective

To obtain a LoopLM that can exit early while maintaining performance, we need an objective that jointly
optimizes accuracy and computational efficiency. Let £“) follow (2), then our training objective combines the
next-token prediction loss with an entropy regularization term over qg¢:

Tmax                                                      Tmax
L= » qo(tlr)- LO —  B- H(qo(-|x))       A(q6(-|x)) = — » qo(t|a) log qo(t|)          (3)

entropy regularization

expected task loss

where £“) is the next token prediction loss at loop step t, and H (qo(-|x)) is the entropy of the exit step
distribution. The hyperparameter { controls the exploration-exploitation trade-off: larger G encourages more
uniform distributions (higher entropy), allowing the model to explore different depths, while smaller (6 allows
more concentrated distributions when the model is confident about the optimal depth for a given input.

Alternative perspective: variational inference with uniform prior. The entropy-regularized objective
can be equivalently viewed through the lens of variational inference. If we treat the exit step as a latent
variable z with a prior distribution 7, we can derive an Evidence Lower Bound (ELBO) objective:

Tmax
Letpo = >> as(t| 2) - LM + B- KL(go(- | 2) || 7)

t=1
When we choose a uniform prior 7 = 1/Tinax, Vt, the KL divergence simplifies to:
KL(qo(- | #) ||) = —H(a6(- | &)) + log Tmax

Since log Tmax is constant, minimizing the ELBO with a uniform prior is equivalent to our entropy-regularized
objective. This connection reveals that our approach aligns with adaptive computation methods like Ponder-
Net [30], which also optimize an ELBO objective for dynamic halting.

Why uniform prior? While our formulation is similar to PonderNet [30], a critical difference lies in the
choice of prior. PonderNet and other adaptive computation methods typically employ geometric priors:

1-—    t-1
ne? = a ee   t=1,...,Tinax,   rAE (0, 1)
Similarly, methods like Recurrent Depth [15] use Poisson-lognormal priors that also favor shallow computation.
We argue that such priors conflate two distinct goals: (1) learning when to exit based on input difficulty, and
(2) minimizing average computation cost. In fact, these priors contain a strong inductive bias toward shallow
computation by placing more mass on early steps, explicitly encouraging the model to exit early. Therefore,
they risk under-exploring deeper steps and may fail to fully exploit the benefits of recurrent depth.

In contrast, the uniform prior makes no assumptions about the optimal exit step distribution. It optimizes all
depths uniformly, enabling the model to infer the computational requirements of different inputs without
relying on predefined inductive biases. This is particularly important for complex reasoning tasks, where the
optimal depth should be learned from the data rather than being constrained by the prior. We provide a
detailed empirical validation of this choice against geometric priors in Section A.

3.4 Stage Il: Focused Adaptive Gate Training

Unlike traditional approaches that treat the gating mechanism as an auxiliary component, we directly optimize
the exit gate for effective adaptive computation. The core idea of our gate training approach is to teach the
model to make termination decisions based on actual performance improvements. We adopt a greedy approach
that optimizes the trade-off between improvement via recurrent step and computation efficiency below.



===== PAGE BREAK =====

Stable Training           CT Annealing |__.        LongCT                 Mid-Training                      .                       Reasoning                Ouro-2.6B
Upcycle 2.6B            3T Tokens   1.4T Tokens   20B Tokens             300B Tokens            Ouro-2.6B                 SFT                   Thinking
/                             SJ                             JS
Stable Training
    Warmup                  3T Tokens
>)                             >)                             D)
Stable Training           CT Annealing |__,        LongCT                 Mid-Training __,             _                      Reasoning                Ouro-1.4B
Keep 14B            3T Tokens               1.4T Tokens              20B Tokens              300B Tokens            Ouro-1.4B                  SFT                    Thinking

Figure 4 The Ouro model training pipeline. The process starts with a common Warmup and an initial 3T token Stable
Training phase. The model is then split into two streams: one ‘Keep 1.3B’ (resulting in Ouro-1.4B) and one ‘Upcycle
2.6B’ (resulting in Ouro-2.6B). Both streams independently undergo an identical subsequent four-stage training process:
a second Stable Training (3T tokens), CT Annealing (1.4T tokens), LongCT (20B tokens), and Mid-Training (300B
tokens). This 7.7T token pre-training pipeline produces the base models (Ouro-1.4B and Ouro-2.6B), which are then
passed through a final Reasoning SFT stage to create the Ouro-Thinking models.

In this stage, the adaptive exit objective must exclusively optimize the gating mechanism without interfering
(t)

i,stop at each token

with the language model’s learned representations. We compute the detached loss £
position 2 and define the loss improvement of advancing from step t — 1 to step t by

1 = max(0, £052, — £0)                                   ")

Intuitively, when J is small, the improvement has stagnated and LoopLM should opt for an early exit.
We implement this intuition by computing the ideal continuation probability wl) =o(k- (” —7)) using
k = 50.0 as the slope and 7 = 0.005 as the improvement threshold. When 1) >T, wh) is large and the
model performs the next recurrent step; When I ©) <7, early exit is favored in step t. The adaptive exit loss
at step t takes the form of a weighted cross-entropy averaged over the sequence length M between this ideal
behavior and the actual gate prediction.

£°. axive = a7 Do {w{” low(d — 49) + 1 = aol?) to(a!?)                       (5)

adaptive
i<M

The total adaptive training loss is averaged across all recurrent steps:

1   Tmax
_       S~ cit
Ladaptive _ T.        La daptive
max

t=2

Significance of our adaptive loss. The adaptive loss (5) trains the gate in step t by matching its weights
MY) with the ideal probability w\. This formulation penalizes two failure modes simultaneously:

i                                 i
e Underthinking, where the gate wants to stop when it should continue (when the ideal continuation
(t)

probability w;’ is large, but the early stop gate MD is also large);

e Overthinking, where the gate wants to continue when it should stop (when the ideal continuation
(t)

probability w;” is small, but MO is also small).

By optimizing (5), the gates learn to greedily search for the optimal exit step, to tradeoff computation
efficiency for better performance. For empirical evaluations, see Section 5.4.1.

4 Training Looped Language Models

Our training pipeline for the Ouro model family is a multi-stage process, as illustrated in Figure 4. The
process begins with a common warmup stage, followed by an initial Stable Training phase on 3T tokens. After
this, the model is split into two variants: a 1.4B parameter model and a 2.6B model created via upcycling.
Both variants then undergo an identical series of four subsequent training stages: a second Stable Training
phase (3T tokens), CT Annealing (1.4T tokens), LongCT (20B tokens), and Mid-Training (300B tokens).
This comprehensive pipeline, totaling 7.7T tokens of training data, produces our base models, Ouro-1.4B and


===== PAGE BREAK =====

Ouro-2.6B. Finally, these base models are further refined through a specialized Reasoning SFT (Supervised
Fine-Tuning) stage to create the final, reasoning-focused models: Ouro-1.4B-Thinking and Ouro-2.6B-Thinking.
This section details the model architecture, data composition, and specific configurations used in each of these
training stages.

4.1. Transformer Architecture

Our Ouro models are built upon the standard decoder-only Transformer architecture [31], prioritizing a
clean implementation of the looped computation mechanism without extraneous modifications. The core
architecture consists of a stack of identical Transformer blocks, which are applied recurrently.

Each block uses Multi-Head Attention (MHA) with Rotary Position Embeddings (RoPE) [32] to handle
sequence order. For computational efficiency, The feed-forward network (FFN) in each block utilizes a SwiGLU
activation [33]. To enhance training stability, which is especially critical for deep recurrent computation, we
employ a sandwich normalization structure. This places an RMSNorm layer before both the attention and FFN
sub-layers, an approach noted in prior literature to improve stability in loop transformers [15].

Table 1 Ouro model architecture configurations. Both models share the same vocabulary and core component types,
differing in parameter count and layer depth.

Model  Parameters Layers Hidden Size (dmodei) Attention  FFN  Pos. Embed. Vocab Size
Ouro 1.4B       1.4B          24              2048             MHA     SwiGLU       RoPE         49,152

Ouro 2.6B            2.6B                 48                       2048                       MHA         SwiGLU           RoPE                49,152

For both models, we use a shared vocabulary of 49,152 tokens, reused from the SmolM2 [34] model. This
tokenizer is optimized for languages with a Latin alphabet and code and contains no Chinese tokens. Our
inclusion of Chinese data in Stage 1 thus resulted in highly inefficient tokenization and poor performance.
Consequently, we removed all Chinese data from Stage 2 onwards to focus our training budget on English and
code. This limited vocabulary may also impose constraints on the model’s advanced mathematical reasoning
capabilities due to a potential lack of specialized symbols. This shift is reflected in the data compositions
detailed in the following sections.

4.2 Data

As data defines the capability boundaries of large foundational models, our model is trained on a diverse
collection of datasets spanning multiple domains and stages, including web data, mathematical content, code,
and long-context documents, enabling it to perform acquire fundamental language understanding and perform
advanced reasoning, coding, and long-context understanding through a unified training pipeline. In addition
to standard web crawl datasets, we adopt specialized datasets for mathematical reasoning and code generation
to further enhance the model’s capabilities for complex problem-solving. In Table 2, we summarize the
composition and quantity of our training data across different stages. In the following sections, we detail our
dataset sources, preparation protocols, and data mixing strategies.

4.2.1 Data Composition

The capabilities of modern language models primarily stem from their training data, and this principle holds
true for our model as well. To ensure reproducibility, our training corpus is entirely composed of open-source
datasets, with data statistics summarized in Table 3. We partition the data into four distinct subsets,
called stages, each employing different data construction strategies that align with the Warmup-Stable-Decay
(WSD) [35] learning rate scheduler, which is widely adopted in modern language model pretraining.

Stage 1: Pre-training The pre-training stage supports the warmup and stable phases of training. The
dataset is primarily composed of Web CommonCrawl (CC) data. Since our objective is to train the model on
more than 2T tokens, many commonly used open-source datasets would not suffice: Fineweb-Edu [36] provides
1.3T tokens, and DCLM [37] offers 2.6T tokens. We select Nemotron-CC [38] (6.3T tokens) as the main


===== PAGE BREAK =====

Table 2 Statistics of the training corpus. Since data are randomly sampled during pre-training, the dataset size
does not directly correspond to the total number of seen tokens.

Data Source                        Stage #Tokens(B) # Used Tokens (B)
Nemotron-CC (Web Data)       Stage 1       6386              4404
MAP-CC (Web Data)           Stage 1        800               780
Ultra-FineWeb-zh (Web Data) | Stage 1        120               120
OpenCoder-pretrain             Stage 1        450               450
MegaMath-web                 Stage 1        247               246
MegaMath-high-quailty           Stage 2         64                   64
Nemotron-CC-Math-v1           Stage 2         210                 210
Nemotron-Code                   Stage 2         53                   53
Nemotron-SFT-Code             Stage 2         48                   48
Nemotron-SFT-General           Stage 2         87                   87
OpenCoder-Annealing            Stage 2          7                    7
ProLong-64K                      Stage 3         20                   20
Mid-training SFT Mix            Stage 4         182                  90

Table 3 Data composition for Stage 1 (Pre-training Phase | & Il). Total dataset size: 6T tokens.

Data Source | Nemotron-CC MAP-CC _ Ultra-FineWeb-zh OpenCoder-pretrain MegaMath-web
Proportion (%) |    73.4       13.0        2.0            7.5           4.1

dataset for the stable phase due to its large scale and suitability for our training requirements. To provide the
model with basic Chinese proficiency, we include Ultra-FineWeb-zh [39] and MAP-CC [40]. Additionally, to
enhance coding and mathematical abilities, we incorporate OpenCoder [41] and MegaMath [42].

Stage 2: Continual Training (CT) Annealing The CT annealing stage incorporates higher-quality
data to enhance the model under the annealing learning rate. We construct our dataset using the high-quality
subset of Nemotron-CC as the base. To further strengthen mathematical, coding, and general capabilities, we
incorporate the high-quality subset of MegaMath, Nemotron-CC-Math-v1 [43, 44], OpenCoder-Annealing [41],
Nemotron-Pretraining-Code-v1 [44], and Nemotron-Pretraining-SFT-v1 [44].

Table 4 Data composition for Stage 2 (CT Annealing). Total dataset size: 1.4T tokens.

Data Source                                                                            Proportion (%)
Nemotron-CC-high-quailty                                                                      66.5
Nemotron-CC-Math-v1                                                                             15.0
MegaMath-high-quailty                                                                             4.6
OpenCoder-LLM /opc-annealing-corpus                                                     0.5
Nemotron-Pretraining-Code-v1/Synthetic-Code                                3.8
Nemotron-Pretraining-SFT-v1/Nemotron-SFT-Code                       3.4
Nemotron-Pretraining-SFT-v1 /Nemotron-SFT-General                  6.2

Stage 3: Long Context Training (LongCT) The LongCT stage extends the long-context capabilities of
the model. We adopt the 64K-length subset of ProLong [45], consisting of 20B tokens, to train the model on
longer sequences and improve its ability to handle long contexts.

Stage 4: Mid-training The mid-training stage leverages a wide and diverse set of extremely high-quality
data, consisting of both (Question, Answer) and (Question, CoT, Answer) samples, to further enhance the
model’s advanced abilities. We integrate more than 20 open-source supervised fine-tuning (SFT) datasets to
maximize data diversity, while conducting thorough decontamination to minimize potential overlaps with
mainstream evaluation benchmarks. All samples are converted into ChatML format to reduce alignment

10


===== PAGE BREAK =====

tax in the subsequent post-training stage. After processing the previous datasets, we obtain a total of 182B
tokens, from which we sample 90B tokens to form the newly incorporated datasets. To ensure stable data
distribution during training, 30B tokens from Stage 1 and 180B tokens from Stage 2 are replayed, resulting in
an effective training volume of 300B tokens. Consequently, this stage is designed to push the model to the
limits of its advanced abilities developed during pretraining.

4.3. Pre and Mid-Training

We adopt a multi-stage training strategy using a dynamic mixture of the curated data described, specifically:
a Pre-training stage (split into two phases with different recurrent configurations), a CT Annealing stage for
quality enhancement, a LongCT stage for context extension, and a Mid-training stage for advanced capability
refinement. Throughout our training pipeline, we train the model with a maximum of 4 recurrent steps.

4.3.1. Training Stability and Adaptive Configuration

During training, we prioritized stability over aggressive scaling, making several key adjustments based on
empirical observations of training dynamics. These decisions were critical for achieving stable convergence with
recurrent architectures, which exhibit different optimization characteristics compared to standard transformers.

Recurrent Step Reduction for Stability. Our initial experiments with 8 recurrent steps in Stage la
reveal training instabilities, including loss spikes and gradient oscillations. We hypothesize this stems from
the compounding gradient flow through multiple recurrent iterations, which can amplify small perturbations.
Consequently, we reduced the recurrent steps from 8 to 4 in Stage 1b, finding this sweet spot balanced
computational depth with training stability.

Batch Size Scaling. To further enhance stability, we progressively increased the batch size from 4M to
8M tokens. Larger batch sizes provide more stable gradient estimates, which is particularly important for
recurrent architectures where gradient flow through multiple iterations can introduce additional variance.

KL Divergence Coefficient Reduction. We strategically reduced 8 from 0.1 in Stage la to 0.05 in
subsequent stages. This reduction serves dual purposes: (1) it decreases the conflicting gradients between task
loss and the KL penalty, leading to more stable optimization, and (2) it reduces the “pull” from the uniform

Table 5 Training recipe for both Ouro 1.4B and 2.6B.

Stage 1a       Stage 1b          Stage 2          Stage 3         Stage 4
Pre-train| Pre-trainll CT Annealing      LongCT Mid-training
Hyperparameters
Learning rate (Final) | 3.0x107* 3.0 x 1074      3.0 x 10-°     3.0x10°° 10x 107°
LR scheduler              Constant      Constant      Cosine Decay     Constant Cosine Decay
Weight decay                                              0.1
Gradient norm clip                                            1.0
Optimizer                                    AdamW (1 = 0.9, G2 = 0.95)
Batch size (tokens)      4M— 8M                               8M
Sequence length              4K             4K              16K             64K            32K
Training tokens                3T              3T               1.4T              20B             300B
Recurrent steps               8                                       4
B for KL divergence         0.1                                   0.05
RoPE base                  10K           10K             40K             1M             1M
Data Focus
Web data                   High          High          Medium          Low            Low
Math & Code               Low           Low            High            Low           High
Long-context               None          None            Low            High         Medium
SFT-quality                None          None            Low            Low           High

11


===== PAGE BREAK =====

prior, allowing the model greater freedom to explore beneficial depth patterns without being artificially
constrained. This adjustment was crucial for maintaining stable training dynamics while enabling the model
to learn effective depth allocation.

4.3.2 Stage-wise Training Details

We utilize the flame [46] framework for performing pretraining, built upon torchtitan [47]. To fully utilize our
resources, we adopt an upcycling strategy that enables efficient scaling of model capacity during training.

e Stage 1a: Pre-training Phase | (Exploration Phase). We initially train the model on 3T tokens of web data
from Nemotron-CC with 8 recurrent steps. The training uses the Warmup-Stable-Decay (WSD) learning
rate scheduler with a peak learning rate of 3 x 1074. The sequence length is set to 4K tokens with an initial
batch size of 4M tokens, gradually increased to 8M for stability. During this phase, we observed training
instabilities that prompted our subsequent architectural adjustments.

e Stage 1b: Pre-training Phase II with Stability-Driven Upcycling. After identifying stability issues in Stage la,
we implemented an architectural pivot: reducing recurrent steps from 8 to 4. To maintain computational
efficiency while improving stability, we split our approach into two variants:

— Variant 1: A 1.4B parameter model maintaining 24 layers with 4 recurrent steps
— Variant 2: A 2.6B parameter model created through layer stacking (48 layers) with 4 recurrent steps

The recurrent nature of our architecture makes this upcycling process particularly smooth, as the shared
weights across iterations naturally facilitate layer duplication without the typical instabilities seen in
standard transformer upcycling. Both variants are trained on an additional 3T tokens with the stabilized
configuration. The data composition is carefully balanced as shown in Table 3.

e Stage 2: CT Annealing. Building on the stable foundation from Stage 1b, we enhance the model with
higher-quality data while annealing the learning rate to 3 x 10~°. The training corpus comprises 1.4T
tokens with increased emphasis on mathematical and coding capabilities. >We extend the sequence length
to 16K tokens, exceeding the length of most samples to minimize truncation and better utilize the enhanced
data quality. The recurrent steps remain at 4, having proven optimal for the stability-performance trade-off.
The data composition is carefully balanced as shown in Table 4.

e Stage 3: LongCT. This stage focuses on extending the model’s context window capabilities. We train on
20B tokens from the ProLong-64K dataset with sequences of 64K tokens, maintaining the batch size at 8M
tokens. The reduced KL coefficient (3 = 0.05) continues to provide stable training dynamics even with
these extended sequences.

e Stage 4: Mid-training. The final stage leverages 90B tokens of extremely high-quality SFT data, consisting
of both <Question, Answer> and <Question, CoT, Answer> samples. All SFT-style data is converted to
ChatML format to facilitate subsequent post-training alignment. The learning rate is further reduced to
1 x 107° with a cosine scheduler to help the model better absorb on this diverse, high-quality dataset.

Optimization Configuration. Throughout all stages, we use AdamW optimizer with weight decay set to
0.1, 6, = 0.9, 62 = 0.95, and gradient clipping at 1.0. These conservative settings were chosen specifically to
maintain stability with recurrent architectures.

Learning Rate Considerations. We empirically found that recurrent architectures require smaller learning
rates compared to standard transformers of equivalent parameter count. While resource constraints prevented
exhaustive hyperparameter search, our chosen rates represent conservative values that prioritized stable
convergence over potentially faster but riskier optimization trajectories.

Sequence Length Progression. The sequence length is progressively increased across stages: 4K tokens
for both pre-training phases, 16K for continual training with learning rate annealing, 64K for long-context
training, and 32K for mid-training. This gradual progression helps maintain stability while enhancing the
training throughput and expanding the model’s long-context capability.

12


===== PAGE BREAK =====

4.4 Supervised Fine-Tuning

Data Composition. We perform supervised fine-tuning (SFT) on a diverse corpus of approximately 8.3M
examples drawn from high-quality public datasets. As shown in Table 6, our training mixture emphasizes
mathematical reasoning (3.5M examples) and code generation (3.2M examples), while also incorporating
scientific reasoning (808K examples) and conversational abilities (767K examples).

For mathematical reasoning, we combine OpenThoughts3 [48] and AceReason-1.1-SFT [49] to provide
comprehensive coverage of problem-solving strategies. Our code training data aggregates multiple sources
including AceReason-1.1-SFT, OpenCodeReasoning [50], Llama-Nemotron-Post-Training-Dataset [51], and
OpenThoughts3, ensuring broad exposure to diverse programming paradigms and reasoning patterns. Scientific
reasoning capabilities are developed through OpenThoughts3 and Llama-Nemotron-Post-Training-Dataset,
while conversational proficiency is enhanced using the OO1-Chat-747K? and DeepWriting-20K [52] datasets.

Training Configuration. We train for 2 epochs with a maximum sequence length of 32K tokens using
the LlamaFactory codebase [53]. We employ the Adam optimizer with a learning rate of 2 x 107° and
6 = (0.9,0.95), applying a cosine decay schedule for stable convergence.®

Table 6 Supervised fine-tuning data composition. The training corpus comprises 8.3M examples across four key
capability domains.

Topic         Data Source                                                                                                        Size

Math     OpenThoughts3, AceReason-1.1-SFT                               3.5M

Code     AceReason-1.1-SFT, OpenCodeReasoning, Llama-Nemotron-Post- | 3.2M

Training-Dataset, OpenThoughts3
Science | OpenThoughts3, Llama-Nemotron-Post-Training-Dataset           808k
Chat     OO1-Chat-747K, DeepWriting-20K                                 767K

4.5 Reinforcement Learning Attempts

Following the SFT stage, we conducted exploratory RLVR (Reinforcement Learning with Verifiable Rewards)
alignment experiments using algorithms such as DAPO [54] and GRPO [55] on the DAPO-17K dataset.
However, these attempts did not yield significant performance gains over the final SFT checkpoint. We
hypothesize two primary reasons for this. First, Model Saturation: the models are relatively small and had
already undergone extensive SFT, which may have saturated their capabilities and left little room for further
improvement via RL. Second, Infrastructure Challenges: the unique LoopLM architecture posed difficulties
for our RL infrastructure. Specifically, our vLLM-based system could not efficiently perform rollouts with
dynamic early exits and subsequently use that variable-depth information for the update step. This forced us
to adopt a trade-off solution for training and inference.

We explored two main strategies based on these constraints:

1. Fixed 4-Round RL: We performed both rollouts and updates using a fixed 4 recurrent steps. In this
setup, model performance increased normally but did not surpass the SFT checkpoint. Interestingly,
we found that even when trained at a fixed 4-round depth, the model could still utilize fewer rounds
at inference time, behaving as incentivized by the RL objective. The reason for this generalization is
currently unknown.

2. Adaptive RL: We attempted to perform both rollouts and updates using the model’s native adaptive
early exit mechanism. This approach failed to yield performance improvements, which we primarily
attribute to the infrastructure challenges in handling the dynamic computational graphs.

*https: //huggingface.co/datasets/m-a-p/001-Chat-747K
3Training was interrupted due to infrastructure issues; we resumed from the last saved checkpoint with a learning rate close to
the original cosine decay schedule.

13


===== PAGE BREAK =====

We will further explore RL alignment for this architecture as we continue to develop infrastructure that can
fully support LoopLM’s dynamic computation.

5 Experiments

5.1. Base Model Evaluation

We conduct comprehensive evaluations of the Ouro base models trained on 7.7T tokens using the LoopLM
architecture. The evaluation focuses on their performance across general knowledge, reasoning, mathematics,
science, coding, and multilingual capabilities. All benchmarks are evaluated using 1m-eval-harness [56] and
evalplus [57] frameworks with settings detailed in Appendix. C.1.

For the base model baselines, we compare our Ouro models with leading open-source base models, including
Qwen2.5 [2], Qwen3 [3], Gemma3 [4], Llama3.1 [5], and Llama3.2 [5] series base models. All models are
evaluated using the same evaluation pipeline to ensure fair comparison.

Table 7 Comparison of 1.4B LoopLM model with 1-4B parameter baselines. The best score is bolded, and the
second-best is underlined. LoopLM’s column is highlighted in gray.

Gemma3 Llama3.2 Qwen2.5 Qwen3 Qwen2.5 Llama3.2 Qwen3 Gemma3 Ouro

1B          1.2B         1.5B       1.7B        3B          3B         4B         4B       1.4B R4
Architecture     Dense       Dense      Dense Dense Dense      Dense     Dense     Dense LoopLM
# Params       1.0B       1.0B      1.5B     1.7B     3.0B      3.0B     4.0B     4.0B      1.4B
# Tokens         2T        9T       18T      36T      18T       9T       36T      4T       7.77

General Tasks

MMLU             39.85        45.46       60.99      62.46      65.62       59.69      73.19      58.37       67.35
MMLU-Pro       11.31        11.80       29.11      37.27      37.87       33.34      51.40      34.61       48.62
BBH                30.26        30.72       43.66      53.51      55.37       39.45      70.95      66.32       71.02
ARC-C             39.25        41.98       54.44      55.72      55.46       52.47      63.65      60.92       60.92
HellaSwag         56.12        59.35       67.73      67.09      74.54       73.09      75.66      75.58       74.29

Winogrande    58.72     62.75     66.77    66.30    70.17     69.14    71.19    71.07    72.30
Math & Coding Tasks

GSM8K         2.05       7.05      60.73    70.28    74.60     67.20     72.86     68.69     78.92
MATH500           41.00           7.40           17.60        25.80        42.60         40.80        59.60        68.60        82.40
HumanEval           6.70             19.50           52.40         66.50         68.90           29.90         77.40         34.80          74.40
HumanEval+       5.50           17.40         46.30        59.80        62.20         26.20        70.70        29.30         67.40
MBPP                  12.40          35.70         60.30        68.00        63.00         50.30        78.80        60.60         73.00

MBPP+        10.10      29.10     50.00    58.50    54.20     39.70    65.90    51.10     62.70

Summary of Evaluation Results Based on the overall evaluation results, we highlight key conclusions
about our base models:

(1) Our 1.4B parameter Ouro model (with 4 recurrent steps) achieves performance comparable to the 4B
Qwen3-Base across most benchmarks. Notably, it matches or exceeds the 4B model on challenging
reasoning tasks such as BBH (71.02 vs 70.95), GSM8K (78.92 vs 72.86) and MATH500 (82.40 vs 59.60)

(2) The 2.6B parameter Ouro model outperforms dense models up to 8B parameters on reasoning-intensive
benchmarks. It achieves 55.73 on MMLU-Pro, 80.46 on BBH and 90.85 on MATH500, surpassing the
8B Qwen3-Base (53.72, 77.65 and 62.30 respectively).

(3) The recurrent architecture shows particular strength on tasks requiring multi-step reasoning and
knowledge manipulation, with the most pronounced gains observed on MMLU-Pro, BBH, GSM8K
and MATH500 benchmarks, validating our hypothesis that iterative computation enhances reasoning
capabilities.

14


===== PAGE BREAK =====

Table 8 Comparison of 2.6B LoopLM model with 3-12B parameter baselines. The best score is bolded, and the
second-best is underlined. LoopLM’s column is highlighted in gray.

Qwenz2.5 Llama3.2 Qwen3 Gemma3 Qwenz2.5 Llama3.1 Qwen3 Gemma3 Ouro

3B          3B         4B         4B          7B          8B         8B        12B      2.6B R4
Architecture        Dense     Dense Dense    Dense     Dense     Dense Dense    Dense LoopLM
# Total Params      3.0B       3.0B      4.0B      4.0B       7.0B      8.0B      8.0B     12.0B      2.6B
# Trained Tokens    18T        9T       36T       4T        18T       15T      36T      12T      7.7T

General Tasks
MMLU              65.62      59.69     73.19     58.37     74.20     73.02    76.63     72.14     74.60
MMLU-Pro          37.87     33.34     51.40     34.61      43.55     43.24     53.72     49.21     55.73
BBH                55.37     39.45     71.14     66.32      53.72     71.56     77.65     78.41     80.46
ARC-C              55.46      52.47     63.65     60.75      63.65     60.75     66.10     72.44     66.40
HellaSwag           74.54      73.09     75.66     75.58      79.98     81.97     79.60     83.68     79.69
Winogrande         70.17     69.14     71.19     71.27     76.48     77.11     76.80     77.74     75.85
Math & Coding Tasks

GSM8K             74.60      67.20     72.86     68.69      81.50     78.17    83.09     77.18     81.58
MATH500           42.60     40.80     59.60     68.60      61.20     52.90     62.30     83.20     90.85
HumanEval          68.90      29.90     77.70     34.80      79.30     38.40    84.80    46.30     78.70
HumanEval+        62.20      26.20     70.70     29.30      70.60     31.10    75.30     37.20     70.70
MBPP              63.00      50.30     78.80     60.60      73.80     62.40     79.00     73.50     80.40

MBPP+                 54.20       39.70      65.90      51.10       63.50       51.60      67.90      66.10       66.60

Table 9 Performance comparison across different benchmarks. For AIME24 and AIME25, we report passQ@1/pass@10
metrics. The best score is bolded, and the second-best is underlined.

Model          AIME24     AIME25   Olympiad Beyond HLE Super GPQA

pass@1 pass@10 | pass@1_ pass@10     bench      AIME          GPQA
Ouro-1.4B-Thinking-R4&               65.0           83.3           46.3           73.3             71.6            34.0        5.21 47.4        45.5
Ouro-2.6B-Thinking-R4          64.7       90.0       50.3       76.7         76.4        39.0     5.58    53.7     52.7
Qwen3-1.7B                   32.0      55.6      22.0      33.3       56.4       15.0    4.13 35.9    34.0
Qwen3-4B                    61.3      75.0      51.3      63.3       73.2       31.0    5.21 51.9    54.5
Qwen3-8B                    73.0      86.7      66.7      81.3        75.3       38.0    2.22 48.0     59.1
Deepseek-Distill-Qwen-1.5B        29.6           66.7           23.0          43.33           56.44            9.0         4.2       26.5        33.2
Deepseek-Distill-Qwen-7B          57.3           83.3           36.0           73.3             72.0            30.0        5.14 46.6        51.0

5.2 Reasoning Model Evaluation

We evaluate the reasoning capabilities of our Ouro reasoning models (Ouro-Thinking) with 4 recurrent steps
on challenging mathematical and scientific benchmarks that require multi-step problem solving and deep
reasoning. The evaluation includes AIME 2024/2025 (American Invitational Mathematics Examination),
OlympiadBench, GPQA, SuperGPQA, BeyondAIME, and HLE, representing some of the most challenging
reasoning tasks in the field.

Benchmarks.
e AIME 2024/2025 [58]. 30 questions per year from AIME I and II; integer answers 0-999.

e OlympiadBench [59]. Olympiad-level bilingual scientific problems; supports images for multimodal
inputs.

e GPQA [60]. 448 graduate-level multiple-choice questions in biology, physics, and chemistry; search-
resistant design.

15


===== PAGE BREAK =====

e SuperGPQA [61]. GPQA scaled to about 285 graduate disciplines; curated to remain challenging.
e BeyondAIME [62]. Hard integer-answer math beyond AIME; emphasizes contamination resistance.

e HLE [63]. Multi-disciplinary closed-ended benchmark; expert-written with public splits and a private
test set.

Models compared. We report results for Ouro-1.4B-Thinking and Ouro-2.6B-Thinking, which are LoopLM-
based looped language models with iterative depth. As baselines we include Qwen3-1.7B, Qwen3-4B, Qwen3-8B,
DeepSeek-Distill-Qwen-1.5B, and DeepSeek-Distill-Qwen-7B. We use size-matched baselines whenever available,
otherwise we compare to the next larger widely used model.

Evaluation protocol. All systems are evaluated with a single in-house harness and identical prompting.
We adopt an LLM-as-judge protocol across benchmarks with a fixed rubric and tie-breaking policy. Unless
otherwise noted, decoding uses temperature = 1.0 and top_p = 0.7 for every model.

Evaluation results. Table 9 summarizes outcomes. Iterative reasoning in the LoopLM architecture provides
consistent gains on these tasks. The 1.4B Ouro model with 4 recurrent steps reaches 71.55 on OlympiadBench
(vs. 73.18 for Qwen3-4B) and 34.0 on BeyondAIME (vs. 31.0 for Qwen3-4B). The 2.6B with 4 recurrent steps
variant scores 76.44 on OlympiadBench (vs. 75.25 for Qwen3-8B) and 39.0 on BeyondAIME (vs. 38.0 for
Qwen3-8B).

5.3. Performance by Recurrent Depth and Extrapolation

Table 10 Performance of the Ouro 1.4B base model across different recurrent steps (C-QA is CommonsenseQA [64]).
Steps 5-8 represent extrapolation, as the model was trained with a maximum of 4 steps. Performance peaks at the
trained depth (T= 4) and then degrades.

UT Step ARC-C ARC-E C-QA HellaSwag MMLU Winogrande
(25-shot) (8-shot) (10-shot) (10-shot) (5-shot avg)  (5-shot)

1           37.63      63.85      44.64       55.24         41.21          56.99
2           54.86      80.30      67.98       71.15         60.43         66.69
3           59.47      83.33      74.37       74.07         66.71          71.35
4           60.92     83.96     75.43       74.29         67.45         72.30
Extrapolation (Trained on T=4)

5           58.96      82.91      75.35       73.72         66.64         70.32
6           59.73      82.58      74.94       72.77         65.77         71.03
7           58.96      81.99      74.28       72.35         65.28         70.09
8           58.19      82.07      73.55       71.60         64.49         69.30

We analyze the Ouro model’s performance as a function of its recurrent computational depth. Our models
were trained with a maximum of 4 recurrent steps (T = 4). We investigate this behavior for both our base
models and our SFT Ouro-Thinking models.

Base Model Performance. Tables 10 and 11 present the performance of the Ouro 1.4B and 2.6B base
models, respectively, evaluated at depths from T = 1 to T= 8.

For both base models, performance on standard benchmarks (e.g., MMLU, ARC-C) generally improves
up to the trained depth of T = 4. Steps JT = 5 through T = 8 represent extrapolation beyond the
training configuration. As shown in both tables, benchmark performance sees a moderate degradation when
extrapolating, with a noticeable drop compared to the peak at T= 4.

However, this degradation in task-specific performance contrasts sharply with the model’s safety alignment.
As detailed in Section 7.1, the model’s safety improves as the number of recurrent steps increases, even into the

16


===== PAGE BREAK =====

Table 11 Performance of the Ouro 2.6B base model across different recurrent steps (C-QA is CommonsenseQA [64]).
Steps 5-8 represent extrapolation, as the model was trained with a maximum of 4 steps. Performance is strongest
around the trained depth (T' = 4) and shows varied degradation patterns during extrapolation.

UT Step ARC-C ARC-E C-QA HellaSwag MMLU Winogrande
(25-shot) (8-shot) (10-shot) (10-shot) (5-shot avg)  (5-shot)

1                47.95        72.39        57.58           68.94             51.55              61.48
2            62.37      85.23      76.90        77.61         67.63          70.48
3                65.36        87.33        79.77           79.12             73.57              74.35
4                66.38        86.95        81.65           79.56            74.60              75.53

Extrapolation (Trained on T=4)

5                65.36        86.83        81.24           79.57             74.43              75.93
6                65.02        86.74        81.08           79.63             73.79              75.37
7                65.44        86.57        80.75           79.59             72.92              75.77
8                64.76        86.49        81.08           79.50             72.24              74.59

Table 12 Performance of Ouro-1.4B-Thinking model by recurrent step. The model was trained at T = 4.
Performance peaks around T = 4 or T = 5. All scores are percentages (0-100).

Benchmark     | T=1 T=2 T=3 T=4    T=5 T=6 T=7 T=8

OlympiadBench | 2.22 59.70 70.67 71.55 72.30 69.48 69.04 66.81
SuperGPQA      2.03 33.07 44.50 47.37 48.73 46.15 45.29 42.88
AIME 2024         0.00 37.33 62.33 65.00 60.67 50.67 42.33 38.67
AIME 2025         0.33 25.00 43.33 46.30 47.00 43.00 41.00 38.00

extrapolated regime (T > 4). This suggests that while the model’s fine-grained knowledge for benchmarks may
falter beyond its training depth, the iterative refinement process continues to enhance its safety alignment.

Reasoning Model (SFT) Performance. We conduct a similar analysis on our SFT models, Ouro-Thinking,
to see how recurrent depth affects specialized reasoning tasks. Results for the 1.4B and 2.6B models are
presented in Table 12 and Table 13, respectively.

We conduct a similar analysis on our SFT models, Ouro-Thinking, to see how recurrent depth affects specialized
reasoning tasks. Results for the 1.4B and 2.6B models are presented in Table 12 and Table 13, respectively.

For both SFT models, performance at T = 1 is very low, confirming that iterative refinement is essential for
these complex tasks. Performance generally peaks at or near the trained depth, but shows slightly different
patterns. The 1.4B model (Table 12) peaks around T' = 4 or T = 5. The 2.6B model (Table 13) tends to peak
slightly earlier, at T = 3 or T = 4. Interestingly, neither model peaks strictly at T’ = 4 across all tasks, unlike
the base model evaluations which are often logit-based. This may suggest that the longer decoding required
for these reasoning tasks allows for a more active exploration of capabilities at different recurrent depths.
For both models, performance degrades as they extrapolate to deeper, unseen recurrent steps (T = 6 — 8),
reinforcing that performance is optimized for the depth seen during training.

Table 13 Performance of Ouro-2.6B-Thinking model by recurrent step. The model was trained at T = 4.
Performance peaks at T = 3 or T = 4. All scores are percentages (0-100).

Benchmark | T=1 T=2 T=3 T=4 T=5 T=6 T=? T=8

OlympiadBench | 18.96 68.59 75.56 7644 71.85 69.19 57.63 39.26
SuperGPQA          15.66 48.58 56.70 53.68 56.45 55.44 53.32 46.84
AIME 2024             3.00 52.00 70.33 64.70 57.00 56.33 49.67 39.00
AIME 2025             2.00 40.67 50.67 50.30 49.33 46.00 38.00 24.33

17


===== PAGE BREAK =====

5.4 Early Exit and Adaptive Computation Efficiency

A defining advantage of the LoopLM architecture lies in its capacity for adaptive computation allocation.
Unlike standard transformers with fixed computational budgets, our model can dynamically adjust the number
of recurrent steps based on input complexity. This section investigates various strategies for implementing
adaptive early exit, comparing their effectiveness in balancing computational efficiency with task performance.

5.4.1. Early Exit Strategies

We explore three distinct approaches to determining when the model should terminate its iterative computation
and produce the final output.

Baseline: Static Exit. The simplest strategy forces the model to exit at a predetermined recurrent step,
regardless of the input characteristics. While this approach provides predictable computational costs, it fails
to leverage the model’s potential for adaptive resource allocation. We evaluate static exit at steps 1 through 4
to establish performance bounds and understand the relationship between computational depth and accuracy.

Hidden State Difference Threshold. This heuristic-based approach monitors the magnitude of represen-
tational changes between consecutive recurrent steps. At each step t, we compute Ah, = |/h_ — ht_i||2 and
trigger early exit when Ah, < € for some threshold e.

Learned Gating with Q-Exit Criterion. Our
primary approach employs the learned exit gate
described in Section 4, which produces step-wise

halting probabilities A, based on the model’s cur-       0.65
rent hidden states. During inference, we apply
the Q-exit criterion: at each step t, we compute      0.60

the cumulative distribution function CDF(t) =
S~\_, p(ilx) and exit when CDF(t) exceeds the
threshold gq € [0,1]. The threshold q serves as
a deployment-time hyperparameter that controls
the compute-accuracy trade-off without requiring
model retraining.

°
ra
a

Accuracy

°
uw
fo)

.                               to                    0.45        7                         —e— Using Ponder Gate (Untrained)
We evaluate this strategy under two training config-                  é                           —¢— Ponder Gate (Trained)

urations. The untrained configuration uses the gate        oo                            wa Reh ext Deol

as trained during our standard pretraining pipeline              i       5       >       +5       x0       35       7
with the entropy-regularized objective (uniform                       Average Exit Round

prior KL loss). This represents the gate’s behav-
ior when jointly optimized with language model-
ing throughout Stages 1-4. The trained configura-
tion additionally applies the specialized adaptive
exit loss described in Section 3.4, which explic-
itly teaches the gate to base stopping decisions on
observed task loss improvements.

Figure 5 Comparison of early exit strategies on MMLU.
We evaluate four approaches across different average exit
rounds: static baseline (red triangle), hidden state differ-
ence threshold (green squares), Ponder gate from standard
pretraining (blue circles), and Ponder gate with specialized
adaptive exit training from Section 3.4 (orange diamonds).

Experimental Results. Figure 5 presents the accuracy-efficiency trade-off curves for all strategies on
the MMLU benchmark. By varying the exit threshold (or static exit step for baseline), we obtain multiple
operating points for each method, enabling direct comparison at equivalent computational budgets measured
by average exit round.

Several key findings emerge from this analysis:

1. The Ponder gate with specialized adaptive exit training achieves the best accuracy at every computational
budget, demonstrating that the loss improvement-based training signal described in Section 3.4 provides

18


===== PAGE BREAK =====

clear benefits over standard entropy regularization. At an average exit round of 2.5, the specialized
training reaches 66% accuracy while the standard gate achieves approximately 64%;

2. Even without specialized training, the Ponder gate from standard pretraining substantially outperforms
the static baseline, validating that the entropy-regularized objective with uniform prior successfully
enables adaptive computation. The gate learns to differentiate input difficulty through the general
training dynamics, though it lacks the explicit supervision to correlate stopping decisions with actual
performance improvements. This demonstrates that our base training approach already captures useful
signals for resource allocation;

3. The hidden state difference threshold strategy performs surprisingly competitively, closely tracking both
gate configurations. At moderate computational budgets (2-3 average rounds), it achieves accuracy within
1%-2% of the specialized trained gate, suggesting that representation stability provides a reasonable
proxy for computational convergence. However, the consistently superior performance of the specialized
trained gate across all operating points confirms that explicit supervision via the adaptive exit loss
captures information beyond what can be inferred from representational dynamics alone.

4. Comparing the untrained and trained gate configurations reveals the value proposition of the specialized
training procedure. The gap between these curves, approximately 2%-3% accuracy at most operating
points, represents the benefit of teaching the gate to explicitly monitor task loss improvements i”
rather than relying solely on entropy regularization to discover stopping policies. This empirical result
validates our design choice to introduce the adaptive exit loss as a specialized training objective.

5. The baseline’s monotonic improvement from 1 to 4 rounds confirms the “deeper is better” property
while revealing diminishing returns. The dramatic jump from 1.0 to 2 rounds (40% to 60% accuracy)
contrasts with the marginal gain from 3 to 4 rounds (67.35% accuracy). This pattern explains why
adaptive methods prove effective: most examples achieve near-maximal performance at intermediate
depths, with only a minority requiring full computational depth.

5.4.2 KV Cache Sharing for Inference Efficiency

The recurrent nature of our architecture introduces a challenge: naively, each recurrent step requires maintaining
its own KV cache, leading to 4x memory overhead for our 4-step model. We investigate strategies to reduce
this overhead through KV cache reuse.

Prefilling Phase During the prefilling phase (processing the input prompt), we find that all four recurrent
steps require their own KV caches, as each step transforms the representations in ways that cannot be
approximated by earlier steps. Attempting to reuse KV caches during prefilling leads to performance
degradation (>10 points on GSM8K).

Decoding Phase However, during the decoding phase (auto-regressive generation), we discover that KV
cache reuse becomes viable. We explore two strategies:

1. Last-step reuse: Only maintain KV cache from the final (4th) recurrent step
2. First-step reuse: Only maintain KV cache from the first (1st) recurrent step.
3. Averaged reuse: Maintain an averaged KV cache across all four steps

As shown in Table 14, these strategies yield dramatically different outcomes. Reusing only the first step’s
cache results in a catastrophic performance collapse (e.g., 18.73 on GSM8K, down from 78.92), indicating that
the initial representations are insufficient for subsequent decoding steps. In contrast, both the last-step and
averaged reuse strategies achieve nearly identical performance (within 0.3 points on GSM8K) to the full cache
baseline, while successfully reducing memory requirements by 4x. The last-step strategy performs slightly
better than the averaged approach on MATH-500, suggesting that the final recurrent step’s representations
are most informative for subsequent token generation. This finding enables practical deployment of LoopLM
models with memory footprints comparable to standard transformers of similar parameter count.

19


===== PAGE BREAK =====

Table 14 KV cache sharing strategies during decoding. Both last-step and averaged strategies achieve minimal
performance loss while reducing memory by 4x.

Strategy               GSM8K MATH-500 Memory Reduction
Full (4x cache)     78.92       82.40             1.00x
First-step only         18.73              8.43                         4.00x
Last-step only         78.85            80.40                     4.00x
Averaged                  78.73             78.52                       4.00x

6 Understanding LoopLMs Superiority from a Parametric Knowledge Viewpoint

Why LoopLMs achieve far better performance when the parameter counts do not increase? Although
potential enhanced reasoning capabilities were observed in [8], the source of the advantage remains unclear.
Specifically, do LoopLMs perform better due to the models’ increased knowledge capacity with the same size
of parameters? Or do they have a better capability in extracting and composing the knowledge encoded within
the parameters? Toward understanding the improvement of the phenomenon, we explore what capabilities are
exactly enhanced by simply looping more times. In this section, we perform experiments to test the model’s
abilities to memorize factual knowledge in its parameters, and the capabilities of manipulating and composing
existing knowledge encoded in the parameters based on a set of fully controllable synthetic tasks in [65-67].

6.1 LoopLMs does not increase knowledge capacity

We first explore the knowledge capacity, i.e. the model’s storage capacity of facts in the parameters. We aim
to answer the first question: do LoopLMs achieve better performance by memorizing knowledge when the
parameter count is not increased?

Settings. Following the Capo task setting in Physics of language models [65, 66], we construct synthetic
biographies to test how much information the model memorizes. Specifically, we generate several synthetic
biographic datasets bioS(V) with different number of people N, and train a series of language models to
memorize the information contained in the dataset. Each biography contains the individual’s name and five
attributes a1, a2,...,a5 of the person: gender, birth date, university, major, and employer. The names n
and the attributes a; are randomly selected from a pre-defined set MN and A; and combined together as a
biography using a random template. Based on the random generation process, we have an information-theoretic
lower bound for the model in the minimum bits required to encode all the names and attributes. To check
whether the models memorize the biographic information accurately, we look at the probability of predicting
the ground-truth attributes with the trained models given the biography context. Calculating the sum of
cross-entropy loss on each attribute token positions, we can estimate how much information (estimated in
bits) has already been memorized in the trained language model, which is our knowledge capacity metric.

With this metric, we can compare the knowledge capacity between the original model (with only one recurrent
step) and the looped model (with 4 recurrent steps) with the same parameter count to investigate whether
looping increases knowledge capacity. Moreover, as larger models should encode more information than smaller
models, we also aim to investigate whether looped models have a better scaling effect when the size of the
model grows. We thereby trained GPT-2 style models of different parameter numbers ranging from 1M to
40M (with depth and hidden dimension varied) and measured the number of bits of knowledge learned by
each model. We trained on bioS(V) with N ranging from 20K to 500K individuals for 1000 exposures. More
training details are provided in Section B.1.

Results. The results are visualized in the plot “bits vs. # of parameters”, where we can observe the comparison
between iso-parameter looped and non-looped models. Our results are shown in Figure 6 (Left): looping does
not increase knowledge capacity nor improve capacity scaling. Models with and without loops all attain
around a similar capacity ratio + 2 bits/parameter. Therefore, the number of parameters itself can be seen as
a direct indicator of knowledge capacity, and merely increasing looping does not help enhance knowledge
capacity itself.

20


===== PAGE BREAK =====

108               Scaling: Bits of Knowledge vs. Params                                                L=10    L=16    DL = 24

Legend                                  Baseline model
@ #20k                                                              we
Ot took                                 a                      Base (12 @ 1)     93.6       94.4      34.8
3     #5008                      4 ead  @      e                             2 layer model
ae                    ee                                      Base (2 @ 1)       21.5        8.4         7.5
& | --- 2bit/param         @  woe                                   Loop (2@6      98.1      96.3      78.0
% 107 | —— 1bit/ param                 ea
8                          eee                                3 layer model
S                                                                              Base (3 @ 1)       75.4       29.8       11.0
F                                                       @                                Loop (3 @4        97.9        95.8        92.2
r                                                                                          6 layer model
| @                                                                         Base (6 @ 1)       84.7       59.5       20.0
“le                                                                            Loop (6 @ 2        93.4       88.5       35.1

10°                                                   10’
Number of Parameters (P_params, log scale)

Figure 6 Left. We trained both LoopLM and a standard trasnformer baseline with the same parameters on Capo task
to compare the knowledge capacity gain by looping more times. With the same parameter count, the looped model and
its non-looped baseline has almost the same knowledge capacity measured in bits of knowledge on Capo task. Right.
Accuracy of looped/non-looped models on Mano task. Looped models are better than the iso-param ({2,3, 6} @ 1)
models. They also achieve better or comparable performance comparing to the iso-flop baseline (12 ® 1) model.

6.2 LoopLMs prevails in knowledge manipulation

We have already shown that reusing parameters cannot help the model memorize more atomic factual knowledge.
However, natural language is not only about single-hop factual knowledge. In most of the scenarios, predicting
the next token requires combining different piece of knowledge, which we called knowledge manipulation [65].
Does looping and reusing parameters help LoopLMs in tasks that require flexible usage of knowledge’? We
further consider two synthetic tasks to investigate the hypothesis on knowledge manipulation capacity: the
synthetic Mano task in [66] based on modular arithmetic, and a multi-hop QA task in natural language [67]
composing individual facts.

Mano Task. We first explore the knowledge manipulation task Mano in [66], based on a complex tree structure
with restricted modular arithmetic knowledge. Models need to solve the task without intermediate thinking
process. As illustration, an example could be <bos> + * a b c <eos> requires the model to directly output
(a xb) +c mod 23. To solve this task, the model needs to (1) apply the arithmetic rules modulo 23 as the
factual knowledge encoded in the parameters, and (2) parse the binary tree structure of the arithmetic to
compose all calculations.

To evaluate the manipulation capability thoroughly, we consider the test accuracy across different difficulty
levels based on maximum expression length L, which accounts for the number of operations in the sample. The
model is trained with online samples with all possible expression lengths ¢ € [1, L] and tested on the maximum
expression length L. We prepare three levels of difficulties L = [10, 16,24] to test LoopLM’s superiority over
non-looped models given fixed training budget. We train ({2,3,6,12} @ 1) standard transformers as the
baselines and several looped models (k ® 12/k) with k = 2,3,6. More details are included in Appendix B.2.

Results. The results in Figure 6 show that given the same parameters, looped models always outperform their
non-looped counterpart for all possible k € {2,3,6}. Even with the same number of FLOPs in the model, the
looped models can often perform better. This indicates that LoopLM has a better inductive bias towards
knowledge manipulation: with the same budget on training samples and computation, LoopLM can achieve
comparable or even better performance after training when the task requires manipulation capability (e.g.,
parsing the arithmetic tree) given limited amount of required knowledge (e.g., modular arithmetic rules).

Multi-hop QA. Next, we corroborate our conjecture with a natural language multi-hop reasoning task proposed
in [67], based on synthetic facts on relations R between || different individuals, like The instructor of A is
B and The teacher of B is C. The target is to answer multi-hop questions like ‘Who is the teacher of the
instructor of A?’. We aim to study whether looping enables the original transformer better learn to perform

21


===== PAGE BREAK =====

100} Run family (loops)
—=- Loop] (avg)
—t— Loop2 (avg)
—e Loop4 (avg)

Run family (loops)
—=- Loop] (avg)
—*— Loop2 (avg)
—e Loop4 (avg)

80

ES
fo)

60

Accuracy (%)
3

40

Accuracy (%)

N
is)

20

B
°

2  4  6  8 10 12 14 16 18 20     12 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
Unique training samples (104)                                                                         Training Steps (103)

Figure 7 We trained LoopLMs and standard transformer baselines with the same parameters on Multi-hop QA tasks.
To investigate the sample efficiency of LoopLMs, we vary the number of unique training samples (from 2.5%
to 25% all possible QA pairs) for models with different loops. We compare the final performance using the same
compute budget in total training tokens. Left. As shown, models with more loops requires fewer samples to learn
the 3-hop QA task. Right. As an example, we train with 15% of all possible QA pairs (12000 unique samples) for
20000 steps with context length 1024 and batch size 2048. Models with more loops learn faster and achieve better
performance comparing with models without loops.

internal multi-hop reasoning in a natural language setting. Compared to the Mano task, the task requires the
model to memorize more factual knowledge with layer-wise data structure, which is closer to practical natural
language multi-hop reasoning.

Multi-hop QA tasks require huge amount of samples to learn according to [67] when training standard
transformers. To study whether LoopLMs accelerate the learning process of this multi-hop knowledge
manipulation task, we consider sample efficiency in learning. Specifically, we study how many different QA
pairs are necessary for the trained model to achieve 100% accuracy, as well as the performance after training
on a fixed budget of unique training samples. For simplicity, we focus on the task with 3-hop QA pairs. We
separate all possible QA pairs into training subsets of different sizes, and compare when each model perfectly
generalizes on the leave-out test set. Similarly to the Mano task, we train a standard (6 @ 1) transformer as
the baseline, and compare it with looped models (6 ® {2,3, 4}) to study the effect of the universal transformer.
We also train an iso-flop model (24 ® 1) for comparison. More details are included in Appendix B.3.

Results. The results in Figure 7 show that looped models generally learn the multi-hop QA task with fewer
examples compared to both the non-looped iso-parameter model when the training budget is the same.
Moreover, LoopLMs learn the multi-hop task much faster than the non-looped model with the same number
of unique QA samples. The improved sample efficiency on the multi-hop reasoning task further demonstrates
that LoopLM has a better ability to learn to compose and manipulate atomic factual knowledge.

Based on the results in both Mano and multi-hop QAs, we can conclude that LoopLMs have a better inductive
bias towards more flexible manipulation of learned knowledge, instead of increasing the knowledge capacity.
The conclusion holds for both synthetic tasks regardless of whether the task is more reasoning-heavy (Mano)
or knowledge-heavy (Multi-hop QA). This also corresponds to the analysis (see Appendix B.4) on the existing
benchmarks (e.g. MMLU): adding more recurrent steps significantly improves the performance on more
reasoning-heavy categories, while the improvements on more knowledge-heavy tasks is limited.

6.3 Discussion: towards understanding why LoopLM helps knowledge manipulation

Why does LoopLM naturally bias towards better manipulation of the knowledge encoded in the parameter
space? We conjecture that the reason lies in the inherent recurrent structure of LoopLM. Given that the
knowledge capacity is limited by the parameter counts, looping enables LoopLM to better utilize the knowledge
encoded in the parameters. LoopLM can reuse the knowledge in each looped block, retrieve new necessary
factual information, or apply structured procedures to obtain the final prediction.

Search on the parametric knowledge graph. During pretraining, language models often obtain an enormous

22


===== PAGE BREAK =====

amount of factual knowledge and learn analysis procedures with a rather shallow thinking depth. To perform
more challenging tasks, the model needs to use multiple pieces of knowledge in the parameter space, which
requires the model to search in-depth in the knowledge graph with directional dependencies formed by the
atomic facts or knowledge. LoopLM naturally support an efficient reuse of the knowledge and algorithms
stored in the parameter spaces: even though the knowledge piece is not retrieved or used in the previous
calculations, the recurrent structure enables LoopLM to redo the procedure and extract necessary information.

Based on the abstraction above, we try to understand why LoopLMs are able to search on knowledge graph
without adding more parameters. Specifically, we study the expressivity of LoopLM on a synthetic task.
We consider the extensively studied search problem in the literature of latent reasoning [24, 68, 69]: graph
reachability on a knowledge graph. Here, we consider that only part of the knowledge graph Getx is included
in the context, and most of the knowledge relations G must be encoded in the parameters. The model must
learn to compose the context knowledge Gtx and the learned knowledge G. Compared to traditional CoT
and recent proposed latent CoT [24, 68], we show that LoopLM is a parallelizable latent reasoning paradigm
that requires fewer sequential reasoning steps.

Theorem 1 (Informal). Fix n as the maximum size of the combined knowledge graph G. Given the adjacency
matrix of the context graph Geix and a query pair (s,t), there exists a one-layer transformer independent of
Getz with loops O(log, D) times that checks whether there exists a path from s to t in the combined knowledge
graph (G+ Giz), where D is the diameter of (G+ Getz).

Latent reasoning method       | Discrete CoT Continuous CoT Universal Transformer

Sequential computation steps |      O(n?)               O(D)                   O(log D)

The proof and the discussion on LoopLM’s efficiency are deferred to Appendix B.5. We claim that the
universal transformers maximize the parallelism in exploring all-pair connectivity and reduce the sequential
computation steps exponentially from O(n?) to O(log D), making the latent reasoning much more efficient
than the traditional CoT view of looping [8] and continuous CoT [68]. The potential efficient latent reasoning
ability may account for the superiority of LoopLM in knowledge manipulation, which also may contribute to
the superior performance in reasoning-heavy tasks.

Recurrence improves sample efficiency. The expressiveness result of LoopLM does not explain why the
transformers with loops often learns knowledge manipulation tasks with samples much fewer than its iso- FLOP
counterpart. We conjecture that the reason lies again in the recurrent structure of LoopLM. Assuming the
reasoning tasks require multiple manipulation and recursion using learned parametric knowledge or algorithmic
procedure, the models have to learn a repeated structure across layers of different depth. For deep transformer
models without looping, they potentially have to explore a large function class where each block of parameters
are not tied. The parameter-sharing layers may help the model explore a much smaller realizable hypothesis
class, thus reducing the sample complexity of learning those manipulation tasks. It could be a possible
statistical reason that LoopLM enjoys a better sample complexity on those reasoning /manipulation tasks.

7 Safety, Faithfulness and Consistency

7.1 Safety

We assess model safety using HEx-PHI dataset [17], which contains 330 examples covering 11 prohibited
categories. HEx-PHI employs GPT-4o as a judge to assign each model response a harmfulness score from 1 to
5; a higher score indicates a less safe output. Additionally, we compute the harmfulness rate, defined as the
proportion of the test cases that receive the highest harmfulness score of 5. For Ouro Base models, we use
greedy decoding with max_new_tokens=128; For Ouro Thinking models, we sample with temperature=1.0,
top_ p=0.7 with max_new_tokens=8192. We evaluate Ouro 1.4B and 2.6B models with recurrent steps
ranging from 1 to 8, and report the result in Figure 8a. Notably, while our models were trained with only 4
recurrent steps, both models show their extrapolation capability by extending recurrence steps to 5-8 during
inference. This demonstrates the model’s ability to generalize to deeper computation than seen during training.
The Ouro Thinking checkpoints further enhance safety alignment, reducing harmful rates to 0.009 for Ouro

23


===== PAGE BREAK =====

—e Ouro 1.4B -e Ouro 1.4B Thinking -*- Ouro 2.6B —* Ouro 2.6B Thinking
0.64

°
IN

|

Harmfulness Score
N     LC)
Harmful Rate
(eo)
N

Recurrent Steps                                                    Recurrent Steps

(a) HEx-PHI evaluation

Recurrent Steps=1                   Recurrent Steps=2                   Recurrent Steps=3                   Recurrent Steps=4

20                                     20                                     20                                     20

Score
104                104                104                104                 5
N       Sou, ankae       nN        ee:        nw      RxKe  |  sat    n    wg My Je         4
eK x       2 *

-104               -104               -104               -104                 2
1

-20                -20                -20                -20

-10 -5 0 5. 10     -10 -5 0 5. 10     -10 -5 0 5. 10     -10 -5 0 5. 10
PCL                PCL                PCL                PCL

(b) PCA analysis on Ouro 1.4B

Figure 8 (a) For both 1.4B and 2.6B models, Ouro demonstrates improved safety alignment on HEx-PHI as the
recurrent steps increase. Note that models were trained with 4 recurrent steps; evaluations at steps 5-8 demonstrate
successful extrapolation beyond the training configuration. (b) As the recurrent steps increase, Ouro 1.4B can
better distinguish the benign prompts and harmful prompts, leading to safer responses. We perform PCA on
the hidden representation of the last input token from the model’s top layer. Harmful prompts with a harmfulness
score of 4 or 5 at recurrent step 1 are marked with x, while other harmful prompts are shown as circles. The color of
each point reflects the harmfulness score of the corresponding response. Benign prompts are shown as green squares.

1.4B Thinking and 0.003 for Ouro 2.6B Thinking at 4 recurrent steps, comparable to Qwen3-4B-Thinking
(0.009).

To further investigate how increasing recurrent steps affects the model’s safety alignment, we conduct Principal
Component Analysis (PCA) on the hidden representation of the last input token from the top model layer.
For a controlled analysis, we select 100 benign and 100 harmful questions with identical formats (all the
examples are the questions starting with “How to”) from Zheng et al.(2024) [70]*. Additionally, we evaluate
the model’s responses to the 100 harmful questions and compute a 5-level harmfulness score (same as the one
used in HEx-PHI) for each response. We plot our PCA analysis on Ouro 1.4B in Figure 8b, from which we
have the following observations. First, as the number of recurrent steps increases, the model becomes more
capable of separating benign and harmful prompts, resulting in safer responses, as indicated by the decreasing
number of red points. Furthermore, most points associated with unsafe responses appear near the middle
of the plot, which represents the boundary between the “benign” and “harmful” clusters. This suggests that
difficulty in distinguishing harmfulness may lead to unsafe responses, which can be alleviated by increasing
the number of recurrent steps.

7.2 Faithfulness

We call a model’s thinking process faithful if it is (i) procedurally correct and (ii) causally coupled to the final
answer. Concretely, a faithful process should satisfy a counterfactual criterion: if the justification is intervened
on (e.g., altered to a different intermediate state), the final prediction should change accordingly. A growing

4Harmful questions: https://github.com/chujiezheng/LLM-Safeguard/blob/main/code/data/custom.txt; Benign ques-
tions: https://github.com/chujiezheng/LLM-Safeguard/blob/main/code/data_harmless/custom.txt

24


===== PAGE BREAK =====

- 1000
1.0
R2-           Sel     361     305     3353)     394     326

- 900
R3- 551                  788       726       716       745       705

0.9
- 800

R4- 361       788

Qwen3-4B-Instruct :

Qwen3-4BThinking :       4

Ouro 1.4B (R2)      E       SR5- 305 | 726

Ouro 1.4B (R3)       :        2

Ouro 1.4B (R4)       :                                                                                  y 600
:           :      R6- 333   716

- 700

ROC AUC
°
(o)
Count

0.7
- 500

R7- 394     745

06                                                                                                                                              - 400
‘                                                     R8- 326   705

20                  40                  60                  80                                      rag         re)         rs         ©         ©         }          &
Layer Index                                                                     Rounds

Figure 9 Left. ROCAUC of linear probes by layer on Quora Question Pairs. Each colored curve shows a probe
trained on hidden states within a given 2 to 8 recurrent steps to predict that loop’s answer; Qwen3-4B models are the
baselines. Vertical dotted lines mark loop boundaries. In recurrent step 7 = 2,3, 4, the ROC AUC rises quickly within
a recurrent step, then partially resets at the next loop, indicating that intra-step answers are determined early while
cross-step updates modify the provisional answer. Right. Agreement across recurrent steps. Heat map (A) over
1,000 Quora Question Pairs. Entry Ali, 7] is the number of items for which steps (i) and (j) assign the same label.

body of work [71-74] shows that standard LLMs often appear to decide on an answer before generating
chain-of-thought text and then use that text to rationalize the already-formed decision.

In LoopLM, the reasoning substrate is the sequence of latent states H@) > H@) >...>H), Each transition
H“*) + H+ performs non-trivial computation using the same shared-weight block, and each step is trained
to improve the task objective. Thus, the causal path to the answer is this latent trajectory, not any optional
natural-language trace. When we decode intermediate text Text(R;,) from H“) via the LM head, we treat it
as an instrumented readout of the internal state rather than the mechanism itself. Because H“) is directly
supervised by the LM loss, its projection into token space provides a faithful snapshot of what the model
currently represents.

Standard evaluation of faithfulness is often based on the manipulation of the reasoning process, CoT, and
check if the average treatment effect of CoT is significant. In our case, we cannot manipulate the latent
reasoning process. Instead, we adopt an observational proxy for mediation: we read out intermediate hidden
representations and test whether predictions change as recurrence deepens on inputs that admit multiple
plausible labels. Concretely, we assess whether intermediate “thinking” genuinely mediates decisions by
measuring step-by-step predictability and agreement patterns. We use the Quora Question Pairs dataset [75],
which asks whether two short questions are semantically equivalent: a setting with ambiguity and weakly-
defined decision boundaries. There are a lot of ambiguous questions in this dataset:

Example: Ambiguous questions in Quora dataset

Question: does the following two questions have the same intent?

Pair 1:                                                                  Pair 2:

1. What are the questions should not ask on Quora?        1. How do we prepare for Union Public Service Commission?
2. Which question should I ask on Quora?                   2. How do I prepare for civil service?

Answer: False                                             Answer: True

If a thinking process merely rationalizes the pre-committed answer, even if the questions are very ambiguous,
the answers will not change after the reasoning process. This has been reported for Gemma-2 9B and
reproduced by us on Qwen-3-4B-Thinking. As shown in the left part of Figure 9, the simple linear probe on
the final-token logits on the Qwen3-4B-Thinking model shows 0.99 ROC AUC predicting the model’s eventual

25


===== PAGE BREAK =====

answer, which means the thinking process almost does not affect the results.

In our model, the situation is very different. our 1.4B x4 model uses 24 layers per recurrent step. We train
linear probes on hidden states from layers 1 through 242 to predict the step-i answer, for i€ {2,3,4}. Within
a single recurrent step, the step-i answer is well predicted by a probe on representation within layer 247,
indicating strong intra-step alignment between state and decision, which is similar to the non-reasoning model
Qwen-4B-Instruct, showing in left part of Figure 9. Crucially, probes on the preceding representation (layer
24(i—1)) do not reliably predict the step-i decision for i € {2,3,4}, showing that the new recurrent pass
performs additional computation that can revise a provisional choice.

To further examine the consistency between the results of different rounds. We also compute a step-by-step
agreement matrix A over 1,000 Quora Question Pairs, where A[2, 7] counts identical labels between step i
and step j (diagonal = 1000 by construction). See the right side of Figure 9. Adjacent steps never reach
full agreement; for example, A[3, 4] =361 indicates only 36.1% of step-3 answers match step-4. A[2,3]=551
indicates only 44.9% of step-2 answers match step-3. We also notice that when i > 4, the overlap consistency
between step-i and step-i + 1, A[i,i+ 1], is close to 1000. We think this phenomenon comes from: (1) the
model does not learn to reason recursively when i > 4. The model is trained within 4 loops; (2) as the number
of loops increases, the answer gradually converges to a fixed point.

All in all, this systematic disagreement across steps when i < 4 is precisely what a faithful latent process
should exhibit: the model is updating its decision as recurrence deepens, and intermediate predictions are not
frozen rationalizations of the final output.

7.3 More Discussion

The practical barrier for safety-critical deployment is that a model’s articulated reasoning and its final answer
may diverge. The LoopLM architecture reduces this gap by exposing a sequence of intermediate predictors
that are strongly aligned with the final predictor and can be used both for acceleration and for pre-emptive
control. We summarize three deployment advantages.

Built-in draft model for speculative decoding. Let Text(R,) denote the language-model head attached
to the latent state after recurrent step t, and let T be the maximum step used at deployment. The pair

Text(Rs), Text  ), l<s<T.
(Text(Rs) Tex (Rr)        s<

proposal       verifier

forms a native proposal—verification decomposition for speculative decoding without training an external draft
model. Proposals are sampled from Text(R,) and verified under Text(Rr) using standard acceptance tests;
rejected tokens are rolled back as usual. Because both heads share the same parameters up to step s, cached
activations and KV states can be reused, reducing verifier overhead. This turns the recurrent structure into
an architectural primitive for draft—verify decoding rather than an add-on.

Joint acceleration and pre-emptive safety. Using the same proposal-—verification split, safety checks
can be interleaved with speculative decoding without extra models. At step s:

1. Generate draft tokens with Text(R;) and compute their acceptance under Text(Rr).

2. Run safety screening on the draft distribution or sampled drafts before any token is surfaced to the user.
Screening can operate on logits, beams, or short candidate spans.

3. If a violation is detected, halt or reroute the response before streaming; otherwise, accept tokens that pass
both verification and safety checks.

Because Text(R,) and Text(Rr) share the latent trajectory, intermediate predictions are well-aligned with
the final answer distribution. This alignment makes the step-s output a reliable proxy for the step-T output
for the purpose of early screening, while the verifier maintains final quality. The Q-exit threshold q further
provides a single deployment knob that simultaneously adjusts compute, consistency, and safety strictness by
shifting the average exit depth.

26


===== PAGE BREAK =====

Anytime generation with monotone refinement. The training objective in Section 3.4 optimizes the
expected task loss across steps while preserving the deeper-is-better property. Consequently, for next-token
prediction loss,

LY] < EL], 1<t<T,

so each additional loop refines the distribution toward higher-quality predictions. This yields an anytime
algorithm: decoding may begin from any intermediate step s and continue streaming while later steps continue
to verify or revise. Unlike chain-of-thought pipelines, which often require completing a reasoning prefix before
emitting answers, LoopLM exposes a single predictive interface at every step, enabling immediate fallback to
a smaller compute budget when latency constraints apply.

8 Conclusion

In this work, we introduced Ouro, a family of Looped Language Models that demonstrate exceptional
parameter efficiency by integrating iterative computation and adaptive depth directly into pre-training on
7.7T tokens. Our 1.4B and 2.6B models consistently match or exceed the performance of 4B and 8B standard
transformers, showcasing a 2-3x efficiency gain. We demonstrated this advantage stems not from increased
knowledge storage, but from a fundamentally superior capability for knowledge manipulation, supported
by synthetic experiments and theoretical analysis. We also presented a practical training objective using
entropy regularization with a uniform prior to learn adaptive depth, and validated efficient KV cache sharing
strategies that make LoopLMs viable for real-world deployment.

Beyond performance, the LoopLM architecture exhibits unique properties: its iterative refinement process
provides a causally faithful reasoning trace, mitigating the post-hoc rationalization issues seen in standard
CoT, and its safety alignment uniquely improves with increased recurrent steps, even when extrapolating.
This work establishes iterative latent computation as a critical third scaling axis beyond parameters and data.
Future research should focus on enhancing performance extrapolation at greater depths and exploring more
complex recurrent mechanisms, solidifying this parameter-efficient approach as a necessary direction in a
data-constrained era.

Acknowledgement

We sincerely thank Zeyuan Allen-Zhu for his in-depth discussion on the physics of language model part and
his enlightening insights on knowledge manipulation. We also thank Yonghui Wu, Guang Shi, Shu Zhong,
Tenglong Ao, Chen Chen, Songlin Yang, Wenhao Chai, and Yuhong Chou for their insightful discussions.
Special thanks to Wenjia Zhu — his words opened our eyes to what the real problems are in current models,
and inspired us to explore this direction.

27


===== PAGE BREAK =====

Contributions

Project Lead
Rui-Jie Zhu, Zixuan Wang, Kai Hua, Ge Zhang
Core Contributors

Rui-Jie Zhu: Proposes the project and leads the pre-training of Ouro. Optimizes pre-training and inference
infrastructure, develops the initial VLLM implementation, and explores RLVR.

Zixuan Wang: Leads the analysis on understanding LoopLM superiority and is responsible for related
experiments. He contributes to the design of adaptive early exit strategies, training, and the safety analysis.

Kai Hua: Designs and curates all pre-training data mixtures and provides key insights during the pre-training
process.

Ge Zhang: Co-leads and supervises the Ouro. Provides several key insights during the pre-training and
post-training process.

Tianyu Zhang: Leads the analysis of Ouro on consistency, safety, and faithfulness. He designs the pipeline
evaluation on faithfulness. He contributes to post-training, probing and efficient KV cache design.

Ziniu Li: Leads the post-training phase, developing supervised fine-tuning and providing key contributions to
RLVR exploration.

Haoran Que: Leads the scaling law analysis for LoopLM, investigating the relationship between performance,
model size, and recurrent depth.

Boyi Wei: Contributes to the safety analysis, conducting evaluations on the HEx-PHI benchmark and
performing PCA on model representations.

Zixin Wen: Contributes to the theoretical analysis, Physics of LLMs experiments, paper writing, and RLVR.

Fan Yin: Optimizes the vLLM and SGLang implementations for Ouro, contributing core pull requests to
improve inference efficiency.

He Xing: Contributes to the vLLM infrastructure development and optimization.
Contributors

Lu Li, Jiajun Shi, Kaijing Ma, Shanda Li, Taylor Kergan, Andrew Smith, Xingwei Qu, Mude Hui, Bohong
Wu, Xun Zhou, Qiyang Min, Hongzhi Huang, Wei Ye, Jiaheng Liu, Jian Yang, Yunfeng Shi, Chenghua Lin,
Enduo Zhao, Tianle Cai

Supervision
Ge Zhang, Wenhao Huang, Yoshua Bengio, Jason Eshraghian
References
[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind

Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
Advances in neural information processing systems, 33:1877-1901, 2020.

[2] Qwen Team et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2:3, 2024.

[3] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen
Huang, Chenxu Ly, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025.

[4] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin,
Tatiana Matejovicova, Alexandre Ramé, Morgane Riviére, et al. Gemma 3 technical report. arXiv preprint
arXiv:2503.19786, 2025.

28


===== PAGE BREAK =====

[5]

[6]

[7|

[8]

[9]

[10]

[11]

[12]

[13]

[14]

[15]

[16]

[17|

[18]

[19]

[20]

[21]

[22|

[23]

[24]

Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,
Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pages
arXiv—2407, 2024.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.
Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing
systems, 35:24824—24837, 2022.

Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal transformers.
arXiv preprint arXiv:1807.03819, 2018.

Nikunj Saunshi, Nishanth Dikkala, Zhiyuan Li, Sanjiv Kumar, and Sashank J Reddi. Reasoning with latent
thoughts: On the power of looped transformers. arXiv preprint arXiv:2502.17416, 2025.

Khashayar Gatmiry, Nikunj Saunshi, Sashank J Reddi, Stefanie Jegelka, and Sanjiv Kumar. Can looped trans-
formers learn to implement multi-step gradient descent for in-context learning? arXiv preprint arXiv:2410.08292,
2024.

Khashayar Gatmiry, Nikunj Saunshi, Sashank J Reddi, Stefanie Jegelka, and Sanjiv Kumar. On the role of depth
and looping for in-context learning with task diversity. arXiv preprint arXiv:2410.21698, 2024.

Jianhao Huang, Zixuan Wang, and Jason D Lee. Transformers learn to implement multi-step gradient descent
with chain of thought. arXiv preprint arXiv:2502.21212, 2025.

William Merrill and Ashish Sabharwal. A little depth goes a long way: The expressive power of log-depth
transformers. arXiv preprint arXiv:2503.03961, 2025.

William Merrill and Ashish Sabharwal. Exact expressive power of transformers with padding. arXiv preprint
arXiv:2505.18948, 2025.

Sangmin Bae, Adam Fisch, Hrayr Harutyunyan, Ziwei Ji, Seungyeon Kim, and Tal Schuster. Relaxed recursive
transformers: Effective parameter sharing with layer-wise lora. arXiv preprint arXiv:2410.20672, 2024.

Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R Bartoldson, Bhavya
Kailkhura, Abhinav Bhatele, and Tom Goldstein. Scaling up test-time compute with latent reasoning: A recurrent
depth approach. arXiv preprint arXiv:2502.05171, 2025.

Boyi Zeng, Shixiang Song, Siyuan Huang, Yixuan Wang, He Li, Ziwei He, Xinbing Wang, Zhiyu Li, and Zhouhan
Lin. Pretraining language models to ponder in continuous space. arXiv preprint arXiv:2505.20674, 2025.

Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. Fine-tuning
aligned language models compromises safety, even when users do not intend to! In The Twelfth International
Conference on Learning Representations.

Yilong Chen, Junyuan Shang, Zhenyu Zhang, Yanxi Xie, Jiawei Sheng, Tingwen Liu, Shuohuan Wang, Yu Sun,
Hua Wu, and Haifeng Wang. Inner thinking transformer: Leveraging dynamic depth scaling to foster adaptive
internal thinking. arXiv preprint arXiv:2502.13842, 2025.

Sangmin Bae, Yujin Kim, Reza Bayat, Sungnyun Kim, Jiyoun Ha, Tal Schuster, Adam Fisch, Hrayr Harutyunyan,
Ziwei Ji, Aaron Courville, et al. Mixture-of-recursions: Learning dynamic recursive depths for adaptive token-level
computation. arXiv preprint arXiv:2507.10524, 2025.

Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A
lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019.

Raj Dabre and Atsushi Fujita. Recurrent stacking of layers for compact neural machine translation models. In
Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 6292-6299, 2019.

Sho Takase and Shun Kiyono. Lessons on parameter sharing across layers in transformers. arXiv preprint
arXiv:2104.06022, 2021.

Boxun Li, Yadong Li, Zhiyuan Li, Congyi Liu, Weilin Liu, Guowei Niu, Zheyue Tan, Haiyang Xu, Zhuyu Yao,
Tao Yuan, et al. Megrez2 technical report. arXiv preprint arXiv:2507.17728, 2025.

Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training
large language models to reason in a continuous latent space. arXiv preprint arXiv:2412.06769, 2024.

29


===== PAGE BREAK =====

[25]

[26]

[27|

[28]

[29]

[30]

[31]

[32|

[33]
[34]

[35]

[36]

[37|

[38]

[39]

[40]

[41]

[42|

[43]

[44]

Amirkeivan Mohtashami, Matteo Pagliardini, and Martin Jaggi. Cotformer: More tokens with attention make up
for less depth. In Workshop on Advancing Neural Network Training: Computational Efficiency, Scalability, and
Resource Optimization (WANT@ NeurIPS 2023), 2023.

Bohong Wu, Shen Yan, Sijun Zhang, Jianqiao Lu, Yutao Zeng, Ya Wang, and Xun Zhou. Efficient pretraining
length scaling. arXiv preprint arXiv:2504.14992, 2025.

Tian Ye, Zicheng Xu, Yuanzhi Li, and Zeyuan Allen-Zhu. Physics of language models: Part 2.1, grade-school
math and the hidden reasoning process. arXiv preprint arXiv:2407.20311, 2024.

Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang,
Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning
for Ilm reasoning. arXiv preprint arXiv:2506.01939, 2025.

Nikita Balagansky and Daniil Gavrilov. Palbert: Teaching albert to ponder. Advances in Neural Information
Processing Systems, 35:14002-14012, 2022.

Andrea Banino, Jan Balaguer, and Charles Blundell. Pondernet: Learning to ponder. arXiv preprint
arXiv:2107.05407, 2021.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and
Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.

Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer
with rotary position embedding, 2023.

Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.

Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martin Blazquez, Guilherme Penedo, Lewis Tunstall,
Andrés Marafioti, Hynek Kydlitek, Agustin Piqueres Lajarin, Vaibhav Srivastav, et al. Smollm2: When smol goes
big—data-centric training of a small language model. arXiv preprint arXiv:2502.02737, 2025.

Kaiyue Wen, Zhiyuan Li, Jason Wang, David Hall, Percy Liang, and Tengyu Ma. Understanding warmup-stable-
decay learning rates: A river valley loss landscape perspective. arXiv preprint arXiv:2410.05192, 2024.

Guilherme Penedo, Hynek Kydliéek, Anton Lozhkov, Margaret Mitchell, Colin A Raffel, Leandro Von Werra,
Thomas Wolf, et al. The fineweb datasets: Decanting the web for the finest text data at scale. Advances in Neural
Information Processing Systems, 37:30811—30849, 2024.

Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Yitzhak Gadre, Hritik Bansal, Etash
Guha, Sedrick Scott Keh, Kushal Arora, et al. Datacomp-Im: In search of the next generation of training sets for
language models. Advances in Neural Information Processing Systems, 37:14200-14282, 2024.

Dan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl, Mostofa Patwary, Mohammad
Shoeybi, and Bryan Catanzaro. Nemotron-cc: Transforming common craw] into a refined long-horizon pretraining
dataset. arXiv preprint arXiv:2412.02595, 2024.

Yudong Wang, Zixuan Fu, Jie Cai, Peijun Tang, Hongya Lyu, Yewei Fang, Zhi Zheng, Jie Zhou, Guoyang Zeng,
Chaojun Xiao, et al. Ultra-fineweb: Efficient data filtering and verification for high-quality llm training data.
arXiv preprint arXiv:2505.05427, 2025.

Xinrun Du, Zhouliang Yu, Songyang Gao, Ding Pan, Yuyang Cheng, Ziyang Ma, Ruibin Yuan, Xingwei Qu,
Jiaheng Liu, Tianyu Zheng, et al. Chinese tiny llm: Pretraining a chinese-centric large language model. arXiv
preprint arXiv:2404.04167, 2024.

Siming Huang, Tianhao Cheng, Jason Klein Liu, Jiaran Hao, Liuyihan Song, Yang Xu, J. Yang, J. H. Liu,
Chenchen Zhang, Linzheng Chai, Ruifeng Yuan, Zhaoxiang Zhang, Jie Fu, Qian Liu, Ge Zhang, Zili Wang, Yuan
Qi, Yinghui Xu, and Wei Chu. Opencoder: The open cookbook for top-tier code large language models. 2024.

Fan Zhou, Zengzhi Wang, Nikhil Ranjan, Zhoujun Cheng, Liping Tang, Guowei He, Zhengzhong Liu, and Eric P.
Xing. Megamath: Pushing the limits of open math corpora. arXiv preprint arXiv:2504.02807, 2025. Preprint.

Rabeeh Karimi Mahabadi, Sanjeev Satheesh, Shrimai Prabhumoye, Mostofa Patwary, Mohammad Shoeybi, and
Bryan Catanzaro. Nemotron-cc-math: A 133 billion-token-scale high quality math pretraining dataset. 2025.

NVIDIA, Aarti Basant, Abhijit Khairnar, Abhijit Paithankar, Abhinav Khattar, Adithya Renduchintala, Aditya
Malte, Akhiad Bercovich, Akshay Hazare, Alejandra Rico, Aleksander Ficek, Alex Kondratenko, Alex Shaposhnikov,

30


===== PAGE BREAK =====

[45]

[46]
[47|

[48]

[49]

[50]

[51]

[52|

Alexander Bukharin, Ali Taghibakhshi, Amelia Barton, Ameya Sunil Mahabaleshwarkar, Amy Shen, Andrew Tao,
Ann Guan, Anna Shors, Anubhav Mandarwal, Arham Mehta, Arun Venkatesan, Ashton Sharabiani, Ashwath
Aithal, Ashwin Poojary, Ayush Dattagupta, Balaram Buddharaju, Banghua Zhu, Barnaby Simkin, Bilal Kartal,
Bita Darvish Rouhani, Bobby Chen, Boris Ginsburg, Brandon Norick, Brian Yu, Bryan Catanzaro, Charles Wang,
Charlie Truong, Chetan Mungekar, Chintan Patel, Chris Alexiuk, Christian Munley, Christopher Parisien, Dan
Su, Daniel Afrimi, Daniel Korzekwa, Daniel Rohrer, Daria Gitman, David Mosallanezhad, Deepak Narayanan,
Dima Rekesh, Dina Yared, Dmytro Pykhtar, Dong Ahn, Duncan Riach, Eileen Long, Elliott Ning, Eric Chung,
Erick Galinkin, Evelina Bakhturina, Gargi Prasad, Gerald Shen, Haifeng Qian, Haim Elisha, Harsh Sharma,
Hayley Ross, Helen Ngo, Herman Sahota, Hexin Wang, Hoo Chang Shin, Hua Huang, Iain Cunningham, Igor
Gitman, Ivan Moshkov, Jaehun Jung, Jan Kautz, Jane Polak Scowcroft, Jared Casper, Jian Zhang, Jiaqi Zeng,
Jimmy Zhang, Jinze Xue, Jocelyn Huang, Joey Conway, John Kamalu, Jonathan Cohen, Joseph Jennings,
Julien Veron Vialard, Junkeun Yi, Jupinder Parmar, Kari Briski, Katherine Cheung, Katherine Luna, Keith Wyss,
Keshav Santhanam, Kezhi Kong, Krzysztof Pawelec, Kumar Anik, Kunlun Li, Kushan Ahmadian, Lawrence
McAfee, Laya Sleiman, Leon Derczynski, Luis Vega, Maer Rodrigues de Melo, Makesh Narsimhan Sreedhar,
Marcin Chochowski, Mark Cai, Markus Kliegl, Marta Stepniewska-Dziubinska, Matvei Novikov, Mehrzad Samadi,
Meredith Price, Meriem Boubdir, Michael Boone, Michael Evans, Michal Bien, Michal Zawalski, Miguel Martinez,
Mike Chrzanowski, Mohammad Shoeybi, Mostofa Patwary, Namit Dhameja, Nave Assaf, Negar Habibi, Nidhi
Bhatia, Nikki Pope, Nima Tajbakhsh, Nirmal Kumar Juluru, Oleg Rybakov, Oleksii Hrinchuk, Oleksii Kuchaiev,
Oluwatobi Olabiyi, Pablo Ribalta, Padmavathy Subramanian, Parth Chadha, Pavlo Molchanov, Peter Dykas,
Peter Jin, Piotr Bialecki, Piotr Januszewski, Pradeep Thalasta, Prashant Gaikwad, Prasoon Varshney, Pritam
Gundecha, Przemek Tredak, Rabeeh Karimi Mahabadi, Rajen Patel, Ran El-Yaniv, Ranjit Rajan, Ria Cheruvu,
Rima Shahbazyan, Ritika Borkar, Ritu Gala, Roger Waleffe, Ruoxi Zhang, Russell J. Hewett, Ryan Prenger,
Sahil Jain, Samuel Kriman, Sanjeev Satheesh, Saori Kaji, Sarah Yurick, Saurav Muralidharan, Sean Narenthiran,
Seonmyeong Bak, Sepehr Sameni, Seungju Han, Shanmugam Ramasamy, Shaona Ghosh, Sharath Turuvekere
Sreenivas, Shelby Thomas, Shizhe Diao, Shreya Gopal, Shrimai Prabhumoye, Shubham Toshniwal, Shuoyang Ding,
Siddharth Singh, Siddhartha Jain, Somshubra Majumdar, Soumye Singhal, Stefania Alborghetti, Syeda Nahida
Akter, Terry Kong, Tim Moon, Tomasz Hliwiak, Tomer Asida, Tony Wang, Tugrul Konuk, Twinkle Vashishth,
Tyler Poon, Udi Karpas, Vahid Noroozi, Venkat Srinivasan, Vijay Korthikanti, Vikram Fugro, Vineeth Kalluru,
Vitaly Kurin, Vitaly Lavrukhin, Wasi Uddin Ahmad, Wei Du, Wonmin Byeon, Ximing Lu, Xin Dong, Yashaswi
Karnati, Yejin Choi, Yian Zhang, Ying Lin, Yonggan Fu, Yoshi Suhara, Zhen Dong, Zhiyu Li, Zhongbo Zhu, and
Zijia Chen. Nvidia nemotron nano 2: An accurate and efficient hybrid mamba-transformer reasoning model, 2025.

Tianyu Gao, Alexander Wettig, Howard Yen, and Danqi Chen. How to train long-context language models
(effectively). arXiv preprint arXiv:2410.02660, 2024.

Yu Zhang and Songlin Yang. Flame: Flash language modeling made easy, January 2025.

Wanchao Liang, Tianyu Liu, Less Wright, Will Constable, Andrew Gu, Chien-Chin Huang, Iris Zhang, Wei Feng,
Howard Huang, Junjie Wang, Sanket Purandare, Gokul Nadathur, and Stratos Idreos. Torchtitan: One-stop
pytorch native solution for production ready LLM pretraining. In The Thirteenth International Conference on
Learning Representations, 2025.

Etash Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna Nezhurina,
Jean Mercat, Trung Vu, Zayne Sprague, et al. Openthoughts: Data recipes for reasoning models. arXiv preprint
arXiv:2506.04178, 2025.

Zihan Liu, Zhuolin Yang, Yang Chen, Chankyu Lee, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping.
Acereason-nemotron 1.1: Advancing math and code reasoning through sft and rl synergy. arXiv preprint
arXiv:2506.13284, 2025.

Wasi Uddin Ahmad, Sean Narenthiran, Somshubra Majumdar, Aleksander Ficek, Siddhartha Jain, Jocelyn Huang,
Vahid Noroozi, and Boris Ginsburg. Opencodereasoning: Advancing data distillation for competitive coding.
arXiv preprint arXiv:2504.01943, 2025.

Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach
Moshe, Tomer Ronen, Najeeb Nabwani, et al. Llama-nemotron: Efficient reasoning models. arXiv preprint
arXiv:2505.00949, 2025.

Haozhe Wang, Haoran Que, Qixin Xu, Minghao Liu, Wangchunshu Zhou, Jiazhan Feng, Wanjun Zhong, Wei
Ye, Tong Yang, Wenhao Huang, et al. Reverse-engineered reasoning for open-ended generation. arXiv preprint
arXiv:2509.06160, 2025.

31


===== PAGE BREAK =====

[53]

[54]

[55]

[56]

[57|

[58]

[59]

[60]

[61]

[62|

[63]

[64]

[65]

[66]

[67|

[68]

[69]

[70]

[71]

Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma.
Llamafactory: Unified efficient fine-tuning of 100+ language models. arXiv preprint arXiv:2403.13372, 2024.

Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong
Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint
arXiv:2503.14476, 2025.

Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang,
YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models.
arXiv preprint arXiv:2402.03300, 2024.

Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence
Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason
Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang,
Kevin Wang, and Andy Zou. The language model evaluation harness, 07 2024.

Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatGPT really
correct? rigorous evaluation of large language models for code generation. In Thirty-seventh Conference on Neural
Information Processing Systems, 2023.

HuggingFaceH4. Aime 2024. https: //huggingface.co/datasets/HuggingFaceH4/aime_2024, 2024. 30 problems
from AIME I & II 2024.

Chaoqun He, Renjie Luo, Yuzhuo Bai, et al. Olympiadbench: A challenging benchmark for promoting agi with
olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024.

David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian
Michael, and Samuel R. Bowman. GPQA: A graduate-level google-proof q&a benchmark. arXiv preprint
arXiv:2311.12022, 2023.

M-A-P Team, Xinrun Du, Yifan Yao, et al. Supergpqa: Scaling Ilm evaluation across 285 graduate disciplines.
arXiv preprint arXiv:2502.14739, 2025.

ByteDance-Seed. Beyondaime. https: //huggingface.co/datasets/ByteDance-Seed/BeyondAIME, 2025. CC0-1.0
license.

Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, et al. Humanity’s last exam. arXiv preprint arXiv:2501.14249,
2025.

Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: A question answering
challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
Short Papers), pages 4149-4158, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.

Zeyuan Allen-Zhu and Yuanzhi Li. Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws.
In Proceedings of the 13th International Conference on Learning Representations, ICLR ’25, April 2025. Full
version available at https://ssrn.com/abstract=5250617.

Zeyuan Allen-Zhu. Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers.
SSRN Electronic Journal, May 2025. https://ssrn.com/abstract=5240330.

Yuekun Yao, Yupei Du, Dawei Zhu, Michael Hahn, and Alexander Koller. Language models can learn implicit
multi-hop reasoning, but only if they have lots of training data. arXiv preprint arXiv:2505.17923, 2025.

Hanlin Zhu, Shibo Hao, Zhiting Hu, Jiantao Jiao, Stuart Russell, and Yuandong Tian. Reasoning by superposition:
A theoretical perspective on chain of continuous thought. arXiv preprint arXiv:2505.12514, 2025.

Shu Zhong, Mingyu Xu, Tenglong Ao, and Guang Shi. Understanding transformer from the perspective of
associative memory. arXiv preprint arXiv:2505.19488, 2025.

Chujie Zheng, Fan Yin, Hao Zhou, Fandong Meng, Jie Zhou, Kai-Wei Chang, Minlie Huang, and Nanyun Peng.
On prompt-driven safeguarding for large language models. In Proceedings of the 41st International Conference
on Machine Learning, pages 61593-61613, 2024.

Kyle Cox. Post-hoc reasoning in chain of thought, December 2024. Blog post.

32


===== PAGE BREAK =====

[72|

[73]

[74]

[75]

[76

[77|

[78]

[79]

[80]

[81]

[82]

[83]

[84]

[85]

[86]

[87|

[88]

Ivan Arcuschin, Jett Janiak, Robert Krzyzanowski, Senthooran Rajamanoharan, Neel Nanda, and Arthur Conmy.
Chain-of-thought reasoning in the wild is not always faithful. arXiv preprint arXiv: 2503.08679, 2025.

Fazl Barez, Tung-Yu Wu, Ivan Arcuschin, Michael Lan, Vincent Wang, Noah Siegel, Nicolas Collignon, Clement
Neo, Isabelle Lee, Alasdair Paren, Adel Bibi, Robert Trager, Damiano Fornasiere, John Yan, Yanai Elazar, and
Yoshua Bengio. Chain-of-thought is not explainability. 2025.

Tomek Korbak, Mikita Balesni, Elizabeth Barnes, Yoshua Bengio, Joe Benton, Joseph Bloom, Mark Chen, Alan
Cooney, Allan Dafoe, Anca Dragan, Scott Emmons, Owain Evans, David Farhi, Ryan Greenblatt, Dan Hendrycks,
Marius Hobbhahn, Evan Hubinger, Geoffrey Irving, Erik Jenner, Daniel Kokotajlo, Victoria Krakovna, Shane
Legg, David Lindner, David Luan, Aleksander Madry, Julian Michael, Neel Nanda, Dave Orr, Jakub Pachocki,
Ethan Perez, Mary Phuong, Fabien Roger, Joshua Saxe, Buck Shlegeris, Martin Soto, Eric Steinberger, Jasmine
Wang, Wojciech Zaremba, Bowen Baker, Rohin Shah, and Vlad Mikulik. Chain of thought monitorability: A new
and fragile opportunity for ai safety. arXiv preprint arXiv: 2507.11473, 2025.

Quora. Quora question pairs. https: //www.kaggle.com/competitions/quora-question-pairs/, 2017. Kaggle
competition.

Clayton Sanford, Bahare Fatemi, Ethan Hall, Anton Tsitsulin, Mehran Kazemi, Jonathan Halcrow, Bryan Perozzi,
and Vahab Mirrokni. Understanding transformer reasoning capabilities via graph algorithms. In A. Globerson,
L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information
Processing Systems, volume 37, pages 78320-78370. Curran Associates, Inc., 2024.

Clayton Sanford, Daniel Hsu, and Matus Telgarsky. Transformers, parallel computation, and logarithmic depth.
arXiv preprint arXiv:2402.09268, 2024.

Bingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Transformers learn shortcuts
to automata. arXiv preprint arXiv:2210.10749, 2022.

Zixuan Wang, Eshaan Nichani, Alberto Bietti, Alex Damian, Daniel Hsu, Jason D Lee, and Denny Wu. Learning
compositional functions with transformers from easy-to-hard data. arXiv preprint arXiv:2505.23683, 2025.

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
Measuring massive multitask language understanding. In International Conference on Learning Representations
(ICLR), 2021.

Yizhong Wang, Yada Pruksachatkun, Sheng Chen, Zexuan Zhong, Pengfei Chen, et al. MMLU-Pro: A more
challenging and reliable evaluation for massive multitask language understanding. arXiv preprint arXiv:2406.01574,
2024.

Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha
Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Challenging big-bench tasks and whether
chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.

Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457,
2018.

Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine really finish
your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,
pages 4791-4800, Florence, Italy, 2019. Association for Computational Linguistics.

Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd
schema challenge at scale. arXiv preprint arXiv:1907.10641, 2019.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert,
Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve
math word problems. arXiv preprint arXiv:2110.14168, 2021.

Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob
Steinhardt. Measuring mathematical problem solving with the MATH dataset. In NeurIPS 2021 Datasets and
Benchmarks Track, 2021.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri
Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael
Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,

33


===== PAGE BREAK =====

[89]

[90]

[91]

[92]

[93]

[94]

95]

Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such,
Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen
Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William
Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec
Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario
Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on
code. arXiv preprint arXiv:2107.03374, 2021.

Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang,
Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program synthesis with large language models. arXiv
preprint arXiv:2108.07732, 2021.

Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457v1, 2018.

Denis Paperno, German Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle,
Marco Baroni, Gemma Boleda, and Raquel Fernandez. The lambada dataset, Aug 2016.

Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a
new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018.

Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in
natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 7432-7439,
2020.

Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego
de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language
models. arXiv preprint arXiv:2203.15556, 2022.

Haoran Que, Jiaheng Liu, Ge Zhang, Chenchen Zhang, Xingwei Qu, Yinghao Ma, Feiyu Duan, Zhiqi Bai, Jiakai
Wang, Yuanxing Zhang, et al. D-cpt law: domain-specific continual pre-training scaling law for large language
models. In Proceedings of the 38th International Conference on Neural Information Processing Systems, pages
90318-90354, 2024.

34


===== PAGE BREAK =====

A_ Empirical Validation of Prior Choice

Training Loss Comparison                                               10                Distribution Probabilities
— Geometric A=0.1                                                    —e— Geometric A=0.1
2.450                                            —— Geometric A=0.2                                                    —e— Geometric A=0.2
— Geometric A=0.3                                                    —e— Geometric A=0.3

2.425                                                               G       an         0.8                                                                       a
ry                                                 —— Geometric A=0.4                                                   —e— Geometric \=0.4
&                                                 —— Geometric A=0.5                                                    —e— Geometric A=0.5
$ 2.400                                              —— Geometric A=0.6                                                     —e— Geometric A=0.6
D                                                                       —— Geometric A=0.7    >, 0.6                                                                —e— Geometric A=0.7
§ 2.375                                                                       i           E=}                                                                           i
3                                                            —— Geometric A=0.8 5                                                            —e— Geometric A=0.8
a                                                 — Geometric A=0.9 §                                               —©— Geometric A=0.9
2 2.350                                                        —— Uniform            &                                                            —=— Uniform
}                                                                                            0.4
fo}
@ 2.325
a
S

2.300                                                                                     02

2.275

20000      25000      30000      35000      40000                                       1               2               3               4
Training Steps                                                                                     UT Step

Figure 10 Effect of the prior over exit steps. Left: training loss (300-step sliding average) for a LoopLM with
Tmax = 4 under different priors on z. Colored curves correspond to geometric priors with parameter » € {0.1,...,0.9};
the red curve uses a uniform prior. Shaded regions indicate variability across runs. Right: prior probability over
LoopLM steps induced by each (uniform shown in red). Stronger geometric bias (larger 4) concentrates mass on
shallow steps, reducing credit assignment to deeper computation.

Experimental setup. Unless otherwise noted, we keep the model, data, optimizer, and schedule identical
across conditions and only change the prior 7 used in the KL term of the loss. All results are obtained
on a 776M-parameter LoopLM with Tmax = 4 recurrent steps. Training is performed on the FineWeb-Edu
corpus [36] for a total of 20B tokens with a global batch of 50K tokens per optimization step, i.e., roughly
40K steps in total.°? For geometric priors we sweep \€ {0.1,0.2,...,0.9}; the uniform prior assigns equal
mass to all steps. To assess variability, we repeat each condition with multiple random seeds; shaded areas in
Figure 10 denote the variability across runs. All other hyperparameters follow our training recipe, keeping (
fixed across prior choices.

Convergence and final loss. As shown on the left of Figure 10, the uniform prior consistently achieves
lower training loss and cleaner convergence on the 776M LoopLM. Geometric priors plateau higher, with
the gap widening as \ grows (i.e., stronger bias toward early exit), reflecting weaker supervision for deeper
iterations.

Stability and exploration. Geometric priors exhibit larger late-training oscillations, consistent with
premature collapse of qg(z |) onto shallow steps and reduced entropy. The uniform prior imposes no
structural depth preference, so the KL term behaves as pure entropy regularization: exploration is maintained
longer, and the model can allocate probability mass across multiple depths until it has learned which examples
benefit from deeper computation.

Depth utilization. The right panel of Figure 10 visualizes the priors. Large-\ geometric priors concentrate
mass at t=1-2, starving deeper steps (t>3) of credit assignment; this undermines the “deeper is better’
property. With a uniform prior, all depths receive comparable signal, enabling later iterations to specialize
and deliver higher accuracy when maximum depth is allowed at inference.

9

Compute—accuracy trade-off. Although the uniform prior does not explicitly favor early exit, it does not
preclude efficient inference: at test time we can still cap steps or apply a halting threshold. For a fixed average

5The loss curves plot a 300-step sliding average over the training trajectory.

35


===== PAGE BREAK =====

step budget, models trained with a uniform prior achieve a strictly better accuracy-compute Pareto frontier
than those trained with geometric priors, indicating that unbiased depth exploration during pretraining turns
into better deployment trade-offs.

B Physics of LoopLMs

In this appendix, we conclude all the experimental settings and details in Section 6. Section B.1 includes the
experiments on knowledge capacity; section B.2 includes the settings on knowledge manipulation synthetic
tasks. Section B.3 introduces the detailed setting on the synthetic QA task following [67]. Finally, Section B.5
provides the theoretical results, detailed proof, and the discussion with the current theoretical results.

B.1 Capo: knowledge capacity

In this section, we introduce the knowledge capacity proposed in [65, 66]. The task evaluates models’ efficiency
in memorizing factual knowledge within its parameters, which is measured by bits per parameter. We tested
different sizes of models and visualize the knowledge scaling law through plotting bits v.s. parameter number.

Dataset: Synthetic Biographies We synthesize fake biographies following the bioS(N) dataset in [65].
Specifically, we generate N biographies of a random generated person together with their date of birth, city of
birth, university, major, and employer. In our work, we online sample the individual attributes and generate
the biographies in natural language using a random selected fixed template. An illustrative example is:

Layla Jack Beasley celebrates their birthday on January 24, 1914. They spent formative years in Portland,
ME. They focused on Business Analytics. They supported operations for Delta Air Lines Inc. in Atlanta,
GA. They received their education at Pepperdine University.

Model We use original GPT2 architecture and replace the positional encoding with RoPE [32]. In the Capo
task, we tie the LM head and the embedding layer. To test the capability of universal transformer, we also
added looping module s.t. the transformer blocks can be looped several times. We explore a broad range
of model sizes varying in hidden dimension and depth. The notation a-b-lc represents the model with 64a
hidden dimensions (a attention heads with each head 64 dimensions), b layers, and c LoopLM steps (loops).
The context length is set to 512.

Training details We use AdamW optimizer by setting (81, 82) = (0.9, 0.98),¢ = 10~® with 1000 steps of
warmup followed by a cosine learning rate schedule from 1 to 0.1x of the original learning rate. We use bf1l6
training and packing is used during training. We masked different pieces of biographies from each other in
each concatenated chunk.

We pass each data piece for 1000 times (similar to the 1000-exposure in [65]) during training. Since the final
performance is not sensitive to learning rate choices, we consider learning rate 7 = 0.001, wd = 0.02, and total
batch size 192. We pick N € {20K,50K, 100K, 200K, 500K}.

Evaluation: Knowledge Capacity Ratio After pretraining on the bioS(N) dataset, we assess a model’s
knowledge capacity, defined as the number of bits of information it can reliably store. To make this measure
comparable across models of different sizes, the raw bit count is normalized by the number of model parameters,
yielding a “bits per parameter” metric. The derivation and motivation of the metric in discussed in [65]. For
readers, we refer the detailed setting to Section 2.1 of [65].

Definition 1. Given a model F with P parameters trained over the bioS(N) dataset Z, suppose it gives
Pi = losSpame(Z) and po = lossyatue(Z), which are the sum of cross entropy loss on the name tokens and
attribute tokens, respectively. The capacity ratio and the maximum achievable capacity ratio are defined as

ef N logy X® + N logs So e??     ef Nlogy No-N+N1
R(F) def  So epi . Sz So €  Rmax(F)  og2 No =  082 50

for No = 400 x 400 x 1000, So = 2 x (12- 28-200) x 200 x 300 x 100 x 263 as all possible configurations.

36


===== PAGE BREAK =====

Ignoring names, each person encodes approximately logs(So) ~ 47.6 bits of knowledge. The evaluation
accounts for partial correctness. For instance, if a model recalls the year of a person’s birth but not the exact
date, the partially correct information still contributes to the overall bit-level computation. This approach
allows for a fine-grained measurement of knowledge retention, rather than relying on a strict all-or-nothing
scoring.

B.2. Mano: knowledge manipulation

We followed [66] and used the Mano task to investigate the models’ capability of manipulating stored knowledge
within the parameters without intermediate thoughts.

Dataset The dataset consists of modular arithmetic instances with tree structures of £ operations, where the
number of operations @ < L as the maximum length. ¢ is uniformly sampled from [1, L]. The expressions are
presented in prefix notation. For example, a length-3 instance is:

<bos> <len_3> - * a b + cd <ans> ans

which corresponds to (a* b)+(c—d) mod 23. All the operations are on F23. The task only involves (+, —, *).
The only tokens we use are the operations, numbers from 0 to 22, and the special <bos>, <ans> and length
tokens len_{i} with 7 € [0, L].

Training details We use AdamW optimizer with (61, 62) = (0.9, 0.98), ¢ = 10~° and gradient clipping with
maximum norm 1.0. We employ 1000 steps of warmup followed by a cosine learning rate schedule to minimal
learning rate 0.1 of the peak learning rate. We use bf16 training with packing and set the context length
to 1024 tokens. Different pieces of mano problems are masked from each other in each concatenated chunk
during training.

We conduct hyperparameter search over learning rates lr € {0.00005, 0.0001, 0.0002, 0.0005} with weight decay
0.1 and global batch size 128. We experiment with model depths L € {10, 16, 24} layers and hidden dimension
1024. Training is performed for {80K,110K,200K} steps respectively for different difficulties. We run all
experiments across 3 random seeds and report the best performance.

Evaluation During evaluation, we only use the expressions with the hardest length @ = L. Accuracy is
computed separately due to the masks. We consider exact match accuracy since the final answer is single-token.

B.3 Multi-hop question answering on synthetic relations

We followed [67] to construct the natural language multi-hop QA task. Comparing with Mano, the QA task is
more knowledge-heavy and with a slightly simpler structure. [67] found that the model needs exponential
many k-hop data for traditional transformer to learn. We chose this task to investigate if recursive structure
in the reused-parameters can improve the sample efficiency of the task, showing better manipulation capability
of LoopLM.

Dataset The dataset contains |E| entities—each with a unique name—and N relation types. We created 500
distinct single-token person names (e.g., Jennifer) and 20 single-token relation names (e.g., instructor) to serve
as namespaces for entities and relations. We reused the name list in [67]. The complete list of relation names
and a partial list of entity names appear in Tables 5 and 6 in [67]. The multi-hop questions are generated
through a K = 5 hierarchical layers, where each layer has 100 individuals. Each entity is connected to |R|
randomly chosen person in the next layer. This structure naturally generates |E|/5 x |R|* k-hop questions. In
our setting, since we only consider 3-hop questions, the number should be 8 x 10°.

For training, we use part of all the 3-hop training set and test on the leave-out 3000 test questions. For each
test instance, we greedy decode the single token answer given the question prompt (e.g. ‘Who is the instructor
of the teacher of Bob? \n Answer:’). We evaluate the exact match accuracy.

37


===== PAGE BREAK =====

Run family (loops)                          100} Run family (loops)
16
—=— Loop] (avg)                                                            —=- Loop] (avg)
—t— Loop2 (avg)                                                            —t— Loop2 (avg)
147 ~~ Loop4 (avg)                                                    80} —®— Loop4 (avg)
xs                                                  xs
=                                                     = 60
@ 10                                                 fe
g                                                  g
5                                                  5
o 8                                           o a0
<x                                                  <x
6
20
4
2                                                   0
12 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20          12 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
Training Steps (103)                                                                                  Training Steps (103)

Figure 11 Left & Right. We further train with 100000 and 140000 unique QA pairs for 20000 steps with context
length 1024 and batch size 2048. Similar to the main text, models with more loops learn faster and achieve better
performance comparing with models without loops.

16     Run family (loops)                                                                            60     Run family (loops)
—# Loop] (isoflop) (avg)                                                    —s- Loop] (isoflop) (avg)
—t— Loop2 (avg)                                                          —*— Loop2 (avg)
147 —~e— Loop4 (avg)                                                    50} —®— Loop4 (avg)
g                                                     E40
Zio                                                   zo
g                                                     g
5                                                                                            5 30
g 8                                          g
<                                                     <
6                                                     20
4                                                     10
2
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20         12 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
Training Steps (103)                                                                                 Training Steps (103)

Figure 12 Left & Right. We further train with 100000 and 120000 unique QA pairs for 20000 steps with context length
1024 and batch size 2048. We train the baseline with 24 layers, which is equivalent flops with the loop 4 transformers.
Similar to the main text, models with more loops learn faster and achieve better performance comparing with models
without loops, even with iso-flop transformers. The loop 2 average performance is weaker than the iso-flop version
transformer since it has less equivalent depth when N = 10°, but it surpasses the baseline with more data provided.

Training details We use AdamW optimizer with (6), 82) = (0.9,0.98),¢ = 10~°© and gradient clipping 1.0.
We run 1000 steps of linear warmup followed by a cosine learning rate schedule to minimal learning rate 0.1
of the peak learning rate. We use bf16 training with packing with context length 1024 tokens. QA pairs from
distinct samples are masked from each other during training.

We use a base model architecture with 1024 hidden dimensions, 16 attention heads, and 6 layers. We allow it
to loop in {1,2,3,4} times. Following the experimental setup in [67], we set the learning rate to 0.0005 with
1000 warmup steps and train for a total of 20,000 steps using batch size 2048. We run all experiments across
4 random seeds and report the average performance.

B.3.1 Additional experimental results

As the supplement of the main text, we present additional experiments to show that the superiority of LoopLM
is general across different number of unique samples. For presentation, we only consider the interval of
{10°, 1.2 x 10°, 1.4 x 10°} to exhibit the difference between looped models and non-looped baselines. We also
checked iso-flop baseline models with the same hidden dimension and 24 layers®. The results are presented
below in Figure 11 and Figure 12.

6We note that in Figure 12, the iso-flop baseline with N = 1.2 x 105 does not perform significantly better than the shallower
version in the main paper. We conjecture that it could be because of the randomness, or insufficient hyperparameter tuning. We
believe further follow-up experiments should be necessary to further validate this conclusion here in the appendix.

38


===== PAGE BREAK =====

B.4 Case study: improvements across different categories in MMLU

To validate our findings from synthetic tasks on a broad, real-world benchmark, we conducted a granular
analysis of performance gains across all 57 sub-categories of MMLU. Our hypothesis is that if LoopLMs
primarily enhance knowledge manipulation and reasoning, the largest performance gains should appear in
procedural, reasoning-heavy tasks, while knowledge-heavy, retrieval-based subjects should see less improvement.

We measured the relative improvement by comparing the accuracy at the single recurrent step (Loop 1)
against the accuracy at our fully trained depth (Loop 4). The detailed results for all 57 categories are
available in Table 15. The analysis strongly supports our hypothesis. The categories with the most significant
improvements are those requiring logical, maths, or procedural reasoning; conversely, categories that depend
more on retrieving specific, memorized facts or nuanced world knowledge showed the most modest gains.

Example: Categories with the most significant /modest improvements

With most significant improvements:                     With most modest improvements:
Moral Scenarios: +7.8%

Global Facts: +8.3%

Virology: +13.7%

Anatomy: +21.4%

Elementary Mathematics: +155.6%
Formal Logic: +143.3%

Logical Fallacies: +127.8%

High School Statistics: +126.9%

This stark contrast indicates that the iterative computation is not simply increasing the model’s accessible
knowledge (as seen in the nearly flat ‘global_ facts‘ improvement) but is actively performing the multi-
step symbolic manipulation required for complex subjects like logic and math. This real-world benchmark
result corroborates our synthetic findings in Section 6.2, confirming that the LoopLM architecture’s primary
advantage lies in enhancing knowledge manipulation, not raw storage.

B.5 Theory: latent thought with LoopLM

In this section, we prove that LoopLM can solve the graph reachability problem (with part of the graph
knowledge learned in the parameters) in O(log) steps. The results are closely related to the expressivity
power of transformers and looped transformers with padding [12, 13, 76, 77]. It matches the expressiveness
lower bound results in [76, 77], and also resembles the state tracking tasks [78, 79] which requires O(log n)
depths. We first define the task rigorously and state our main theorem. We finally discuss the theoretical
improvement, caveats of the results, and all related theoretical results.

We first define our task based on the intuition of knowledge manipulation. Challenging knowledge manipulation
tasks often have multiple steps or hierarchical structures, which requires the model to search in the knowledge
graph with directional dependencies formed by the atomic facts or knowledge. Moreover, the context also
contains conditions or new facts necessary for the problem. Therefore, we consider the searching task that
requires the model to both encode the fixed hidden knowledge graph G in the parameters and utilize the
contextual information (additional graph) Gx. The goal is to check if two queried nodes are connected. The
formal definition is as follows (modified from [68]):

Definition 2 (Graph reachability on knowledge graph). Let V = {v1,v2,...,Un} is the set of vertices
and E = {e1,€2,...,€m} is the set of edges. Let G = (V,E) be a directed hidden knowledge graph, and
Gete = (V, Ectx) be an input additional knowledge graph. Given a source node s a target node t, the task is to
output 1 when there exists a path from s to t on the combined graph G+ Gets := (V,E + Ecc), and output 0
when s cannot reach t on the combined graph.

Transformer architecture In this setting, we consider a simple single-head transformer architecture. We
only use one-head and a two-layer gated MLP layer. For clearer theoretical demonstration, we use a special
normalization layer LN() to threshold on H: LN(#),;,; = 1{H;,; > 0}. And the overall architecture for each
loop is (where Q, K,V,W1, W2 are all shared through layers)

Hisos = LN(A; + Attng.«.v(Hi)), Attng,«,v (Hi) = V Hisoftmax(H;' K'QH;)

39


===== PAGE BREAK =====

Table 15 Performance metrics across different depths by category in MMLU.

Category                                    Loop 1 Loop2 Loop3 Loop 4 Improvement%
elementary_mathematics                            0.3095 0.6190 0.7460 0.7910 155.5556
formal_ logic                                               0.2381 0.4841 0.5238 0.5794 143.3333
logical fallacies                                          0.3313 0.7178 0.7546 0.7546 = =127.7778
high school_ statistics                                0.3102 0.5833 0.6620 =—-0.7037 =: 126.8657
high _school_ macroeconomics                     0.3692 0.6564 0.7513 0.7718 109.0278
management                                               0.3883 0.6699 0.7864 0.7961 105.0000
high _school_ government _and_politics 0.3938 0.7202 0.8238 0.7979 102.6316
high _school_ microeconomics                     0.4076 0.7143 0.8361 0.8193 101.0309
high_school_ psychology                             0.4183 0.7817 = 0.8294 =: 0.8385 ~—- 100.4386
high_school_ biology                                  0.4129 0.7452 0.8129 0.8161 97.6563
college chemistry                                       0.2600 0.5000 0.5500 0.5000 92.3077
conceptual physics                                         0.3830 0.6340 0.7149 0.7319 91.1111
college biology                                           0.3889 0.6806 0.7569 0.7431 91.0714
machine _ learning                                       0.2500 0.4286 0.5089 0.4732 89.2857
miscellaneous                                              0.3870 0.6564 0.7101 0.7178 85.4785
high _school_ geography                              0.4444 0.7121 0.7980 0.8182 84.0909
high _school_ physics                                  0.2649 0.3841 0.5033 0.4834 82.5000
high _school_ chemistry                               0.3054 0.53820 0.5665 0.5517 80.6452
college medicine                                        0.3584 0.5607 0.6416 0.6358 77.4194
college _computer_ science                          0.3500 0.5100 0.6000 0.6100 74.2857
professional accounting                              0.2872 0.4539 0.4929 0.5000 74.0741
world _ religions                                               0.4211 0.6374 0.6959 0.7251 72.2222
high _school_computer_ science                  0.4600 0.6800 0.7600 0.7800 69.5652
clinical_ knowledge                                      0.4151 0.5887 0.6415 0.6943 67.2727
astronomy                                 0.4013 0.6184 0.6711 0.6711 67.2131
prehistory                                                   0.3765 0.5586 0.6389 0.6235 65.5738
high _school_us_ history                             0.4314 0.6912 0.7304 0.7108 64.7727
professional psychology                              0.3709 0.53892 0.5850 0.6095 64.3172
philosophy                                                  0.4405 0.6495 0.7042 0.7106 61.3139
business ethics                                           0.4200 0.6400 0.6500 0.6700 59.5238
high _school_ mathematics                          0.3000 0.4444 0.5037 0.4778 59.2593
high school_ european _ history                  0.5030 0.6909 0.7273 0.8000 59.0361
medical_ genetics                                        0.4500 0.6900 0.7100 0.7100 57.7778
human _ sexuality                                        0.4580 0.6336 0.7023 0.7176 56.6667
computer_ security                                      0.4500 0.6200 0.6700 0.7000 55.5556
college physics                                           0.2549 0.3333 0.38922 0.3922 53.8462
international_law                                       0.5124 0.7107 0.7769 0.7851 53.2258
marketing                                                   0.5726 0.8547 0.8803 0.8761 52.9851
nutrition                                                     0.4510 0.6634 0.6863 0.6863 52.1739
college mathematics                                  0.2900 0.3700 0.4200 0.4400 51.7241
econometrics                                               0.3421 0.4123 0.5088 0.5175 51.2821
sociology                                                     0.5373 0.7413 0.7910 0.8010 49.0741
professional_ medicine                                 0.3787 0.5368 0.5735 0.5625 48.5437
high _school_ world_ history                        0.5274 0.7300 0.7595 0.7722 46.4000
human_ aging                                             0.4484 0.6143 0.6457 0.6502 45.0000
security studies                                          0.5265 0.6898 0.7510 0.7592 44.1860
professional _law                                         0.3246 0.4055 0.4596 0.4570 40.7631
public_ relations                                          0.4636 0.6182 0.6727 0.6364 37.2549
us_ foreign policy                                      0.5900 0.7200 0.8100 0.8000 35.5932
electrical_ engineering                                 0.4483 0.5655 = 0.5862 0.6069 35.3846
abstract algebra                                        0.2700 0.3000 0.3700 0.3600 33.3333
moral_ disputes                                           0.5491 0.6503 0.6850 0.6994 27.3684
anatomy                                  0.4148 0.5111 0.5333 0.5037 21.4286
jurisprudence                                              0.5926 0.6944 0.7593 0.7130 20.3125
virology                                                      0.4398 0.5000 0.4880 0.5000 13.6986
global_ facts                                                0.3600 0.3700 0.3600 0.3900 8.3333
moral_ scenarios                                          0.2436 0.2693 0.2492 0.2626 7.7982

40


===== PAGE BREAK =====

Hy41 = LN(Ai+0.5 + W2ReLU(W1 Ai+0.5))

Input Format We define the adjacency matrix of the graph G as A = [a1, ag, ...,@n] € R"*”. Similarly, we
define Acts = [@1,ctx,@2,ctx; +++» An,ctz|. We use one-hot embeddings v; to denote the vertex embedding. We
consider the following input sequence format with length n +1 for this task (assuming we already have the
embeddings):
Hy =    U1       V2     ute     Un    = R2rxn
Qi.cta G2 ctx ‘*' On, ctx

where the first n tokens are the input context graph adjacency matrix. We assume the LoopLM recurrent for
L steps, and we denote the hidden state sequence for the i-th recurrence:

a) a@® 2.

For simplicity, we ignore the encoding and decoding process and have a direct output protocol: the final
:       _             (L)
output for query (s,¢) is the t-th entry of as”.

Now we state the main theorem given the previous setting.

Theorem 2 (LoopLM solves reachability in log D steps). Fiz n as the maximum size of the combined
knowledge graph G. Taking the adjacency matrix of the context graph Getz € R”*”" fed in as a n-token
sequence and given a query pair (s,t), there exists a one-layer, single-head transformer independent of Getz,
with recurrent O(log, D) times and a hidden dimension of de = 2n that can check whether there exists a path
from s to t in the combined knowledge graph (G+ Geix), where D is the diameter of (G + Getz).

We directly construct the attention and the MLP layer for the LoopLM to implement an algorithm that is
similar to Warshall’s algorithm, using Boolean matrix powers with repeated squaring. The proof idea is to do
a parallel search on all pairs connectivity, doubling the reachable distance with each loop. Since the maximum
distance (i.e. diameter) is D, we only need O(log D) rounds to decide whether two nodes are connected or
not. The attention enables each loop to iterative square the current adjacency matrix, and the MLP stores
the hidden encoded graph G’s adjacency matrix to help internal knowledge manipulation.

Proof. We assign the parameters Q, K, V,W1, W2 as follows (6 — +00 is a large scalar):

Recall that the input sequence is

V1            v2         eee        Un

Ho _— | = R2rxn

Qijcta G2 ctr °'* OAn,cta

which only contains the input context graph’s adjacency matrix. We assume the LoopLM loops for L steps,
and we denote the hidden state sequence for the 7-th loop:

al) al) tee a

For simplicity, we directly output the ¢-th entry of as” for query (s,t). The model should check whether (s, ¢)

are connected, i.e. (aS), = 1 or 0. We are going to prove by induction that for each recursion, a\) contains

all the vertices vz (i.e.  (a), = 1) that vertex v; is connected to, and the distance between v,; and v are less
than or equal to 2'~!. Therefore, we only need log D + 1 loops to get the final answer.

Base. When i = 1, the constructed parameters ensure that the j-th node v; attend to all the nodes that are
directly connected to v; with the same attention score 3. This guarantees that the attention layer will average

Al


===== PAGE BREAK =====

the nodes’ tokens that v; is connected to. The j-th column of the attention before the thresholding layer
becomes (|a;,ct2| means the nodes that v; connects to)

= Loted eal. bt
a;    Qj ctx   |; cta| k:(a;.cae)e=1 Qk ctx

This updated adjacency vector naturally contains all nodes that has distance < 2 to vj in context graph Getz
after the thresholding layer. It naturally includes nodes with distance 1.

Now we consider the output of the MLP layer, which only adds the adjacency matrix of hidden knowledge
graph G to the residual stream.

Uy) — | U5          Uj  | UR   On
st] = [af] +wansvo (ws []) = [2] [at

which combines the adjacency matrices of the context graph Get, and the hidden knowledge graph G. After
the thresholding in the end, all non-zero entries become 1, which already includes all distance 1 nodes for all
nodes. Therefore, we have that al)

joctx contains all reachable nodes within distance 1 of node v; after the first
recursion.

(2)
J
of v;. Now the hidden state sequence is going through loop 7 + 1. To finish the proof, we show that a

contains all reachable vertices within distance 2’~!
(i+1)
J

Induction. Assume at the recursion step i (4 > 1), all a

contains all reachable vertices within distance 2¢ of Uj.

wr) now, and the j-th node v; uniformly attend to all the nodes that

are connected to v; within distance 2’~'. The j-th column of the attention before the thresholding layer

The attention for this stage looks at a

becomes (ja | means the number of nodes)

      j   la; | da) p=  k

After thresholding, the adjacency vector aggregates a) and all al where vz has distance < 2*~' to v;

J
in combined graph Getz + G. Meanwhile, al? contains all vertices connected to vz with distance < 2*~!.
Therefore, the combined vector includes all nodes within distance 2" of v;.

Finally, we consider the final MLP:

UF |             Uj                                         U5               _                 v5               On
ast] = EN [te] + Waet (| tbo])) =EN([jotho] + [0]

which also contains all vertices connected to v; with distance 2’. That means (aft), is a precise indicator of

whether (s,t) is connected within 2* distance. By induction, we finish the proof and with L = [log, D] +1
recursion steps, the model can correctly solve reachability on the combined graph G+ Getz.

Discussion on related theoretical results We provided a modified construction from [12], which requires n3

padding tokens that increase the computation complexity to O(n®). However, our construction requires
O(n) hidden dimension as the continuous CoT did in [68]. The O(n) requirement is necessary because the
superposition in latent space needs to (in the worst case) encode O(n) nodes’ information, which can be
a theoretical limitation. The requirement can be relaxed when there is an upper bound for the maximum
number of vertices that some vertex is connected to.

Input format In our construction, the input format is the adjacency matrix of the graph. As a natural
alternative, [68] used a sequence with length O(n?) as the input to encode all the different edges. To make
LoopLM also work on this setting, some additional induction head mechanism (similar to [68]) is needed to
extract all edges and combine them into the adjacency matrix. With slight modification, we can still get the
solution with sequential steps O(log D).

42


===== PAGE BREAK =====

C Evaluations

C.1_ Evaluation Settings

Base Model Evaluation Settings Table 16 details the evaluation settings and frameworks used for the
Ouro base models.

Table 16 Evaluation settings and benchmark sources for base models.

Benchmark          Settings                  Framework
General
MMLU [80]         logprobs, 5-shot            lm-eval-harness
MMLU-Pro [81]     strict match, 5-shot CoT 1m-eval-harness
BBH [82]           strict match, 3-shot CoT 1m-eval-harness
ARC-C [83]         logprobs, 25-shot          1m-eval-harness
HellaSwag [84]      logprobs, 10-shot           lm-eval-harness
Winogrande [85]     logprobs, 5-shot            lm-eval-harness
Math
GSMB8k [86]         strict match, 3-shot CoT 1m-eval-harness
MATH500 [87]      strict match, 5-shot CoT In-house
Code
HumanEval [88]     pass@1                     evalplus
HumanEval+ [57] pass@1                  evalplus
MBPP [89]         pass@1                   evalplus
MBPP-+ [57]        pass@1                   evalplus

Reasoning Model Evaluation Settings Table 17 details the evaluation settings and protocol used for the
Ouro-Thinking reasoning models, as described in Section 5.2. All reasoning benchmarks utilized an in-house
evaluation harness and an LLM-as-judge protocol with a fixed rubric.

Table 17 Evaluation settings and protocol for reasoning models (Ouro-Thinking).

Benchmark        Protocol                            Decoding Settings

AIME 2024/2025 In-house harness; LLM-as-judge temp=1.0, top_ p=0.7
OlympiadBench     In-house harness; LLM-as-judge temp=1.0, top_ p=0.7

GPQA             In-house harness; LLM-as-judge temp=1.0, top_ p=0.7
SuperGPQA        In-house harness; LLM-as-judge temp=1.0, top_ p=0.7
BeyondAIME       In-house harness; LLM-as-judge temp=1.0, top_ p=0.7
HLE               In-house harness; LLM-as-judge temp=1.0, top_ p=0.7

D Scaling Law for LoopLMs

To further explore the potential of LoopLM, we conduct a series of small-scale experiments to investigate
the scalability and predictability inherent in LoopLM. Specifically, our work focuses on the following three
research questions:

e RQ1: What is the performance gap between standard models and LoopLM?
e RQ2: How do recurrent steps impact the total loss and step-wise loss in the context of LoopLM?

e RQ3: What is the inherent connection between total loss and step-wise loss?

D.1 RQ1: What is the performance gap between standard models and LoopLM?

To understand the performance gap between standard models and LoopLM, we quantify this difference in
terms of benchmark performance. We also observe how this gap varies with changes in recurrent step and
model size to guide the further scaling and iteration of LoopLM.

43


===== PAGE BREAK =====

Experimental Setup We prepare five model sizes: 53M, 134M, 374M, 778M, and 1.36B. For recurrent steps,
we prepare four different depths: 1, 2, 4, and 8. It is worth noting that for standard models, different recurrent
steps effectively multiply the number of layers in the model. We evaluate the performance on the following
benchmarks: ARC-Challenge [90], ARC-Easy [90], HellaSwag [84], LAMBADA [91], OpenBookQA [92], and
PIQA [93]. In all sub-experiments, we train on 20B tokens using the FineWeb-Edu corpus [36]. We present
the benchmark performance from the final step. For LoopLM, the recurrent step used for evaluating is the
maximum recurrent step.

By observing the trends in the curves, we derive the following observations:

1. Whether standard models or LoopLM, the model’s performance improves with increasing model size and
recurrent step. As shown in Figure 13, for all recurrent steps, the benchmark performance of both the LoopLM
and Standard models increases as the model size grows, which aligns with the principle of LLMs: larger is
better. As shown in Figure 14, except for the LoopLM at 778M and 1.364B, both the LoopLM and Standard
models show that benchmark performance increases as the recurrent step increases. This indicates that latent
reasoning is indeed useful for both the LoopLM and Standard Transformer.

Recurrent Step: 1                                           Recurrent Step: 2

0.500 4

0467 __»— RLM                           —e— RLM
—e— Standard                      0.4757" —e— Standard

°
B
a
°

Avg Benchmark Score
Avg Benchmark Score

0.350 4

0.325 4
0.34 4

0     200    400     600     800    1000    1200    1400         0     200     400     600     800    1000    1200    1400
Model Size (M)                                           Model Size (M)
Recurrent Step: 4                                            Recurrent Step: 8

0.525 4

—e— RLM
—e— Standard

0.500}. —*— RLM

—e— Standard                                        0.500 4

°
B
N
a

°
nS
fs)
6
°
BR
fs)
fo

Avg Benchmark Score

Avg Benchmark Score

°
w
x
a

0.350 4                                                                                  0.350 4

0.325 4                                                                                                0.325 4
()          200        400         600         800        1000       1200       1400                  ()          200         400         600         800        1000       1200       1400

Model Size (M)                                           Model Size (M)

Figure 13 The average benchmark performance of LoopLM and Standard Transformer models under different recurrent
steps as model size varies. With a recurrent step of 1 (top left), both models have identical architectures, resulting in
overlapping curves. Overall, as the model size increases, the benchmark performance improves. The average benchmark
score demonstrates the average results of the six benchmarks.

2. Overall, the performance of the standard model exceeds that of LoopLM under the same conditions. This
gap increases with the recurrent step and decreases with the model size. Observing Figure 13 and Figure 14,
it is clear that the benchmark performance of the Standard model is consistently higher than that of LoopLM,
indicating that the Standard model has a scoring advantage without considering computational budget.

44


===== PAGE BREAK =====

Model Size: 53M

Model Size: 134M

Model Size: 374M

Model Size: 778M

Model Size: 1364M

v
S oso} —e- RLM                                                                                            |                                   an
Wn        —e— Standard
s 0.45 4                           |                           4                           |                           4
oe
£
S 0.40 4                           | 4     |
5                                    o——_e——___
)
© 0,354                           |
2 | -—_—
2              6      8      2              6      8      2              6      8      2              6      8      2              6      8

4                        4                        4                        4                        4
Recurrent Step         Recurrent Step         Recurrent Step         Recurrent Step         Recurrent Step

Figure 14 The average benchmark performance of LoopLM and Standard Transformer models under different model
sizes as recurrent step varies. Except for the LoopLM at the model size of 778M and 1.364B, in all other cases, the

benchmark performance of the model increases with the increase in recurrent steps.

Furthermore, we define the benchmark performance gap as the benchmark performance of the Standard
model minus that of LoopLM, and this value is positive in all our experiments. As shown in Table 18, as
the recurrent step increases, the benchmark performance gap also increases, suggesting that as the number
of recurrences rises, the effect of models not sharing parameters gradually surpasses that of models sharing
parameters. Besides, we find that the benchmark performance gap generally has a negative correlation with
model size when the maximum recurrent step is relatively low, meaning that as the model size increases, the
performance of LoopLM becomes closer to that of the Standard model, resulting in a smaller gap between the
two. This trend is particularly consistent when the recurrent step is 4.

Table 18 The average benchmark performance gap between LoopLM and Standard models as the recurrent step

varies at different model sizes. The gap is defined as (Standard model score - LoopLM score). As the recurrent step
increases, the performance gap generally increases.

Average Performance Gap

Model Size Step 2                      Step 4
170M         0.021               0.039
340M                  0.023                                0.037
680M                  0.015                                0.026
1.3B               0.017                        0.025

D.2 RQ2: How do recurrent step impact the total loss and step-wise loss in the context
of LoopLM?

In this subsection, we investigate the predictability and generalizability of LoopLM from the perspective
of training loss, examining the impact of recurrent step on the trends in total loss and step-wise loss. The
experiment is set up in complete consistency with Section D.1, but we focus more on the total loss and
step-wise loss during the training process. Here, step-wise loss refers to the loss of the same LoopLM at
different recurrent step.

Here, we have following variables: model size N, training data size D, maximum recurrent step T,,, recurrent
step T, total loss L,, and step-wise loss L,. Following Chinchilla [94], we first attempt to fit the relationship
between L; and N,D,T,, in the form of a power law:

A   B    C

I, =E4        }        ;
‘     (N+t))® | (D+t2)8 © (Tm +t3)7

The purpose of t,, t2, and t3 is to prevent the variables from exploding in value near zero, allowing the
fitting curve to be smoother. We refer to above formula as the Total Loss Scaling Law. First, to validate the

45


===== PAGE BREAK =====

predictability of LoopLM, we fit all the data points, and the resulting curve is shown in Figure 15. We find
that the actual loss curve and the predicted loss curve are highly consistent, demonstrating the predictability
of LoopLM in terms of model size, training data size, and max recurrent step. We quantify the consistency of
the scaling law using the coefficient of determination R?. An absolute value of the R? closer to 1 indicates a
better fit, with positive values representing a positive correlation and negative values representing a negative
correlation. Fitting the Total Loss Scaling Law using all data points and calculating R? with all data points,
we obtain an R? value of 0.9596. This confirms the strong correlation between total loss and model size,
training data size, and max recurrent step, demonstrating the predictability of the Total Loss Scaling Law.

Tm =2,N = 53M               Tm = 2,N=134M              Tm = 2,N = 374M              Tm =2,N = 778M              Tm = 2,N=1.36B

— Real      |             — Real                   — Real                   — Real                   — Real
-- Pred     |           a--- Pred                ---: Pred                ---- Pred                ---- Pred

Total Loss

20              0                                  20              0                                  20              fy                                  20              0                                  20

3      Fi      Fis                        3      Ft)      Fis                        3      Ft)      Fy                         3      Fi      Fy                        3      Fi      Fis
Tokens(B)                            Tokens(B)                            Tokens(B)                            Tokens(B)                             Tokens(B)
Tm = 4,N = 53M               Tm = 4, N= 134M              Tm= 4,N=374M              Tm = 4,N = 778M              Tm = 4,N=1.36B

— Real      |             —— Real                   — Real                   —— Real                   — Real
— Pred     |           Pred                —-~ Pred                —— Pred                ——— Pred

Total Loss

20              °                                  20              °                                  20              0                                  20              °                                  20

3      Fig      Fa                        3      Fy      Fa                        3      Fy      Fa                         3      Fig      Fa                        3      Fig      Fa
Tokens(B)                            Tokens(B)                            Tokens(B)                            Tokens(B)                             Tokens(B)
Tm = 8,N = 53M               Tm = 8, N= 134M              Tm = 8, N= 374M              Tm = 8,N =778M              Tm = 8, N=1.36B

m

—— Real      |             —— Real                   —— Real                   —— Real                   —— Real
-- Pred                ---~ Pred                ---~ Pred                ---- Pred                ---- Pred

Total Loss

By     Fi    Fis                   3     Ft)    Fis                   3     Ft)    Fy                   3     Fi    Fy                   Hy     Fi    Fis
Tokens(B)                       Tokens(B)                       Tokens(B)                       Tokens(B)                       Tokens(B)

Figure 15 Illustration of the actual loss curve and the loss curve predicted by the scaling law. To demonstrate the
predictability of LoopLM, we have used all data points for fitting, proving its predictability in terms of model size,
training data size, and max recurrent step. The orange dashed line represents the prediction, while the blue solid line
represents the actual loss.

In addition to its predictability, we further explore the generalizability of the Total Loss Scaling Law.
Predictability refers to the ability of the Scaling Law to fit all data points into a unified curve when all data
points are available. Generalizability, on the other hand, indicates whether the Scaling Law can predict unseen
data points when fitting is done with a subset of data points. For example, generalizability tests whether the
performance of a 14B model can be predicted using the known performances of 1B and 7B models [95]. To
verify its generalizability across model size N, training data size D, and maximum recurrent step T,,,, we have
conducted related experiments, details can be found in Appendix E.1.

During the LoopLM training process, we compute the cross-entropy loss at each recurrent step, which we
refer to as step-wise loss L,. We aim to explore the relationship between step-wise loss LZ, and the current
recurrent step T’, model size N, and training data size D. Similarly, we can fit the scaling law between L,
and N,D,T, with the formula as follows:



===== PAGE BREAK =====

We refer to the above formula as the Step-wise Loss Scaling Law. We also present the fitting effectiveness
from the perspectives of predictability and generalizability. Regarding predictability, we fit all data points.
Even with the same recurrent step, the loss curve can vary significantly across different maximum recurrent
steps. To ensure the independence of the variable recurrent step T’, we do not consider the maximum recurrent
step in the Step-wise Loss Scaling Law formula and focus solely on the relationship between LD, and N, D,T.
Therefore, we have a total of three major experiments, each representing the fitting of the Step-wise Loss
Scaling Law for maximum recurrent steps of 2, 4, and 8. The fitting results of the Step-wise Loss Scaling
Law are shown in Figure 16, Figure 17, and Figure 18, which illustrate the trends of the actual and fitted
curves for maximum recurrent steps of 2, 4, and 8, respectively. We find that in some cases in Figure 17 and
Figure 18, L, increases with the increase in D. We consider this a special case and will discuss it in detail
in Section D.3; we will ignore these outlier data points during fitting. The R? for the three max recurrent
steps are 0.8898, 0.8146, and 0.795, respectively. As the maximum recurrent step increases, the increase in
the number of data points leads to lower R? values. The step-wise loss itself is less stable than the total
loss, resulting in greater variability. Thus, the obtained R? values are not as high as those of the Total Loss
Scaling Law. However, it is still evident that the scaling law is able to capture the overall trend of the curves,
demonstrating the predictability of the Step-wise Loss Scaling Law. The fitting parameter y of the Step-wise
Loss Scaling Law is positive, indicating that LD, decreases as the recurrent step increases. This aligns with our
original intent in the design of the recurrence. Besides, we present the generalizability of the Step-wise Loss
Scaling Law in Appendix E.2.

T=1,N=53M                T=1,N=134M               T=1,N=374M               T=1,N=778M               T=1,N=1.36B8

— Real      |             — Real                   — Real                   — Real                   — Real
--- Pred                 ---~ Pred                 ---~ Pred

Fy                             ---- Pred

Step-wise Loss

20              0                                  20              0                                  20              fy                                  20              0                                  20

By      Fi      Fis                        3      Ft)      Fis                        3      Ft)      Fy                         3      Fi      Fy                        Hy      Fi      Fis
Tokens(B)                            Tokens(B)                            Tokens(B)                            Tokens(B)                             Tokens(B)
T=2,N=53M                T=2,N=134M               T=2,N=374M               T=2,N=778M               T=2,N=1.36B

— Real                              — Real                              — Real                              — Real                              — Real
Fry                       v= Pred                              s--= Pred                              c--: Pred                              s-o= Pred                              s--= Pred

Step-wise Loss

3     Fi    Fa                   3     Fy    Fa                   3     Fy    Fy                   3     Fi    %                   3     Fi    Fa
Tokens(B)                       Tokens(B)                       Tokens(B)                       Tokens(B)                       Tokens(B)

Figure 16 Illustration of the actual loss curve and the loss curve predicted by the Step-wise Loss Scaling Law when
the maximum recurrent step is equal to 2.

In summary, both total loss and step-wise loss exhibit a strong correlation with N,D,T/Ti. The fitting
results demonstrate the predictability and generalizability of the Scaling Law for LoopLM. In next section, we
will explore the relationship between total loss and step-wise loss in greater depth.

D.3 RQ3: What is the inherent connection between total loss and step-wise loss?

We first review the training objectives of LoopLM:
Tm
Li =o ag(2=t| 2) LO — 8- H(qo(z | 2))

T=1

LY) represents the step-wise loss at the recurrent step T’. By analyzing the above form, we can see that the

47


===== PAGE BREAK =====

T=1,N=53M                 T=1,N=134M                T=1,N=374M                T=1,N=778M                T=1,N=1.36B
T       r                                T       r                                r       r                                T       r                                T       r
— Real                    — Real                    —— Real                    — Real                    — Real
w                -——= Pred                     “= Pred                    ---- Pred                     -——= Pred                     “= Pred |
ne
a
co}
g «|
ge
a                                                                                                   |
3S 4                                                                 mn     | {       i                   mt                           .
2         ss -
wn
2                                                                                                   I
°
o      3      Fi      Fis      7%           o      3      Ft)      Fis     2           o      3      Ft)      Fy      2          é      3      Fi      Fy      2           o      3      Fi      Fis      7%
Tokens(B)                            Tokens(B)                            Tokens(B)                            Tokens(B)                             Tokens(B)
T=2,N=53M                 T=2,N=134M                T=2,N=374M                T=2,N=778M                T=2,N=1.36B
T       T                                T       T                                T       T                                T       T                                T       T
— Real                    — Real                    — Real                    — Real                    — Real
2                soo= Pred                     ---= Pred                    s--- Pred                     soo= Pred                     soo= Pred
ne
a
3 +{|
&°
2
a,               AD                           \         iy         4                                                            :       \
2
wn
2
°
°      3      Fi      Fa      20           °      3      Fy      Fa     20           °      3      Fy      Fy      20          °      3      Fi      %      20           °      3      Fi      Fa      20
Tokens(B)                            Tokens(B)                            Tokens(B)                            Tokens(B)                             Tokens(B)
T=3,N=53M                 T=3,N=134M                T=3,N=374M                T=3,N=778M                T=3,N=1.36B
T       r                                T       r                                r       r                                T       r                                T       r
— Real                    — Real                    —— Real                    —— Real                    — Real
w                -—-= Pred                     “= Pred                    = Pred                     ———= Pred                     = Pred
ne
a
¢ {|
2°
2
o 4               n                      |          |        1
Pa]
wn
2
°
°      3      Fi      Fa      20           °      3      Fy      Fa     20           °      3      Fy      Fa      20          °      3      Fi      Fa      20           °      3      Fi      Fa      20
Tokens(B)                            Tokens(B)                            Tokens(B)                            Tokens(B)                             Tokens(B)
T=4,N=53M                 T=4,N=134M                T=4,N=374M                T=4,N=778M                T=4,N=1.36B
T       T                                T       T                                T       T                                T       T                                T       T
— Real                    — Real                    — Real                    — Real                    — Real
fd                so-= Pred                     ---- Pred                    s--- Pred                     soo= Pred                     s--= Pred
ne
8
a
3 +||                                     |
2
o 4                m         n               |           |        a                                                            ni                      q
2
wn
2
°

20                   0                                                20                   0                                                20                   0                                                20                   0                                                20

3         Fry        Fa                                     3         Fy         Fa                                     3         Fy         5                                     3         Fry         Ey                                     3         Fry        Fa
Tokens(B)                     Tokens(B)                     Tokens(B)                     Tokens(B)                     Tokens(B)

Figure 17 Illustration of the actual loss curve and the loss curve predicted by the Step-wise Loss Scaling Law when
the maximum recurrent step is equal to 4.

48


===== PAGE BREAK =====

T=1,N=53M                    T=1,N=134M                   T=1,N=374M                   T=1,N=778M                   T=1,N=1.36B

—— Real                              —— Real                              —— Real                              —— Real
-                                                                        ~~ Pred |                                 -—- Pred |                                 -— Pred |                                 ~~ Pred |
4
a
3
§
3
Be
=
Fs
3 Le                                                                       |                                      |                                      |
a
Tokens(B)                                 Tokens(B)                                 Tokens(B)                                 Tokens(B)                                 Tokens(B)
T=2,N=53M                    T=2,N=134M                   T=2,N=374M                   T=2,N=778M                   T=2,N=1.36B
—— Real                              —— Real                              —— Real                              —— Real                              —— Real
te                           -— Pred |                                 ~~ Pred |                                 -—- Pred |                                 -— Pred |                                 ~~ Pred |
4
a                               f
3
§
3
Be
=
ra                                                                     | |                                 ||             \                    [
Tokens(B)                                 Tokens(B)                                 Tokens(B)                                 Tokens(B)                                 Tokens(B)
T=3,N=53M                    T=3,N=134M                   T=3,N=374M                   T=3,N=778M                   T=3,N=1.36B
—— Real                              —— Real                              —— Real                                                                     —— Real
te                                                                        ~~ Pred |                                 -—- Pred |
4
B
8
g
g.
=
$.                  ra                          if           \                  |i             ||               ina                        rr                          |                  |
a
Tokens(B)                                 Tokens(B)                                 Tokens(B)                                 Tokens(B)                                 Tokens(B)
T=4,N=53M                    T=4,N=134M                   T=4,N=374M                   T=4,N=778M                   T=4,N=1.36B
—— Real                                   —— Real                                   —— Real
te                                                                        ~~ Pred |                                 -—- Pred |
Be
8
g
3
Be
=
g.                  Lt           i             if           \                  4             [|               i                        if          __|
a
Tokens(B)                                 Tokens(B)                                 Tokens(B)                                 Tokens(B)                                 Tokens(B)
T=5,N=53M                    T=5,N=134M                   T=5,N=374M                   T=5,N=778M                   T=5,N=1.36B
— Real                              — Real
ae.                                                                   Pred |                                      |
Be
8
g
3
Be
=
e.                  |           F             |                                            if               nen                        or
a
Tokens(B)                                 Tokens(B)                                 Tokens(B)                                 Tokens(B)                                 Tokens(B)
T=6,N=53M                    T=6,N=134M                   T=6,N=374M                   T=6,N=778M                   T=6,N=1.36B
—— Real                                   —— Real                                   —— Real
te                                                                        ~~ Pred |                                 -—- Pred |
Be
8
g
3
Be
=
ae                                            |                                            | |               why                        | |          _ |,                          | |                 f
a
Tokens(B)                                 Tokens(B)                                 Tokens(B)                                 Tokens(B)                                 Tokens(B)
T=7,N=53M                    T=7,N=134M                   T=7,N=374M                   T=7,N=778M                   T=7,N=1.36B
— Real                                   —— Real                                   —— Real                                   —— Real
-                                                                        ~~ Pred |                                 -—- Pred |                                 -— Pred |
4
B
8
g
3
Be
=
$s.               \} |         |,
FA
Tokens(B)                                 Tokens(B)                                 Tokens(B)                                 Tokens(B)                                 Tokens(B)
T=8,N=53M                    T=8,N=134M                   T=8,N=374M                   T=8,N=778M                   T=8,N=1.36B
—— Real                                   —— Real                                   —— Real
-                                                                        -~ Pred |                                 <= Pred |                                 --+ Pred |
Be
Ss
@
Be
A                                   |
$s.                  {1 |            1                                                                                                         ||          '        :
FA
Tokens(B)                                 Tokens(B)                                 Tokens(B)                                 Tokens(B)                                 Tokens(B)

Figure 18 Illustration of the actual loss curve and the loss curve predicted by the Step-wise Loss Scaling Law when
the maximum recurrent step is equal to 8.

49


===== PAGE BREAK =====

total loss consists of two components. The first part is the expected task loss, which is a weighted sum of the
step-wise loss. The second part is entropy regularization, whose primary purpose is to ensure that the learned
gating mechanism qg does not converge to a specific recurrent step.

In our extensive small-scale experiments, we have observed an interesting phenomenon: the exploitation of
total loss for shallow step-wise loss. To be specific, as shown in Figure 17 and Figure 18, when the model
size is insufficient, the shallow step-wise loss increases with the growing amount of training data. This is an
unusual phenomenon, typically, all step-wise losses should decrease as the amount of training data increases.
We attempt to explain this phenomenon. In Section D.2, it is mentioned that the step-wise loss decreases
with an increasing recurrent step, indicating that deeper recurrent steps result in lower L,. To minimize the
expected task loss, the learned gating mechanism assigns more weight to deeper recurrent steps. However,
entropy regularization ensures that the learned gating mechanism does not deviate too much from the prior
distribution. When the model size is insufficient, the amount of information it can encode is limited. To
further reduce the total loss, this results in an increase in shallow step-wise loss, which in turn allows the
weights to favor higher recurrent steps to lower the total loss. Thus, to ensure that the trend of step-wise loss
remains normal, a larger model size may be more effective for LoopLM.

As mentioned in Section D.2, the scaling law for LoopLM is predictable and generalizable for both total loss
and step-wise loss. We have:

L -E fl    At    j    Bi    f    Ct
POE (N+ t)® | (D+ tar)? © (Din + tae)
LO = B64     As     |     Bs     |     Cs
s    *  (N+tis)® (D+ tes)® © (T+ t3s)7

The subscripts s and t represent the fitting parameters for the Step-wise Loss Scaling Law and the Total Loss
Scaling Law, respectively. By substituting the Step-wise Loss Scaling Law into the training objectives, we
have:

~                 As        B,        Cs;

For the first three terms in Le), the sum of gg equals 1, allowing us to factor it out, which gives us:

A,              B,         &                       C

FSB tye * Dt a=) ay O Hlael= |)

As the amount of training data increases, the learned gating mechanism qg stabilizes, and we observe that the
value of the entropy regularization term becomes relatively low, accounting for approximately 1% to 5% of
the total loss. If the form of gg gradually stabilizes, we treat it as a constant term, and the formula becomes:

As           B;
i= E  +             t             t Eo  er
t          (N + tis)    (D + tz.)      th

In Section D.2, when considering the Step-wise Loss Scaling Law, we will fix the maximum recurrent step.
Once we determine the model’s maximum recurrent step T;,,, the forms of the above formula and the Total
Loss Scaling Law are completely consistent, indicating that there is a trend consistency in the scaling law
between total loss and step-wise loss.

We further demonstrate this through practical experiments. We take the situation where the max recurrent
step is equal to 4. First, we perform a standard fitting of the Step-wise Loss Scaling Law to obtain the
fitting parameters FE, A,,B,,C;, and so on. Next, we observe and record the distribution of gg for each

50


===== PAGE BREAK =====

Round 1 (Mean: 0.0004)                                  Round 2 (Mean: 0.0855)                                  Round 3 (Mean: 0.3793)                                  Round 4 (Mean: 0.5348)

000)                      --- Mean: 0.0004                             =-- Mean: 0.0855    2000 ~~~ Mean: 0.3793                                                 --- Mean: 0.5348
12000                                      2500                                      1750

10000                                                                 2000                                                                 1500

1250                                                                                        9

Frequency
Frequency
3
8
Frequency

i
i
i
i
I
'
'
'
'
'                                                                                                                  :
                                                                  1000                                                                                        5
'                                                                                                                                                               &
i

i

B 2 @
&é 38 8
83s
6&8 8
a
ty
2
g
8

1000

8

s
iS
f}
8

2000                                                                                             500

3
8
8

[oH           Foose

°                                                                                                                                                                 °

0.00        0.05         0.10         0.15        0.20               0.00    0.05    0.10    0.15    0.20    0.25    0.30                 0.0        0.1        0.2        0.3        0.4        0.5                0.3     0.4    05    0.6    07     0.8    0.9     1.0
Weight Value                                        Weight Value                                        Weight Value                                        Weight Value

Figure 19 Distribution of the learned ponder weights (qg(z = t | x)) for each recurrent step t when the maximum
recurrent step T;, = 4. These weights were collected during inference on the MMLU benchmark.

recurrent step t when the maximum recurrent step T;, = 4, as shown in Figure 19. For convenience, we take
the average value of qg at different recurrent steps and treat it as a normal discrete distribution, resulting
in the distribution {0.0004, 0.0855, 0.3793, 0.5348}. We then substitute this distribution and the T values
into the training objective, ignoring the entropy regularization term (after the training stabilizes, it becomes
relatively low, for simplicity, we will just ignore it). This leads to a fitting formula, and upon substituting the
actual fitting data points N and D, the computed R? value is 0.961, with the fitting results illustrated in
Figure 20. We can see that the fitting accuracy is high, and the predicted curve closely matches the actual
curve, indicating that, under a relatively rough estimate, step-wise loss can be transformed into total loss,
thus indirectly suggesting an intrinsic connection between the two.

N=53M                        N=134M                       N=374M                       N=778M                       N=1.36B

— Real                   — Real                   — Real
to            ~~ Pred                ---~ Pred                ---~ Pred

Total Loss

3     Fi    Fis                   3     Ft)    Fis                   3     Ft)    Fy                   3     Fi    Fy                   3     Fi    Fis
Tokens(B)                       Tokens(B)                       Tokens(B)                       Tokens(B)                       Tokens(B)

Figure 20 Illustration of the actual loss curve and the loss curve predicted by the estimated Scaling Law when the
maximum recurrent step is equal to 4.

E Details of the Scaling Law for LoopLM

E.1 Generalizability for the Total Loss Scaling Law

To demonstrate the generalizability of the Total Loss Scaling Law across model size, training data, and
maximum recurrent step, we have conducted relevant experiments. Our evaluation metric is the coefficient
of determination R?. To evaluate the fitting effectiveness of the Scaling Law, we calculate the coefficient of
determination of all data points.

Model Size Generalizability For model size generalizability, our total data points include five different model
sizes: 53M, 134M, 374M, 778M, and 1.364B. We select three model sizes as fitting data points, resulting in
(3) = 10 possible combinations. After fitting, the average R? across the 10 combinations is 0.9542, which is
similar to the result obtained with the full data points, demonstrating the model size generalizability of the

Total Loss Scaling Law. Figure 21 illustrates an example.

Training Data Generalizability Regarding the training data size, we are primarily interested in whether
the Scaling Law can predict model performance as training data increases. Therefore, we typically use the
preceding data points to predict future data points. To align with this starting point, we have conducted
three sets of experiments, using the current 25%, 50%, and 75% of the data points as fitting data to predict
the overall fitting performance.The R? values for using the first 25%, 50%, and 75% of the data. as fitting

51


===== PAGE BREAK =====

Tm =2,N = 53M               Tm = 2,N=134M              Tm = 2,N = 374M              Tm =2,N = 778M              Tm = 2,N=1.36B

24}             —— Real       |             — Real       -              — Real                    —— Real                    —— Real
|                    —--- Pred          |                    ---~ Pred          |                    ---- Pred          |                    ---— Pred          |                    ~~ Pred
Fry
n
a
ay
a
£
ee
4
2
fy      3      Fi      Fa      20           fy      3      Fy      Fa      20           fy      3      Fy      Fy      20           3      3      Fi      %      2           fy      3      Fi      Fa      20
Tokens(B)                            Tokens(B)                            Tokens(B)                            Tokens(B)                             Tokens(B)
Tm = 4,N = 53M               Tm = 4, N= 134M              Tm= 4,N=374M              Tm = 4,N = 778M              Tm = 4,N=1.36B
v4}             — Real       |             — Real       |              — Real                    — Real                    — Real
|                    ---- Pred          |                    ----) Pred          |                    o---) Pred          |                    ----. Pred          |                    ---- Pred
Fry
n
a
any
a
g
ec  6
4
2
ry      3      Fi      Fis      %           ry      3      Ft)      Fis      30           ry      3      Ft)      Fy      30           é      3      Fi      Fy      Fy           ry      3      Fi      Fis      %
Tokens(B)                            Tokens(B)                            Tokens(B)                            Tokens(B)                             Tokens(B)
Tm = 8,N = 53M               Tm = 8, N= 134M              Tm= 8,N=374M              Tm = 8,N=778M              Tm = 8,N=1.36B
24}             — Real       t             — Real       |              — Real                    — Real                    — Real
|                    Pred          |                    ---~ Pred          |                    Pred          {                    ——~ Pred          |                    Pred
Fry
n
Se
a
£
ee
4
2
3      3      Fig      Fa      20           3      3      Fy      Fa      20           3      3      Fy      Fa      20           3      3      Fig      Fa      2           3      3      Fig      Fa      20
Tokens(B)                            Tokens(B)                            Tokens(B)                            Tokens(B)                             Tokens(B)

Figure 21 Illustration of model size generalizability for the Total Loss Scaling Law. The fitting data includes model
sizes of 374M, 778M, and 1.364B. The predicted curves for the unseen model sizes of 53M and 134M closely align with
the actual curves, demonstrating the generalizability of the Total Loss Scaling Law with respect to model size.

52


===== PAGE BREAK =====

points are 0.9385, 0.9609, and 0.962, respectively. It is evident that as the number of data points increases,
the consistency between the fitted curves and the actual curves improves. In other words, if you want to
predict model performance at larger training sizes, collecting data points closer to those of larger model sizes
will yield better prediction results.

Max Recurrent Step Generalizability We have conducted a total of three different maximum recurrent steps:
2, 4, and 8. To verify the generalizability with respect to maximum recurrent step, we select two of these
as fitting data points and perform the fitting, followed by validation on the full data points and calculation
of R?. The average R? for the three sets of experiments is 0.9581, demonstrating the generalizability of the
Total Loss Scaling Law with respect to maximum recurrent step.

E.2 Generalizability for the Step-wise Loss Scaling Law

Following the same approach as in Section E.1, we seek to explore the performance of the Scaling Law on
unseen data points, specifically regarding the generalizability of the Scaling Law. In this subsection, we
explore the generalizability of the Step-wise Loss Scaling Law from three aspects: model size generalizability,
training data generalizability, and recurrent step generalizability. The evaluation metric remains the coefficient
of determination R?. To evaluate performance on unseen data points, we will calculate the coefficient of
determination using all data points, while fitting will only use a subset of the data points.

Model Size Generalizability The Scaling Law experiments include five different model sizes: 53M, 134M,
374M, 778M, and 1.364B. To verify the generalizability of model size, we select three of these as fitting data
points. In each fitting experiment, the Scaling Law does not have access to the remaining two model size data
points during fitting, ensuring the reasonableness and validity of the results through repeated experiments.
Specifically, to save on resources, we have conducted experiments only for max recurrent step of 2 and 4,
resulting in a total of (3) x 2 = 20 small experiments. The final experimental results show that for max
recurrent step of 2, the average R? value is 0.8815, while for max recurrent step of 4, the average R? value is
0.797. This difference is not significant compared to the R? values obtained from the full data points (0.8898
and 0.8146), demonstrating the generalizability of the Step-wise Loss Scaling Law with respect to model size.
To illustrate the results more clearly, we show example fitting curves in Figure 22. It is important to note
that using only a subset of data points for fitting may lead to miscalculations of the Scaling Law on unseen
data points. Due to the nature of the power law, if the values are too small, it may result in a very large
computed value, causing inaccuracies. To ensure the validity of the fitting, we can attempt to adjust the
initial fitting values or impose some constraints on the fitting algorithm. For convenience, we adjust the initial
fitting values to make the fitting formula effective over a broader range of data points.

Training Data Generalizability Following Section E.1, to ensure the validity of the fitting, we have selected
the first 25%, 50%, and 75% of the data points for fitting. In the case of max recurrent step of 2, the R?
values are 0.8686, 0.8882, and 0.8896, respectively. For max recurrent step of 4, the R? values are 0.793,
0.813, and 0.8142. It can be observed that as the number of fitting data points increases, the fitting accuracy
improves. This aligns with the intuition that fitting with more data points generally yields better results.
Additionally, these results are similar to those obtained from fitting with the full data points (0.8898 and
0.8146), demonstrating the generalizability of the Step-wise Loss Scaling Law with respect to training data.

Recurrent Step Generalizability In the case of max recurrent step equal to 2, there are only two recurrent
step values, making it unreasonable to conduct generalizability experiments. Therefore, we choose to perform
experiments with max recurrent step equal to 4. In this situation, we have four different recurrent step
values: 1, 2, 3, and 4. We randomly select three of these as fitting data points, resulting in a total of (3) =4
experiments. The average R? value obtained from these four experiments is 0.8118, which is similar to the R?
value of 0.8146 obtained from the full data points, demonstrating the generalizability of the Step-wise Loss
Scaling Law with respect to recurrent step. Figure 23 presents a specific example, showing a high degree of
consistency between the fitted curve and the actual curve.

53


===== PAGE BREAK =====

Fry

Step-wise Loss

Fry

Step-wise Loss

Fry

Step-wise Loss

Fry

Step-wise Loss

Figure 22 Illustration of model size generalizability for the Step-wise Loss Scaling Law. The fitting data comprises
three medium model sizes: 134M, 374M, and 778M. To verify the fitting consistency of the model on unseen larger
model size 1.364B and unseen smaller model size 53M, we can observe that the predicted curves reflect the trends of
the actual data points, demonstrating the generalizability of the Step-wise Loss Scaling Law with respect to the model

size.

T=1,N=53M                 T=1,N=134M                T=1,N=374M                T=1,N=778M                T=1,N=1.36B
T       T                                T       T                                T       T                                T       T                                T       T
— Real                    — Real                    — Real                    — Real                    — Real
soo= Pred                     s--= Pred                    so--) Pred                     soo= Pred                     soo= Pred
{    | | {      i                  ft                         .
1                                                              al
3      3      Fig      Fa      20                  3      Fy      Fa     20                  3      Fy      Fa      20                  3      Fig      Fa      20           3      3      Fig      Fa      20
Tokens(B)                            Tokens(B)                            Tokens(B)                            Tokens(B)                             Tokens(B)
T=2,N=53M                 T=2,N=134M                T=2,N=374M                T=2,N=778M                T=2,N=1.36B
T       r                                T       r                                r       r                                T       r                                T       r
— Real                    — Real                    — Real                    —— Real                    — Real
——-= Pred                          = Pred                          = Pred                          -—-= Pred                          “= Pred
3      3      Fi      Fa      20                  3      Fy      Fa     20                                                         3      Fi      Fa      20           3      3      Fi      Fa      20
Tokens(B)                            Tokens(B)                                                                    Tokens(B)                             Tokens(B)
T=3,N=53M                 T=3,N=134M                T=3,N=374M                T=3,N=778M                T=3,N=1.36B
T       T                                T       T                                T       T                                T       T                                T       T
— Real                    — Real                    — Real                    — Real                    — Real
soo= Pred                     s--= Pred                    s--- Pred                     soo= Pred                     soo= Pred
            +t          1                            |          }
fy      3      Fi      Fa      20                  3      Fy      Fa     20                  3      Fy      Fy      20                  3      Fi      %      20           fy      3      Fi      Fa      20
Tokens(B)                            Tokens(B)                            Tokens(B)                            Tokens(B)                             Tokens(B)
T=4,N=53M                 T=4,N=134M                T=4,N=374M                T=4,N=778M                T=4,N=1.36B
— Real                    — Real                    — Real                    — Real                    — Real
-——= Pred                     = Pred |                    = Pred                     -—-= Pred                     “= Pred |

°     3

20

Fry        Fa
Tokens(B)

5

20

Fy    Fa
Tokens(B)

5

Fy    35
Tokens(B)

54

20

5

Fry    Ey
Tokens(B)

20

°     3               20

Fry        Fa
Tokens(B)


===== PAGE BREAK =====

T=1,N=53M                                              T=1,N=374M                T=1,N=778M                T=1,N=1.36B8
T     T                                                      T                             T     T                        T

— Real                    — Real                    — Real                    — Real                    — Real
wo                = Pred |                     ---= Pred |                    ---- Pred |                     ---- Pred |                     ---- Pred |

T=1,N=134M
T

Step-wise Loss
__—

°      3      Fry      Fa      %           °      3      Fy      Fa     2           °      3      Fy      5      2          ry      3      Fry      Ey      Ey           °      3      Fry      Fa      %
Tokens(B)                            Tokens(B)                            Tokens(B)                            Tokens(B)                             Tokens(B)
T=2,N=53M              T=2,N=134M             T=2,N = 374M             T=2,N=778M             T=2,N=1.36B
— Real                    — Real                    — Real                    — Real                    — Real

0                — Pred |                     ---~ Pred |                    —~ Pred |                     — Pred |

Step-wise Loss
{             y
3
&
i

°       3       Fry      Fa      %            °       3       Fy      Fa      2            °       3       Fy      35      2            ry       3       Fry      Ey      2            °       3       Fry      Fa      %
Tokens(B)                                Tokens(B)                                Tokens(B)                                Tokens(B)                                Tokens(B)
T=3,N=53M                 T=3,N=134M                T=3,N=374M                T=3,N=778M                T=3,N=1.36B
T        1                                    T        T                                    T        T                                    T        T                                    T        1
— Real                                  — Real                                  — Real                                  — Real                                  — Real
w                          —— Pred |                                --= Pred |                                —--- Pred |                                —— Pred |                                --— Pred |

__—
|

Step-wise Loss

0     3

f

20                   0           5                         5          20                   0           5                        5          20                   0            5                        5          20                   0           3                         5          20

Fry    F:                       Fry    FY
Tokens(B)                          Tokens(B)

Fy      F
Tokens(B)
T=4,N=374M                T=4,N=778M                T=4,N=1.36B

T                                       T       1                                T       1

1
— Real                    — Real                    — Real                    — Real                    — Real
wo                ---- Pred |                     ---= Pred |                    =~ Pred |                     = Pred |                     ---- Pred |

Fy      FY

Tokens(B)

T=4,N=134M
T

Fry      Fa

Tokens(B)

T=4,N=53M
T

Step-wise Loss

hi

0           3                                    20                   0           5                                   20                   0           5                                   20                   0            5                                   20                   0                                                20

Fry        Fa                                               Fy         Fa                                               Fy         5                                               Fry         Ey                                               Fry        Fa
Tokens(B)                     Tokens(B)                     Tokens(B)                     Tokens(B)                     Tokens(B)

Figure 23 Illustration of recurrent step generalizability for the Step-wise Loss Scaling Law. The fitting data includes
three different recurrent steps: recurrent step = 1, 2, and 3. At the unseen data points of recurrent step = 4, the
predicted curve closely matches the actual curve, demonstrating the generalizability of the Step-wise Loss Scaling Law
with respect to recurrent step.

59
