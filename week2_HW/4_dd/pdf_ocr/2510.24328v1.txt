arX1v:2510.24328vl1 [cs.CL] 28 Oct 2025

Beyond MCQ: An Open-Ended Arabic Cultural QA Benchmark
with Dialect Variants

Hunzalah Hassan Bhatti, Firoj Alam
Qatar Computing Research Institute, Qatar
fialam@hbku.edu.ga, hunzalahhassan@gmail.com

Abstract
Large Language Models (LLMs) are increasingly used to answer everyday questions, yet their performance on
culturally grounded and dialectal content remains uneven across languages. We propose a comprehensive method
that (i) translates Modern Standard Arabic (MSA) multiple-choice questions (MCQs) into English and several Arabic
dialects, (ii) converts them into open-ended questions (OEQs), (iii) benchmarks a range of zero-shot and fine-tuned
LLMs under both MCQ and OEQ settings, and (iv) generates chain-of-thought (CoT) rationales to fine-tune models for
step-by-step reasoning. Using this method, we extend an existing dataset in which QAs are parallelly aligned across
multiple language varieties, making it, to our knowledge, the first of its kind. We conduct extensive experiments
with both open and closed models. Our findings show that (i) models underperform on Arabic dialects, revealing
persistent gaps in culturally grounded and dialect-specific knowledge; (ii) Arabic-centric models perform well on
MCQs but struggle with OEQs; and (iii) CoT improves judged correctness while yielding mixed n-gram-based metrics.
The developed dataset will be publicly released to support further research on culturally and linguistically inclusive

evaluation.

Keywords: Cultural Knowledge; Everyday Knowledge, Open-Ended Question, Chain-of-Thought

1. Introduction

Cultural information underpins human iden-
tity, behavior, and social interaction, en-
compassing shared beliefs, values, customs,
languages, traditions, and collective prac-
tices. In today’s tightly coupled information-
communication ecosystem, hundreds of mil-
lions of users interact with LLMs for everyday
queries, often asking about local norms, hol-
idays, cuisine, or etiquette, where culturally
grounded interpretations are essential (Pawar
et al., 2025; Hasan et al., 2025). Yet despite
rapid progress in multilingual understanding
and reasoning, LLM performance remains un-
even across languages, dialects, and culturally
specific domains (Wei et al., 2022; Muennighoff
et al., 2023). The issue is especially salient for
Arabic, where Modern Standard Arabic (MSA)
coexists with numerous regional dialects that
differ in phonology, morphology, lexicon, and
usage (Alwajih et al., 2025a; Sadallah et al.,
2025). Beyond modeling challenges, widely
used multiple-choice (MCQ) evaluations can
mask deficiencies in reasoning by enabling

McQ                                                  OEQ
TAsnal gl Aare Syl ide Cay gS) Cabran ple csi (gf    gh gle Cap gS cilaeam le col (gf
Translation: /n what year did Kuwait win its                             SA pal gf Ane

first Olympic medal?                                    In what year did Kuwait win
Options: (A) 1996 (B) 2004 (C) 1984 (D)      its first Olympic medal?
2000 Answer: 2000                                        Answer: 2000
CoT for OEQ              Ae
eB GAG All (ol NYS pal) GLa Aa Sis A Can gSll Assad gi Atlane ish et LS (ish wt) lashes Sigua
2000 co3) Absy Addl ng be caries cbapal gh cod pill

Translation: The question is factual. Historically, Kuwait’s first Olympic medal was the
bronze in shooting (trap) won by Fehaid Al-Deehani at the Sydney Olympics, which
pinpoints the year as 2000

Language varieties: Modern Standard Arabic (MSA, la), English (==), Levantine (= = =
©), Gulf(@ @ Se @ &), Egyptian (=), Maghrebi (mt & ol = Ba)

Figure 1: Example QA shown in two formats
(MCQ and OEQ). MCQ: Multiple-Choice Ques-
tion; OEQ: Open-Ended Question. Flags in
parentheses indicate representative countries
where each dialect is widely spoken.

shortcut strategies such as label bias or option-
guessing, complicating fair cross-lingual and
cross-format comparison (Raman et al., 2025;
Li et al., 2024b). A central open problem is
how to measure and improve an LLM’s abil-
ity to understand and generate responses to
such culturally embedded queries, especially
in multilingual settings with substantial dialec-
tal variation. Another noteworthy aspect is that
multiple-choice questions (MCQs) have long


===== PAGE BREAK =====

been the dominant format for evaluating QA
performance in large language models (LLMs)
due to their simplicity, automatic scoring, and
structured answer space (Myrzakhan et al.,
2024). However, models can sometimes ex-
ploit the test format rather than genuinely un-
derstanding the question, leading to a form of
selection bias, for instance, consistently favor-
ing certain options (e.g., always choosing “A”)
regardless of content.

To address these challenges, parallel efforts
have emerged to develop culturally aligned lan-
guage models (Wang et al., 2023) and to en-
able their efficient deployment in low-compute
environments (Hu et al., 2022). At the same
time, new culturally relevant datasets, targeted
benchmarks, and evaluation protocols are be-
ginning to operationalize the measurement of
everyday cultural knowledge (Myung et al.,
2024; Li et al., 2024a; Mousi et al., 2025; Alam
et al., 2025). Collectively, these trends demon-
strate the need for new resources, evaluations,
and models that are grounded in underrepre-
sented dialectal varieties and culturally contex-
tualized content.

To shade a light on the challenges, we intro-
duce a comprehensive method for developing
a new resource for under-representative lan-
guage verities. Starting from an existing MSA
MCQ dataset (Alwajih et al., 2025b), we per-
form the following steps: (i) translate the ques-
tions into several Arabic dialects and English,
(ii) convert the MCQs into open-ended ques-
tions (OEQ) that require free-form answers, (iii)
evaluate a range of zero-shot and fine-tuned
LLMs on the resulting benchmark, and (iv) cre-
ate and fine-tune models on chain-of-thought
(CoT) annotations to encourage explicit reason-
ing for O0EQ. An example of MCQ, OEQ with
CoT is shown in Figure 1.

Our approach allows us to isolate and study
the impact of question format, language vari-
ety, and reasoning supervision on model per-
formance. We find that OEQ settings present
greater challenges than MCQ, especially in di-
alectal Arabic. Our contributions are as follows:
¢ We construct a multilingual and multidialectal

QA dataset by translating MSA MCQs into

English and multiple Arabic dialects.

* We convert the dataset to OEQs in all lan-
guage variants, enabling more rigorous eval-
uation of model knowledge.

¢ We benchmark a range of zero-shot and fine-
tuned LLMs under both MCQ and OEQ set-
tings.

* We generate chain-of-thought (CoT) annota-
tions for OEQ and fine-tune models.

This work represents the first effort to unify
dialectal Arabic QA, open-ended reasoning,
and CoT fine-tuning in a single benchmark, of-
fering new insights into LLM performance on
culturally rich, linguistically diverse data.

2. Related Work

General Capabilities of LLMs. Large lan-
guage models (LLMs) have demonstrated im-
pressive general capabilities across a variety
of NLP tasks, including text generation, transla-
tion, summarization, and reasoning (Abdelali
et al., 2024). At sufficient scale, LLMs exhibit
emergent abilities, such as multi-step inference
and commonsense reasoning (Bubeck et al.,
2023; Wei et al., 2022). Prompting techniques
like few-shot and chain-of-thought (CoT) sig-
nificantly enhance performance on reasoning-
heavy tasks (Kojima et al., 2022; Wei et al.,
2022). However, most evaluations focus on
English or high-resource languages. Perfor-
mance often degrades on morphologically rich
or low-resource languages such as Arabic, par-
ticularly in dialectal contexts (Mousi et al., 2025;
Muennighoff et al., 2023).

Cultural and Everyday Knowledge. Recent
research has highlighted the limitations of
LLMs in capturing culturally grounded, every-
day knowledge. Myung et al. (2024) intro-
duced BLEnD, a multilingual benchmark com-
prising 52.6K QA pairs across 13 languages
and 16 regions, designed to evaluate models’
understanding of daily-life knowledge. Simi-
larly, Hasan et al. (Hasan et al., 2025) devel-
oped MultiNativQA, featuring 64K QA pairs
covering nine locations in seven languages.
Across these studies, results consistently show
that LLMs underperform on questions reflect-
ing underrepresented cultures, often default-


===== PAGE BREAK =====

ing to Western-centric norms. In the Arabic
context, Sadallah et al. (2025) proposed ARAB-
CuLTurRE, a benchmark of 3.5K MSA-based
multiple-choice questions authored by native
speakers from 13 Arab countries to assess cul-
turally specific commonsense reasoning. Like-
wise, Alwajih et al. (2025a) introduced Pat, a
dialect-rich instruction dataset encompassing
all 22 Arab countries.

Challenges in Converting MCQ to OEQ.
Many evaluation benchmarks use MCQs be-
cause they allow straightforward automatic
scoring, in which the model selects an op-
tion (A/B/C/D) that can be directly compared
with the correct answer. However, recent stud-
ies show that this format may introduce arti-
ficial performance gains and mask a model’s
actual reasoning ability (Molfese et al., 2025;
Chandak et al., 2025; Myrzakhan et al., 2024).
For instance, LLMs often display a selection
bias, favoring certain options (e.g., consis-
tently choosing “A”) due to training artifacts. To
mitigate these issues, several works propose
converting MCQs into OEQs that require the
model to generate answers without predefined
choices (Myrzakhan et al., 2024). This forces
reliance on internal Knowledge and reasoning
rather than elimination or guessing. Yet, this
conversion introduces new challenges: some
MCQs become ambiguous once options are
removed, and others may yield multiple valid
answers unless carefully rephrased. Moreover,
evaluating free-form responses is inherently
harder, as correctness depends on compar-
ing generated text with gold answers that may
differ in wording. Prior work addresses this
by using LLM-based evaluation pipelines (e.g.,
GPT-4) to judge open-ended answers against
human references with high reliability (Myrza-
khan et al., 2024). Overall, shifting from MCQ
to open-ended formats holds promise for re-
vealing deeper model understanding, but it de-
mands careful question selection and robust
evaluation protocols.

Chain-of-Thought (CoT) Reasoning. Chain-
of-thought (Col) prompting has emerged as
a powerful technique for enhancing reasoning

capabilities in large language models (LLMs).
Instead of producing an answer directly, the
model is encouraged to generate an explicit,
step-by-step reasoning path before reaching
a final conclusion (Wei et al., 2022). By artic-
ulating these intermediate steps, models can
decompose complex problems into manage-
able components, leading to substantial gains
in accuracy. Remarkably, even without task-
specific training, simply prefixing the prompt
with “Let’s think step by step” can induce this
behavior in sufficiently large models, a method
known as zero-shot CoT (Qin et al., 2023).
This simple prompting strategy has demon-
strated significant improvements across a wide
range of reasoning tasks, including mathemat-
ical problem solving and commonsense rea-
soning. Furthermore, Qin et al. (2023) intro-
duced a self-consistency mechanism, in which
the model generates multiple reasoning chains
and selects the most frequent answer, further
enhancing performance. While most existing
studies emphasize inference-time CoT, recent
research has explored CoT fine-tuning to trans-
fer reasoning skills to smaller or multilingual
models (Puerto et al., 2025). However, to the
best of our Knowledge, no prior work has ap-
plied CoT fine-tuning to Arabic open-ended QA
datasets, particularly those covering dialectal
varieties, which constitutes a key contribution
of our study.

3. Datasets

Our data is based on the PalmX 2025 - Gen-
eral Culture Evaluation (PalmX-GC) dataset,
which assesses a model’s understanding of
Arab culture, including customs, history, geog-
raphy, arts, cuisine, notable figures, and ev-
eryday life across the 22 Arab League coun-
tries. All questions and answers are written in
MSA and manually verified, providing a robust
benchmark for culturally grounded QA (Alwajih
et al., 2025b). The dataset comprises 2,000
training, 500 development, and 2000 test exam-
ples, all in MCQ format. We use PalmX-GC as
the foundation for creating dialectal MCQ and
OEQ variants. Figure 2 illustrates the dataset
construction process. In the entire pipeline,


===== PAGE BREAK =====

Manual
Checking

Dialectal MCQ
MSA to Dialects

Dialectal OEQ

MCQ to OEQ                         MSA to Dialects

Figure 2: Pipeline for the dataset construction
process.

we used LLMs (specifically GPT-4.1) for both
translation and data conversion. Our choice of
this model was primarily based on its reliability
and our available paid access.

3.1. Dialectal MCQ

To broaden cultural and linguistic coverage
beyond MSA, we translate PalmX into four
Arabic dialect Egyptian, Levantine, Gulf, and
Maghrebi, and into English GPT-4.1 with a
quality check. These dialects were selected
because (i) they collectively cover the largest
speaker populations and the widest geographic
spread across the Arab world; (ii) they cap-
ture major points on the Arabic dialect con-
tinuum with substantial lexical, morphological,
and pragmatic divergence from MSA; and (iii)
they are the primary medium of everyday com-
munication and online discourse, where cul-
turally grounded queries naturally occur. In-
cluding English serves two purposes: it pro-
vides a lingua-franca baseline for cross-lingual
comparison (disentangling language modeling
from culture-specific knowledge) and reflects
real usage, where users often pose culturally
focused questions in English about Arabic con-
texts. This design enables us to probe (a) for-
mat sensitivity (MCQ—+OEQ), (b) dialect sensi-
tivity (MSA vs. regional varieties), and (c) cross-
lingual transfer (Arabic<+English) within a sin-
gle, controlled benchmark.

We employed controlled prompting to trans-
late each MSA MCQ into four dialects and En-
glish. The prompts explicitly enforced semantic
equivalence while allowing lexical and stylistic
adaptation to dialectal norms. This approach
ensured that the dialectal phrasing preserved

the original question’s intent without causing
any semantic drift from its MSA counterpart.

3.2. MCQ to OEQ

We converted the MSA MCQs into OEQs us-
ing GPT-4.1. Each MCQ was transformed into
a natural question—answer pair by rephrasing
the original question and its correct option into
a single, self-contained QA instance. The re-
maining distractors were used only to guide
contextual understanding but were excluded
from the final prompt. We filtered out QA items
where conversion was structurally infeasible,
such as questions dependent on visible alter-
natives, to avoid ill-posed or underspecified
open-ended forms. This process ensured that
the resulting OEQs were faithful derivations
of verified MCQs rather than arbitrary genera-
tions.

3.3. Dialectal OEQ

We then translated the OEQs into dialectal vari-
ants using similar controlled prompting princi-
ples, promoting authentic dialectal translation
while preserving both semantic and pragmatic
fidelity to the original MSA version. The result-
ing dataset constitutes a parallel corpus span-
ning five dialects and English, each aligned
with consistent cultural grounding and verified
equivalence. This parallel structure enables
systematic evaluation of dialectal reasoning
and transfer capabilities in generative settings.

3.4. OEQ with CoT

Inspired by prior work (Yu et al., 2025; Zelik-
man et al., 2022), we transform each OEQ
instance x = (q,a*) € dataset D, where q de-
notes the question and a* the gold or reference
answer, into one or more CoT training sam-
ples using a four-stage pipeline. The pipeline
generates multiple reasoning chains without
revealing the gold answer, optionally produces
gold-conditioned rationalizations, and verifies
accepted chains. While generating Cols, we
also prompt the LLM to classify each q as either
factual or subjective. Identifying the question
type enables type-specific model development


===== PAGE BREAK =====

and evaluation. For instance, factual questions
may require reference evidence or source attri-
bution for their answers.

Preliminaries. Let N      =      samples
be the number of chain attempts,
T = rationalize_target the minimum

number of gold-aligned chains to retain, and
p = accept_ratio € (0,1] the acceptance
threshold. For each attempt € {1,...,N}, we
obtain (é;, a;,@;), where ¢; € C is a generated
chain-of-thought, a; ¢ Y is the generated
answer, and £; € £ = {factual, subjective} is a
label. Let also denote the collection of attempts
as S = {(é;, a, €;)}®_, and the accepted sub-
set as K C S. Acceptance is determined via
a matching function match : Y x Y > {0,1}
(see Matcu) against the gold answer a’, with
indicator m; = I{match(a;,a*) = 1}. We
enforce |K| > T and the empirical acceptance
ratio |K|/N > p.

1. CoT generation. LetG:Q>CxYxCL
denote the rationale-answer-label generator.
For each g € O, we sample (¢;, @;, 2;) = G(q).
For each attempt, compute the match flag
m, = Wmatch(4;, a*) = 1} and collect the kept
subset K = {(é;,4:,6;) € S: mj; = 1}.

2. CoT rationalize with gold. Let R : Q x
yY—>Cx yx L£ denote the gold-conditioned
rationalizer. If |] < T (as obtained in Step 1),
we draw additional chains via (¢;,@;,¢;) =
R(q, a*) and retain those that match the gold
answer: m,; = I{match(a;,a*) = 1}, K =
{(é,4;,0)) : m; = 1}. We then update
K <—KUK until || > T (and, if applicable,
|K|/N > p). This stage ensures a sufficient
pool of gold-aligned CoT for downstream use.

3. Verification. Let VV :QOx YVxCx
Y — [0,1] x {pass, fail} x Report denote the
verifier, where the two Y components corre-
spond to the gold answer a* and the candidate
answer a,, respectively. For each retained
item (cr, az, £e) € K, compute (o4,%,7K) =
V(q,a*,Cr, ax), where ox € [0,1] is a con-
fidence score, 1, € {pass, fail} is the ver-
dict under a default threshold + = 0.8 (i.e.,
Vv, = pass iff o, > 7), and r,; is a brief is-
sue report. We then form the verified subset

Kyer = {(ck, an, lk) € K : vy = pass}, noting
that ¢; is carried forward but not used by V.

Answer matching match. We follow very
weak answer matching approach. Given a gen-
erated answer a and gold a’, define match :
Yxy > {0,1} by match(a,a*) = 1 iff at
least one holds: (i) exact normalized equal-
ity, norm(@) = norm(a*); (ii) high token Jac-
card, J(P,G) = ma PooH > 0.75, where
P = tokset(@) and G = tokset(a*); (iii) small-
set containment, (|P| <6A PCG) v (|G|<
6AGC P); (iv) high character similarity,
sim(norm(4@), norm(a*)) > 7, with 7 = 0.88 and
sim Computed sequence matching algorithm!
Otherwise, match(a, a*) = 0.

To facilitate the answer matching, we
use language-aware normalization norm(-).
For Arabic, we remove diacritics, and
drop non-{Arabic _ letters/digits/_} — char-
acters.    For non-Arabic, we apply uni-
code normalization, lowercase, and re-
move non-{a—z, 0-9} characters. We set
tokset(s) = set(norm(s) split on spaces).

3.5. Manual Checking

As shown in Figure 2, we conducted a targeted
manual evaluation on small samples from each
task (e.g., dialectal translation checks and
MCQ-+OEQ conversions). For each dialect,
one native Arabic speaker (fluent in English) re-
viewed the items. Annotators participated on a
voluntary basis. We acknowledge that having
a single annotator per dialect is a limitation;
nonetheless, this pass provided a practical,
low-latency quality screen before larger-scale
studies.

Our evaluation rubrics capture complemen-
tary aspects of downstream utility: (i) Dialectal
naturalness for sociolinguistic authenticity, (ii)
Meaning preservation for semantic fidelity, (iii)
Logical coherence to avoid ill-posed or incon-
sistent items, (iv) Question-type appropriate-
ness to ensure valid MCQ—OEQ conversion,

"https://docs. python. org/3/library/
difflib. html


===== PAGE BREAK =====

and (v) Linguistic quality and clarity for gram-
maticality and readability. We use a five-point
Likert scale as it provides (a) sufficient gran-
ularity to capture meaningful differences, (b)
symmetric anchors with a neutral midpoint for
ambiguity, and (c) easy aggregation across an-
notators and tasks. Each dimension is rated
onl,...,5.

Each rubric is briefly described below.
Dialectal naturalness: Do the question and
(if applicable) options sound authentic, fluent,
and idiomatic in the target dialect?
Meaning preservation: Does the OEQ/-
translation convey the same meaning and
intent as the original MCQ/source?

Logical coherence: Are the question and (if
applicable) options logically consistent, fac-
tually sound, and contextually appropriate?
Question-type appropriateness: Is the
chosen format valid for the content (MCQs
are decidable by selection; OEQs are gen-
uinely open-ended)?

Linguistic quality and clarity: Are gram-
mar, wording, and orthography correct and
easily understood by native speakers?
Table 1 summarizes the manual annotation
results, with an overall average of 4.4, between
mostly true and true. Maghrebi received the
highest average - 4.8, while English scored low-
est - 4.1. Meanwhile, for MSA MCQ to OEQ
transformation performed well - 4.3, largely pre-
serving meaning and structure.

Metric            MSA Lv Eg Gf En Mg

Dialectal naturalness       4.2 44 43 43 4.2 4.7
Meaning preservation       46 46 4.3 43 4.0 4.7
Logical coherence         42 44 43 43 40 48

Question-type appropriateness 4.1 4.7 4.3 44 42 48
Linguistic quality and clarity   42 4544 44 40 48

Average              43 45 43 44 41 48

Table 1: Average Likert score from manual
annotations on a sample of 50 dialectal MCQs
and MSA OEQs derived from MSA MCQs. Lv:
Levantine, Eg: Egyptian, Gf: Gulf, En: English,
Mg: Maghrebi.

4. Experiments

Models. For the experiments, we used a
range of open and closed-source multilin-

gual and Arabic-centric models, covering ca-
pacities from small open models to frontier
systems. The models include Falcon3-10B-
Instruct (Malartic et al., 2024), NileChat-3B
(Mekki et al., 2025), Fanar-1-9B-Instruct (Team
et al., 2025), Qwen2.5-3B and Qwen2.5-7B
(Wang et al., 2024), GPT-4.1 and GPT-5 (Ope-
nAl, 2025), and ALLaM-7B-Instruct-preview
(Bari et al., 2025). This selection covers both
high-performing proprietary and open models
under 10B parameters, suitable for controlled
fine-tuning and reproducible evaluation.

Benchmarking. All models were evaluated
in a zero-shot setting across multiple language
varieties. Prior work on cross-lingual prompt-
ing (Kmainasi et al., 2025) has shown that
non-native (English) prompts consistently out-
perform native prompts in reasoning and fac-
tual tasks, even for Arabic-centric models-while
mixed prompts yield intermediate results. Fol-
lowing these findings, all model evaluations
in this study were conducted using English
prompts.

For the MCQ evaluation, a structured prompt-
ing template was designed to present the ques-
tion along with its answer options. For OEQ,
a separate open-ended prompting template is
used to generate responses.

Training. We adopt fine-tuning configura-
tions consistent with prior work on Arabic cul-
tural QA tasks, as reported in (Bhatti et al.,
2025). Fine-tuning is conducted over 3 epochs
using LoRA adapters (Hu et al., 2022), with a
maximum sequence length of 512 for MCQ
training and 2048 for OEQ training. The
learning rate is set to 2 x 10~+, with a LoRA
rank of 64 and a = 16. All models are
fine-tuned for MCQ evaluation, while only
ALLaM-7B-Instruct-preview is fine-tuned for
the OEQ task.

Evaluation and Metrics. For MCQ, we re-
port accuracy, which is an standard metric for
MCQ. For OEQ, we employ semantic evalu-
ation using BERTScore (Zhang et al., 2020)
and ROUGE-L (Lin, 2004) to assess precision,
recall, and overall semantic overlap with the


===== PAGE BREAK =====

Model                            MSA_ Egyptian Levantine Magrebi Gulf [English Average
Falcon3-10B-Instruct             46.05 43.95       44.10       42.70 45.15 66.50      48.48
Falcon3-10B-Instruct FT           57.65 55.15         54.25        53.60 55.95 71.90       58.17
NileChat-3B                      67.55 64.75       64.65       64.45 66.00 65.15      65.00
NileChat-3B FT                    69.20 67.75        67.65       66.90 67.45 69.05      67.76
Fanar-1-9B-Instruct                     65.75 62.95           62.40          61.00 61.45 65.30        62.62
Fanar-1-9B-Instruct FT              72.55 69.85         70.55         69.70 70.75 72.65       70.70
Qwen2.5-3B                                59.65 53.70           54.50          52.65 54.85 61.50        55.44
Qwen2.5-3B FT                  63.75 62.80       62.80       62.45 62.60 69.55      64.04
Qwen2.5-7B                                61.95 60.25           60.65          57.05 60.60 65.15        60.74
Qwen2.5-7B FT                  67.50 65.85       65.95       63.25 66.00 71.50      66.51
ALLaM-7B-Instruct-preview      67.25     65.70       64.90       64.35 66.20 62.15      64.66
ALLaM-7B-Instruct-preview FT 71.95 70.55        69.85       69.85 70.40 67.70      69.67
Avg. Arabic-Centric           66.85 64.47       63.98      63.27 64.55 64.20     64.09
Avg. Arabic-Centric FT         71.23 69.38       69.35       68.82 69.53 69.80      69.38
Avg. Base All                    61.37 58.55       58.53       57.03 59.04 64.29      59.49
Avg. FT All                       67.10 65.33       65.18       64.29 65.53 70.39      66.14
GPT-4.1                          7742 79.08       78.29       80.24 79.33 78.57      79.10
GPT-5                            79.59 79.10       78.88       77.70 79.31 77.17      78.43

Table 2: MCQ accuracy (%) across different language variants. Fine-tuned models (FT) are
shaded in light blue, GPT models in gray, and averages for Arabic-centric models (NileChat,
Fanar, ALLaM) are highlighted in light green. Bold values indicate the best-performing open

model per dialect.

Model          MSA   Egyptian Levantine Magrebi   Gulf   English Average

Fi RL Ft RL F1 RL F1 RE F1 RE F1 RE Ft RL
Falcon3-10B-Instruct 0.43 0.12 0.41 0.10 0.41 0.09 0.41 0.09 0.41 0.10 0.54 0.23 0.44 0.12
NileChat-3B     0.48 0.17 0.49 0.18 0.50 0.17 0.50 0.18 0.49 0.17 0.49 0.15 0.49 0.17
Fanar-1-9B-Instruct 0.52 0.20 0.50 0.17 0.50 0.16 0.50 0.17 0.51 0.17 0.53 0.18 0.51 0.18
Qwen2.5-3B     0.45 0.13 0.43 0.11 0.43 0.10 0.44 0.11 0.44 0.11 0.47 0.11 0.44 0.11
Qwen2.5-7B     0.55 0.24 0.51 0.20 0.51 0.19 0.53 0.21 0.52 0.20 0.53 0.20 0.53 0.21
ALLaM-7B-Instruct 0.49 0.20 0.47 0.16 0.47 0.15 0.46 0.15 0.48 0.17 0.52 0.22 0.48 0.17
Avg. Arabic-Centric 0.50 0.19 0.49 0.17 0.49 0.16 0.49 0.17 0.49 0.17 0.51 0.18 0.50 0.17
Avg. Base      0.49 0.18 0.47 0.16 0.47 0.14 0.47 0.15 0.47 0.15 0.51 0.18 0.48 0.16
GPT-4.1       0.55 0.27 0.53 0.24 0.53 0.21 0.54 0.24 0.54 0.24 0.56 0.25 0.54 0.24
GPT-5        0.57 0.28 0.54 0.24 0.54 0.22 0.55 0.24 0.55 0.25 0.54 0.22 0.55 0.24

Table 3: OEQ performance

across different language variants. Averages for Arabic-centric

models (NileChat, Fanar, ALLaM) are highlighted in light green. GPT models are shaded in gray.

gold answers. Arabic responses are evaluated
using arabert-v2 (Antoun et al., 2020), and
English responses with bert-base-uncased.
This setup allows direct comparability between
multilingual and dialectal outputs across all
evaluated models. Additionally, for OEQ, we
use GPT-4.1 as LLM-as-judge following MT-
Bench (Bai et al., 2024), where responses are
rated on a 1 to 10 rubric (helpfulness, rele-
vance, accuracy, faithfulness).

5. Results

We compare performance across four condi-
tions: (i) MCQ base vs. fine-tuned, (ii) OEQ
base, (iii) OEQ fine-tuned without CoT, and (iv)

OEQ fine-tuned with CoT. Tables 2, 3, and 4

present the results for the

MCQ, OEQ, and

OEQ (with vs. without CoT) evaluations, re-

spectively.
Lang          Base                   FT              FT with COT
J  F1 RL  J  Fi RL  J  F1. RL
MSA |5.50 0.49 0.20/|6.02 0.76 0.56|6.12 0.70 0.48
Eg  4.93 0.47 0.16)/5.90 0.71 0.46/6.10 0.66 0.41
Lv  4.95 0.47 0.15/5.93 0.70 0.45/6.13 0.66 0.40
Mg  4.80 0.46 0.15) 5.88 0.70 0.45/6.08 0.65 0.39
Gf  4.97 0.48 0.17)5.94 0.70 0.45|6.14 0.66 0.41
En  4.49 0.52 0.22/5.55 0.74 0.57|5.48 0.67 0.43
Avg. | 4.94 0.48 0.17 | 5.87 0.72 0.49]6.01 0.67 0.42

Table 4: Performance on

the OEQ across

ALLaM-7B base, fine-tuned (FT), and fine-
tuned with CoT models. J: LLM-as-a-judge.


===== PAGE BREAK =====

Performance Gap for MCQ. As presented in
Table 2, the average performance among the
Arabic language variants is relatively higher for
MSA across open models, followed by Gulf,
Egyptian, and others. The average perfor-
mance for English is higher compared to Ara-
bic across open models, mainly due to the
strong performance of non-Arabic-centric mod-
els such as Falcon and Qwen. The average
performance for Arabic-centric models in the
base and fine-tuned (FT) settings is 64.09%
and 69.38%, respectively.

The performance of closed models (i.e.,
GPT*) are higher than closed models in all
language variants. The MCQ performance
for MSA is highly comparable with the Palmx
shared task results where top-system achieved
an accuracy of 72.15% (Alwajih et al., 2025b).

Among the smaller open models (i.e.,
size 3B), in the base setting, NileChat-3B
achieves the highest average accuracy of
65.43, while Fanar-1-9B-Instruct is the best-
performing fine-tuned model with an accu-
racy of 71.01. Among the open models,
the fine-tuned ALLaM-7B-Instruct performs
best for Egyptian and Maghrebi, whereas
Fanar-1-9B-Instruct-FT achieves the highest
performance for MSA, Le, Gf, and En.
Performance Gap for OEQ. Across language
variants, we observe a pattern consistent with
MCQ results: the average F1 for MSA ex-
ceeds that of other Arabic dialects; however,
the gap is smaller than in the MCQ setting
(Table 3). Similarly, English attains higher F1
than the Arabic variants. Notably, for OEQ, the
Qwen2.5-7B open model outperforms the other
open models, including Arabic-centric ones.

Among all base models, GPT-5 achieves the
highest overall performance (F'1 = 0.55), fol-
lowed closely by GPT-4.1 (Fl = 0.54). GPT-5
performs best on MSA, while GPT-4.1 shows
strong results on both English and MSA.

Did CoT help for OEQ? In Table 4, we report
the performance of OEQ with a comparison to
the base model, fine-tuning without CoT (FT),
and fine-tuning with CoT. Other than F1 and
Rouge-L score, we also report LLM-as-a-judge
scores. On token-overlap metrics, FT yields
the strongest scores (F1/RL), whereas the CoT-

tuned model attains the highest average LLMV-
as-a-judge score. This divergence indicates
that Col improves semantic acceptability but
reduces /exical overlap with the references. A
manual pass over low-F1 cases shows that the
CoT model frequently returns briefer answers
that judges deem correct, yet they share fewer
n-grams with the (often longer) gold strings,
decreasing F1 and RL. Overall, CoT helps on
judged correctness but not on n-gram overlap.

This pattern aligns with prior findings that
CoT is not uniformly beneficial. For instance,
Zhu et al. (2025) show that adding rationales
can sometimes hurt performance, while Li et al.
(2025) find that fine-tuning smaller models
on lengthy, teacher-generated CoT traces per-
forms no better, or worse, than training without
Col. Together with our results, these observa-
tions highlight the need to examine when CoT
is advantageous, particularly regarding task
type, rationale length, and model size.

6. Conclusions and Future Work

We presented a comprehensive pipeline for
converting MCQ into OEQ and extended an
existing MCQ dataset into OEQ across mul-
tiple language varieties, including MSA, En-
glish, and several Arabic dialects. To our knowl-
edge, this is the first Arabic cultural OEQ re-
source with parallel dialectal variants along-
side English, providing a foundation for cultur-
ally grounded evaluation beyond MSA. Our re-
source also differs from related datasets such
as Palm and Palmx: in our case, QAs are par-
allelly aligned across all language variants. We
benchmarked the dataset using a spectrum of
open, closed, and fine-tuned models. Fine-
tuned models consistently outperform their
base counterparts yet still trail strong closed
models; performance is generally higher for
MSA than for dialects. Arabic-centric mod-
els show advantages on Arabic variants for
MCQ but smaller gains on OEQ, highlighting
the added difficulty of generative, culturally
grounded answering. Our initial CoT results
improve judged correctness but yield mixed
n-gram—based scores. Future work includes
broader human validation, variety-aware nor-


===== PAGE BREAK =====

malization and scoring and extensions to other
low-resource languages and modalities.

7. Ethics statement

We do not anticipate ethical concerns arising
from this work. We build on publicly avail-
able datasets that permit research use, and
we comply with their licenses and terms. For
the manual annotations, contributors partici-
pated voluntarily after being fully briefed on the
task and its purpose. No personal or sensitive
data were collected beyond what is contained
in the source datasets.

8. Limitations

Our extensions to publicly available Arabic-
dialect datasets rely on LLM-assisted trans-
lation and MCQ->0EQ conversion, which may
introduce modeling biases (e.g., paraphrase
drift, dialectal normalization) and occasional
errors. Due to limited annotation capacity, we
performed manual checks on small samples
rather than exhaustive human verification. A
full, dialect-sensitive manual check across va-
rieties remains future work and would substan-
tially improve the dataset’s reliability and utility
for benchmarking dialectal cultural knowledge.

Ahmed Abdelali, Hamdy Mubarak, Shammur
Chowdhury, Maram Hasanain, Basel Mousi,
Sabri Boughorbel, Samir Abdaljalil, Yassine
El Kheir, Daniel Izham, Fahim Dalvi, Majd
Hawasly, Nizi Nazar, Youssef Elshahawy,
Ahmed Ali, Nadir Durrani, Natasa Milic-
Frayling, and Firoj Alam. 2024. LAraBench:
Benchmarking Arabic Al with large language
models. In Proceedings of the 18th Confer-
ence of the European Chapter of the Associ-
ation for Computational Linguistics (Volume
1: Long Papers), pages 487-520, St. Ju-
lian’s, Malta. Association for Computational
Linguistics.

Firoj Alam, Md Arid Hasan, Sahinur Rahman
Laskar, Mucahid Kutlu, and Shammur Absar

Chowdhury. 2025. NativQA Framework: En-
abling Ilms with native, local, and everyday
knowledge. arXiv preprint arXiv:2504.05995.

Fakhraddin Alwajin, Abdellan El Mekki,
Samar Mohamed Magdy, AbdelRahim A.
Elmadany, Omer Nacar, El Moatez Billah
Nagoudi, Reem Abdel-Salam, Hanin Atwany,
Youssef Nafea, Abdulfattah Mohammed
Yahya, Rahaf Alhamouri, Hamzah A. Al-
sayadi, Hiba Zayed, Sara Shatnawi, Serry
Sibaee, Yasir Ech-chammakhy, Walid
Al-Dhabyani, Marwa Mohamed Ali, Imen
Jarraya, Anmed Oumar El-Shangiti, Aisha
Alraeesi, Mohammed Anwar AL-Ghrawi,
Abdulrahman S. Al-Batati, Elgizouli Mo-
hamed, Noha Taha Elgindi, Muhammed
Saeed, Houdaifa Atou, Issam Ajit Yahia,
Abdelhak Bouayad, Mohammed Machrouh,
Amal Makouar, Dania Alkawi, Mukhtar Mo-
hamed, Safaa Taher Abdelfadil, Amine Ziad
Ounnoughene, Anfel Rouabhia, Rwaa
Assi, Ahmed Sorkatti, Mohamedou Cheikh
Tourad, Anis Koubaa, Ismail Berrada,
Mustafa Jarrar, Shady Shehata, and
Muhammad Abdul-Mageed. 2025a. Palm: A
culturally inclusive and linguistically diverse
dataset for Arabic LLMs. In Proceedings of
the 63rd Annual Meeting of the Association
for Computational Linguistics (Volume 1:
Long Papers), pages 32871-32894, Vienna,
Austria. Association for Computational
Linguistics.

Fakhraddin Alwajih, Abdellah El Mekki, Hamdy
Mubarak, Majd Hawasly, Abubakr Mohamed,
and Muhammad Abdul-Mageed. 2025b.
PalmX 2025: The first shared task on bench-
marking Ilms on arabic and islamic culture.
In Proceedings of the Third Arabic Natural
Language Processing Conference (Arabic-
NLP 2025), Suzhou, China. Association for
Computational Linguistics. Co-located with
EMNLP 2025, November 5-9.

Wissam Antoun, Fady Baly, and Hazem Hajj.
2020. AraBERT: Transformer-based model
for Arabic language understanding. In Pro-
ceedings of the 4th Workshop on Open-
Source Arabic Corpora and Processing


===== PAGE BREAK =====

Tools, with a Shared Task on Offensive Lan-
guage Detection, pages 9-15, Marseille,
France. European Language Resource As-
sociation.

Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He,
Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin,
Wenbo Su, Tiezheng Ge, Bo Zheng, and
Wanli Ouyang. 2024. MT-bench-101: A fine-
grained benchmark for evaluating large lan-
guage models in multi-turn dialogues. In
Proceedings of the 62nd Annual Meeting of
the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 7421-
7454, Bangkok, Thailand. Association for
Computational Linguistics.

M Saiful Bari, Yazeed Alnumay, Norah A.
Alzahrani, Nouf M. Alotaibi, Hisham Abdullah
Alyahya, Sultan AlRashed, Faisal Abdulrah-
man Mirza, Shaykhah Z. Alsubaie, Hassan A.
Alahmed, Ghadah Alabduljabbar, Raghad
Alkhathran, Yousef Almushaygih, Raneem
Alnajim, Salman Alsubaihi, Maryam Al Man-
sour, Saad Amin Hassan, Dr. Majed Al-
rubaian, Ali Alammari, Zaki Alawami, Ab-
dulmohsen Al-Thubaity, Anmed Abdelaii,
Jeril Kuriakose, Abdalghani Abujabal, Nora
Al-Twairesh, Areeb Alowisheq, and Haidar
Khan. 2025. ALLam: Large language mod-
els for arabic and english. In The Thirteenth
International Conference on Learning Rep-
resentations.

Hunzalah Hassan Bhatti, Youssef Ahmed,
Md Arid Hasan, and Firoj Alam. 2025. Cul-
tranai at palmx 2025: Data augmentation for
cultural knowledge representation.

Sébastien Bubeck, Varun Chandrasekaran,
Ronen Eldan, Johannes Gehrke, Eric
Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee,
Yuanzhi Li, Scott Lundberg, Harsha Nori,
Hamid Palangi, Marco Tulio Ribeiro, and
Yi Zhang. 2023. Sparks of artificial general
intelligence: Early experiments with GPT-4.
Technical report, Microsoft Research.

Nikhil Chandak, Shashwat Goel, Ameya
Prabhu, Moritz Hardt, and Jonas Geiping.

2025. Answer matching outperforms mul-
tiple choice for language model evaluation.
arXiv preprint arXiv:2507.02856.

Md. Arid Hasan, Maram Hasanain, Fatema
Ahmad, Sahinur Rahman Laskar, Sunaya
Upadhyay, Vrunda N Sukhadia, Mucahid
Kutlu, Shammur Absar Chowdhury, and Firoj
Alam. 2025. NativQA: Multilingual culturally-
aligned natural query for LLMs. In Findings
of the Association for Computational Linguis-
tics: ACL 2025, pages 14886-14909, Vi-
enna, Austria. Association for Computational
Linguistics.

Edward J Hu, Yelong Shen, Phillip Wallis,
Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
Lu Wang, Weizhu Chen, et al. 2022. Lora:
Low-rank adaptation of large language moa-
els. ICLR, 1(2):3.

Mohamed Bayan Kmainasi, Rakif Khan, Ali Ez-
zat Shahroor, Boushra Bendou, Maram
Hasanain, and Firoj Alam. 2025. Native vs
non-native language prompting: A compar-
ative analysis. In Web Information Systems
Engineering — WISE 2024, pages 406-420,
Singapore. Springer Nature Singapore.

Takeshi Kojima, Shixiang Shane Gu, Machel
Reid, Yutaka Matsuo, and Yusuke lwasawa.
2022. Large language models are zero-shot
reasoners. Advances in neural information
processing systems, 35:22199-22213.

Cheng Li, Damien Teney, Linyi Yang, Qing-
song Wen, Xing Xie, and Jindong Wang.
2024a. CulturePark: Boosting cross-cultural
understanding in large language models. In
Aavances in Neural Information Processing
Systems, volume 37, pages 65183-65216.

Wangyue Li, Liangzhi Li, Tong Xiang, Xiao Liu,
Wei Deng, and Noa Garcia. 2024b. Can
multiple-choice questions really be useful
in detecting the abilities of Ilms? In Pro-
ceedings of the 2024 Joint International Con-
ference on Computational Linguistics, Lan-
guage Resources and Evaluation (LREC-
COLING 2024), pages 2819-2834.


===== PAGE BREAK =====

Yuetai Li, Xiang Yue, Zhangchen Xu, Fengging
Jiang, Luyao Niu, Bill Yuchen Lin, Bhaskar
Ramasubramanian, and Radha Pooven-
dran. 2025. Small models struggle to
learn from strong reasoners. arXiv preprint
arXiv:2502. 12143.

Chin-Yew Lin. 2004. ROUGE: A package for
automatic evaluation of summaries. In Text
Summarization Branches Out: Proceedings
of the ACL Workshop.

Quentin Malartic, Nilabhra Roy Chowdhury,
Ruxandra Cojocaru, Mugariya Farooq, Giu-
lia Campesan, Yasser Abdelaziz Dahou Djji-
lali, Sanath Narayan, Ankit Singh, Maksim
Velikanov, Basma El Amel Boussaha, et al.
2024. Falcon2-11b technical report. arXiv
preprint arXiv:2407.14885.

Abdellah El Mekki, Houdaifa Atou, Omer
Nacar, Shady Shehata, and Muhammad
Abdul-Mageed. 2025. Nilechat: Towards
linguistically diverse and culturally aware
Ilms for local communities. arXiv preprint
arXiv:2505. 18383.

Francesco Maria Molfese, Luca Moroni, Luca
Gioffré, Alessandro Sciré, Simone Conia,
and Roberto Navigli. 2025. Right an-
swer, wrong score: Uncovering the in-
consistencies of Ilm evaluation in multiple-
choice question answering. arXiv preprint
arXiv:2503. 14996.

Basel Mousi, Nadir Durrani, Fatema Ahmad,
Md Arid Hasan, Maram Hasanain, Tameem
Kabbani, Fahim Dalvi, Shammur Absar
Chowdhury, and Firoj Alam. 2025. AraDiCE:
Benchmarks for dialectal and cultural capa-
bilities in LLMs. In Proceedings of the 31st
International Conference on Computational
Linguistics, pages 4186—4218, Abu Dhabi,
UAE. Association for Computational Linguis-
tics.

Niklas Muennighoff, Thomas Wang, Lintang
Sutawika, Adam Roberts, Stella Biderman,
Teven Le Scao, M Saiful Bari, Sheng Shen,
Zheng Xin Yong, Hailey Schoelkopf, Xian-
gru Tang, Dragomir Radev, Alham Fikri Aji,

Khalid Almubarak, Samuel Albanie, Zaid
Alyafeai, Albert Webson, Edward Raff, and
Colin Raffel. 2023. Crosslingual generaliza-
tion through multitask finetuning. In Proceed-
ings of the 61st Annual Meeting of the As-
sociation for Computational Linguistics (Vol-
ume 1: Long Papers), pages 15991-16111,
Toronto, Canada. Association for Computa-
tional Linguistics.

Aidar Myrzakhan, Sondos Mahmoud Bsharat,

and Zhigiang Shen. 2024.  Open-llm-
leaderboard:   From multi-choice to
open-style questions for Ilms_ evalua-

tion, benchmark, and arena. arXiv preprint
arXiv:2406.07545.

Junho Myung, Nayeon Lee, Yi Zhou, Jiho
Jin, Rifki Afina Putri, Dimosthenis Antypas,
Hsuvas Borkakoty, Eunsu Kim, Carla Perez-
Almendros, Abinew Ali Ayele, et al. 2024.
BLEnD: A benchmark for Ilms on every-
day knowledge in diverse cultures and lan-
guages. In Proceedings of the 38th Con-
ference on Neural Information Processing
Systems (NeurlPS), Vancouver, Canada.

OpenAl. 2025. Gpt-5 technical overview. https:

//openai.com/research/gpt-5.

Siddhesh Pawar, Junyeong Park, Jiho Jin,
Arnav Arora, Junho Myung, Srishti Yadav,
Faiz Ghifari Haznitrama, Inhwa Song, Alice
Oh, and Isabelle Augenstein. 2025. Survey
of cultural awareness in language models:
Text and beyond. Computational Linguistics,
pages 1-96.

Haritz Puerto, Tilek Chubakov, Xiaodan
Zhu, Harish Tayyar Madabushi, and Iryna
Gurevych. 2025. Fine-tuning on diverse rea-
soning chains drives within-inference CoT
refinement in LLMs. In Proceedings of the
63rd Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long
Papers), pages 3789-3808, Vienna, Austria.
Association for Computational Linguistics.

Libo Qin, Qiguang Chen, Fuxuan Wei, Shi-
jue Huang, and Wanxiang Che. 2023.


===== PAGE BREAK =====

Cross-lingual prompting: Improving zero-
shot chain-of-thought reasoning across lan-
guages. In Proceedings of the 2023 Confer-
ence on Empirical Methods in Natural Lan-
guage Processing, pages 2695-2709, Sin-
gapore. Association for Computational Lin-
guistics.

Narun Raman, Taylor Lundy, and Kevin Leyton-
Brown. 2025. Reasoning models are test
exploiters: Rethinking multiple-choice. arXiv
preprint arXiv:2507. 15337.

Abdelrahman Sadallah, Junior Cedric Tonga,
Khalid Almubarak, Saeed Almheiri, Farah
Atif, Chatrine Qwaider, Karima Kadaoui,
Sara Shatnawi, Yaser Alesh, and Fajri Koto.
2025. Commonsense reasoning in Arab cul-
ture. In Proceedings of the 63rd Annual
Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages
7695-7710, Vienna, Austria. Association for
Computational Linguistics.

Fanar Team, Ummar Abbas, Mohammad Shah-
meer Ahmad, Firoj Alam, Enes Altinisik,
Ehsannedin Asgari, Yazan Boshmaf, Sabri
Boughorbel, Sanjay Chawla, Shammur
Chowdhury, Fahim Dalvi, Kareem Darwish,
Nadir Durrani, Mohamed Elfeky, Anmed EI-
magarmid, Mohamed Eltabakh, Masoomaii
Fatehkia, Anastasios Fragkopoulos, Maram
Hasanain, Majd Hawasly, Mus’ab Husaini,
Soon-Gyo Jung, Ji Kim Lucas, Walid Magdy,
Safa Messaoud, Abubakr Mohamed, Tasnim
Mohiuddin, Basel Mousi, Hamdy Mubarak,
Ahmad Musleh, Zan Naeem, Mourad Ouz-
zani, Dorde Popovic, Amin Sadeghi, Hus-
rev Taha Sencar, Mohammed Shinoy, Omar
Sinan, Yifan Zhang, Ahmed Ali, Yassine El
Kheir, Xiaosong Ma, and Chaoyi Ruan. 2025.
Fanar: An arabic-centric multimodal genera-
tive ai platform.

Peng Wang, Shuai Bai, Sinan Tan, Shi-
jie Wang, Zhihao Fan, Jinze Bai, Keqin
Chen, Xuejing Liu, Jialin Wang, Wenbin
Ge, et al. 2024. Qwen2-vl: Enhancing
vision-language model’s perception of the
world at any resolution. arXiv preprint
arXiv:2409. 12191.

Yufei Wang, Wanjun Zhong, Liangyou Li, Fei
Mi, Xingshan Zeng, Wenyong Huang, Lifeng
Shang, Xin Jiang, and Qun Liu. 2023. Align-
ing large language models with human: A
survey. arXiv preprint arXiv:2307. 12966.

Jason Wei, Xuezhi Wang, Dale Schuurmans,
Maarten Bosma, Fei Xia, Ed H Chi, Quoc V
Le, Denny Zhou, et al. 2022. Chain-of-
thought prompting elicits reasoning in large
language models. In Advances in Neural
Information Processing Systems.

Ping Yu, Jack Lanchantin, Tianlu Wang,
Weizhe Yuan, Olga Golovneva, Ilia Kulikov,
Sainbayar Sukhbaatar, Jason Weston, and
Jing Xu. 2025.  Cot-self-instruct: Build-
ing high-quality synthetic data for reason-
ing and non-reasoning tasks. arXiv preprint
arXiv:2507.23751.

Eric Zelikman, Yuhuai Wu, Jesse Mu, and
Noah Goodman. 2022. STaR: Bootstrap-
ping reasoning with reasoning. Advances
in Neural Information Processing Systems,
35:15476-15488.

Tianyi Zhang, Varsha Kishore, Felix Wu, Kil-
ian Q. Weinberger, and Yoav Artzi. 2020.
BERTScore: Evaluating text generation with
bert. In Proceedings of ICLR 2020.

Chiwei Zhu, Benfeng Xu, An Yang, Junyang
Lin, Quan Wang, Chang Zhou, and Zhen-
dong Mao. 2025. Rationales are not sil-
ver bullets: Measuring the impact of ratio-
nales on model performance and reliability.
In Findings of the Association for Computa-
tional Linguistics: ACL 2025, pages 5808—
5835, Vienna, Austria. Association for Com-
putational Linguistics.
