arXiv:2510.25017v1 [cs.DB] 28 Oct 2025

StorageX Tuner: An LLM Agent-Driven Automatic Tuning Framework for
Heterogeneous Storage Systems

Qi Lin', Zhenyu Zhang}, Viraj Thakkar', Zhenjie Sun', Mai Zheng’, Zhichao Cao!
‘Arizona State University, Iowa State University

Abstract

Automatically configuring storage systems is hard: param-
eter spaces are vast and conditions shift across workloads,
deployments, and versions. Heuristic and ML tuners are usu-
ally tied to one system, rely on manual glue, and often lose
effectiveness under changes. LLM-based proposals help, but
most cast tuning as a single-shot, system-specific task, limit-
ing cross-system reuse, constraining exploration, and offering
weak validation.

We present StorageXTuner, an LLM-agent-based auto-
tuning framework for heterogeneous storage engines. Stor-
ageXTuner separates concerns across four agents—Executor
(sandboxed benchmarking), Extractor (performance digest),
Searcher (insight-guided configuration exploration), and Re-
flector (insight generation and management). The design cou-
ples an insight-driven tree search with a layered memory
that promotes empirically validated insights and employs
lightweight checkers to guard against unsafe actions. We im-
plement a prototype and evaluate it on RocksDB, LevelDB,
CacheLib, and MySQL InnoDB with YCSB, MixGraph, and
TPC-HIC. Relative to out-of-the-box settings and to ELMo-
Tune, StorageXTuner reaches up to 575% and 111% higher
throughput, reduces p99 latency by as much as 88% and 56%,
and converges with fewer trials.

1 Introduction

Storage systems are widely used in modern IT ecosystems for
data persistence and management. For instance, the social net-
work infrastructures at Meta combine caching systems (e.g.,
TAO [33], Memcached [77], CacheLib [32]), graph stores
(e.g., MyRocks [7], Neo4j [19], and Neptune [18]), key-value
stores (ZippyDB [49] and RocksDB [75]), and object stores
(e.g., S3 [14], HDFS [12,27]) for various types of social data.
Each of these storage systems provides numerous configu-
ration parameters, and prior studies have demonstrated that
tailored configuration tuning can substantially improve perfor-
mance and resource utilization, often surpassing what default
configurations can deliver [36, 74, 83].

Tuning the storage system configuration typically follows
a multi-stage pipeline. Initially, engineers need to understand
targeted workload characterizations, upper-layer application
behaviors, hardware specs, system performance metrics, as
well as the software stack. Then, based on that information,
engineers perform an iterative process of configuration explo-
ration, evaluation, and re-adjustment [48]. Considering the
hundreds of configuration parameters spanning diverse types
with intricate dependencies [8, 22,75], engineers usually use

different algorithms like heuristic [48] or Bayesian [80] for
tuning [62,89]. The new configurations will be evaluated via
benchmarking, trace replay, or shadow testing for iterative
adjustment until the expected tuning goal is satisfied [45].

To achieve fast and efficient tuning of storage systems, dif-
ferent manual solutions are explored, including rule-based
tuning [35] and search-based approaches like PAS [76] and
OASIS [97]. These methods typically rely on expert knowl-
edge or guided exploration to navigate the configuration space.
To automate the tuning process with less manual effort, Ma-
chine Learning (ML) is used in several auto-tuning studies,
such as Endure [57], Dremel [105], Ottertune [90], CDB-
Tune [103], QTune [65]. These methods build models that
capture the relationship between configurations and perfor-
mance metrics to automate the adjustments. More recently,
several studies show that Large Language Modules (LLMs)
hold strong promise for system tuning [51, 52, 63, 87, 88].
ELMo-Tune [88] applies prompt engineering to leverage
LLM reasoning for key-value store tuning, overcoming the
brittleness of expert-crafted heuristics. GPTuner [63] and A-
Tune [51,52] use LLMs to guide database knob selection.

In general, LLM-based tuning solutions shift much of the
tuning effort from handcrafted rules and system-specific mod-
els to automated reasoning [63], and are able to automate the
whole tuning pipeline. Also, they show strong storage-system
generality foundations with different system deployments (A-
Tune for Postgres and ELMo-Tune for RocksDB use the same
underlying LLM model) and have the human-like reasoning
ability to capture complex relationships across workloads,
hardware, software, and configuration dependencies. Yet, cur-
rent LLM-based tuning approaches remain limited. They are
closely tied to specific system designs, which require substan-
tial manual intervention and hard-coded engineering. More-
over, they typically collect all available information and feed
it as a single LLM prompt, hoping that the LLM’s reasoning
capabilities can handle the tuning complexity and produce
effective configuration recommendations. However, aggregat-
ing such heterogeneous information can weaken the LLM
reasoning process [69]. Further, these approaches inherit well-
known drawbacks of LLMs: slow response, recurring API
costs, and susceptibility to errors [41].

To overcome these limitations for achieving an efficient
LLM-based storage system tuning solution, several key chal-
lenges must be addressed: 1) Existing LLM-based tuning
frameworks are tightly coupled to a specific storage system
design or an implementation version. A key challenge is how
to decouple system-specific semantics (e.g., system APIs,


===== PAGE BREAK =====

version-specific behaviors, and implementation-specific de-
tails) and reduce manual effort. 2) LLMs prefer to work with
clear, task-specific, and structured contexts [70, 85]. It is crit-
ical to divide the whole tuning pipeline into subtasks with
clear scope and context, so that LLMs can focus on accurate
reasoning and decision-making. 3) Guiding LLMs to explore
large configuration spaces with high efficiency is challenging.
It is essential to explore search algorithms to avoid costly
exhaustive searches and minimize the overhead of trial-and-
error tuning. And 4) given LLMs’ tendency to hallucinate and
overfit to prompt phrasing [58, 82], a key challenge is how to
ensure LLM tuning suggestions are accurate and reliable.

To address the aforementioned challenges, we propose Stor-
ageXTuner, an end-to-end, fully automated, and general stor-
age system tuning framework that integrates LLM reason-
ing while ensuring efficiency and robustness. Inspired by
the recent success of LLM agents, which extend LLMs with
planning, tool use, and collaboration abilities [96, 100], Stor-
ageX Tuner introduces three key innovations:

¢ Collaborative Multi-Agent Tuning Framework: StorageX-
Tuner decomposes and abstracts the tuning process into
four stages with dedicated LLM agents that operate in
an iterative pipeline: 1) Executor is responsible for stor-
age system deployment, benchmarking, and monitoring; 2)
Extractor analyzes deployment, workload, benchmarking
statistics, and log data collected by Executor and summa-
rize them into structured tuning input; 3) Searcher explores
the configuration space, decide the next round of configura-
tions based on the input from Extractor and Reflector, and
summarize the tuning experience; And 4) Reflector collect,
analyze, and manage the tuning insights from the past tun-
ing experiences from Searcher. Moreover, we design a set
of agent-specific checkers to validate LLM outputs before
execution to further enhance robustness.

Insight-Driven Exploration of Configuration Space: Stor-
ageXTuner combines LLM reasoning with tree-based ex-
ploration to navigate complex configuration spaces. It ex-
tracts high-level tuning insights from past trials and in-
jects them into the Searcher’s context, guiding the gen-
eration of candidate configurations as new tree branches.
Searcher selects the most promising candidates from multi-
ple branches for further expansion based on the analysis re-
sults from Extractor, efficiently focusing on high-potential
configurations while avoiding redundant or suboptimal
paths.

Memory-Efficient Management of Tuning Insights: Stor-
ageXTuner generates tuning insights and manages them
in a layered memory by Reflector: Short-Term Mem-
ory (STM) for tentative insights and Long-Term Memory
(LTM) for validated insights. A confidence score is as-
sociated with each insight to reflect the reliability and is
dynamically adjusted. Tuning insights are retrieved based
on contextual similarity and confidence scores during the
configuration searching process, enhancing LLM reason-

ing efficiency and adaptability for different workloads and
environments.

We implement a prototype of StorageXTuner with code
released on GitHub!. We evaluate StorageXTuner across
various single-node storage systems (e.g., key-value stores
and caches), including RocksDB [75], LevelDB [56], Cache-
Lib [32], and MySQL InnoDB [8]. We do not include dis-
tributed storage systems, as they introduce additional chal-
lenges such as network variability, coordination overhead, and
fault tolerance, which are beyond the scope of this paper.

StorageX Tuner delivers consistent performance improve-
ments across multiple baselines, including default configura-
tions, LLM-Default, and state-of-the-art tuners (ELMO-Tune,
ADOC, Endure for RocksDB; SMAC, DDPG, and A-Tune
for SQL InnoDB). Real-world workloads such as YCSB [44],
MixGraph [40], and TPC-H/C [11] reveal throughput gains up
to 575% and p99 latency reductions up to 88%. For CacheLib,
StorageXTuner improves hit ratios by up to 3.1 percentage
points and achieves up to 22% higher throughput compared
to other baselines across write-intensive, read-intensive, and
mixed workloads. In MySQL InnoDB, StorageXTuner yields
up to 709% improvement in transaction throughput on TPC-C
and 71% improvement in query performance on TPC-H. We
further evaluate StorageXTuner ’s flexibility across system
versions (four RocksDB and two LevelDB releases), tree-
search branching factors (child nodes), insight counts, and dif-
ferent LLMs (GPT-3.5, GPT-40, GPT-03-mini, and LLaMA
variants), demonstrating robust performance under diverse
conditions.

In addition, we present our learnings from StorageX Tuner,
presenting how the configuration search process majorly re-
volves around a few key parameters, why LLMs perform
better when asked to edit instead of create configurations,
and having a closed-loop LLM-reasoning architecture signifi-
cantly reduces configuration convergence iterations.

2 Background & Motivations

2.1 Configuring and Tuning Storage Systems

Storage systems like caching systems (e.g., CacheLib [22],
Memcached [26], and Redis [9]), persistent key-value stor-
age engines (e.g., RocksDB [75], LevelDB [56], and MySQL
InnoDB [8]), and file systems (e.g., Ext4 [16], XFS [20], Tec-
tonic [79], and HDFS [27]) are important for various appli-
cations to store and manage a large volume of data. Out-of-
the-box configurations usually fall short in meeting applica-
tion requirements of performance metrics like throughput
and latency in different scenarios [83]. Typically, each stor-
age system exposes a number of tunable configurations (e.g.,
more than 170 in RocksDB, and 140 in MySQL InnoDB),
which control diverse aspects such as I/O options, buffer
sizes, threads, and other critical storage system behaviors.

| https://github.com/gdaythcli/StorageXTuner (Anonimized)


===== PAGE BREAK =====

The large configurational space, along with the targeted work-
loads [28, 29, 39,44] and hardware deployments [47, 104] in
which these systems are used, makes tuning configurations a
challenging task.

Domain experts or engineers typically tune configurations
[92] with a multi-step process. They profile targeted work-
loads into representative benchmarks (e.g., social graph OLTP
workload from Meta as the MixGraph benchmark [40]) to en-
able iterative offline testing. Then, the storage system config-
urations are iteratively tuned and evaluated via benchmarking
to observe the impact on key performance metrics. During
the tuning process, a number of factors, including storage
system design, interplay of the workload profile (e.g., Zifian
access pattern, read-heavy workloads), deployment setup (e.g.,
storage devices, CPU, and Memory), hardware and software
characteristics, and resource constraints, are also considered.
Once the experts achieve satisfactory performance as required
by the application, the configurations are pushed for produc-
tion deployment.

To automate the iterative tuning process, Machine Learn-
ing (ML) is used in several auto-tuning studies, such as En-
dure [57], Dremel [105], Ottertune [90], CDBTune [103],
QTune [65], CAPES [67], CAMAL [102], ADSTS [71], and
H5Tuner [31]. These solutions usually focus on exploring
parameters affecting performance and building an automated
tuning framework that leverages learned patterns to tune the
configurations. More recently, interest has grown in tuning
solutions that are cohesive with Large Language Models
(LLMs). For example, ELMo-Tune [88] introduces an LLM-
based framework for tuning RocksDB, a key-value store,
across different workloads and deployment environments,
A-Tune [52] demonstrates that LLMs can generate effective
database configuration suggestions under diverse workloads,
and GPTuner [63] integrates LLMs with Bayesian optimiza-
tion to improve configuration exploration efficiency. Those
studies highlight new opportunities for addressing the com-
plex task of storage system tuning with LLMs.

2.2 LLMs and LLM Agents

Large Language Models (LLMs). Built upon transformer
architectures, LLMs are advanced AI systems that recognize
complex patterns in language and context, powering a wide
range of NLP tasks such as translation, question answering,
summarization, and content creation [5, 6,78]. Prominent ex-
amples include GPT [78], Gemini [4], Claude [1], and Copi-
lot [2]. LLMs are pretrained on massive corpora, giving them
a broad knowledge base and strong generalization capabilities
across different domains [6,34,78]. The human-like reasoning
helps to infer the purpose of configurations and relationships
between configuration changes and performance [42].

LLM Agents. Unlike standalone LLMs, LLM agents can
integrate additional actions, access plugins, call APIs, and or-
chestrate complex workflows, effectively acting as intelligent
assistants capable of more than natural language responses.

Techniques such as Chain-of-Thought prompting [95] en-
courage the model to generate intermediate reasoning steps
explicitly, enabling more accurate and explainable outputs.
Tree-of-Thoughts [99] allows the model to explore multiple
branching reasoning paths in parallel, evaluating alternatives
before committing to a solution. Self-refinement [72] lets
the model iteratively critique its own outputs and make im-
provements, improving reliability and reducing errors. These
innovations have demonstrated success across domains such
as software engineering, automation, optimization, question-
answering, and information retrieval [46, 53-55,91, 93,94].

LLM: The New Silver Bullet? LLMs show strong potential
for storage system tuning, offering generalization across het-
erogeneous workloads and deployments with minimal human
intervention [87, 88]. Pretrained on massive and diverse cor-
pora [17,78], LLMs already have rich knowledge of most of
the open-source storage systems and can reason across multi-
ple dimensions of tuning complexity, such as workload diver-
sity, hardware constraints, software behaviors, and configu-
ration dependencies [42]. LLMs can map high-level insights,
like “increasing cache size improves read performance,” to
system-specific actions such as adjusting block_cache_size in
RocksDB or innodb_buffer_pool_size in MySQL.

2.3 Limitations of State-of-The-Art

Despite their potential, current LLM-based solutions for stor-
age system tuning remain limited. First, existing LLM-based
solutions typically rely on a single LLM inference cycle for
narrow subtasks, such as directly generating configuration
suggestions from workload statistics [88]. However, a single
LLM struggles with the multi-stage nature of storage sys-
tem tuning, which involves workload characterization, perfor-
mance diagnosis, configuration search, and validation. Feed-
ing heterogeneous information all at once often overwhelms
the model and degrades its reasoning [69]. To illustrate this,
we compare two tuning approaches for RocksDB using the
Elmo-Tune framework:

- Single-shot input: feeding all relevant information (e.g.,
hardware, software, configuration parameters, and work-
load characteristics) into a single LLM at one prompt.

¢ Multi-shot input: dividing the same information into se-
mantically coherent batches and sending each batch to
separate LLM agents. Each agent processes its context
independently, and their outputs are then integrated to pro-
duce final tuning recommendations.

To ensure a fair comparison, both approaches use the same
input content and information. Under the MixGraph work-
load [39] in RocksDB, the multi-shot approach significantly
outperforms the single-shot method (as shown in Table 1),
highlighting the advantage of structured, focused reasoning
for multi-stage storage system tuning.

Second, existing LLM-based approaches lack mechanisms
to efficiently explore large and complex configuration spaces,
often resulting in costly trial-and-error searches. For instance,


===== PAGE BREAK =====

Table 1: Comparison of Input Types and Search Methods.

Input     Strategy             Throughput (kops/s) —_ Latency (us)
Type — Single-shot input                      104                                   78
Multi-shot input                        231                                     31
Search    Strategy                Total Token (K)      Per-Round (K)
Methods Random search                  142                       6.76
Tree search                          63                           12.6

Table | shows that under the MixGraph workload in RocksDB
using the Elmo-Tune framework, random search (the default
in ELMo-Tune) incurs substantially higher exploration over-
head than using tree search in Elmo-Tune when achieving the
same performance level. This occurs because ELMo-Tune
relies on a single LLM inference cycle per suggestion, which
causes it to repeatedly evaluate the same or similar settings
before converging on effective configurations.

Furthermore, existing LLM-based approaches [52, 63,88]
tightly couple system-specific semantics with the tuning
framework, leading to low generality and compatibility. For
example, Elmo-Tune [88] requires developers to manually
hard-code RocksDB interfaces and modify the tuning frame-
work source code to apply LLM-generated suggestions. More-
over, Elmo-Tune cannot successfully tune different RocksDB
versions directly. Through careful testing across different
RocksDB versions, we found that ELMo-Tune works cor-
rectly on version 8.8.1, but fails on version 5.7.1 due to ver-
sion compatibility issues. Therefore, existing LLM-based ap-
proaches are neither fully automated nor general, and they
still demand substantial human effort to ensure compatibility
with particular implementations or release versions.

Additionally, current approaches cannot retain or reuse the
tuning knowledge from previous tuning cycles or even cross
different tuning sessions (but engineers will learn from those
past trials). In other words, even if a workload has been tuned
before, ELMo-Tune treats each new tuning session fully inde-
pendently, requiring repeated exploration of the configuration
space from scratch. Table 2 shows that ELMo-Tune with prior
tuning knowledge (e.g., historical configurations and their
observed performance) effectively reduces the number of ex-
ploration iterations and achieves better RocksDB performance
than the default ELMo-Tune across different workloads.

Together, these limitations motivate the need for a holistic,
automatic storage system tuning framework that integrates
LLM reasoning across all stages of the tuning process while
enabling high generality, efficient configuration search, tuning
knowledge reuse, and fast validations.

Table 2: Impact of Previous Knowledge on Iteration Rounds
and Final Performance

With Previous Without Previous Achieved Performance
Workload

Knowledge            Knowledge         (% of With Knowledge)
Fillrandom                  6                               15                                      0.84
Readrandom               8                               12                                      0.82
MixGraph                    9                               17                                      0.76

3 StorageXTuner

In this paper, we propose StorageXTuner, a general, fully
automated, end-to-end tuning framework for various storage
systems that incorporates LLM reasoning while ensuring both
efficiency and robustness. We first discuss the challenges be-
ing addressed in StorageXTuner. Then, we present the design
and implementation details.

3.1 Challenges

First, the tuning process spans multiple stages, each with dis-
tinct contexts, reasoning requirements, and feedback loops.
How to decompose the workflow into modular subtasks, de-
fine clear scopes and contexts for each, and coordinate reason-
ing across multiple modules is challenging. Second, current
tuning approaches are tightly coupled to individual storage
system design, specific versions, and low-level implementa-
tion details, making them brittle and labor-intensive to extend.
The key challenge is how to abstract common tuning seman-
tics and design representations that enable LLMs to reason
across diverse storage systems without extensive human in-
tervention. Third, configuration spaces are high-dimensional,
and often include complex parameter interactions. Exhaustive
search is infeasible, while naive trial-and-error can lead to
costly exploration overheads. How to design strategies that
help LLMs prioritize promising configurations, prune redun-
dant or low-value trials, and adaptively balance exploration
with exploitation? Finally, LLM outputs can be hallucinated,
and tuning processes may fail due to environmental variability
or execution errors. Can we capture, manage, and leverage
knowledge from past tuning trials to improve search efficiency
and correctness? Also, how to validate LLM-generated sug-
gestions, detect errors, and recover gracefully with minimal
human intervention.

3.2 StorageXTuner Architecture Overview

The overall architecture of StorageXTuner is shown in Fig-
ure |, StorageXTuner automates workload benchmarking,
performance analysis, configuration search, and tuning expe-
riences management with 4 dedicated LLM agents, including
Executor, Extractor, Searcher, and Reflector:

e Executor launches the sandbox engine to deploy the stor-
age system in the targeted setups and run benchmarks
based on given configurations and resource constraints. At
the same time, it monitors runtime statistics (e.g., CPU
utilization, I/O bandwidth) and collects structured (e.g.,
JSON files) or unstructured (e.g., text logs) output from
benchmarking.

¢ Extractor analyzes benchmark output and runtime statis-
tics from the Executor, extracts structured performance
metrics (referred to as the performance digest), and passes
them to the Searcher as benchmarking and deployment re-
sults for the given configuration.

¢ Searcher explores the configuration space, proposes and
selects the next round of configuration candidates based on


===== PAGE BREAK =====

Executor
Benchmarker                                       Monitoring

—  Workload,    = fees
sas box

Extractor

Agent £3

Write ‘SiS

analyze scripts         Unified Structured Outputs
Reflector

Searcher

| Node Selection

Perception:
Tuning Insights
Tree Node Info
Search Path

Child Node 1,

2  cJ
\Configs Proposal! LLM beat  /’| Tuning Node
l | Agent  Gh -
Perception: ;          Resi
Tuning Insights
Tree Node Info      N14 |/N2 |.
Search Path                     MN
Action:          N3 || N4 || N5 | NG
|

Action:                                                  Iteration 1

Iteration 2

S LLM Agent

i                   Insights Management
Short-term Memory
'|4( Insight 1 {..., "confidence": 0.7} |
{ Insight 2 {..., "confidence": 0.8} |

Insights
'Generation

Upgrade

I                       =
i[ Insight N     Long term Memory

Degrade

Iteration 3
>

Select a node             Child Node 2

Collect tuning experiences

 Insight M     { Insight A {..., "confidence": 0.9} |

Figure 1: StorageXTuner framework

the performance digest from the Extractor, and tuning in-
sights (i.e., summary of analysis from past tuning history)
from Reflector. It also records tuning experiences (e.g., ex-
plored configurations and their performance digest), which
are then sent to the Reflector.

¢ Reflector collects, analyzes, and manages tuning experi-
ences from the Searcher, and summarizes them into high-
level tuning insights and dynamically updates and manages
those insights based on feedback from the Searcher.

StorageX Tuner operates through two intertwined iterative
cycles during a tuning round, as shown in Figure |. 1) Cycle
A (red lines) involves the Executor, Extractor, and Searcher,
which collaboratively propose new configuration candidates,
run benchmarks, and analyze the results of each candidate.
2) Cycle B (blue lines) involves the Searcher and Reflector,
which collect and transform tuning experiences into high-
level insights. Searcher uses these insights to guide the con-
figuration search, and Reflector continuously updates them
with new benchmarking results. These two cycles operate
continuously, enabling the framework to progressively refine
its understanding of storage system behavior and improve
storage system tuning efficiency over time.

With dedicated LLM agents for each major task of the
tuning process (i.e., Executor, Extractor, Searcher, and Re-
flector), StorageXTuner decouples the dependencies between
storage system-specific knowledge, designs, and executions
from the common tuning workflows and handles them via
LLM agents. StorageXTuner only requires lightweight inter-
faces for running benchmarks and adjusting configurations
for different storage systems and implementation versions,
achieving high generality. Moreover, StorageXTuner provides
LLM agents with structured, system-related, task-specific con-
texts, enabling more accurate reasoning and decision-making
at each stage of the tuning process.

3.3. Automated Benchmarking and Analysis

In a tuning process, a new configuration is evaluated (e.g.,
via benchmarks or traces) and compared against other can-
didate configurations to guide the configuration adjustment.

However, these processes are tightly coupled with the storage
system and often require custom scripts to automate bench-
marking. Furthermore, analyzing the benchmarking results
usually requires manual efforts due to the complexity of the
data content and format (e.g., runtime statistics, execution
logs, system monitoring data, and benchmark outputs). Stor-
ageXTuner delegates benchmarking and analysis actions to
two specialized LLM-powered agents, Executor and Extrac-
tor.

Executor. As shown in Figure 2, the Executor uses a pre-
cise specification for each benchmark experiment, including
a candidate configuration file (e.g., RocksDB configuration
with write_buffer_size = 64MB, block_cache_size = 256MB),
resource constraints (e.g., 2 CPU cores, 1|GB memory), and
a specific workload. The workload can be created from syn-
thetic micro-benchmarks (e.g., fillrandom from db_bench in
RocksDB) or trace replay. The Executor sets up the sandbox
environment (e.g., Docker), applies the given configuration,
and generates the workload within the specified resource enve-
lope to isolate tuning experiments and enable safe, repeatable
testing. This design can isolate potential errors or unsafe LLM-
generated configurations. Executor also monitors runtime sta-
tus (e.g., container health, CPU load, memory usage) and
collects all sandbox outputs, including performance metrics
from benchmarking output (e.g., throughput and tail latency),
storage system operational logs, and any system warnings.

Extractor. The massive structured and unstructured output
data from Executor raises the challenge of how to analyze
them for configuration exploration. Conventional approaches
rely on manual parsing or rigid, fixed-format parsers, which
are labor-intensive and difficult to scale [39]. When output for-
mats change with new system releases, these methods require
costly re-engineering. While LLMs can directly analyze such
files to extract performance metrics, directly feeding massive
logs into an LLM risks token overflows and hallucinations.
Therefore, StorageXTuner exploits the LLM’s program-
ming capabilities and uses a separate LLM agent, Extractor,
to generate a targeted Python parser on demand. We provide
the LLM with descriptions of the performance metrics of inter-


===== PAGE BREAK =====

| |17447, 385, [i

2 CPU cores, 4 GB Memory

Tuning Task Inputs              _             Structured/Unstructured

eo        a        Benchmark Outputs

'| Configuration File _|,         =a-------2==                          Puts _

' \write_buffer_size = 64MB, ||        —         S                    Text Log}   ;

! block_cache_size = 256MB |!          Sandbox         \                            \

iF                                       JSON |]

1        Workload File        \       Set Environment           |                                    1

'lTrace         Synthetic      \     Apply Configuration         |) "p99 Latency":              I

l  Replay   Microbenchmark | ;                                 !   —                 av     I

,  Resource Description |!        Sandbox Output || | |Time, Latency,
I

|                                                                                                |
l

|                                                                                                |
l

7.

oO                          Executor
Structured JSON Object

3
®
=
1D
n
=>
3
3
m
x
Cy
°
<
=
°

Performance Digest

| <——— Extractor <—
|

| |Run fixed  Write Python
\¢ | parsers e analyze scripts

YA

Exceed Retry) Regenerate | a
Threshold?                 Coal

|

|

|

              AN               N

iY                              Run scripts
*< Success?

\                                         locally

Figure 2: Automated Benchmarking and Analysis

est (e.g., definitions of throughput and latency), together with
relevant system information (e.g., system version), and data
samples (e.g., log snippets). Guided by this context, the LLM
infers the possible output formats and synthesizes Python
code to process them (e.g., import different libraries to handle
text, JSON, or CSV files). Then, the Extractor executes the
Python script locally to transform raw outputs into structured
key-value pairs to store extracted performance metrics and
non-metric information. This design enables StorageXTuner
to adapt on the fly to the unique logging formats of various
storage systems without heavy engineering effort. Finally, Ex-
tractor constructs the Performance Digest, a structured JSON
object that includes quantitative performance metrics and
qualitative evaluation summary (i.e., a concise interpretation
of benchmark results, including key trends, trade-offs, or any
anomalies generated by LLM). The performance digest, com-
bined with the configuration file, workload description, and
system resource information, forms the tuning node in the
search tree used by Searcher for configuration exploration
(detailed in Section 3.4).

Validation. To ensure the reliability of the generated Python
scripts, Extractor incorporates a self-correcting mechanism.
As shown in Figure 2, if a generated script fails to execute or
produces invalid output (e.g., incorrect types), this failure is
detected by a pre-defined checker and reported back to the
LLM to generate a refined script. If regeneration attempts
exceed a predefined threshold, the system rolls back to pre-
defined, fixed-format human-written parsers. A script may run
successfully but still compute metrics incorrectly. To address
this, Extractor performs pre-defined abnormal value checks
(e.g., throughput exceeding hardware limits) to identify suspi-
cious outputs and trigger regeneration.

3.4 Insight-Driven Configuration Exploration

While LLM can directly propose candidate configurations
based on the performance digest from the last tuning cycle, as
Elmo-Tune did [88], it cannot efficiently explore configura-

Root NO         N1            N3 | 'Top-K Tuning Insights Example                                             4
\         ~    a   | 1Increasing write buffer pool size significantly improves throughput |
oo1

| ConfigurationNO || Resource ||    ,  See ONS
max_background_jobs = 2 |p cpy           write _burler_size=

write_buffer_size = 64MB, Il Gp Memory   block_cache_size =12 MB
block_cache_size = 12MB ||---

Configuration N3
write_buffer_size=54+2 768MB,
block_cache_size =42 256MB

Configuration N2

Configuration N4

Digest             Workload       i           ize=

i\jwrite_buffer_size=12 MB,              —
"ops_per_sec": 11000       Random | Nblock_cache_size          Hibernia
™         Me                             1                                   —      —_    ~
“Summary 1 XXX                  Write      \                                   wes

42 512MB

Figure 3: Insight-Driven Configuration Exploration

tions that need past tuning experiences. The LLM reasoning
is session-bound and limited by context windows, making it
impractical to feed all past trial results without performance
degradation or hallucinations [66]. In addition, LLMs lack
iterative feedback integration and tend to generate stochastic,
unguided exploration paths, which often lead to redundancy
and incomplete coverage [61].

We propose to integrate LLM with a tree search process for
configuration exploration, which addresses these limitations
by structuring exploration into explicit steps, recording past
outcomes, and guiding the search toward promising directions.
By maintaining parent-child relationships between config-
uration candidates, tree search expands promising branches
while pruning unpromising ones. This structure combines
planning, branching, and feedback-driven prioritization, en-
abling LLMs to concentrate their reasoning on high-potential
areas of the configuration space.

StorageX Tuner achieves this by delegating the configura-
tion tree search task to a specialized LLM agent, the Searcher.
Unlike traditional tree search, which depends on scalar re-
wards or hand-tuned utility functions (demanding substantial
human effort to predefine and tune), the Searcher leverages
high-level tuning insights, which are summarized by the LLM
from past tuning trials (See details in Section 3.5), to decide
how and where to explore.

How to Propose Configuration Candidates? The explo-
ration process begins with the initial configuration (e.g., the
default configuration), which serves as the root node in the
search tree. Tuning insights, expressed in natural language,
capture plausible relationships between configuration changes
and observed storage system behavior. For instance, an in-
sight might state that increasing the write buffer pool size
significantly improves throughput for RocksDB. Guided by
these insights, the Searcher proposes a set of configuration
candidates, which become the child nodes of the current node
(the root in this case) in the search tree. The LLM is directed
to prioritize exploration consistent with these insights, such
as re-balancing the write buffer and block cache size.

As shown in Figure 3, consider tuning RocksDB un-
der a random write workload with 2 CPU cores and 1
GB of memory, where the initial configuration (root node)
sets max_background_jobs=2, write_buffer_size=64MB, and
block_cache_size=12MB. First, Searcher constructs a struc-
tured prompt that incorporates the current tuning node (root
node), task descriptions, and tuning insights. Figure 4 shows
how the prompt template organizes these elements into a


===== PAGE BREAK =====

System
‘You are a RocksDB system expert performing the system configuration tuning task.
You will use tree search to find the best configurations.

You have these restrictions on your exploration: [BLACKLIST] <_ Restrictions on Exploration <

User
Here are the tuning insights you can reference: [INSIGHT 1, INSIGHT 2, ...]
Here are the tuning nodes and their corresponding information:

[NODE ID: {"Configs": xx, "Workload": xx, "Resource": xx, "Digest":{"xx"}}]                          -
[NODE ID: {"Configs": xx, "Workload": xx, "Resource": xx, "Digest":{"xx"}}]

Here are the relationships between these nodes: [RELATIONSHIP 1, RELATIONSHIP 2, ...]
Task requirements: Based on the given tuning insights, please explore these tuning nodes. First, select the
most promising node; then generate xx children nodes from it, each child node only changes xx configs at most.
Task Instruction: Please first explain why to explore that node, then why to generate these child nodes.
Response Requirement: You should follow these examples to output your suggestions:

[EXAMPLE 1: {"Select node": xx, "Reason": xx, "Child node": {Node 1, Node 2}}]
Figure 4: Example Prompt for Configuration Proposal

natural-language instruction. This prompt is then passed to
the LLM to propose child configurations for exploration.
Then, the Searcher expands the root node by proposing two
candidate configurations, increasing write_buffer_size and
block_cache_size to 512MB each to fully utilize the 1 GB
memory budget. Next, the Searcher explores re-balancing
strategies within the same memory budget. Guided by the
insight that increasing write_buffer_size can improve through-
put under write-intensive workloads, it reallocates memory
by assigning 768MB to write_buffer_size and 256MB to
block_cache_size. In practice, Searcher may propose mul-
tiple child nodes.

Which Configuration Should Be Further Explored? After
the Searcher proposes candidate configurations, a new ques-
tion arises: which child node should be explored in the next
round? Before selecting the most promising node, we send the
benchmark tasks, including candidate configurations, resource
descriptions, and workload descriptions, to the Executor. The
Executor benchmarks these candidates, and the Extractor ana-
lyzes the results to generate a new set of performance digests.
This performance digest serves as the utility for each node. As
shown in Figure 4, we combine the performance digests with
the corresponding tuning nodes, insert them into the prompt
template, and guide the LLM to compare these nodes, reason
about which node is more promising, and select it for the next
round of exploration.

Since the performance digest is a structured summary rather
than a directly comparable numeric value, the LLM relies
on its reasoning capabilities to weigh trade-offs and select
the most promising node for exploration. This process may
choose a suboptimal node, for example, a configuration with
the second-highest throughput. We consider this acceptable
because even if the current node does not achieve the highest
performance, exploring it can still provide valuable informa-
tion. Furthermore, if future iterations reveal poor outcomes
for this node, the LLM can return to select the optimal node
in subsequent rounds. In addition, we can also add specific
insights to guide the LLM toward prioritizing the optimal
node. For instance, an insight stating that “throughput is our
main target” encourages the selection of higher-throughput
configurations. This soft comparison approach enables the
LLM to explore promising directions while maintaining a
degree of randomness in its exploration.

How to Validate Configuration Candidates? Given the in-

herent uncertainty of LLM reasoning, StorageXTuner relies
on a two-layer validation strategy to ensure the correctness
and safety. In the first layer, users can specify domain con-
straints, such as persistence guarantees or concurrency limits.
The LLM applies these constraints to filter out unsafe or irrel-
evant candidate configurations before evaluation. Configura-
tions that pass this filtering stage undergo empirical validation
in the second layer. Here, the Extractor performs consistency
checks using pre-defined scripts. These checks include detect-
ing anomalous metrics, enforcing blacklists to reject unsafe
configuration changes, and reporting errors back to the LLM.
This layered validation ensures that while LLMs provide flex-
ible semantic reasoning, all tuning decisions remain firmly
grounded in reliable, observable feedback.

When to Terminate the Exploration? To ensure bounded
search, StorageXTuner supports multiple termination strate-
gies. The search process halts automatically when the perfor-
mance improvement between iterations falls below a thresh-
old (e.g., 1% over three rounds), indicating convergence. Alter-
natively, searching can be terminated based on user-specified
constraints such as maximum LLM token usage, total tuning
time, or a system resource budget. These controls provide
both flexibility and safety, allowing StorageXTuner to be de-
ployed in diverse operational settings ranging from offline
experimentation to online tuning in production environments.

3.5 Tuning Insights Management

Tuning insights play a critical role in configuration explo-
ration. The Searcher relies on them to decide how and where
to explore within the complex configuration space.

Insights Generation. As shown in Figure 5, insights are
derived from past tuning experiences. In each search itera-
tion, the Searcher collects tuning experiences, which include
node records (e.g., executed configurations and their perfor-
mance digests) and the search paths (e.g., the relationships
between explored nodes), and sends them to the Reflector.
Those records are inserted into a prompt (as shown in Fig-
ure 6 (Blue Text segments)). Reflector is then instructed to
compare performance digests across different nodes, learn the
relationship within these nodes, and infer generalized rules,
such as which parameter adjustments consistently lead to per-
formance gains or losses. Each insight is expressed in natural
language and is associated with an initial confidence score,
which is assigned by the Reflector based on its initial assess-
ment and updated as more evidence becomes available. The
confidence score denotes the confidence level of the insight,
reflecting how strongly it is supported by past evidence. Be-
sides, each insight also records the source nodes from which
it was inferred.

Insights Management. After generating tuning insights, man-
aging and selectively providing them becomes critical for
improving configuration exploration efficiency. We may use
Retrieval-Augmented Generation (RAG) to manage the in-
sights [64], which retrieves relevant insights via vector sim-


===== PAGE BREAK =====

Uf ToRKTSGRS Reflector Memory Updates

Insights Generation       <

(Searcher

Two-layer Memory

Two-layer Memory                      (Insight N {..., "confidence": 0.8} }'.

Long-term Memory (Degrade <=0.8)

Long-term Memory

| {insight N+1 {..., "confidence": 0.7}) j

---+/= InsightA {..., "confidence": 0.8}

i
a4

(Insight A {..., “confidence”: 0.9, 0.8}) \/ DOWNVOTE   ufinsight N+2 {..., "confidence": 0.3})

\Short-term Memory
(Discard <=0.2 & Upgrade >=0.9)
ir  Insight A {..., "confidence": 0.8} |

Search Path

Insight N                               1
Short-term Memory                                                   {"Content": Increasing buffer   i
[Insight 1 {..., "confidence": 0.7, 0.8}|/\ UPVOTE                 pool size                       id

(Insight 2 {..., "confidence": 8-8, 0.9}|/\ UPVOTE

can significantly improve the ia

throughput,

:O-->0                           | Insight 1 {..., "confidence": 0.8} |

(Insight 3 {..., "confidence": 0-3, 0.2})\/ DOWNVOTE

!

|

i

i

i

Insight 2 {..., "confidence": 0.9} | ||
i

£                                          I
i

i

i

i

I

i

“Source Node": {N2},

-- Insight 2 {..., "confidenc

—— Insight 3 {..., "confidence"

o@ Re-updates  B=
or Rollback      —

Prompt T2 |@

Q| Insert to Short-
term Memory

"Confidence": 0.8}
eB>

Prompt T1

(1)   Tuning Experiences

Figure 5: Tuning Insight Generation and Management. The LLM updates an insight’s confidence through Upvote or Downvote
based on observed results (e.g., Node 2 shows performance consistent with Insight N’s prediction, while Insight 3 contradicts it).
Insights with high confidence are promoted from STM to LTM as validated, reusable knowledge, while low-confidence insights

are discarded or demoted for further evaluation.

System
You are a RocksDB system expert performing the system configuration tuning task
[You are summarizing high-level tuning insights from previous tuning experiences.]
[You are reviewing previous insights based on the latest testing records and examples.<Define Operations
[You have two operations: Upvote: Insights become more valid. Downvote: Insights become more invalid.]
User
Here are the tuning nodes and their corresponding information:
[Node 1, Node 2, ...]< Tree Node Information —
Here are the relationships between these nodes: [RELATIONSHIP 1, RELATIONSHIP 2, ...]  < Search Path <
[Here are the insights you may review:
[INSIGHT 1, INSIGHT 2, ...]] <__ Existing Insights <
Task requirements: Based on the given tuning node records and relationships, please
[extract] [review] tuning insights.
Task Instruction: First, please explain what insights you want to [summarize] [review].
Then, specify which [node you referred to in order to obtain these insights] [operation you want to take].
Finally, [provide confidence scores, from 0 to 1, for these insights] [explain why you take this operation]
Response Requirement: You should follow these examples to output your suggestions:
[EXAMPLE 1: {"Insight content": xx, "Source nodes": {Node 1, Node 2, ...}, "Confidence score": {xx}}]
[EXAMPLE 1: {"Insight ID": xx, "Operation": {Upvote, Downvote, ...}, "Reason": {xx}}]

Figure 6: Prompt for Insight Generation and Updating

ilarity and injects them as auxiliary context into the LLM’s
prompt. However, RAG is insufficient for the storage system
tuning task. First, it relies on a static, homogeneous knowl-
edge base, making no distinction between volatile, session-
specific insights and validated, reusable ones. Second, it re-
trieves knowledge solely by embedding similarity, which is
brittle when insight utility depends on subtle workload dynam-
ics. Embedding similarity only captures surface-level context
similarity and lacks a feedback loop, preventing real tuning
outcomes from reinforcing or revising stored insights.

To address those limitations, StorageXTuner adopts two
key mechanisms in Reflector. First, it maintains a layered
memory structure: a Short-Term Memory (STM) for session-
specific insights and a Long-Term Memory (LTM) for vali-
dated, reusable insights across sessions. Second, it applies a
Confidence Scoring-based Management, where retrieval con-
siders both contextual similarity and dynamically updated con-
fidence scores that reflect empirical tuning outcomes. STM
serves as a workspace for newly generated or session-specific
insights, while LTM accumulates validated insights that have
demonstrated consistent effectiveness across different tuning
executions. Each new insight is first placed into STM with a
provisional confidence score reflecting the LLM’s initial as-
sessment of its potential utility. Since such confidence scores
are unreliable at generation time, STM functions as a staging
area where tentative knowledge is empirically tested against
benchmark results.

For example, under a write-heavy workload on a 2-core
system for RocksDB, an STM insight might be “increasing

max_background_jobs from 2 to 4 could improve compaction
throughput.” Such an insight is specific to the current session
and may not generalize. As evidence accumulates, insights in
STM are either discarded if contradicted or promoted to LTM
once repeatedly validated. LTM, in turn, stores cross-session
knowledge such as “larger block_cache_size consistently re-
duces read latency”. This separation of transient and persistent
knowledge enables StorageX Tuner to adapt dynamically to
workload-specific behaviors while gradually consolidating
durable expertise.

The Reflector leverages the Confidence Scoring-based
Management to adjust and manage the insights. The con-
fidence score of an insight represents the level of confidence,
indicating how strongly an insight is expected to produce the
predicted effect. Each insight receives an initial confidence
score from the LLM during generation, but this score is pro-
visional and continuously updated as new benchmarking evi-
dence arrives. We adjust the score through two LLM-driven
operations: Upvote, which raises confidence when observed
benchmarking outcomes align with the insight, and Down-
vote, which lowers confidence when results contradict the
guidance. As shown in Figure 6 (Red Text segments), the
Reflector applies dedicated prompts to interpret benchmark
results and update the confidence scores of insights. Over
time, this dynamic updating allows the Reflector to validate,
demote, or discard insights, ensuring that only empirically
grounded knowledge persists in LTM.

Insights Retrieval. The Reflector combines each insight’s
updated confidence score with its contextual similarity (e.g.,
cosine similarity [64]) to produce a similarity metric weighted
by confidence. The top-k insights (e.g., k = 10) according to
this metric are then returned to the Searcher, guiding the
next round of configuration proposals with both empirically
validated and contextually relevant knowledge. In this way,
the retrieval workflow closes the loop between reasoning,
empirical validation, and insight management, ensuring that
subsequent search steps are progressively better informed and
increasingly effective.

Validation. StorageXTuner includes a lightweight Validator


===== PAGE BREAK =====

that cross-checks LLM-proposed confidence updates against
actual benchmarking results. For each insight, the Validator
verifies whether associated nodes show performance changes
consistent with the insight’s predicted effect. Confidence up-
dates are then accepted or rejected based on this verification,
ensuring that insights reflect concrete benchmark outcomes
rather than relying solely on LLM reasoning.

4 Evaluations

4.1 Prototype Implementation

We implement StorageXTuner in Python (v3.10) as a modu-
lar, LLM-driven framework composed of collaborative LLM
agents. Our evaluation spans three representative storage sys-
tems: 1) the most widely used key-value store RocksDB
(v8.8.1) [75], 2) a powerful and modularized cache engine
CacheLib developed by Meta [22], and MySQL InnoDB [8],
to cover a diverse range of storage systems. Note that for
MySQL we focus on the configuraitons of its default storage
engine InnoDB, and leave other optimizations (e.g., query-
level tuning) as future work.

We extend standard benchmarking tools, including YCSB
for RocksDB [23], CacheBench for CacheLib [21], and TPC-
H/C workloads for MySQL [11], to support workload gener-
ation and configurable tuning parameters. By default, Stor-
ageX Tuner uses OpenAI’s GPT-40 [6] as its backend LLM
model, and the model can be replaced. The source code for
StorageXTuner is available on GitHub [25].

4.2 Experimental Setup

Hardware and System Setup. Our experimental evalua-
tion is conducted on a workstation equipped with Intel(R)
Core(TM) i9-10920X CPU @ 3.50GHz, 32 GB of RAM, a
512GB NVMe SSD running Ubuntu 24.04.1 LTS. We lever-
age cgroups [15] and Docker [3] to isolate workloads and
enforce resource constraints. Unless otherwise specified, all
tests are limited to 2 CPU cores, 4GB of DRAM, and 4GB of
swap space.

Baselines. We use the default configuration file provided
by each application as our primary baselines, including the
default RocksDB configurations (RocksDB-Default), the de-
fault configurations of CacheLib (CacheLib-Default), and
the default configurations of InnoDB dnnoDB-Default). In
addition, we compare StorageXTuner against state-of-the-art
tuning solutions. Specifically:

e RocksDB: ADOC [101] (heuristic-based tuner), Endure
[57] (ML-based tuner), ELMo-Tune [88] (LLM-based
state-of-the-art tuner), LLM-Default (simple one-shot
LLM tuning), and StorageXTuner (our solution).

e CacheLib: LLM-Default and StorageXTuner.

¢ InnoDB: SMAC [68] (Bayesian optimization for tuning),
DDPG [50] (Reinforcement-learning based tuning), A-
Tune [51,52] (a LLM-based tuner), LLM-Default, and
StorageX Tuner.

Workloads and Benchmarks. We evaluate StorageX Tuner
across a variety of real-world workloads tailored to each sys-
tem under test. For RocksDB, we use macro-benchmarks
based on Yahoo’s YCSB [44] and Meta’s MixGraph [40] to
capture realistic access patterns. Unless otherwise specified,
all write-intensive workloads operate on 50 million key-value
(KV) pairs, and all read-related tests access at least 20 mil-
lion KV pairs, with each key and value set to 16 bytes and 48
bytes, respectively. For CacheLib, we evaluate three workload
profiles: write-intensive, read-intensive, and mixed workloads.
Each test involves a total of 50 million operations, the average
cache item size is set to 128 bytes, and the access pattern is
defined by a configurable lookup-to-insert ratio, set to 9:1 for
read-intensive workloads, 1:9 for write-intensive workloads,
and 5:5 for mixed workloads. For MySQL InnoDB, we bench-
mark the system using TPC-C and TPC-H workloads [10, 1 1],
with database sizes set to 1GB and 5GB, respectively. All
reported results are the average of three or more runs.
Performance Metrics. Since LLM-driven tuning is a fully
new research area, traditional measurement metrics like even-
tual throughput or latency improvement are insufficient to
capture the unique dynamics of LLM-based solutions. LLMs
introduce reasoning overheads and semantic decisions that
can yield both significant gains and unpredictable outcomes.
Therefore, we propose four new measurement metrics tai-
lored for LLM-driven tuning, which can better guide the
designs and evaluations. Max Performance Gain (MPG)
captures the relative improvement over the baseline perfor-
mance metrics (e.g., throughput). Token Cost to 95% Max
Performance (TC95) measures the total number of LLM
tokens consumed to reach 95% of the peak performance. We
use 95% of peak performance instead of the absolute maxi-
mum because it better reflects typical token usage in practice.
Token Efficiency (TE) quantifies how much performance
gain is achieved per 1,000 tokens used, helping assess the
cost-effectiveness of tuning. Token-Weighted Error Rate
(TWER) represents the number of invalid or error-prone con-
figurations generated per 1,000 tokens. Invalid configurations
include syntax errors, unrecognized options, or configurations
that result in execution failures. These metrics collectively
offer insight into the quality, efficiency, and robustness of the
LLM-driven tuning solutions.

4.3. Overall Performance Evaluation

Key-Value Stores. We assess StorageXTuner ’s performance
on real-world benchmarks using MixGraph [40] and YCSB
[44] workloads for RocksDB. With MixGraph workload, as
shown in Figure 7, StorageXTuner achieves up to a 575%
increase in throughput and a reduction in latency by up to
88% compared to other baselines. Under the YCSB work-
load, StorageXTuner achieves throughput improvements of
up to 111% and p99 latency reductions of up to 56% com-
pared to ELMo-Tune, the current state-of-the-art LLM-based
tuning framework for RocksDB. These results demonstrate


===== PAGE BREAK =====

RocksDB-Default     KXY ADOC     E="] Endure

Throughput
(kops/s)

YCSB-A YCSB-B

LLM-Default

N W B
o Oo 8S
o oOo 98O

B
fo}

latency (ps
Oo

-A YCSB-B

0

YCSB

Figure 7: Performance Comparison under YCSB and Mixgraph Workload in RocksDB

ZZA CacheLib-Default         LLM-Default    x InfiniTuner
4 40004                               — 100

20                                s

Sa                                2

2 & 2000]                     % 90  A

eg                            ;   3

id                                ¥       H

F     0              l              = 80           aa

Mix
Worklaod

Read
Intensive

Write
Intensive

Write                        Mix
Intensive                   Worklaod

Figure 8: Performance with Different Workload in CacheLib

Read
Intensive

—e*—LambdaTune
——infiniTuner

DDPG
—+*—LLM-Default

—=—-SQL Default
—e-SMAC

3000

2000

1000

Throughput (tx/s)

4                       10

Iteration

6

Figure 9: Best Performance over Iterations on InnoDB.

StorageXTuner ’s more effective use of LLMs and structured
feedback. ADOC, Endure, rule-based, and ML-based tuning
solutions improve throughput over the RocksDB-Default base-
line by up to 43% and 141%, respectively. However, they still
fall short of StorageXTuner: StorageXTuner surpasses these
two methods by up to 369% in throughput. Additionally, com-
pared to the LLM-Default baseline, a naive one-shot interac-
tion with an LLM lacking insight feedback and validation,
StorageXTuner achieves a throughput improvement of up to
80% and significantly more stable tail latencies. These results
highlight the importance of structured tuning experiences,
insight-driven search, and feedback validation in enabling
efficient and reliable LLM-powered tuning.

Cache Systems. For CacheLib, we evaluate StorageX Tuner
across three workload types: write-intensive, read-intensive,
and mixed. We use the cachebench tool with configurable
access patterns and object sizes to simulate realistic cache
usage scenarios. To the best of our knowledge, no existing
LLM based tuning framework has been applied to CacheLib,
making StorageXTuner the first to address this configuration
space using an LLM-driven approach. As shown in Figure 8,
compared to the CacheLib-Default baseline, StorageXTuner
achieves up to a 26% improvement in throughput and im-
proves cache hit ratio by up to 3.1 percentage points across
workloads. Specifically, for the write-intensive workload, Stor-
ageXTuner increases the hit ratio from 94.5% to 96.5%, and
for the read-intensive workload, the hit ratio improves by 2.9

10

Table 3: Standardized Metric Analysis.

Metric                                         StorageXTunerELMo-          LLM-
Tune          Default
Max Performance Gain                      5.75             3.88             2.04
Token Cost to 95% Max                         56K               138K             214K
Token Efficiency                                          0.10                 0.02                0.009
Token-Weighted Error Rate                    0.017               0.024               0.028

percentage points, demonstrating more efficient memory uti-
lization and improved eviction policy decisions. Additionally,
it outperforms the LLM-Default baseline by up to 15%, il-
lustrating the advantage of StorageXTuner ’s insight-driven,
feedback-aware tuning in optimizing complex caching sys-
tems.

SQL Storage Engines. For InnoDB, the default storage en-
gine in MySQL, StorageXTuner is evaluated using two widely
adopted transactional and analytical benchmarks: TPC-C
and TPC-H, respectively. StorageXTuner consistently out-
performs all baselines across both benchmarks. As shown
in Figure 9, under the TPC-C workload, StorageXTuner in-
creases throughput by up to 709% over InnoDB-Default and
29% over LLM-Default. StorageXTuner achieves over 15%
higher throughput compared to A-Tune. In the TPC-H work-
load, StorageXTuner reduces query latency by up to 71%
(from 8.9s to 2.5s) across all baselines, showcasing its ability
to improve analytical query performance through better buffer
pool sizing and I/O tuning. This underscores the effectiveness
of its insight-driven and feedback validation tuning, even in
relational database storage engines.

Standardized Metric Analysis. Table 3 highlights the ad-
vantages of StorageXTuner over the LLM-Default baseline
across several standardized evaluation dimensions. First, Stor-
ageX Tuner achieves a significantly higher peak performance,
demonstrating its ability to discover more effective config-
urations. More importantly, it reaches 95% of its peak per-
formance using fewer LLM tokens (56K vs. 214K), which
translates to both lower latency in interaction rounds and re-
duced monetary cost. This improvement in cost-efficiency is
further underscored by its higher token efficiency, achieving
11x and 5x improvements compared to LLM-Default and
ELMo-Tune, respectively. Beyond efficiency, StorageXTuner
exhibits greater robustness. It accumulates fewer errors in the
tuning process, as evidenced by a lower Token- Weighted Error
Rate (TWER = 0.017 vs. 0.028), indicating better stability and
reduced need for human correction or fallback mechanisms.


===== PAGE BREAK =====

(ZZ) LLM-No Info

[=] +Workload Info

 +Hardware Info
~

(x1 +Tree Search
AN) +Static Insights

[= +Dynamic Insights
+Validation

N
°
o

go 9
NOK

a
fo)
ro}

rror Ratio

Throughpu
(kops/s)

EERE

Figure 10: Ablation Study

°

E
°
°

Table 4: Performance comparison across different versions on
the fillrandom workload (throughput in kops/s). X indicates
the method fails to run.

RocksDB                                       LevelDB
Method         5.7.1 611.6 75.3 88.1 1.23 1.21
Default          47.6     126     196    234    108    64
SILK                              114              x                x               x              x             x
ELMO                             x                x              210          242          119          72
StorageXTuner 126              142             221            246            124            74

4.4 Ablation Study

To understand the impact of progressively enriching the tuning
context for LLM-driven configuration generation, we conduct
an ablation study on RocksDB using MixGraph. In addition
to varying the levels of input information, we also evaluate
the contributions of our design components, including tree
search, dynamic tuning insights, and the validation checker.
We measure how each element affects both throughput and
error ratio to quantify their individual and combined benefits.

The evaluation is conducted with the following setups:

LLM-No Info: The baseline where the LLM receives only
a general tuning task description prompt, with no other tun-
ing context. +Workload Info: Adds workload benchmarking
results. +Hardware Info: Adds hardware metadata, such
as CPU and memory information. +Tree Search: Adds Tree
Search without insights. +Static Insights: Adds static insights.
+Dynamic Insights: Adds dynamic insights (insights man-
agement). +Validation: Combines validation mechanisms.

As shown in Figure 10, each additional layer of context
contributes to measurable improvements. Starting with the
LLM without any guidance, throughput is the lowest (65,908
ops/sec) with a high error ratio (56%). Adding workload
information increases throughput to 87,664 ops/sec and re-
duces the error ratio to 41%. Incorporating hardware infor-
mation further improves throughput to 98,470 ops/sec and
lowers the error ratio to 38%. Introducing tree search context
boosts throughput to 161,847 ops/sec and reduces the error
ratio to 22%. Adding static insights from historical tuning
records further increases throughput to 185,542 ops/sec and
lowers error ratio to 18%. Incorporating dynamic insights,
which include runtime feedback and performance refinement,
raises throughput to 247,257 ops/sec and reduces the error
ratio to 10%. Finally, the full StorageXTuner setup with vali-
dation mechanisms achieves 246,352 ops/sec and eliminates
all invalid configuration attempts.

11

Table 5: Impact of Different Number of Child Nodes.

Child Nodes              1         2         3         4         5
Max Performance Gain 2.15 3.58 5.75 5.86 5.92
Token Efficiency          0.07 0.09 0.10 0.07 0.06

Table 6: Impact of Different Numbers of Insights.

K Value                 1         2         4         8         16
Max Performance Gain 1.48 2.49 4.68 5.84 5.89
Token Efficiency           0.04 0.06 0.09 O.11 0.04

4.5 Sensitivity Analysis

To evaluate the robustness of StorageXTuner, we conduct a
sensitivity analysis across a variety of resource and model-
level parameters. This study demonstrates how the perfor-
mance of StorageXTuner responds to changes in system ver-
sion diversity, exploration child node, number of selected top
insights, and the capabilities of the underlying LLM.

Different Storage System Implementations and Version.
Table 4 reports throughput under the fillrandom workload of
RocksDB and LevelDB (two different implementations of
LSM-based key-value stores) across multiple versions. We
observe that StorageXTuner consistently achieves the highest
performance and runs across all tested versions, while Elmo-
Tune and SILK [30] (an I/O scheduler for RocksDB) fail on
several versions due to compatibility issues. This highlights
StorageX Tuner ’s ability to deliver robust and generalizable
performance across diverse system versions.

Different Number of Child Node. Table 5 shows the impact
of varying the number of child nodes on tuning effectiveness.
Increasing the number of child nodes generally improves
maximum performance gain, with diminishing returns be-
yond three nodes. However, token efficiency does not scale
proportionally and even decreases when the branching fac-
tor becomes too large. This highlights a trade-off between
performance gain and efficiency, suggesting that moderate
branching achieves the best balance.

Different Number of Insight. As shown in Table 6 com-
pares the effect of different numbers of Top-K insights on
tuning performance. As K increases from | to 8, the max-
imum performance gain improves significantly, indicating
that leveraging more insights helps the tuner explore better
configurations. However, increasing K beyond 8 yields dimin-
ishing returns, and token efficiency drops for K = 16 due to
higher token overhead during reasoning. This demonstrates
that choosing an appropriate number of Top-K insights is
important to balance performance gains and efficiency.

LLM Model Selection.. As shown in Figure |1, we evalu-
ate StorageXTuner with a range of LLMs to understand the
impact of model size and reasoning capabilities on tuning
quality. Specifically, we utilize OpenAI’s GPT-3.5, GPT-40,
ol, ol-mini, and 03-mini models. Where applicable, we also
analyze the effect of different reasoning levels within the
03-mini family. To explore the trade-offs between small and
large open-source models, we select various configurations


===== PAGE BREAK =====

(ZZZAGPT-3.5       KX GPT-o1                 [22] GPT-03-mini-high Hj LLaMA-3-8B
E==] GPT-4o0       (I GPT-03-mini-low       KANN LLaMA-3.2-1B     f= LLaMA-3.1-70B
E==] GPT-ol-mini == GPT-03-mini-medium [= LLaMA-3.2-3B     HE LLaMA-3.3-70B

2          =        010
Bg 100   5  |      i  505  T

Figure 11: StorageXTuner on Different LLM Models

from the LLaMA series—namely, LLaMA-3.2-1B, 3.2-3B,
3.1-8B, 3.1-70B, and 3.3-70B—all of which publicly disclose
model sizes. The results show that larger, reasoning-capable
models consistently yield higher-quality insights and tuning
decisions. In contrast, models with limited reasoning abil-
ity—such as 01-mini and the low- and medium-capacity vari-
ants of o3-mini—perform worse than GPT-40, which, while
powerful, lacks explicit reasoning prompts. These results sug-
gest that even a high-capacity model without dedicated rea-
soning prompts can achieve strong performance, but enabling
robust reasoning capabilities further improves tuning effec-
tiveness. Overall, these results show that StorageXTuner can
work with different models effectively, and it is expected that
the tuning effectiveness will scale as LLM evolves

5 Lessons Learned

In this section, we summarize a few key lessons learned from
building and evaluating StorageXTuner:

A small set of key insights dominates the search trajec-
tory. The search process is mainly guided by a small set of
human-auditable high-level insights. Treating these insights
as first-class artifacts to guide exploration significantly re-
duces invalid attempts and costs. Critically, these general
insights are transferable across heterogeneous systems, effec-
tively accelerating cold starts.

Proposal of new configurations is the primary error source.
Errors primarily arise not from editing existing configurations,
but from proposing changes to untouched configurations,
which trigger string/enum mismatches and version incompati-
bilities. Systematic numerical mistakes (e.g., unit confusion,
budget violations) are also common. These are best mitigated
by pre-execution schema validation and enforcing normalized
budget caps, rather than relying on reactive fixes.

Closed-loop agent architecture outperforms monolithic
model reasoning. A closed-loop, multi-agent architecture
with clear role separation delivers more robust and stable
gains than increasing the underlying model’s size. This design
is largely autonomous for well-documented open-source sys-
tems but benefits from lightweight retrieval of local documen-
tation to stabilize performance on closed-source or rapidly
evolving ones.

Human-in-the-loop shifts from low-level tuning to high-
level strategy. With StorageXTuner, the human’s role evolves
from manual tuning to strategic oversight. By providing ex-
plicit guardrails (e.g., budgets, SLOs), experts enable the
agent to explore safely and broadly. Their primary value thus

12

becomes curating high-impact insights and adjudicating trade-
offs across non-aligned objectives, which yields auditable
improvements with minimal manual intervention.

6 Related Work

Analyzing Software Configurations. Great efforts exist to
analyze and/or test configurations of various software sys-
tems [36-38,43,59,60,73,81,84,98]. For example, SPEX [98]
analyzes configuration constraints and uses inferred con-
straints to harden systems against misconfigurations. Sim-
ilarly, cDEP [43] and ConfD [73] analyze configuration de-
pendencies in cloud systems (e.g., Hadoop and OpenStack)
and file systems (e.g., Ext4, XFS), respectively. Insights de-
rived from these efforts could potentially be leveraged to
guide auto-tuning and are complementary to StorageXTuner.

Heuristic-based Approaches. To alleviate tedious manual
configuration, heuristic approaches [36, 86] encode expert
knowledge into predefined rules or strategies to guide parame-
ter adjustments. For example, a workload-aware heuristic can
recommend a larger cache size for read-heavy workloads or
an infrequent compaction policy for write-heavy workloads.
While such heuristics capture certain domain insights, rules
crafted for one system or version rarely transfer to others,
forcing experts to rebuild strategies.

ML-based Approaches. To overcome these limitations, re-
searchers have proposed automated approaches that typically
follow a two-phase workflow. First, they collect representative
workload characteristics (e.g., I/O patterns, query plan cost
vectors) by automated profiling or user queries. And second,
different optimization engines (like reinforcement-learning
(RL) agents using policy gradients) iteratively propose config-
urations that are tested on offline benchmarks before finaliza-
tion. Prominent examples include CherryPick [24] and DB-
Tune [13], which leverage Gaussian-process—based Bayesian
optimization to reduce tuning overhead by up to 75% on
cloud analytics workloads; QTune [65] and CDBTune [103],
apply deep RL-models to adapt configurations under dy-
namic query and cloud-instance patterns; and OtterTune’s
[90] meta-learning engine fingerprints workloads to transfer
priors across deployments, cutting required trials nearly in
half. While more general than heuristics-based methods, ML
approaches are closely tied to training data and often lose
effectiveness as workload, hardware, or software evolves. Fur-
ther, they often focus on subsets of configurations, as with
Endure [57] and Dremel [105] for key-value store tuning.

7 Conclusion

In this paper, we propose StorageX Tuner, a novel auto-tuning
framework for heterogeneous storage systems that leverages
specialized LLM agents to efficiently explore configurations
while dynamically generating and managing tuning insights.
This framework not only opens new opportunities for manag-
ing insights across storage systems but also introduces new
measurement metrics for evaluating LLM-based configuration
tuning systems.


===== PAGE BREAK =====

References
[1] Claude. URL: https://claude.ai/.

[2] Copilot. URL: https://copilot.microsoft.com/.

[3] Docker: Accelerated Container Application Develop-

ment. URL: https: //www.docker.com/.

[4] Google Gemini. URL: https://gemini.google.co

m.

[5] GPT-4 API general availability and deprecation of

[13

“4

[14

sy

[17]

older models in the Completions API. URL: https:
//openai.com/blog/gpt-4-api-general-avail
ability.

Hello GPT-40. URL: https: //openai.com/index
/hello-gpt-4o0/.

Myrocks. http://myrocks.io/.

MySQL :: MySQL 8.4 Reference Manual :: 17 The
InnoDB Storage Engine. URL: https: //dev.mysql.
com/doc/refman/8.4/en/innodb-storage-eng
ine. html.

Redis. https://redis.io/.

TPC-C Homepage. URL: https://www.tpc.org/
tpcc/default5.asp.

TPC-H Homepage. URL: https://www.tpc.org/
tpch/.

HDFS Architecture Guide, May 2022. URL: https:
//nadoop.apache.org/docs/rl.2.1/hdfs_desig
n.html.

PKU-DAIR/DBTune, December 2024. original-date:
2022-03-23T05:28:08Z. URL: https://github.c
om/PKU-DAIR/DBTune.

Amazon S3 - Cloud Object Storage - AWS, September
2025. URL: https://aws.amazon.com/s3/.

Control Group v2 — The Linux Kernel documentation,
April 2025. URL: https://www.kernel.org/doc
/ntml/latest/admin-guide/cgroup-v2.html.

ext4(5) - Linux manual page, September 2025. URL:
https: //www.man7.org/linux/man-pages/man5/
ext4.5.html.

LLM Training Datasets - a sugatoray Collection, April
2025. URL: https: //huggingface.co/collectio
ns/sugatoray/llm-training-datasets-65dbe
4ab2b0037ec198b0 9ab.

13

[18]

[19]

[20]

[21]

Managed Graph Database - Amazon Neptune - AWS,
September 2025. URL: https://aws.amazon.com
/neptune/.

Neo4j Graph Database & Analytics — The Leader in
Graph Databases, September 2025. URL: https: //
neo4j.com/.

xfs(5) - Linux manual page, September 2025. URL:
https://www.man7.org/linux//man-pages/man
5/xfs.5.html.

Cache bench. https: //cachelib.org/docs/Cach
e_Library_User-sz_Guides/Cachebench_Over
view/, Accessed: June, 2023.

Cachelib github. https: //github.com/facebook/
CacheLib, Accessed: June, 2023.

db_bench. https://github.com/facebook/rock
sdb/wiki/Benchmarking-tools, Accessed: June,
2023.

Omid Alipourfard, Hongqiang Harry Liu, Jianshu
Chen, Shivaram Venkataraman, Minlan Yu, and Ming
Zhang. {CherryPick}: Adaptively Unearthing the Best
Cloud Configurations for Big Data Analytics. pages
469-482, 2017. URL: https: //www.usenix.org/c
onference/nsdil7/technical-sessions/pres
entation/alipourfard.

Anonymous. Infinitune, September 2025. URL: http
s://github.com/gdaythcli/InfiniTune.

Anthony Anthony and Yaganti Naga Malleswara Rao.
Memcached, redis, and aerospike key-value stores em-
pirical comparison.

Apache. Hadoop distributed file system, 2005. URL:
https://hadoop.apache.org/docs/r1.2.1/hdfs
_design.html.

Martin Arlitt and Tai Jin. A workload characterization
study of the 1998 world cup web site. IEEE network,
14(3):30-37, 2000.

Berk Atikoglu, Yuehai Xu, Eitan Frachtenberg, Song
Jiang, and Mike Paleczny. Workload analysis of a
large-scale key-value store. In ACM SIGMETRICS
Performance Evaluation Review, volume 40, pages 53—
64. ACM, 2012.

Oana Balmau, Florin Dinu, Willy Zwaenepoel, Karan
Gupta, Ravishankar Chandhiramoorthi, and Diego Di-
dona. SILK: Preventing latency spikes in Log-
Structured merge Key-Value stores. In 2019 USENIX
Annual Technical Conference (USENIX ATC 19), pages
753-766, Renton, WA, July 2019. USENIX Associa-
tion. URL: https://www.usenix.org/conferenc
e/atcl9/presentation/balmau.


===== PAGE BREAK =====

[31] Babak Behzad, Huong Vu Thanh Luu, Joseph Huchette,

[33

[37

“

“4

—

“4

Surendra Byna, Prabhat, Ruth Aydt, Quincey Koziol,
and Marc Snir. Taming parallel I/O complexity with
auto-tuning. In Proceedings of the International
Conference on High Performance Computing, Net-
working, Storage and Analysis, pages 1-12, Denver
Colorado, November 2013. ACM. URL: https:
//dl.acm.org/doi/10.1145/2503210.2503278,
doi:10.1145/2503210.2503278.

Benjamin Berg, Daniel S Berger, Sara McAllister, Isaac
Grosof, Sathya Gunasekar, Jimmy Lu, Michael Uhlar,
Jim Carrig, Nathan Beckmann, Mor Harchol-Balter,
et al. The cachelib caching engine: Design and ex-
periences at scale. In 14th USENIX Symposium on
Operating Systems Design and Implementation (OSDI
20), pages 753-768, 2020.

Nathan Bronson, Zach Amsden, George Cabrera,
Prasad Chakka, Peter Dimov, Hui Ding, Jack Ferris,
Anthony Giardullo, Sachin Kulkarni, and Harry C Li.
TAO: Facebook’s distributed data store for the social
graph. In 20/3 USENIX Annual Technical Conference
(USENIX ATC 13), pages 49-60, 2013.

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Nee-
lakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
Sandhini Agarwal, Ariel Herbert-Voss, Gretchen
Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,
Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,
Christopher Hesse, Mark Chen, Eric Sigler, Mateusz
Litwin, Scott Gray, Benjamin Chess, Jack Clark,
Christopher Berner, Sam McCandlish, Alec Radford,
Ilya Sutskever, and Dario Amodei. Language models

are few-shot learners, 2020. URL: https://arxiv.

org/abs/2005.14165, arXiv:2005.14165.

Zhen Cao. A Practical Auto-Tuning Framework for
Storage Systems. PhD thesis, PhD thesis, Stony Brook
University, 2019.

Zhen Cao, Geoff Kuenning, and Erez Zadok. Carver:
Finding important parameters for storage system tun-
ing. In Proceedings of the 18th USENIX Conference
on File and Storage Technologies (FAST), 2020.

Zhen Cao, Vasily Tarasov, Hari Prasath Raman, Dean
Hildebrand, and Erez Zadok. On the performance
variation in modern storage stacks. In 15th USENIX
conference on file and storage technologies (FAST 17),
2017.

Zhen Cao, Vasily Tarasov, Sachin Tiwari, and Erez
Zadok. Towards better understanding of black-box
{Auto-Tuning}: A comparative analysis for storage

14

[39

[40

[41

[42

—“

=

sy

]

“4

sy

systems. In 2018 USENIX Annual Technical Confer-
ence (USENIX ATC 18), pages 893-907, 2018.

Zhichao Cao, Siying Dong, Sagar Vemuri, and David
H. C. Du. Characterizing, Modeling, and Benchmark-
ing {RocksDB} {Key-Value} Workloads at Facebook.
pages 209-223, 2020. URL: https: //www.usenix
.org/conference/fast20/presentation/cao-z
hichao.

Zhichao Cao, Siying Dong, Sagar Vemuri, and
David HC Du. Characterizing, modeling, and bench-
marking rocksdb key-value workloads at facebook. In
18th USENIX Conference on File and Storage Tech-
nologies (FAST 20), pages 209-223, 2020.

Lingjiao Chen, Matei Zaharia, and James Zou. Fru-
galgpt: How to use large language models while re-
ducing cost and improving performance, 2023. URL:
https://arxiv.org/abs/2305.05176, arXiv:
2305.05176.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,
Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri
Edwards, Yuri Burda, Nicholas Joseph, Greg Brock-
man, Alex Ray, Raul Puri, Gretchen Krueger, Michael
Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin,
Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
Alethea Power, Lukasz Kaiser, Mohammad Bavarian,
Clemens Winter, Philippe Tillet, Felipe Petroski Such,
Dave Cummings, Matthias Plappert, Fotios Chantzis,
Elizabeth Barnes, Ariel Herbert-Voss, William Heb-
gen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie
Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,
William Saunders, Christopher Hesse, Andrew N. Carr,
Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa,
Alec Radford, Matthew Knight, Miles Brundage, Mira
Murati, Katie Mayer, Peter Welinder, Bob McGrew,
Dario Amodei, Sam McCandlish, Ilya Sutskever, and
Wojciech Zaremba. Evaluating large language models
trained on code, 2021. URL: https: //arxiv.org/
abs/2107.03374, arXiv:2107.03374.

Qingrong Chen, Teng Wang, Owolabi Legunsen, Shan-
shan Li, and Tianyin Xu. Understanding and Discov-
ering Software Configuration Dependencies in Cloud
and Datacenter Systems. In Proceedings of the 28th
ACM Joint Meeting on European Software Engineer-
ing Conference and Symposium on the Foundations of
Software Engineering (ESEC/FSE), 2020.

Brian F Cooper, Adam Silberstein, Erwin Tam, Raghu
Ramakrishnan, and Russell Sears. Benchmarking
cloud serving systems with YCSB. In Proceedings
of the Ist ACM symposium on Cloud computing, pages
143-154. ACM, 2010.


===== PAGE BREAK =====

[45]

[46

=

[47

—

[50

=

[52

“

Carlo Curino, Evan Jones, Yang Zhang, and Sam Mad-
den. Schism: a workload-driven approach to database
replication and partitioning. Proc. VLDB Endow.,
3(1-2):48-57, September 2010. doi:10.14778/1
920841.1920853.

Sixun Dong, Juhua Hu, Mian Zhang, Ming Yin, Yanjie
Fu, and Qi Qian. Mmtok: Multimodal coverage maxi-
mization for efficient inference of vlms. arXiv preprint
arXiv:2508. 18264, 2025.

Siying Dong, Shiva Shankar P, Satadru Pan, Anand
Ananthabhotla, Dhanabal Ekambaram, Abhinav
Sharma, Shobhit Dayal, Nishant Vinaybhai Parikh,
Yanqin Jin, Albert Kim, et al. Disaggregating rocksdb:
A production experience. Proceedings of the ACM on
Management of Data, 1(2):1-24, 2023.

Songyun Duan, Vamsidhar Thummala, and Shivnath
Babu. Tuning database configuration parameters with
ituned. Proc. VLDB Endow., 2(1):1246-1257, August
2009. doi:10.14778/1687627.1687767.

Facebook. Zippydb, 2021. URL: https: //engineer
ing. fb.com/2021/08/06/core-infra/zippydb/.

Rasool Fakoor, Pratik Chaudhari, and Alexander J.
Smola. Ddpg++: Striving for simplicity in continuous-
control off-policy reinforcement learning, 2020. URL:
https://arxiv.org/abs/2006.15199, arXiv:
2006.15199.

Victor Giannakouris and Immanuel Trummer. Demon-
strating A-tune: Exploiting large language models for
workload-adaptive database system tuning. In Com-
panion of the 2024 International Conference on Man-
agement of Data, pages 508-511, 2024.

Victor Giannankouris and Immanuel Trummer.
{\lambda}-tune: Harnessing large language models
for automated database system tuning. arXiv preprint
arXiv:2411.03500, 2024.

Nanxu Gong, Sixun Dong, Haoyue Bai, Xinyuan Wang,
Wangyang Ying, and Yanjie Fu. Agentic feature aug-
mentation: Unifying selection and generation with
teaming, planning, and memories. arXiv preprint
arXiv:2505. 15076, 2025.

Nanxu Gong, Chandan K Reddy, Wangyang Ying,
Haifeng Chen, and Yanjie Fu. Evolutionary large lan-
guage model for automated feature transformation. In
Proceedings of the AAAI conference on artificial intel-
ligence, volume 39, pages 16844-16852, 2025.

15

[55] Nanxu Gong, Xinyuan Wang, Wangyang Ying, Haoyue

[56

[58

[63

=

—

“4

—“

=

sy

“

“4

sy

Bai, Sixun Dong, Haifeng Chen, and Yanjie Fu. Unsu-
pervised feature transformation via in-context genera-
tion, generator-critic Ilm agents, and duet-play teaming.
arXiv preprint arXiv:2504.21304, 2025.

Google. Leveldb, 2011. URL: https: //github.com
/google/leveldb/.

Andy Huynh, Harshal A. Chaudhari, Evimaria Terzi,
and Manos Athanassoulis. Endure: a robust tuning
paradigm for lsm trees under workload uncertainty.
Proc. VLDB Endow.,, 15(8):1605-1618, apr 2022. doi:
10.14778/3529337.3529345.

Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan
Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto,
and Pascale Fung. Survey of hallucination in natu-
ral language generation. ACM Computing Surveys,
55(12):1-38, March 2023. URL: http: //dx.doi.o
rg/10.1145/3571730, doi:10.1145/3571730.

Dongpu Jin, Xiao Qu, Myra B. Cohen, and Brian
Robinson. Configurations everywhere: Implications
for testing and debugging in practice. In Proceedings
of the 36th International Conference on Software En-
gineering (ICSE), 2014.

Lorenzo Keller, Prasang Upadhyaya, and George Can-
dea. ConfErr: A tool for assessing resilience to human
configuration errors. In Proceedings of the 38th IEEE
International Conference on Dependable Systems and

Networks (DSN), 2008.

Aida Kostikova, Zhipin Wang, Deidamea Bajri, Ole
Piitz, Benjamin Paafien, and Steffen Eger. Lilms:
A data-driven survey of evolving research on lim-
itations of large language models. arXiv preprint
arXiv:2505.19240, 2025.

Saurabh Kumar, Gaurav Khandelwal, Arjun Varshney,
and Mukul Arora. Cost-based query optimization with
heuristics. International Journal of Scientific & Engi-
neering Research, 2(9), 2011.

Jiale Lao, Yibo Wang, Yufei Li, Jianping Wang, Yunjia
Zhang, Zhiyuan Cheng, Wanghu Chen, Mingjie Tang,
and Jianguo Wang. GPTuner: A Manual-Reading
Database Tuning System via GPT-Guided Bayesian
Optimization. Proc. VLDB Endow., 17(8):1939-1952,
April 2024. URL: https://dl.acm.org/doi/10.
14778/3659437.3659449, doi:10.14778/3659437
.3659449,

Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio
Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich
Kiittler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel,


===== PAGE BREAK =====

[65

“4

[66

=

[67

—

[69

—“

[70

=

[71]

et al. Retrieval-augmented generation for knowledge-
intensive nlp tasks. Advances in neural information
processing systems, 33:9459-9474, 2020.

Guoliang Li, Xuanhe Zhou, Shifu Li, and Bo Gao.
QTune: a query-aware database tuning system with
deep reinforcement learning. Proc. VLDB Endow.,
12(12):2118-2130, August 2019. URL: https://
dl.acm.org/doi/10.14778/3352063.3352129,
doi:10.14778/3352063.3352129.

Huayang Li, Pat Verga, Priyanka Sen, Bowen Yang,
Vijay Viswanathan, Patrick Lewis, Taro Watanabe, and
Yixuan Su. ALR?: A Retrieve-then-Reason framework
for long-context question answering. arXiv preprint
arXiv:2410.03227, 2024.

Yan Li, Kenneth Chang, Oceane Bel, Ethan L. Miller,
and Darrell D. E. Long. CAPES: unsupervised storage
performance tuning using neural network-based deep
reinforcement learning. In Proceedings of the Interna-
tional Conference for High Performance Computing,
Networking, Storage and Analysis, pages 1-14, Den-
ver Colorado, November 2017. ACM. URL: https:
//dl.acm.org/doi/10.1145/3126908.3126951,
doi:10.1145/3126908.3126951.

Marius Lindauer, Katharina Eggensperger, Matthias
Feurer, André Biedenkapp, Difan Deng, Carolin Ben-
jamins, Tim Ruhkopf, René Sass, and Frank Hutter.
Smac3: A versatile bayesian optimization package
for hyperparameter optimization. Journal of Ma-
chine Learning Research, 23(54):1-9, 2022. URL:
http://jmlr.org/papers/v23/21-0888.html.

Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-
jape, Michele Bevilacqua, Fabio Petroni, and Percy
Liang. Lost in the middle: How language models use
long contexts. Transactions of the Association for
Computational Linguistics, 12:157-—173, 2024. URL:
https://aclanthology.org/2024.tacl-1.9/,
doi:10.1162/tacl_a_00638.

Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,
Hiroaki Hayashi, and Graham Neubig. _ Pre-train,
prompt, and predict: A systematic survey of prompt-
ing methods in natural language processing, 2021.
URL: https://arxiv.org/abs/2107.13586,
arXiv:2107.13586.

Kai Lu, Guokuan Li, Jiguang Wan, Ruixiang Ma, and
Wei Zhao. ADSTS: Automatic Distributed Storage
Tuning System Using Deep Reinforcement Learning.
In Proceedings of the 51st International Conference
on Parallel Processing, pages 1-13, Bordeaux France,
August 2022. ACM. URL: https://dl.acm.org/d

16

[73

[74

[75

[76

(77

[78

“

“4

sy

]

]

—

“4

01/10.1145/3545008.3545012, doi:10.1145/35
45008.3545012.

Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler
Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha
Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank
Gupta, Bodhisattwa Prasad Majumder, Katherine Her-
mann, Sean Welleck, Amir Yazdanbakhsh, and Pe-
ter Clark. Self-refine: Iterative refinement with self-
feedback, 2023. URL: https: //arxiv.org/abs/23
03.17651, arXiv:2303.17651.

Tabassum Mahmud, Om Rameshwar Gatla, Duo
Zhang, Carson Love, Ryan Bumann, and Mai Zheng.
ConfD: Analyzing configuration dependencies of file
systems for fun and profit. In 21st USENIX Conference
on File and Storage Technologies (FAST 23), 2023.

Aleksander Maricq, Dmitry Duplyakin, Ivo Jimenez,
Carlos Maltzahn, Ryan Stutsman, and Robert Ricci.
Taming performance variability. In 13th USENIX Sym-
posium on Operating Systems Design and Implementa-

tion (OSDI 18), pages 409-425, 2018.

Meta. Rocksdb, a persistent key-value store for fast
storage enviroments, 2019. URL: https: //rocksdb.
org.

Kiran-Kumar Muniswamy-Reddy, David A. Holland,
Uri Braun, and Margo Seltzer. Provenance-aware stor-
age systems. In Proceedings of the Annual Conference
on USENIX ’06 Annual Technical Conference, ATEC
06, page 4, USA, 2006. USENIX Association.

Rajesh Nishtala, Hans Fugal, Steven Grimm, Marc
Kwiatkowski, Herman Lee, Harry C Li, Ryan McElroy,
Mike Paleczny, Daniel Peek, and Paul Saab. Scaling
memcache at facebook. In nsdi, volume 13, pages
385-398, 2013.

OpenAI, Josh Achiam, Steven Adler, Sandhini Agar-
wal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-
man, Diogo Almeida, Janko Altenschmidt, Sam Alt-
man, Shyamal Anadkat, Red Avila, Igor Babuschkin,
Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming
Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello,
Jake Berdine, Gabriel Bernadett-Shapiro, Christo-
pher Berner, Lenny Bogdonoff, Oleg Boiko, Made-
laine Boyd, Anna-Luisa Brakman, Greg Brockman,
Tim Brooks, Miles Brundage, Kevin Button, Trevor
Cai, Rosie Campbell, Andrew Cann, Brittany Carey,
Chelsea Carlson, Rory Carmichael, Brooke Chan, Che
Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby
Chen, Jason Chen, Mark Chen, Ben Chess, Chester
Cho, Casey Chu, Hyung Won Chung, Dave Cummings,
Jeremiah Currier, Yunxing Dai, Cory Decareaux,
Thomas Degry, Noah Deutsch, Damien Deville, Arka


===== PAGE BREAK =====

Dhar, David Dohan, Steve Dowling, Sheila Dunning,
Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi,
Liam Fedus, Niko Felix, Simon Posada Fishman, Jus-
ton Forte, Isabella Fulford, Leo Gao, Elie Georges,
Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel
Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Mor-
gan Grafstein, Scott Gray, Ryan Greene, Joshua Gross,
Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse
Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes
Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Pe-
ter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli
Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn
Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun
Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo
Jun, Tomer Kaftan, Lukasz Kaiser, Ali Kamali, In-
gmar Kanitscheider, Nitish Shirish Keskar, Tabarak
Khan, Logan Kilpatrick, Jong Wook Kim, Christina
Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros,
Matt Knight, Daniel Kokotajlo, Lukasz Kondraciuk,
Andrew Kondrich, Aris Konstantinidis, Kyle Kosic,
Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai
Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy,
Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin,
Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia
Lue, Anna Makanju, Kim Malfacini, Sam Manning,
Todor Markov, Yaniv Markovski, Bianca Martin, Katie
Mayer, Andrew Mayne, Bob McGrew, Scott Mayer
McKinney, Christine McLeavey, Paul McMillan, Jake
McNeil, David Medina, Aalok Mehta, Jacob Menick,
Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vin-
nie Monaco, Evan Morikawa, Daniel Mossing, Tong
Mu, Mira Murati, Oleg Murk, David Mély, Ashvin
Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Nee-
lakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang,
Cullen O’ Keefe, Jakub Pachocki, Alex Paino, Joe
Palermo, Ashley Pantuliano, Giambattista Parascan-
dolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail
Pavlov, Andrew Peng, Adam Perelman, Filipe de
Avila Belbute Peres, Michael Petrov, Henrique Ponde
de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass,
Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris
Power, Elizabeth Proehl, Raul Puri, Alec Radford,
Jack Rae, Aditya Ramesh, Cameron Raymond, Francis
Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri
Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders,
Shibani Santurkar, Girish Sastry, Heather Schmidt,
David Schnurr, John Schulman, Daniel Selsam, Kyla
Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker,
Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie
Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Ben-
jamin Sokolowsky, Yang Song, Natalie Staudacher, Fe-
lipe Petroski Such, Natalie Summers, Ilya Sutskever,
Jie Tang, Nikolas Tezak, Madeleine B. Thompson,
Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Pre-

17

[79]

[80]

[81]

[82]

[83]

[84]

ston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-
lipe Cerén Uribe, Andrea Vallone, Arun Vijayvergiya,
Chelsea Voss, Carroll Wainwright, Justin Jay Wang,
Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei,
C. J. Weinmann, Akila Welihinda, Peter Welinder, Ji-
ayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner,
Clemens Winter, Samuel Wolrich, Hannah Wong, Lau-
ren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai
Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wo-
jciech Zaremba, Rowan Zellers, Chong Zhang, Mar-
vin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang
Zhuang, William Zhuk, and Barret Zoph. GPT-4
Technical Report, March 2024. arXiv:2303.08774
[cs]. URL: http://arxiv.org/abs/2303.08774,
doi:10.48550/arXiv.2303.08774.

Satadru Pan, Theano Stavrinos, Yungiao Zhang,
Atul Sikaria, Pavel Zakharov, Abhinav Sharma,
Shiva Shankar P, Mike Shuey, Richard Wareing,
Monika Gangapuram, Guanglei Cao, Christian Preseau,
Pratap Singh, Kestutis Patiejunas, JR Tipton, Ethan
Katz-Bassett, and Wyatt Lloyd. Facebook’s tectonic
filesystem: Efficiency from exascale. In 19th USENIX
Conference on File and Storage Technologies (FAST
21), pages 217-231. USENIX Association, February
2021. URL: https: //www.usenix.org/conferenc
e/fast21/presentation/pan.

By James E. Powell and 2021 September 7. Using Ma-
chine Learning for Automatic Database Tuning. URL:
https://tdwi.org/articles/2021/09/07/dwt
-all-using-machine-learning-for-automatic
-database-tuning.aspx.

Ariel Rabkin and Randy Katz. Static extraction of
program configuration options. In Proceedings of the
33rd International Conference on Software Engineer-

ing (ICSE), 2011.

Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane
Suhr. Quantifying language models’ sensitivity to
spurious features in prompt design or: How i learned
to start worrying about prompt formatting, 2024. URL:
https://arxiv.org/abs/2310.11324, arXiv:
2310.11324.

Priya Sehgal, Vasily Tarasov, and Erez Zadok. Eval-
uating performance and energy in file system server
workloads. In FAST, volume 10, pages 253-266, 2010.

Xudong Sun, Runxiang Cheng, Jianyan Chen, Elaine
Ang, Owolabi Legunsen, and Tianyin Xu. Testing
configuration changes in context to prevent produc-
tion failures. In PProceedings of the 14th USENIX
Symposium on Operating Systems Design and Imple-
mentation (OSDI), 2020.


===== PAGE BREAK =====

=

—

“4

—“

=

sy

“

“4

[85] Zhenjie Sun, Naihao Deng, Haofei Yu, and Jiaxuan

You. Table as thought: Exploring structured thoughts
in Ilm reasoning, 2025. URL: https://arxiv.org/
abs/2501.02152, arXiv:2501.02152.

Yingtian Tang, Han Lu, Xijun Li, Lei Chen, Mingxuan
Yuan, and Jia Zeng. Learning-aided heuristics design
for storage system. In Proceedings of the 2021 Inter-
national Conference on Management of Data, pages

2597-2601, 2021.

Viraj Thakkar, Qi Lin, Kenanya Keandra Adriel Prase-
tyo, Raden Haryosatyo Wisjnunandono, Achmad Imam
Kistijantoro, Reza Fuad Rachmadi, and Zhichao Cao.
Elmo-tune-v2: Llm-assisted full-cycle auto-tuning to
optimize Ism-based key-value stores. arXiv preprint
arXiv:2502. 17606, 2025.

Viraj Thakkar, Madhumitha Sukumar, Jiaxin Dai,
Kaushiki Singh, and Zhichao Cao. Can modern Ilms
tune and configure lsm-based key-value stores? In Pro-
ceedings of the 16th ACM Workshop on Hot Topics in
Storage and File Systems, pages 116-123, 2024.

Petros Tsialiamanis, Lefteris Sidirourgos, Irini Fun-
dulaki, Vassilis Christophides, and Peter Boncz.
Heuristics-based query optimisation for sparql. In Pro-
ceedings of the 15th International Conference on Ex-
tending Database Technology, pages 324-335, 2012.

Dana Van Aken, Dongsheng Yang, Sebastien Bril-
lard, Ari Fiorino, Bohan Zhang, Christian Bilien, and
Andrew Pavlo. An inquiry into machine learning-
based automatic configuration tuning services on real-
world database management systems. Proc. VLDB
Endow., 14(7):1241-1253, March 2021. URL: https:
//dl.acm.org/doi/10.14778/3450980. 3450992,
doi:10.14778/3450980.3450992.

Chenyu Wang, Weixin Luo, Sixun Dong, Xiaohua
Xuan, Zhengxin Li, Lin Ma, and Shenghua Gao. Mllm-
tool: A multimodal large language model for tool agent
learning. In 2025 IEEE/CVF Winter Conference on
Applications of Computer Vision (WACV), pages 6678—
6687. IEEE, 2025.

Jaylen Wang, Udit Gupta, and Akshitha Sriraman.
Characterizing datacenter server generations for life-
time extension and carbon reduction. In Workshop on
NetZero Carbon Computing, 2023.

Xinyuan Wang, Haoyue Bai, Nanxu Gong, Wangyang
Ying, Sixun Dong, Xiquan Cui, and Yanjie Fu. Llm-ml
teaming: Integrated symbolic decoding and gradient
search for valid and stable generative feature transfor-
mation. arXiv preprint arXiv:2506.09085, 2025.

“4

=

—

“4

—“

=

sy

“

[94] Xinyuan Wang, Dongjie Wang, Wangyang Ying,

Haoyue Bai, Nanxu Gong, Sixun Dong, Kunpeng Liu,
and Yanjie Fu. Efficient post-training refinement of la-
tent reasoning in large language models. arXiv preprint
arXiv:2506.08552, 2025.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and
Denny Zhou. Chain-of-thought prompting elicits
reasoning in large language models, 2023. URL:
https://arxiv.org/abs/2201.11903, arXiv:
2201.11903.

Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu,
Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang,
Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah,
Ryen W White, Doug Burger, and Chi Wang. Autogen:
Enabling next-gen Ilm applications via multi-agent
conversation, 2023. URL: https://arxiv.org/ab
$/2308.08155, arXiv:2308.08155.

Yulai Xie, Dan Feng, Yan Li, and Darrell DE Long.
Oasis: An active storage framework for object stor-
age platform. Future Generation Computer Systems,
56:746-758, 2016.

Tianyin Xu, Jiaqi Zhang, Peng Huang, Jing Zheng,
Tianwei Sheng, Ding Yuan, Yuanyuan Zhou, and
Shankar Pasupathy. Do not blame users for misconfig-
urations. In Proceedings of the Twenty-Fourth ACM
Symposium on Operating Systems Principles (SOSP),
2013.

Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,
Thomas L. Griffiths, Yuan Cao, and Karthik
Narasimhan. Tree of thoughts: Deliberate prob-
lem solving with large language models, 2023.
URL: https://arxiv.org/abs/2305.10601,
arXiv:2305.10601.

Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
Shafran, Karthik Narasimhan, and Yuan Cao. React:
Synergizing reasoning and acting in language models,
2023. URL: https://arxiv.org/abs/2210.03629,
arXiv:2210.03629.

Jinghuan Yu, Sam H. Noh, Young ri Choi, and Chun Ja-
son Xue. ADOC: Automatically harmonizing dataflow
between components in Log-Structured Key-Value
stores for improved performance. In 2/st USENIX
Conference on File and Storage Technologies (FAST
23), pages 65-80, Santa Clara, CA, February 2023.
USENIX Association. URL: https://www.usen
ix.org/conference/fast23/presentation/yu.

Weiping Yu, Siqiang Luo, Zihao Yu, and Gao Cong.
Camal: Optimizing Ism-trees via active learning. Pro-


===== PAGE BREAK =====

[103

[104

[105

“4

sy

“4

ceedings of the ACM on Management of Data, 2(4):1-
26, 2024.

Ji Zhang, Ke Zhou, Guoliang Li, Yu Liu, Ming Xie, Bin
Cheng, and Jiashu Xing. $$\hbox {CDBTune}‘*{+}$$:
An efficient deep reinforcement learning-based auto-
matic cloud database tuning system. The VLDB Jour-
nal, 30(6):959-987, November 2021. doi:10.1007/
s00778-021-00670-9.

Yinggiang Zhang, Chaoyi Ruan, Cheng Li, Xinjun
Yang, Wei Cao, Feifei Li, Bo Wang, Jing Fang, Yuhui
Wang, Jingze Huo, et al. Towards cost-effective and
elastic cloud database deployment via memory dis-
aggregation. Proceedings of the VLDB Endowment,
14(10):1900-1912, 2021.

Chenxingyu Zhao, Tapan Chugh, Jaehong Min, Ming
Liu, and Arvind Krishnamurthy. Dremel: Adaptive con-
figuration tuning of rocksdb kv-store. In Abstract Pro-
ceedings of the 2022 ACM SIGMETRICS/IFIP PER-
FORMANCE Joint International Conference on Mea-
surement and Modeling of Computer Systems, SIG-
METRICS/PERFORMANCE ’22, page 61-62, New
York, NY, USA, 2022. Association for Computing Ma-
chinery. doi:10.1145/3489048.3530970.

19
