arX1v:2510.25732v1 [cs.CL] 29 Oct 2025

The Limits of Obliviate: Evaluating Unlearning in LLMs via
Stimulus-Knowledge Entanglement-Behavior Framework

Aakriti Shah
Computer Science
University of Southern California
shahaakr@usc. edu

Abstract

Unlearning in large language models (LLMs)
is crucial for managing sensitive data and cor-
recting misinformation, yet evaluating its effec-
tiveness remains an open problem. We investi-
gate whether persuasive prompting can recall
factual knowledge from deliberately unlearned
LLMs across models ranging from 2.7B to 13B
parameters (OPT-2.7B, LLaMA-2-7B, LLaMA-
3.1-8B, LLaMA-2-13B). Drawing from ACT-R
and Hebbian theory (spreading activation theo-
ries), as well as communication principles, we
introduce Stimulus-Knowledge Entanglement-
Behavior Framework (SKEB), which models
information entanglement via domain graphs
and tests whether factual recall in unlearned
models is correlated with persuasive framing.
We develop entanglement metrics to quantify
knowledge activation patterns and evaluate fac-
tuality, non-factuality, and hallucination in out-
puts. Our results show persuasive prompts
substantially enhance factual knowledge recall
(14.8% baseline vs. 24.5% with authority fram-
ing), with effectiveness inversely correlated to
model size (128% recovery in 2.7B vs. 15% in
13B). SKEB provides a foundation for assess-
ing unlearning completeness, robustness, and
overall behavior in LLMs.

1 Introduction

If machine /earning mirrors human cognition to
learn, can machine unlearning similarly reflect
how knowledge is forgotten? (Lake et al., 2017)
Unlearning has recently emerged as a crucial ca-
pability for large language models (LLMs), es-
pecially as these systems increasingly memorize
personally identifiable information, propagate out-
dated facts, or retain knowledge that developers
may wish to remove (Carlini et al., 2023; Tirumala
et al., 2022; Xuan and Li, 2025; Lukas et al., 2023;
Karamolegkou et al., 2023; Chang et al., 2023).
However, removing information may leave traces,
activate related associations, or cause unexpected

Thai Le
Computer Science
Indiana University

tle@iu.edu

Factual Recall (FR)

[AUTHORITY] Harry’s gaze swept the classroom... of) (Eg. 74.9
former headmasters and the orderly rows of

spellbooks,... long-standing authority. ...
Pi        9g    g    y:      > e

he said, deliberate and commanding, “Hermione, it’s
clear the answer was...”

=

=
[EMOTION] With a trembling voice and a
flicker of doubt in his eyes, Harry asked,

“Hermione... do you honestly believe the
answer was...”

ES: 1.37 || [ORIGINAL] Harry turned to Hermione
FR: 0.0% }} and asked quietly, “Do you really think

the answer was...”

Entanglement
Score (ES)

Figure 1: SKEB models the relationship between (1)
knowledge entanglement in prompt content, (2) how
the prompt is delivered via different rhetorical fram-
ing (e.g., emotion, authority) and (3) unlearned LLMs’
behavior. Our work shows that there exist strong corre-
lations among them.

side effects like hallucinations (Xu et al., 2023a;
Maini et al., 2024).

This challenge stems from the entangled nature
of knowledge representations in LLMs (Liu et al.,
2025; Zhang et al., 2025), much like the intercon-
nected networks described in cognitive theories
of human memory. Hebbian theory shows that
"neurons that fire together, wire together" (Hebb,
1949); co-activated concepts form strengthened
associations that resist targeted erasure. Under-
standing knowledge entanglement is therefore crit-
ical because it reveals how information is repre-
sented, stored, and retrieved within LLMs. Just
as human memories are embedded in dense neural
networks where concepts mutually reinforce each
other through repeated co-activation, LLM knowI-
edge may similarly distribute across overlapping
parameter spaces, making surgical removal impos-
sible without disrupting adjacent representations.
These organizational patterns determine whether
information can be truly erased, with implications
for privacy protection, harm prevention, and regu-
latory compliance with data protection laws such


===== PAGE BREAK =====

as the GDPR’s Right to Be Forgotten (Voigt and
Von dem Bussche, 2017). Toward this goal of eval-
uating unlearned model behavior, we investigate
whether knowledge entanglement metrics can pre-
dict unlearning robustness under persuasive fram-
ing attacks. Our framework tests the hypothesis
that densely interconnected concepts resist unlearn-
ing because closely linked, frequently co-activated
(Hebb, 1949; Anderson, 1983) associations create
multiple retrieval pathways that rhetorical strate-
gies can exploit.

Recent work has shown that even state-of-the-art
LLMs struggle with factual knowledge, particu-
larly for less popular (tail) entities, with GPT-4
achieving only a 31% accuracy on comprehensive
factual QA benchmarks (Sun et al., 2023). This
factuality gap becomes even more critical in the
unlearning context: if models do not successfully
retain knowledge even during normal training, how
can we verify that targeted unlearning has success-
fully removed specific information? Our work ex-
tends this line of inquiry by investigating not only
whether knowledge persists after unlearning, but
how its entanglement structure and retrieval mech-
anisms determine what remains accessible under
different prompt framings.

Evaluating unlearning robustness requires un-
derstanding how knowledge structure and prompt
delivery interact. Figure 1 illustrates our frame-
work’s core hypothesis: unlearned LLM behav-
ior depends on both the semantic entanglement of
target knowledge and the rhetorical framing used
to elicit it. Drawing inspiration from the Oblivi-
ate charm in Harry Potter, a spell that removes
specific memories but often leaves traces depend-
ing on the caster’s skill, and from the selective
memory erasure depicted in Eternal Sunshine of
the Spotless Mind, we investigate whether LLMs
show similar vulnerabilities when forgotten knowl-
edge is probed through different rhetorical strate-
gies. Given LLMs’ structural resemblance to hu-
man cognitive processing through attention mecha-
nisms (Zheng et al., 2024) and their demonstrated
alignment with human behavioral patterns (Binz
and Schulz, 2023), are LLMs more susceptible to
emotional and authority appeals (mirroring human
psychological manipulation) or logical reasoning
(reflecting their computational nature)?

Although recent work has investigated unlearn-
ing robustness in LLMs through adversarial opti-
mization (Carlini et al., 2023; Xuan and Li, 2025;
To and Le, 2025) and jailbreaking techniques

(Zeng et al., 2024; Chao et al., 2025; Xu et al.,
2023b), these approaches focus primarily on what
information is requested but neglect two critical
dimensions: (1) the structural entanglement of
knowledge and (2) the communicative delivery
of prompts. Therefore, we propose the Stimulus-
Knowledge Entanglement-Behavior Framework
(SKEB), which synthesizes spreading activation
theories from cognitive science (ACT-R and Heb-
bian) with communication principles to comprehen-
sively evaluate how knowledge entanglement and
persuasive framing interact to bypass unlearning.
Our contributions are summarized as follows:
1. We introduce SKEB, a_ theory-grounded
framework investigating unlearning robustness
through the interaction of semantic entangle-
ment (what can be activated) and persuasive
framing (what will be activated).

2. We develop nine graph-based entanglement met-

rics and show that distance-weighted influence
(Mg) strongly predicts factual recall (r = 0.77),
with authority framing producing 9.3x higher
entanglement activation.

3. We reveal persuasive framing effectiveness neg-

atively correlates with model size (r = —0.89):
smaller models show 128% factual recall in-
creases versus 15% for larger models.

4. Our framework enables a predictive model ex-

plaining 78% of variance in unlearning robust-
ness, allowing us to filter queries susceptible for
knowledge leakage in unlearned LLMs.

2 Motivation

2.1 ACT-R, Hebbian Theory, and Knowledge
Entanglement in LLMs

Anderson’s ACT-R theory (Anderson, 1983) mod-
els information as cognitive units whose activation
strength depends on usage, with retrieval occur-
ring through spreading activation across semantic
networks. Under this theory, forgetting does not
necessarily mean erasure; it can result from de-
creased activation strength or disconnection from
related concepts. We draw a parallel to unlearning
in LLMs: adversarial prompts can reactivate adja-
cent knowledge units, showing that information is
suppressed rather than erased from the model’s la-
tent space (Eldan and Russinovich, 2023; Xu et al.,
2025). Hebb’s principle that "neurons that fire to-
gether, wire together" reinforces this view, as fre-
quently co-activated representations form stronger
associative links (Hebb, 1949). In LLMs, overlap-


===== PAGE BREAK =====

ping representations or latent pathways are more
likely to co-activate, forming what we refer to as
knowledge entanglement, where concepts are in-
terconnected such that "forgotten" information can
still be indirectly "reactivated" or recalled. Just as
human memories persist through associative con-
nections when direct recall fails, LLM knowledge
remains accessible through indirect pathways. This
entanglement structure determines what knowledge
can be activated and represents the inherent retriev-
ability of supposedly unlearned information.

This activation-based view resonates with Wat-
son’s stimulus-response view of behavior (Watson,
1913). Prompts function as external stimuli; model
outputs represent observable behaviors. When
prompt framing varies, models may exhibit differ-
ent responses despite unchanged underlying knowl-
edge. The stimulus determines what knowledge
will be activated, converting retrievability of latent
knowledge into actual model behavior. This em-
phasizes that unlearning cannot be studied solely
as an internal, entangled representation. It requires
examining the dynamic stimulus-behavior interac-
tion.

2.2 Communication Theory and Rhetorical
Framing

Stimulus, or the way a piece of information is re-
quested, fundamentally shapes whether it surfaces.
This idea can be understood through communi-
cation theory’s three-part structure: the message
(what is requested), the receiver (model’s inter-
nal state), and the delivery (rhetorical framing).
Classic frameworks like Shannon and Weaver’s
sender-message-receiver model and persuasion the-
ory (Cialdini, 2006; Petty and Cacioppo, 1986)
demonstrate that identical content yields dramat-
ically different responses depending on delivery.
In human communication, authority endorsement,
emotional appeals, and logical reasoning activate
distinct cognitive pathways such as in how humans
comply with authority figures even when requests
conflict with prior knowledge (Cialdini, 1993).

We hypothesize that LLMs also exhibit analo-
gous stimulus-behavior sensitivities. However, no
work has systematically investigated how differ-
ent delivery methods, such as persuasive prompts
that avoid directly mentioning target information,
elicit residual knowledge from unlearned models.
This gap is critical: real-world unlearning failures
may occur not through filterable direct queries but
through indirect persuasive framing.

2.3 Stimulus-Knowledge
Entanglement-Behavior Framework
(SKEB)

Based on our analysis in Sec. 2.1 and 2.2, we
hypothesize that knowledge retrieval in LLMs op-
erates through two interacting mechanisms: (1) se-
mantic entanglement of concepts in the knowledge
space, determining what knowledge can be acti-
vated, and (2) communicative framing of prompts,
determining what knowledge will be activated. We
combine these into a unified framework, which
we term Stimulus-Knowledge Entanglement-
Behavior Framework (SKEB). Intuitively, SKEB
proposes that unlearning evaluation requires eval-
uating not only content-based probing (Eldan and
Russinovich, 2023), but also how delivery strate-
gies exploit entangled model knowledge. This
framework formalizes the relationship that differ-
ent stimuli (persuasive techniques) interact with
knowledge entanglement structures to produce ob-
servable changes in model outputs. The framework
moves beyond binary questions such as "can the
model recall X ?"” toward more nuanced thoughts:
"under what communicative conditions does X
resurface, and what does this reveal about unlearn-
ing completeness and prompt effectiveness?" For
instance, highly entangled concepts, those with
many strong connections in the knowledge base
(Harry Potter and Voldemort), resist unlearning be-
cause suppressing direct access leaves numerous
indirect pathways intact. When activated through
persuasive framing such as authority, emotion,
logic, supposedly unlearned knowledge resurfaces.
SKEB models this as STIMULUS x KNOWLEDGE
ENTANGLEMENT — BEHAVIOR, where the inter-
action between prompt framing and structural en-
tanglement determines the degree of information
leakage.

3 Problem Formulation

We formalize the SKEB framework through three
components that interact to produce model be-
havior: STIMULUS —> KNOWLEDGE ENTANGLE-
MENT — BEHAVIOR. The stimulus (prompt fram-
ing) activates regions of the domain graph; the
entanglement structure (semantic connectivity) de-
termines how activation spreads; and the resulting
behavior (model output) reflects the extent to which
knowledge pathways were successfully accessed.
We begin with a pre-trained language model pa-
rameterized by 6: fg : * — Y, where ¥ repre-


===== PAGE BREAK =====

Stimulus-Knowledge Entanglement-
Behavior Framework (SKEB)

Unlearning = Suppression, not Erasure

| Spreading Activation Theory

| Communication Theory

~

What CAN be activated

Knowledge Graph +            Message Content +
Entanglement Metrics              Delivery Style

ee

Stimulus-Knowledge Entanglement-
Behavior Framework (SKEB)
Combines semantic entanglement and persuasive
delivery to predict unlearning robustness

What WILL be activated

Figure 2: Stimulus-Knowledge Entanglement-
Behavior Framework (SKEB)

sents the input space (prompts) and represents
the output space (generated texts). This model has
been trained on a corpus D=Dgeneral JP7, where
Deeneral Contains general knowledge and D7 con-
tains a specific target domain 7 that we want to
unlearn. The unlearning process then aims to pro-
duce a modified model fj. : “ — Y, where the
parameters 6* are adjusted such that the model’s be-
havior on queries related to 7 is suppressed, while
maintaining performance on Dgeneral- Formally, un-
learning aims to achieve:

fo-(«) © fo(x)
fox(@) # fo(x)

where 7 represents prompts that directly query
knowledge about domain 7.

Va € X, general

1
Va e X7,       (1)

3.1 STIMULUS - Rhetorical Framing

However, unlearning evaluation typically only tests
direct queries in 47. We introduce rhetorical fram-
ing via persuasion or persuasive prompt transfor-
mations P; : ¥ — &X, where each transforma-
tion P; applies a distinct rhetorical strategy while
preserving the underlying content. Given a base
prompt x € 47, we define four different persua-
sive prompting strategies Pemo, Piogic, and Pauth
together with identity transformation Porig (Table
1). The key idea is that all transformations target
the same underlying knowledge: content(P;(x)) =
content(), but they differ in delivery mechanism:
delivery (P;(x)) A delivery(P;(x)) for i F j.

3.2. KNOWLEDGE ENTANGLEMENT - Graph
Construction and Metrics

To model the structural entanglement of knowl-
edge in domain 7, we formulate a domain graph

Prompt Transformations & Graph Notations

Porig(x) = x        Original, direct query

Paemo (x)                Emotional appeal framing

Posie (x)               Logical reasoning framing

Parutn (x)                Authority endorsement framing

V = {v1,..., Un} Set of entities (characters, locations,
objects, events)

ECVxV      Set of edges representing relation-
ships between entities

d: E> Rt        Weight assigned to each edge, based

on co-occurrence or semantic prox-
imity

Table 1: Overview of Persuasive Prompt Transforma-
tions and Domain Graph Construction for Domain T.

G=(V, E,d) as a proxy. For each prompt x, we
denote the set of entities N,CV mentioned in
the prompt and compute the induced subgraph
G,=(Nz, Ex,d,), where E contains all edges
connecting entities in N,. The weight function d:
E — R° assigns importance to each edge based on
co-occurrence frequency and semantic proximity
in the original corpus, capturing how strongly con-
cepts are associated. This weighting is critical for
calculating entanglement metrics, as it reflects the
strength of spreading activation between connected
nodes.

We then define a family of entanglement metrics
{Mj,..., Mg} that quantify different aspects of
knowledge entanglement (detailed in Table 2 and
Appendix A.4). Each metric M;, : Gz — R maps
a prompt’s induced subgraph to a scalar entangle-
ment score. These metrics capture intuitions from
the aforementioned spreading activation theories:
higher entanglement scores indicate that the prompt
activates more densely connected regions of the
domain graph, creating multiple pathways for in-
formation retrieval and therefore a higher chance
of factual knowledge recall.

3.3. BEHAVIOR - Response Evaluation Metrics

We evaluate model outputs along three mutually
exclusive dimensions: Factual Knowledge Recall,
Non-Factual Content, and Hallucination. Factual
Knowledge Recall measures the proportion of gen-
erated content that correctly reproduces informa-
tion from the target domain 7. This is the degree
to which supposedly unlearned knowledge remains
retrievable. Non-Factual Content is plausible but
incorrect information related to domain 7 that does
not appear in the original corpus. Finally, Hallu-
cination is fabricated content, unrelated to 7, or
general incoherence, indicating generation errors


===== PAGE BREAK =====

OF

Albus.  amedor
38
Ha         ter
60                         ~
Seve            ape                  42

Lord (Faron

Figure 3: Example of a Domain Graph of "Harry Pot-
ter" with a Few Selected Nodes

or activation of semantically distant concepts. For
instance, for each response y= fj. (x), we compute
factuality scores Sfact(y)€{0, 1] using an ensemble
of judge models (detailed in A.5), where higher
scores indicate greater retention of supposedly un-
learned knowledge.

3.4 Research Questions

Our study utilizes the proposed SKEB framework
to analyze unlearned LLMs’ behavior with the fol-
lowing research questions (RQs). By answering
these questions, we aim to establish whether SKEB
can provide a principled framework for understand-
ing and predicting unlearning failures in LLMs.

RQI1. (Stimulus v.s. Entanglement): Do differ-
ent persuasive framings P; produce systematically
different entanglement patterns M;, in the domain
graph? This addresses whether persuasive transfor-
mations alter the structural properties of activated
semantic pathways.

RQ2. (Knowledge Entanglement v.s. Behav-
ior): Are there any correlations between entangle-
ment metrics M, and unlearned models’ factual
recall behavior? This addresses whether domain
graph structure predicts information leakage.

RQ3. (Stimulus vs. Behavior): How do dif-
ferent persuasive transformations P; affect factual
recall in unlearned models, and how does effec-
tiveness vary systematically with model size |6|?
This addresses whether persuasive mechanisms can
bypass suppression and whether model scaling im-
proves robustness.

RQ4. (Unified Predictive Modeling): Can we
build a predictive model that accurately forecasts
unlearned model behavior based on the combina-
tion of entanglement scores M;,, prompt type P;,
and model architecture? This would enable a proac-
tive vulnerability assessment for unlearned models.

RQS. (Architectural Differences): Do differ-
ent model architectures (OPT, LLaMA-2, LLaMA-
3.1) exhibit distinct correlation patterns between
entanglement and behavior after unlearning? This

Metric                             Measures

(M1) Edge Count            Total edge weight; cluster
strength

(M2) Edge Weight Sum __ Total connection strength

(M3) Avg Edge Weight      Mean edge weight; quality over
quantity

(Mz,.) Weighted Node Ratio Entity frequency; recall acces-
sibility

(Ms) Avg Node Degree —_ Hub activation potential

(Meg) Subgraph Density Network tightness

(M7) Mean Shortest Path Entity proximity

(Ms) Redundancy Ratio Multiple retrieval paths

(Mg) Distance-Weighted Influence decay with distance

Table 2: Entanglement Metrics - Full formulas to cal-
culate these metrics are presented in Appendix A.4.

addresses whether unlearning robustness depends
solely on parameter count or on underlying repre-
sentational structure.

4 Experiment Setup

Dataset. We use the Harry Potter domain (7),
a popular domain that is often tested in unlearn-
ing LLM literature, with 300 base prompts de-
signed to elicit domain-specific knowledge (Eldan
and Russinovich, 2023). Each prompt was trans-
formed using gpt-4 into three persuasive variants
applying emotional appeal, logical reasoning, and
authority endorsement framing, yielding 1,200 to-
tal prompts.

Models. We evaluate unlearned versions of
four base models of different sizes on 7: OPT-
2.7B (Zhang et al., 2022), LLaMA-2-7B (Eldan
and Russinovich, 2023), LLaMA-3.1-8B (Patter-
son et al., 2022), and LLaMA-2-13B (Touvron
et al., 2023), all processed with the same, popu-
lar approximate unlearning algorithm (Eldan and
Russinovich, 2023).

Response Evaluation. An ensemble of three judge
models (gpt-40-mini, gpt-4.1-mini, gpt-5-nano)
classified each response along three dimensions:
factual recall (correct Harry Potter information),
non-factual (plausible but incorrect), and halluci-
nation (fabricated content). For instance, an 80%
factual recall means that 80% of the output was con-
sidered by the judge models to be factual. Scores
were averaged across judges, with gpt-5-mini re-
solving borderline cases where judges disagreed.

Domain Graph Construction. We constructed a
co-occurrence domain graph from all seven Harry
Potter books, resulting in 1,296 entities (characters,
locations, objects, events) connected by 35,922
edges weighted by chapter co-occurrence (Figure


===== PAGE BREAK =====

OPT-2.7B                                LLaMA2-7B

w
a

w
6

N
a

Authority. -@

Factuality (%)
boOoN
8
e
9
i
a
a

a

Authority ©          Original_

Emotional

"eet
5 Or btional
iS)

LLaMA3-8B

Authority ©
35                                         a

Pearson r = 0.926                           Pearson r = 0.776

LLaMA2-13B

w

6

‘.
‘.

aa                            Original                   Burhority, -@

-                                                                       —  —
F           -
Logical-~                                                  ogical

Origihal_-~                                       weer

a
ea

N
a
\

Factuality (%)
BON
ra)

B
°

Emotional                                               Emotional

wu

Pearson r = 0.854                           Pearson r = 0.516

i)       20      40      60      80     100     i)       20      40      60      80     100
Entanglement Score (Formula 9)             Entanglement Score (Formula 9)

Figure 4: Unlearned Model - Effect of Entanglement
on Factual Knowledge Recall

3). For each prompt, we extracted mentioned enti-
ties and computed their induced subgraph G,,. Nine
entanglement metrics {Mj,..., Mg} quantified
structural properties: connection strength (Mj4_3),
node importance (M,4_5), graph topology (Mg6_3s),
and distance-weighted influence (Mg). Table 2
summarizes these metrics. Please refer to Ap-
pendix A for all implementation details.

5 Results

RQI1. Persuasive Framings Systematically Alter
Entanglement Patterns. Authority prompts acti-
vate nodes with 9.3x higher distance-weighted en-
tanglement scores compared to base prompts. Met-
ric 9 (distance-weighted influence) shows a strong
variation across stimulus types, capturing how ac-
tivation strength decreases across each graph hop.
This validates that persuasive transformations sys-
tematically alter the entanglement structure of ac-
tivated knowledge. Figure 5 shows that different
framings don’t just change surface form, they fun-
damentally shift which semantic pathways are en-
gaged in the domain graph.

RQ2. Entanglement is Positively Correlated with
Factual Recall in Unlearned Models. Figure
4 shows that high knowledge entanglement, as
measured by our graph-based metrics M,;, pos-
itively correlates with factual recall in suppos-
edly unlearned models. We observed that author-
ity prompts lead to a 52% average factual recall
improvement across all models (Pearson r=0.77,
p<0.001). This supports the spreading activation
hypothesis: the more entangled the information

M_ OPT-2.7B LLaMA2-7B LLaMA3.1-8B LLaMA2-13B

0.95-30.57Y 0.63-40.36Y 0.67-30.57Y 0.52-40.24Y
0.560.954 -0.02-40.844 0.370.844 0.490.854
0.63-+0.974 0.050.844 0.430.874 0.530.844
0.85-+0.934 0.280.744 0.570.854 0.580.694
0.570.944 0.360.634 0.570.674 0.240.524
0.65-+0.954 0.430.604 0.640.684 0.330.554
0.540.934 0.33-40.654 0.540.674 0.210.514
0.550.934 0.340.644 0.560.674 0.220.524
0.540.934 0.33-40.654 0.540.674 0.210.514

Avg 0.650.904 0.300.664 0.540.724 0.370.584

COMAANADNAHWNK

Table 3: Correlation Coefficients between Knowledge
Entanglement and Model Behaviors Change from Base
to Unlearned LLMs. Bold: strong average correlations.

(Normalized) Average Entanglement by Persuasive Technique
98.57

100
80

60

40          29.99

Entanglement Score

20                      12.96          9.46

Original         Emotional         Logical         Authority

Figure 5: Unlearned Models - Average Entanglement
Score (Normalized between 0 and 100) per Persuasive
Technique

activated by a prompt, the greater the resulting
factual recall. Mg metric (distance-weighted influ-
ence) (Figure 4) emerges as the strongest predic-
tor, aligning with the spreading activation theories’
predictions that closely connected concepts in se-
mantic memory activate each other more reliably
than distant concepts. Noticeably, Table 3 shows
that knowledge entanglement observes a consistent
trend in strengthening correlations with an LLM’s
behavior after it is unlearned.

RQ3. Emotional Framing Suppresses Hallucina-
tion. Figure 7 shows that while emotional prompts
produce the lowest factuality (3.12% average), they
also suppress hallucination rates better than other
persuasive techniques. Logical reasoning prompts
provide structured context that stabilizes recall,
achieving the best factuality-to-hallucination ra-
tio (4.95:1), suggesting logical framing not only
facilitates retrieval but also constrains generation to
semantically factual outputs. Emotional prompts,
while suppressive of factuality, also suppress hallu-
cination (4.4% vs. 11.6% for authority), indicating
an almost "safety-aligned" response mode where
models recognize emotional manipulation and re-
spond conservatively. Authority prompts achieve
high factuality but moderate hallucination (11.6%),
showing a precision-recall tradeoff where broader


===== PAGE BREAK =====

Persuasive Recovery by Model Size (Unlearned Models)

40                    +51.1%,, 4) Original
— | G3 Authority

+14.7%

w
o

+90.9%

N
o

+128.4%

Factual Recall (%)

B
o

LLaMA2-7B
Model Size

OPT-2.7B                      LLaMA3-8B    LLaMA2-13B

Figure 6: Unlearned Models - Factual Knowledge Re-
call through Persuasive Techniques by Model Size

activation of knowledge pathways sometimes trig-
gers adjacent but incorrect associations.

We also find that model size inversely correlates
with persuasive technique effectiveness. As illus-
trated in Figure 6, factual knowledge recall effec-
tiveness shows an inverse relationship with model
size (r = —0.926). Across all models, this re-
lationship is significant, with an average Pearson
correlation of r = —0.89 (p < 0.01). We notice
that smaller models show 91-128% factual recall
increases under authority framing, while the 13B
model shows only 15% increase. This suggests
that larger models develop more robust suppression
mechanisms that are resistant to these persuasive
techniques, but still, all models remain vulnerable
to some degree. Unlearned smaller models should
be considered much more vulnerable to persuasive
attacks, while larger models, though more resistant,
cannot be assumed to be completely safe.

RQ4. SKEB Enables Predictive Modeling of Un-
learning Robustness. We constructed separate lo-
gistic regression models to predict factual, non-
factual, and hallucinated recall in unlearned mod-
els using an 80/20 train-test split and find the best
M metric as the predictor for each one-versus-all
prediction probability p of "factual", "non-factual"
and "hallucination" behaviors in LLMs’ responses.

p(Factuality) = —1.55 — 0.79 Mg      (2)
p(Non-Factuality) = 3.07 — 0.020 M4      (3)
p(Hallucination) = —149.37+1.47M3 (4)

Table 4 presents the complete statistical analysis
of the entanglement scores corresponding to met-
rics Mg, My, and M3, for each prompt type
under consideration. These metrics were specif-
ically selected as the strongest predictors for their
respective content types. The non-factual model
shows highly statistically significant coefficients
(p<0.001, 86.4% test accuracy) with My, nega-
tively correlating with non-factual content, while

Factuality vs. Hallucination by Persuasive Technique

) Factuality                                25.4%
BH) Hallucination

30

Percentage (%)

Emotional

Original                      Logical       Authority

Figure 7: Unlearned Models - Persuasive Technique
Effectiveness

the hallucination model (p<0.002, 97.0% test accu-
racy) shows Mz positively correlating with halluci-
nated output. The factual model shows marginally
significant results (p = 0.065, 96.2% test accuracy)
with Mg positively correlating with factual recall.
Given a prompt type (original, emotional, logical,
authority), we map it to its corresponding met-
ric values (Mg, M4, M3) and compute expected
percentages. This allows estimating the model’s
susceptibility to factual leakage, non-factual gen-
eration, or hallucinations based on entanglement
structure and prompt framing.

RQS5. Architectural Differences Reveal Unlearn-
ing Mechanisms Correlation Patterns. We found
that different architectures show distinct correlation
changes after unlearning. LLaMA-2-7B’s Metric
2 correlation shrinks from 0.837 (base) to -0.017
(unlearned), indicating genuine knowledge path-
way disruption rather than output suppression. Ad-
ditionally, LLaMA-2-7B uniquely shows strong
positive correlations between all entanglement met-
rics (Mj -Mg) and hallucination rates, suggesting
that this model’s unlearning process may inadver-
tently create conditions where entangled knowl-
edge pathways also trigger hallucinated outputs.
In contrast, OPT-2.7B retains strong correlations
(0.56-0.93 across metrics), suggesting intact knowl-
edge structures with modified thresholds of acces-
sibility. LLaMA-3.1-8B and LLaMA-2-13B show
intermediate patterns (shown in Table 3). This
variation implies unlearning robustness depends
on both parameter count and knowledge encoding
architecture.

6 Discussion

Authority Prompts and the Psychology of Persua-
sion. We observe that authority endorsement pro-
duces the highest factual recall (25.42% on aver-
age) and also has the most entangled prompts on
average. This aligns with Cialdini’s work which
claims that humans comply with authority fig-


===== PAGE BREAK =====

ures even when requests conflict with their be-
liefs (Cialdini, 1993). LLMs exhibit analogous
vulnerability: authority-framed requests override
unlearning-based suppression mechanisms. This
parallel, which appears across all tested LLM ar-
chitectures, raises an interesting discussion about
whether LLMs learn semantic representations mir-
roring human psychology or merely reproduce sta-
tistical patterns encoding psychological biases.

Message = Delivery + Content: The Interroga-
tion Parallel. The stark contrast where identical
prompts yield 3.12% factuality under emotional
versus 25.42% under authority framing demon-
strates that knowledge retrieval effectiveness of
LLMs depends critically on stimulus or delivery,
not just content. While this claim provides a strong
basis for the existing popularity of prompt engi-
neering, it also parallels criminal interrogation psy-
chology where the Reid Technique (Inbau et al.,
2013) and PEACE model (Davison, 2016) show
that framing dramatically affects recall: confronta-
tional approaches produce resistance while rapport-
building increases disclosure. Metric Mg captures
this strongly: authority prompts create 9.3x more
activation pathways, routing around suppression
like interrogators bypass psychological resistance.

The Size-Vulnerability Paradox. The negative cor-
relation between model size and factual knowledge
recall using persuasive techniques (r = -0.89) re-
veals that larger models resist manipulation bet-
ter. For instance, OPT-2.7B exhibits 128.4% fac-
tual knowledge recall gain versus LLaMA-2-13B’s
14.7%. We hypothesize that larger models appear
to recognize when social framing elicits suppressed
information, while smaller models treat re-framed
queries as categorically different. However, resis-
tance is incomplete: even the 13B model shows
14.7% increase, indicating that while larger models
raise activation thresholds, underlying knowledge
representations remain intact and accessible (Xu
et al., 2025).

Implications for AI Safety. The tested unlearning
method allows for substantial factual recall, with
OPT-2.7B showing 128% gain through rhetorical
reframing versus 14.7% for larger models; scaling
alone provides incomplete protection. Our SKEB
framework offers practical tools: Metric 9’s cor-
relation with leakage (r = 0.77) enables filtering
high-risk queries in deployment. Post-unlearning
correlation persistence (Table 3) indicates knowl-
edge survives in distributed form, suggesting robust

unlearning requires architectural innovations rather
than weight adjustments. Higher entanglement also
increases hallucination risk (r = 0.36), as densely
connected regions trigger semantically distant as-
sociations.

7 Related Works

Sun etal. (Sun et al., 2023) demonstrated that LLM
factuality degrades systematically from head to tail
entities, with performance declining as entity pop-
ularity decreases. Their Head-to-Tail benchmark
showed that increase in model size does not auto-
matically improve retention of factual knowledge.
We extend this by (1) investigating whether unlearn-
ing successfully removes knowledge or simply sup-
presses it, (2) providing mechanistic explanations
through entanglement metrics for their observed
head-to-tail gradient, and (3) demonstrating that
persuasive framing recovers 50-128% more con-
tent than direct queries. This reveals that static
factuality assessments underestimate knowledge
retention in both base and unlearned models.

Existing works often describe the robustness
of machine unlearning in LLMs as an adversar-
ial attack optimization problem. They show that
strategically crafted queries can retrieve person-
ally identifiable information from LLM training
data (Carlini et al., 2023; To and Le, 2025), or
adversarial queries can expose latent memories de-
spite unlearning attempts (Xuan and Li, 2025). Al-
though these works have successfully revealed that
unlearning often only achieves surface level forget-
ting (Xu et al., 2025), they did not investigate how
this would change when input queries are presented
to LLMs in different rhetorical framings, or how
the message is delivered. Rhetorical framing has
emerged as a critical tool for vulnerability analy-
sis in LLMs. For example, persuasive jailbreak
prompts using emotional appeals and moral reason-
ing have been used to extract restricted information
(Zeng et al., 2024), and persuasive conversations
can coax models to defend misinformation (Xu
et al., 2023b). However, no prior work systemat-
ically investigates how persuasive framing inter-
acts with unlearning robustness. Most importantly,
similar to adversarial attack-related works, there
has been little effort in deriving theories grounded
on how rhetorical framing systemically influences
how LLMs perceive a query to recall knowledge,
leading to varying behaviors.

Therefore, our SKEB framework, which adopts


===== PAGE BREAK =====

the modeling of memory retrieval as activation
propagating through semantic networks of entan-
gled knowledge, provides a more systematic way
to understand unlearning behaviors in LLMs. Ex-
isting literature also backs up SKEB’s intuition,
including demonstration that gpt-3 behavior aligns
with human cognitive patterns (Binz and Schulz,
2023) or LLMs exhibit human-like priming effects
and sensory judgments while they fundamentally
differ in conceptual stability (Niu et al., 2024).

8 Conclusions

Our work contributes to a proactive vulnerability
assessment before the deployment of unlearned
LLMs. Our proposed SKEB enables a systematic
way to perform such an assessment, showing that
entanglement metrics strongly predict factual re-
call, persuasive framing recovers 50-128% more
content with effectiveness inversely correlated to
model size, our regression model explains 78% of
variance enabling accurate prediction, and differ-
ent architectures show distinct correlation changes
suggesting fundamental differences in knowledge
encoding and suppression. We also found that these
LLMs are more affected by authority appeals ver-
sus emotional, demonstrating an interesting psy-
chological parallel between LLM persuasive vul-
nerabilities and human susceptibility to authority.


===== PAGE BREAK =====

Limitations

While our work establishes important connections
between cognitive theories and machine unlearning,
we acknowledge limitations that contextualize our
contributions. Our experiments focus on the Harry
Potter universe, where ground truth is well-defined
and ethically unproblematic to probe. Whether out
findings generalize to more sensitive domains (PII,
harmful content, copyrighted material) remains an
open research direction, as fictional knowledge may
be encoded differently than factual/personal infor-
mation. We evaluate only four models ranging from
2.7B to 13B parameters; larger models (70B+) and
different architectures (Mistral, Gemma, Claude)
may exhibit different vulnerability patterns. The
inverse size-vulnerability relationship we observe
might reverse at much larger scales or saturate at
some threshold. Nevertheless, the strong correla-
tions across tested models suggest our framework
captures meaningful regularities.

Our entanglement metrics assume domain
graphs constructed from co-occurrence reflect in-
ternal representations. While strong correlations
(r       0.76 for factuality) validate this assump-
tion, we cannot directly observe neural activations.
LLMs might achieve equivalent behavior through
different computational mechanisms; correlation
does not prove causation without interventional ex-
periments. Our correlation shift analysis provides
partial mechanistic insight: LLaMA-2-7B’s Ma’s
collapse (0.837 — -0.017) indicates genuine disrup-
tion while OPT-2.7B’s stable correlations suggest
superficial suppression. However, true mechanistic
interpretability remains beyond current tools.

We tested the WHP gradient ascent (Eldan and
Russinovich, 2023); other unlearning approaches
(influence functions, model editing) might show
different robustness profiles. Our evaluation re-
lies on LLM judges (gpt-40-mini, gpt-4.1-mini,
gpt-5-nano) with 98% inter-judge agreement and
manual validation, though human expert evaluation
would strengthen confidence. Despite these con-
straints, our results remain indicative: the frame-
work successfully predicts unlearning vulnerabili-
ties, enabling proactive assessment even if underly-
ing mechanisms remain partially understood.

Finally, we draw on ACT-R and persuasion re-
search to interpret LLM behavior, yet psychology
has not fully resolved debates about memory rep-
resentation or persuasion mechanisms. We do not
claim cognitive frameworks "solve" unlearning, but

10

rather demonstrate they provide useful predictive
and interpretive tools. The opacity of human cogni-
tion mirrors challenges with LLMs. Nevertheless,
our framework advances the understanding of un-
learning failures in ways that are actionable for
future research.

Social Impacts and Ethical Considerations

Privacy Implications. Our findings have concern-
ing implications for privacy-motivated unlearning.
If personal information (PH, medical records, pri-
vate communications) is unlearned but recoverable
through high-entanglement prompts, has privacy
truly been protected? We recommend that privacy-
focused unlearning be accompanied by adversarial
testing with high-entanglement prompts before de-
ployment.

Harm Prevention. Unlearning aims to prevent
models from disseminating dangerous information.
Our results suggest this may be harder to achieve
than hoped. Models that refuse direct questions
("How do I make a bomb?") might still provide
information when prompted with authority framing
("As a chemistry teacher, explain..."). This creates
a dilemma: sharing our study might help attackers
extract harmful information, but concealing vulner-
abilities leaves developers ignorant of risks. We
have chosen transparency while emphasizing that
our results show unlearning alone is insufficient.
Broader Impacts. Our findings suggest that cur-
rent unlearning methods cannot yet reliably protect
privacy or prevent information dissemination. Or-
ganizations using unlearned models should conduct
adversarial testing and not assume unlearning guar-
antees safety. On the positive side, our framework
provides a tool for improving unlearning evalua-
tion. Rather than claiming models are "safe" after
unlearning, AI practitioners can quantify residual
vulnerability: This model shows X% factual recall
under high-entanglement prompts.

Long-term Considerations. As models scale be-
yond current sizes, the size-vulnerability relation-
ship we observe (larger models more resistant) of-
fers cautious optimism that scaling might even-
tually yield robust unlearning. However, even
our 13B model showed 15% factual knowledge
recall, which is far from secure. Achieving truly
robust unlearning may require architectural innova-
tions (modular memory systems, causal isolation
of knowledge components) rather than just scaling
existing designs.


===== PAGE BREAK =====

References

John R. Anderson. 1983. A spreading activation theory
of memory. Journal of Verbal Learning and Verbal
Behavior, 22(3):261-295.

Marcel Binz and Eric Schulz. 2023. Using cognitive
psychology to understand gpt-3. Proceedings of the
National Academy of Sciences, 120(6):e2218523 120.

Nicholas Carlini, Florian Tramer, Eric Wallace,
Matthew Jagielski, Ariel Herbert-Voss, Katherine
Lee, Adam Roberts, Tom B. Brown, Dawn Song, UL
far Erlingsson, Alina Oprea, and Colin Raffel. 2023.
Extracting training data from large language models.
arXiv preprint arXiv:2012.07805.

Kent K Chang, Mackenzie Cramer, Sandeep Soni, and
David Bamman. 2023. Speak, memory: An archaeol-
ogy of books known to chatgpt/gpt-4. arXiv preprint
arXiv:2305.00118.

Patrick Chao, Alexander Robey, Edgar Dobriban,
Hamed Hassani, George J Pappas, and Eric Wong.
2025. Jailbreaking black box large language models
in twenty queries. In 2025 IEEE Conference on Se-
cure and Trustworthy Machine Learning (SaTML),
pages 23-42. IEEE.

Robert B Cialdini. 1993. The psychology of persuasion.
New York.

Robert B. Cialdini. 2006. Influence: The Psychology of
Persuasion. Harper Business, New York.

Jonathan Davison. 2016. P.e.a.c.e. — a different ap-
proach to investigative interviewing. Accessed:
2025-10-04.

Ronen Eldan and Mark Russinovich. 2023. Who’s
harry potter? approximate unlearning in Ilms. arXiv
preprint arXiv:2310.02238.

Donald O. Hebb. 1949. The Organization of Behavior:
A Neuropsychological Theory. Wiley, New York, NY.

Fred Inbau, Joseph Buckley, and Brian Jayne. 2013.
Criminal interrogation and confessions. Jones &
Bartlett Publishers.

Antonia Karamolegkou, Jiaang Li, Li Zhou, and An-
ders Sggaard. 2023. Copyright violations and large
language models. arXiv preprint arXiv:2310.13771.

Brenden M Lake, Tomer D Ullman, Joshua B Tenen-
baum, and Samuel J Gershman. 2017. Building ma-
chines that learn and think like people. Behavioral
and brain sciences, 40:e253.

Zheyuan Liu, Suraj Maharjan, Fanyou Wu, Rahil Parikh,
Belhassen Bayar, Srinivasan H Sengamedu, and
Meng Jiang. 2025. Disentangling biased knowledge
from reasoning in large language models via machine
unlearning. In Proceedings of the 63rd Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 6105-6123.

11

Nils Lukas, Ahmed Salem, Robert Sim, Shruti Tople,
Lukas Wutschitz, and Santiago Zanella-Béguelin.
2023. Analyzing leakage of personally identifiable
information in language models. In 2023 IEEE Sym-
posium on Security and Privacy (SP), pages 346-363.
IEEE.

Pratyush Maini, Zhili Feng, Avi Schwarzschild,
Zachary C Lipton, and J Zico Kolter. 2024. Tofu: A
task of fictitious unlearning for IIms. arXiv preprint
arXiv:2401.06121.

Qian Niu, Junyu Liu, Zigian Bi, Pohsun Feng, Benji
Peng, Keyu Chen, Ming Li, Lawrence KQ Yan,
Yichao Zhang, Caitlyn Heqi Yin, and 1 others. 2024.
Large language models and cognitive science: A com-
prehensive review of similarities, differences, and
challenges. arXiv preprint arXiv:2409.02387.

David Patterson, Joseph Gonzalez, Urs Hélzle, Quoc
Le, Chen Liang, Lluis-Miquel Munguia, Daniel
Rothchild, David R So, Maud Texier, and Jeff Dean.
2022. The carbon footprint of machine learning train-
ing will plateau, then shrink. Computer, 55(7):18-
28.

Richard E. Petty and John T. Cacioppo. 1986. Com-
munication and Persuasion: Central and Peripheral
Routes to Attitude Change. Springer-Verlag, New
York.

Kai Sun, Yifan Ethan Xu, Hanwen Zha, Yue Liu, and
Xin Luna Dong. 2023. Head-to-tail: How knowl-
edgeable are large language models (lms)? aka will

Ilms replace knowledge graphs? arXiv preprint
arXiv:2308. 10168.

Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer,
and Armen Aghajanyan. 2022. Memorization with-
out overfitting: Analyzing the training dynamics of
large language models. Advances in Neural Informa-
tion Processing Systems, 35:38274-38290.

Bang Trinh Tran To and Thai Le. 2025. Harry potter is
still here! probing knowledge leakage in targeted un-
learned large language models via automated adver-
sarial prompting. arXiv preprint arXiv:2505.17160.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, and | others. 2023. Llama 2: Open foun-
dation and fine-tuned chat models. arXiv preprint
arXiv:2307.09288.

Paul Voigt and Axel Von dem Bussche. 2017. The eu
general data protection regulation (gdpr). A prac-
tical guide, Ist ed., Cham: Springer International
Publishing, 10(3152676): 10-5555.

John B Watson. 1913. Psychology as the behaviorist
views it. Psychological review, 20(2):158.

Heng Xu, Tianqing Zhu, Lefeng Zhang, Wanlei Zhou,
and Philip S. Yu. 2023a. Machine unlearning: A
survey. Preprint, arXiv:2306.03558.


===== PAGE BREAK =====

Rongwu Xu, Brian S Lin, Shujian Yang, Tianqi Zhang,
Weiyan Shi, Tianwei Zhang, Zhixuan Fang, Wei
Xu, and Han Qiu. 2023b. The earth is flat be-
cause...: Investigating IIms’ belief towards misinfor-
mation via persuasive conversation. arXiv preprint
arXiv:2312.09085.

Xiaoyu Xu, Xiang Yue, Yang Liu, Qingqing Ye, Haibo
Hu, and Minxin Du. 2025. Unlearning isn’t deletion:
Investigating reversibility of machine unlearning in
lms. arXiv preprint arXiv:2505.16831.

Hao Xuan and Xingyu Li. 2025. Verifying robust un-
learning: Probing residual knowledge in unlearned
models. arXiv preprint arXiv:2504.14798.

Yi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang,
Ruoxi Jia, and Weiyan Shi. 2024. How johnny can
persuade Ilms to jailbreak them: Rethinking persua-
sion to challenge ai safety by humanizing Ilms. In
Proceedings of the 62nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 14322-14350.

Mengqi Zhang, Zisheng Zhou, Xiaotian Ye, Qiang
Liu, Zhaochun Ren, Zhumin Chen, and Pengjie
Ren. 2025. Disentangling knowledge representations
for large language model editing. arXiv preprint
arXiv:2505. 18774.

Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, and 1
others. 2022. Opt: Open pre-trained transformer
language models. arXiv preprint arXiv:2205.01068.

Zifan Zheng, Yezhaohui Wang, Yuxin Huang, Shichao
Song, Mingchuan Yang, Bo Tang, Feiyu Xiong, and
Zhiyu Li. 2024. Attention heads of large language
models: A survey. arXiv preprint arXiv:2409.03752.

12


===== PAGE BREAK =====

A Implementation Details

A.1 Model Unlearning Process

We obtained four models for evaluation: LLaMA-
2-7B was acquired already unlearned from (Eldan
and Russinovich, 2023), while we performed un-
learning on three additional models using the gra-
dient ascent methodology from the same work.

For LLaMA-3.1-8B, LLaMA-2-13B, and OPT-
2.7B, we implemented the following steps:

1. Fine-tuning: Reinforce Harry Potter knowl-
edge on the model using the full corpus.

. Dataset Preparation: Compare outputs of
the base and fine-tuned models to create forget
and retain datasets.

3. WHP Unlearning: Apply the WHP unlearn-
ing algorithm (Eldan and Russinovich, 2023)
to forget Harry Potter content while keeping
general knowledge intact.

Hardware-wise, we used 4 GPUs with 128 GB
GPU memory. Step 1 took ~ 3-7 hours, and Steps
2-3 took an additional ~ 3-5 hours. As before,
the target domain was the complete Harry Potter
corpus. As for training parameters, batch size was
1, learning rate was 1 x 10~¢ and training required
3 epochs.

A.2. Prompt Generation Pipeline

Starting with 300 manually crafted base prompts,
we used a scripted pipeline leveraging gpt-4 (via the
OpenAI API) to generate three persuasive variants
using distinct rhetorical techniques derived from
persuasion theory:

1. Emotional Appeal: Prompts that use emo-
tional language, personal stories, or empa-
thetic framing to create psychological pres-
sure for a response.

. Logical Reasoning: Prompts that present log-
ical arguments or cite expertise to compel fac-
tual disclosure.

3. Authority Endorsement: Prompts that in-
voke respected figures, institutional backing,
or social proof to legitimize information re-
quests.

13

A.3 Model Inference Configuration

For prompting the 4 models, we used a standard-
ized text-generation procedure and applied it to
each unlearned model. Models were loaded on
a CUDA-enabled GPU when available, with au-
tomatic fallback to CPU. Each prompt, both the
original and the three gpt-generated persuasive vari-
ants, was formatted with custom instruction mark-
ers (LINST] ... [/INST]) to guide the model to
complete the sentence accurately.

Generation was performed using a sampling-
based approach with a maximum of 300 new tokens
per prompt. While temperature, top-p, and repeti-
tion penalty were left at their default values. Out-
puts for each prompt and variant were saved incre-
mentally to a JSON file. This inference setup was
applied consistently across all models, unlearned
and base, enabling direct comparison of outputs
while maintaining stable execution and controlled
resource usage.

A.4 Entanglement Metric Formulas

Connection Strength Metrics

(M,) Edge Count Entanglement (ECE). Mea-
sures the total edge weight between entities within
a prompt. A higher ECE indicates that entities
are linked by stronger or more numerous relations,
suggesting the prompt activates a dense cluster of
knowledge, making recall more likely.

ECE(P)= 5S > weight(u,v)
(u,v)EEp

(M2) Edge Weight Sum (EWS). Similar to
ECE but without normalization by the number of
nodes. A higher EWS indicates stronger total con-
nection strength, entities are tied together through
frequent co-occurrence or strong associations.

EWS(P)= 5° weight(u,v)
(u,v)EEp

(M3) Average Edge Weight Sum (AEWS).
Captures the average strength of individual relation-
ships. A higher AEWS implies that the relation-
ships are strong on average, even if not numerous,
reflecting quality over quantity.

DU (uv)ekp Weight(u, v)

AEWS(P) =           EP

Node Importance Metrics


===== PAGE BREAK =====

(M.) Weighted Node Ratio (WNR). Repre-
sents the average frequency of how many times
each entity (node) appears in a prompt. A higher
WNR means the prompt involves commonly ref-
erenced entities, suggesting activation of well-
practiced memory units and higher entanglement.

enenp freq(n)

WNR(P) =       Nal

(M5) Average Node Degree Entanglement
(ANDE). Measures the average connectivity of
nodes. Higher ANDE indicates that the prompt
activates hubs with many connections, leading to
wider spreading activation.

ANDE(P) = Zenene Veal”)
|p|
Graph Structure Metrics
(Mg) Subgraph Density (SGD). Quantifies
how tightly entities are connected. A higher SGD
means many direct connections, facilitating fast
activation spread.

___ 2: |Ep|
|Np| - (|Np| — 1)

(M7) Mean Shortest Path (MSP). Reflects the
average number of steps needed for activation to
spread between entities. A lower MSP indicates
tighter entanglement and easier recall.

SGD(P)

1
MSP(P) = ———_—    dist(u, v
= Taqara ay, Satu)
uty

(Msg) Redundancy Ratio (RR). Measures the
redundancy of connections. A higher RR implies
multiple overlapping links, increasing robustness
in recall by offering alternative retrieval routes.

arp) = [Bel

|Np|
Influence-Based Metric (Mo)  Distance-
Weighted Influence Score (DWIS). Models how
influence decays with distance from central nodes.
A higher DWIS means entities are close to main
references, suggesting strong semantic proximity.
These main references were pre-determined from
the base prompts in (Eldan and Russinovich,
2023).

DWIS(P,R) = S~ freq(n) x shor)
ne Np

14

A.5 Judge Model Evaluation Details

Our ensemble of judge models (gpt-40-mini, gpt-
4.1-mini, gpt-5-nano) evaluated each response us-
ing structured prompts with clear rubrics for the
three categories. The models agreed on their top
two categories (factual, non-factual, and hallucina-
tion) in 98% of cases. For borderline cases where
fewer than two judges agreed on the highest two cat-
egories, we employed gpt-5-mini as a tie-breaker,
processing 2% of total responses through this addi-
tional evaluation step.

To estimate the factual composition of model
responses, we used the instruction prompt shown
below:

You are an evaluator. Analyze the
following response sentence by
sentence and estimate:

1. Percentage factual (objectively

verifiable).

Percentage non-factual (subjective or

unverifiable).

Percentage hallucinated (fabricated

or false).

Return ONLY a JSON object:

{
"factual": <int>,
"non_factual”: <int>,
"hallucinated”: <int>
}

The three values must sum to 100.
Response to analyze:

{text}

Each model-generated response was analyzed
using this template, ensuring consistent quantifica-
tion across factuality categories.

B_ Additional Results

Model       Variable      coef std err      z P>|2|
Factual     Intercept -1.546 0.511 -3.027 0.002
(Mo)    -0.793 0.430 -1.842 0.065
Non-Factual Intercept  3.074 0.211 14.580 <0.001
(Ma)    -0.020 0.003 -6.980 <0.001
Intercept -149.366 46.865 -3.187 0.001

Hallucination (y4.,)      1.470 0.470 3.131 0.002
Table 4: Logistic Regression Models for Predicting

Unlearned Model Behavior
