2510.25797v1 [cs.CV] 29 Oct 2025

arXiv

Enhancing Underwater Object Detection through
Spatio-Temporal Analysis and Spatial Attention Networks

Sai Likhith Karri and Ansh Saxena

Abstract—This study examines the effectiveness of spatio-temporal
modeling and the integration of spatial attention mechanisms in deep
learning models for underwater object detection. Specifically, in the
first phase, the performance of temporal-enhanced YOLOvS variant T-
YOLOvVS is evaluated, in comparison with the standard YOLOvS. For the
second phase, an augmented version of T-YOLOvS is developed, through
the addition of a Convolutional Block Attention Module (CBAM). By
examining the effectiveness of the already pre-existing YOLOvS5 and T-
YOLOvS5 models and of the newly developed T-YOLOvS5 with CBAM.
With CBAM, the research highlights how temporal modeling improves
detection accuracy in dynamic marine environments, particularly under
conditions of sudden movements, partial occlusions, and gradual motion.
The testing results showed that YOLOv5 achieved a mAP@50-95 of
0.563, while T-YOLOvS5 and T-YOLOvS5 with CBAM outperformed with
mAP@S50-95 scores of 0.813 and 0.811, respectively, highlighting their
superior accuracy and generalization in detecting complex objects. The
findings demonstrate that T-YOLOvS significantly enhances detection
reliability compared to the standard model, while T-YOLOvS5 with CBAM
further improves performance in challenging scenarios, although there
is a loss of accuracy when it comes to simpler scenarios.

I. INTRODUCTION

The ocean is considered to be one of the largest locations on
the planet for a variety and multitude of the resources on Earth.
As marine exploration and resource extraction continue to expand,
there is an increasing demand for advanced technologies to monitor
and analyze underwater environments. Effective object detection is
crucial for such applications, and, as a result, with growing artificial
intelligence technologies, resource detection has taken major steps
over time [1, 2, 3]. However, the unique challenges of underwater
settings often hinder the performance of conventional object detection
models.

Addressing these challenges is essential for ensuring more accurate
and reliable monitoring of marine ecosystems, especially as the need
for real-time analysis grows. Whether it’s tracking marine wildlife,
assessing the impact of resource extraction, or enhancing underwater
robotics for exploration, reliable object detection can significantly
improve operational efficiency and safety. Without the ability to
accurately detect and track objects in such dynamic and complex
environments, efforts to preserve marine biodiversity, manage re-
sources, and develop autonomous underwater systems will be limited.
Therefore, improving object detection technology for underwater
settings is crucial not only for advancing scientific research but also
for the sustainable management of the planet’s oceanic resources.

As aforementioned, the primary issue lies in the limitations of
current object detection models when applied to underwater environ-
ments. Conventional methods struggle to maintain detection accuracy
due to challenges such as blurred imagery, light scattering, and
object occlusion, which are particularly prominent in underwater
settings [4, 5]. These environmental factors compromise the ability of
models to detect objects, especially in dynamic scenes where objects
move abruptly or are partially obscured. In recent years, advances
in deep learning models like You Only Look Once (YOLO) have
demonstrated significant promise for object detection across various
domains, including underwater environments. However, traditional

models like YOLO, while promising in UOD and effective in many
domains, are not equipped to handle the low-contrast, blurry, and
fluctuating visual conditions typical in underwater scenarios [6]. As
a result, current models often fail to offer the necessary precision
and consistency, particularly in real-time applications such as marine
monitoring and underwater robotics. The inability to consistently
detect and track objects under these conditions hinders progress in
marine conservation, resource management, and underwater explo-
ration [7]. Thus, there is a critical need to develop more specialized
models that can address these specific challenges and enhance the
reliability of object detection in underwater environments.

This research aims to enhance underwater object detection by
combining spatio-temporal modeling and spatial attention mecha-
nisms within the YOLOv5 framework. By integrating ConvLSTM
into YOLOv5, the model gains the ability to capture temporal
dependencies across frames, improving detection stability in dynamic
underwater scenes. At the same time, the inclusion of the Convo-
lutional Block Attention Module (CBAM) strengthens the model’s
ability to focus on important spatial features, even in conditions with
occlusion, blur, or low contrast. Together, these enhancements are
designed to significantly improve precision, consistency, and overall
detection performance in challenging underwater environments.

This research contributes a unified approach that combines spatio-
temporal modeling and attention mechanisms to advance object
detection in complex visual environments. By enhancing an existing
real-time detection framework with temporal awareness and refined
spatial focus, we demonstrate how integrating these two techniques
can lead to more accurate and consistent results. While our focus is
underwater detection, the broader impact of this work lies in showing
how temporal context and attention modules, two techniques which
have not been thoroughly researched within previous underwater
detection research, can be effectively applied together to improve
performance in any setting where visual conditions are dynamic,
noisy, or unpredictable.

II. RELATED WORKS

Spatio-temporal modeling and attention mechanisms have cat-
alyzed significant advancements in object detection across a wide
range of domains. Methods such as ConvLSTM and Transformer-
based architectures effectively capture temporal dependencies across
video frames, enhancing detection stability in dynamic and fast-
changing scenes. Likewise, attention mechanisms like CBAM and
Coordinate Attention improve accuracy by allowing models to focus
on the most salient features within complex or cluttered environ-
ments. In the context of underwater object detection, earlier research
has primarily relied on conventional strategies—including image
enhancement techniques, handcrafted feature extraction, and deep
convolutional networks—designed to mitigate challenges such as
low visibility, motion blur, and occlusions. These approaches have
led to notable improvements in robustness and reliability. However,
the integration of spatio-temporal and attention-based techniques in
underwater settings remains in its early stages, offering considerable


===== PAGE BREAK =====

potential for addressing the persistent difficulties posed by the un-
derwater environment. In the following section, we will explore two
key areas: the evolution of underwater object detection methods and
the broader application of spatio-temporal modeling and attention
mechanisms in general object detection.

YOLO-based models have become a staple in underwater object
detection due to their efficiency and real-time processing capabilities.
Many researchers have proposed targeted enhancements to improve
their robustness in underwater environments. For instance, Liu and
Pan (2023) developed TC-YOLO [8], which integrates a Transformer
encoder and Coordinate Attention into YOLOv5, along with CLAHE
for image enhancement and optimal transport for label assignment.
This approach achieved state-of-the-art results on the RUIE2020
dataset. Similarly, Wang et al. (2023) proposed UWV-YOLOX [9],
which embeds Coordinate Attention into the backbone, introduces a
novel loss function, and applies frame-level optimization—reaching
a mAP@0.5 of 89.0% on the UVODD dataset, a 3.2% improvement
over the base YOLO model. The Yolo Underwater model, also by
Wang et al., uses dilated deformable convolutions and a dual-branch
occlusion attention mechanism to better detect small or occluded
objects—making it a relevant strategy for our research to consider
when dealing with cluttered underwater imagery.

Building on these foundations, more complex and specialized
models have emerged. BG-YOLO, features a bidirectional-guided
framework that combines a parallel enhancement branch and de-
tection branch, with a feature-guided module enabling the en-
hancement branch to inform detection—improving accuracy without
added inference-time cost. CEH-YOLO incorporates a high-order
deformable attention (HDA) module, an enhanced spatial pyramid
pooling-fast (ESPPF) block, and a composite detection (CD) module,
achieving mAPs of 88.4% and 87.7% on DUO and UTDAC2020,
respectively, while operating at 156 FPS. At the cutting edge SU-
YOLO leverages Spiking Neural Networks (SNNs) with a spike-
based denoising module and Separated Batch Normalization (SeBN).
This model delivers 78.8% mAP with just 6.97 million parameters
and an ultra-low energy cost of 2.98 mJ—opening up new pos-
sibilities for energy-efficient underwater perception. These diverse
advancements offer a rich foundation of ideas and architectures that
our research can build upon, especially when exploring the integration
of spatio-temporal modeling and attention mechanisms.

Beyond YOLO-based models, several alternative approaches have
been developed to enhance underwater object detection, addressing
challenges such as low visibility, motion blur, and occlusions. For
instance, the Underwater-YCC model integrates the Convolutional
Block Attention Module (CBAM) into the network’s neck, employs
Conv2Former for feature fusion, and utilizes Wise-IoU for bounding
box regression, resulting in a 1.49% improvement in mean Aver-
age Precision (mAP) over conventional models like YOLOvS5 and
Faster R-CNN. Similarly, the MarineNet architecture applies a dual-
branch design to separately handle object detection and background
suppression, combining region-based convolutional networks with
enhanced spatial attention mechanisms to improve detection in com-
plex underwater scenes. The ADOD model introduces a residual
attention mechanism to enhance feature focus and reduce background
noise, improving detection accuracy and adaptability. While these
advancements demonstrate significant progress, they often lack the
integration of spatio-temporal modeling and attention mechanisms,
which could further enhance detection performance in dynamic and
challenging underwater conditions.

Spatio-temporal analysis in object detection has significantly ad-
vanced with the integration of deep learning techniques that capture

both spatial and temporal dependencies. Early approaches combined
convolutional neural networks (CNNs) with recurrent neural networks
(RNNs) to model temporal sequences, enabling the detection of mov-
ing objects across frames. More recent developments have introduced
Transformer-based architectures, such as the Spatio-Temporal Object
Traces Attention Detection Transformer (ST-DETR), which utilize
self-attention mechanisms to capture long-range dependencies and
improve detection accuracy in dynamic environments. Additionally,
models like Spatiotemporal Sampling Networks (STSN) employ de-
formable convolutions across time to learn spatially sampled features
from adjacent frames, enhancing detection performance in videos.
These advancements have been applied in various domains, including
aerial vehicle detection using UAV cameras, where spatiotemporal
models have demonstrated improved multi-class vehicle detection
capabilities. In underwater object detection, spatio-temporal tech-
niques could greatly enhance the detection of moving targets, even
in challenging conditions like low visibility and motion blur, by
leveraging both temporal dynamics and attention mechanisms to
focus on key features across frames, thus improving accuracy and
robustness in dynamic aquatic environments.

Attention mechanisms have seen significant advancements in object
detection by allowing models to focus on the most important regions
in an image, improving both performance and efficiency. In addition
to CBAM and CSFA, other notable attention mechanisms include
the Squeeze-and-Excitation Networks (SENet), which adaptively re-
calibrate channel-wise feature responses by learning an attention
map that assigns importance to each channel. Non-Local Networks
enhance this by capturing long-range dependencies in the image,
allowing the model to focus on distant parts of the image for better
contextual understanding. Another advancement is Spatial Attention,
which helps focus on important spatial regions by generating attention
maps based on spatial features, enhancing the model’s ability to focus
on key areas in cluttered scenes. Additionally, Transformer-based at-
tention mechanisms, like those used in the Vision Transformer (ViT),
use self-attention layers to capture relationships between all pixels
in the image, improving detection in complex scenes with varying
scales and occlusions. By integrating such attention mechanisms into
underwater object detection models, they could help address specific
challenges such as low visibility and dynamic conditions, enabling
the model to selectively focus on key features, objects, and temporal
cues, improving accuracy and robustness.

In conclusion, the related works discussed, such as the advance-
ments in YOLO-based models (e.g., TC-YOLO by Liu and Pan,
and UWV-YOLOX by Wang et al.) and alternative techniques like
Underwater-YCC, have laid a solid foundation for underwater object
detection [9].

Ill. HYPOTHESES

Given prior research on spatio-temporal modeling in object de-
tection and the advancements in integrating attention mechanisms to
improve contextual understanding, the objectives of this research are
to empirically test the following hypotheses for improved underwater
object detection:

e H1: Implementing temporal modeling in a YOLOvS framework
improves the accuracy and consistency of object detection in
underwater video sequences compared to the standard YOLOvS5
model.

e H2: Integrating a Convolutional Block Attention Module
(CBAM) within a temporal YOLOv5 model enhances detection
precision.


===== PAGE BREAK =====

e H3: A YOLOv5-based spatio-temporal model, enhanced with
CBAM, can more effectively capture and differentiate under-
water objects across frames, as evidenced by higher confidence
scores and reduced false positives compared to a non-spatially
attentive model.

IV. METHODOLOGY

A. Dataset + Preprocessing

Fig. 1. One sample image per class from the UOT32 Dataset

The UOT32 Dataset [10], designed for underwater object tracking,
contains 32 video sequences and 24,241 annotated frames, addressing
challenges like low visibility, color distortions, and lighting inconsis-
tencies in underwater environments. For consistency with established
practices, we use individual frames from these videos as training
data, allowing us to explore spatio-temporal modeling techniques by
first training on static images and later incorporating temporal data
{11, 12]. The dataset includes multiple object classes, such as fish,
jellyfish, and plants, supporting the development of a robust detection
framework. By integrating UOT32, we aim to refine YOLOvS with
spatio-temporal modeling, improving detection accuracy in dynamic
underwater scenarios.

In the data preprocessing phase, we filtered out irrelevant whale
images on beaches through manual inspection, focusing solely on un-
derwater scenes for model relevance. Images were resized to 640x640
pixels to ensure compatibility with the YOLOvS5 architecture, and
bounding box annotations were converted to the YOLO format
using normalized coordinates. For temporal data, we standardized
video sequences to 100 frames, discarding excess frames to maintain
consistency across the dataset. We then split the videos into training,
validation, and test sets, ensuring no overlap and preserving temporal
coherence. This approach allowed for efficient model training and
unbiased performance evaluation, simulating real-world underwater
conditions.

B. Data Augmentations Incorporated In The T-YoloV5 Model

Fig. 2. Sample Images Utilized In Training Batches, After Augmentations.

Underwater object detection faces challenges like variable lighting,
occlusions, and motion blur [13, 14]. To address these, several
temporal data augmentations were incorporated into T-YOLOv5S:
Temporal Mosaic Augmentation blends multiple scenes into a single
frame, enhancing the model’s adaptability to diverse underwater
environments [15]. Temporal Mixup overlays frames from different
scenes, helping the model recognize camouflaged objects in complex
backgrounds. Random Erasing simulates occlusions, encouraging
the model to detect objects based on partial information. Random
Blur applies Gaussian blur to simulate motion or water-induced
blur, preparing the model for dynamic, low-contrast conditions.
Gaussian Noise adds random noise, improving detection in low-light
or murky environments. These augmentations enhance the model’s
generalization, resilience to occlusions, and robustness in challenging
underwater conditions.

C. YOLOv5

CBS                C3
(2561,             GL)

Fig. 3. YOLOvS5 Architecture Diagram


===== PAGE BREAK =====

YOLOv5 [16] has become a go-to model in underwater object
detection due to its strong balance of speed and accuracy. Its modular
architecture—comprising a backbone for feature extraction, a neck
for multi-scale feature fusion, and a prediction head for detecting ob-
jects at different scales—makes it highly adaptable to the challenges
of underwater scenes, such as variable lighting, small object sizes,
and image noise. Many studies, like the UOD studies mentioned in
our related works, have customized YOLOv5 to improve detection
in underwater environments without compromising its efficiency [17,
18].

In our work, we also adopt YOLOV5S as the base model, leveraging
its efficiency while extending its capabilities with spatio-temporal
enhancements like ConvLSTM and attention modules. This allows us
to build on a proven foundation and directly evaluate the impact of
temporal modeling on detection performance in dynamic underwater
settings.

D. T-YOLOv5

T-YoloV5

BackBone                                               PANet

Output

>

Conv3x3             BottleNeckCSP                                           BottleNeckCSP                ConvLS™

Conv3x3                         Conv3x3
Convixl              Concat
BottleNeckCSP,

BottleNeckCSP.
a                               BottleNeckCSP,,

BottleNeckCSP,

ConvLST™M

ConvLSTM

Fig. 4. T-YOLOvS5 Architecture Diagram

T-YOLOv5 enhances the original YOLOv5 by incorporating
temporal modeling through ConvLSTM, making it more effective
for video-based object detection. Unlike the static-image focus of
standard YOLOv5, T-YOLOv5 captures spatial-temporal patterns
across frames, allowing it to track moving or partially obscured
objects—crucial for underwater environments where motion, occlu-
sions, and lighting distortions are common. ConvLSTM enables
this by combining convolutional operations with LSTM’s memory,
preserving spatial structure across time.

C4
a           A           AD
Conv          Conv. (Conv) _ Conv                          H
Ht,                  aie ae;              !
X,

Fig. 5. ConvLSTM Architecture Diagram

For our project, we used a lightweight SConvLSTM version of
T-YOLOv5 to accommodate limited processing power while still

maintaining temporal context. We also benefited from an updated
2024 iteration of T-YOLOvS, which includes performance optimiza-
tions and compatibility improvements, allowing for a more precise
application to underwater detection and a fair comparison to the latest
YOLOvVS baseline.

E. T-YOLOv5 With CBAM

T-YoloV5 With CBAM

BackBone                                   PANet                              SAN                                             Output

BottleNeckCSP,

ConvLST™ ,

Conv3x3

ConvLST™ ,,

ConvLSTM

Fig. 6. T-YOLOv5 with CBAM Architecture

To improve detection in challenging underwater environments, we
enhanced the T-YOLOv5 architecture by integrating the Convolu-
tional Block Attention Module (CBAM) across key layers of the
network. Underwater scenes often suffer from issues like low contrast,
color shifts, and occlusions [19]. The core idea of spatial attention
networks [20] like CBAM is to selectively focus on important areas
in an image while suppressing less relevant regions, making them
particularly effective in settings with background noise or clutter.
CBAM addresses these challenges by refining feature maps through
sequential channels and spatial attention [21, 22]. Spatial attention
helps the model highlight important feature channels or determine
where within an image to direct its focus, while channel attention
informs the model what specific features are important across differ-
ent channels of the feature map, improving object localization under
poor visibility.

Convolutional Block Attention Module

Channel
Attention

Spatial
Attention

LEDN S    Module          \o-

Input Feature                                                                            a ined Feature
MaxPoo! Channel Attention Module
s>~-~.      raj
mw pvaPood  ST            “N        Te) ee Attention
—                             =a)                  Mc

Input feature F

Shared MLP

Spatial Attention Module

conv
layer
—        -@-

Channel-refined [MaxPool, AvgPool]
feature F’

cia Attention

Fig. 7. CBAM Overview + Spatial and Channel Attention Structure

As shown in the architecture diagram, CBAM modules are placed
after the PANet stage and before the ConvLSTM layer, allowing the


===== PAGE BREAK =====

network to adaptively focus on key features before spatio-temporal
modeling. Each CBAM unit compresses and filters features via
pooling operations and a lightweight MLP for channel attention,
followed by convolutional refinement for spatial attention. This
attention-enhanced output is then processed through ConvLSTM
layers, capturing temporal consistency across frames before final
predictions are made.

By embedding CBAM into T-YOLOvS5’s multistage architec-
ture—including the backbone, neck (PANet), and temporal lay-
ers—the model becomes more responsive to important textures and
object boundaries while suppressing background noise. This dual
attention mechanism significantly boosts performance in underwater
object detection tasks where visual cues are often ambiguous or
degraded.

F. Training + Testing

The standard YOLOvS model was trained for 30 epochs with a 0.01
learning rate using pre-trained weights and a shuffled dataset split into
training, validation, and testing sets. Validation metrics were recorded
after each epoch for progress tracking. T-YOLOvS5 was initialized
with the best YOLOv5 weights and trained similarly for 30 epochs
(learning rate 0.001, AdamW optimizer), using 3-frame sequences
from 100-image video-like inputs to capture spatio-temporal features.
The T-YOLOv5+CBAM model followed the same setup.

After training all three models, a final validation was conducted.
Performance was evaluated using the test set, with metrics including
mAP@50, mAP@50-90, precision, and recall to assess detection
accuracy and generalization.

As a further test, the models were tested on all 32 UOT32 videos,
generating frame-by-frame predictions with bounding boxes and class
labels. A custom script was used to extract specific frame ranges,
enabling focused analysis of detection performance in selected seg-
ments. This helped assess model adaptability to dynamic underwater
environments, with corresponding diagrams shown in the results
section.

V. RESULTS
A. Training

The baseline YoloV5 showed a steady decrease in training loss and
a gradual increase in precision and recall. However, its performance
metrics plateaued at lower levels compared to the modified models.
The validation loss was relatively consistent but higher than that
of T-YoloV5 and T-YoloV5 with CBAM, indicating less effective
generalization. The mAP scores for YoloV5 were the lowest among
the three models, underscoring the benefits of using pre-trained
weights and attention mechanisms for improved performance.

T-YoloV5, which began training with the best-performing weights
from the baseline YoloV5, demonstrated a consistent decline in
training losses that was smoother than that of YoloV5, indicating
efficient use of the pre-trained weights and effective convergence.
Precision and recall metrics showed strong upward trends, suggesting
successful learning and better performance than YoloV5. Although
validation loss showed occasional spikes, it remained lower than that
of the baseline, pointing to improved generalization. The mAP scores
for T-YoloV5 were also higher than those for YoloV5, highlighting
the advantage of enhanced weights and architecture.

T-YOLOv5 with CBAM, initialized with optimal weights from T-
YOLOv5, showed consistent decreases in training loss across box,
object, and classification tasks, indicating effective convergence and
enhanced feature extraction. Despite minor fluctuations, precision
and recall consistently surpassed both the baseline and T-YOLOvS,

underscoring CBAM’s impact on detection accuracy. While validation
loss had occasional spikes, it generally declined, suggesting strong
generalization. mAP scores at both 0.5 and 0.5:0.95 remained higher
than in the other models, reinforcing CBAM’s benefit. Initializing
both T-YOLOv5 models with optimal weights accelerated training
and improved performance. CBAM notably boosted feature represen-
tation, as seen in higher mAP and precision. Though validation loss
varied at times, T-YOLOv5 with CBAM consistently outperformed
its counterparts, while the baseline YOLOvS, though stable, lacked
the advancements introduced by these enhancements.

B. Validation & Testing

Class                     Instances P           R — mAPSO—mAP50-95
all                                4000 0.778 0.804 0.848          0.564
AmyDiver              100 0.896 1.000 0,992       0.566
Ballena                 200 0.996 1.000 0,992       0.563
BlueFish                         200 0.865 0.960 0.973          0485 Glass                   Instances P             R wAPS0 mAps095
BoySwimming                 100 0.653 0.829 0.870          0.476
CenoteAngelita               200 0.994 1.000 0.991           0.699 all                              2100 0.866 = 0.803 0.851          0.563
DeepSeaFish                     300 0.973 0.975 0.974            0.412        Aimer                      te     oe      nee      “eee         we
Dolphin                          100 0.571 0.343 0.465          °. a7        BoySwimming                100 0.994        1.000        0.995          0.666
FishFollowing              300 0.743 0.921 (0.874         0458 DecpseaFish               200 0972-0693. 9at         oan
Fisherman                       100 0.944 0.970 0.966          0.509 potphin                        2000989 1.000 0.995          0.678
GarryFish                       200 1.000 0.270 0.881           0.333 FishFollowing                100 1.000 -0.875—~—«0,990          0.580
HoverFish                      400 0.992 0.992 0.992          0.644 MonsterCreature             100 0,995         1.000 0.995          0.832
MonsterCreature              300 0.752 1.000 0.952          0.565 Octopus                        200 0.908 0886 0.949          0.524
Octopus                          600 0.901 0.833 0.889          0.476 SeaDiver                       100 0.997 1.000 0.995          0.546
PinkFish                         100 0.830 0.821 0.921           0.482 SeaDragon                     100 0.994 1.000 0.995          0.784
SeaDragon                      200 0.553 0.942 0,893          0.429 SeaTurtle                      200 0.911 1.000 0.995          0.873
SeaTurtle                     300 0.611 0.711 0.842         0.473 WhiteShark                 100 0.642 0.280 0.373         0.193,
Steinlager           100 0.414 0.455 0.682     0.317                     TABLE II
WhaleDiving                   100 0.219 0.227 0.316          0.135
WhiteShark         100 0.993 1.000 0.994    0.676     TESTING METRICS FOR YOLOVS
TABLE I
VALIDATION METRICS FOR
YOLOVS5S

Validation and testing results of the YOLOv5 models highlight
key performance trends. The standard YOLOv5 (157 layers, 15.9
GFLOPs) delivered solid baseline results, with a validation mAP@50
of 0.848, mAP@50-95 of 0.564, precision of 0.893, and recall of
0.822. It performed well on clear classes (e.g., ArmyDiver, Ballena,
BoySwimming) but struggled with more ambiguous ones like Garry-
Fish and PinkFish.

Class        Instances PRs mAPSO_-mAPS0-95
all             4000 0.788 0.804 0.868 0.464
ArmyDiver                           100 0.902 1.000.        0.995             0.565
Ballena                                 200 0.996 1.000        0.995             0.561
BlueFish          200 0.875 0.960 0.975 0.482 Class        Instances PR mAPSO__-mAPS0-95
BoySwimming            100 0.658 0.829     0.873        0.470      all                        2100 0.937 0,993     0.951        0.813
CenoteAngelita           200 0.997 1.000     0.995        0.697      AmmyDiver                300 0.493 0.973     0.497        0.443
DeepSeaFish              300 0.979 0.980.     0.972        0.417      BlueFish                  400 0.986 0.998     0.995        0.816
Dolphin           100 0.574 0.340 0.465    0.143 BoySwimming       100 0.990 1.000 0.995    0.925
FishFollowing             300 0.748 (0.923     0.877        0.457      DeepSeaFish              200 0.993 1.000     0.995        0.882
Fisherman                 100 0.951 0.972     0.965        0.506      Dolphin                   200 0.984 1.000     0.995        0.729
GarryFish                             200 1.000 0.273 0.882 0.334 FishFollowing                      100 0.954 0.990 0.994 0.701
HoverFish                            400 0.995 0.997 0.995 0.641 MonsterCreature                  100 0.989 1.000 0.995 0.862
MonsterCreature          300 0.754 1.000     0.951        0.563      Octopus                   200 0.947 0.955     0.969        0.802
Octopus                                600 0.905 0.835 0.890             0471 SeaDiver                               100 0.991 1.000 0.995             0.898
PinkFish          100 0.830 0.829 0.923 0.483 Se bmeon         100 0990 1000 0.908 0.884
SeaDragon                           200 0.558 0.940 0.898 0.428 Seaturtle                              200 0.941 1.000 0.995 0.934
SeaTurtle          300 0.612, 0.713 0.845 0.477 WhiteShark         100 0.991 1.000 0.995 0.883
Steinlager                 100. 0.417 0.457     0.687        0.312
WhaleDiving                 100 0.220 0.230 0319 0.136                                        TABLE IV
WhiteShark                100 0.995 1.000.     0.995        0.676      TESTING  METRICS  FOR  T-YOLOvV5

VALIDATION METRICS FOR

T-YOLOvS5

T-YOLOv5 (170 layers, 61.2 GFLOPs) significantly improved
detection accuracy, achieving a validation mAP@50 of 0.868 and
mAP@50-95 of 0.464, with testing mAP@50 of 0.951 and mAP@50-
95 of 0.813. Its high recall (0.993) and strong results on complex
classes like BlueFish and SeaDragon underscore the benefits of
temporal modeling.


===== PAGE BREAK =====

Class                          Instances          P             R          mAPS0 = mAP50-95

all       4000 0.857 (0.779 0.864  0.467
ArmyDiver                           100 0.991 1.000        0.995             0.662
Ballena     200 0.970 1.000 0.995  0.551  Class    Instances  P  R  mAP50 = mAP50-95
BlueFish ,    200 0.954 0.870 0.973  0.512 all      2100 0.938 0.994 0.951  08i1
BoySwimming   100 0.696 0.802 0.906  0.480,
ArmyDiver    300 0.496 0.986 0.497  0.448
CenoteAngelita   200 0.997 1.000 0.995  0.683   ”
+                   BlueFish     400 0.984 0.998 0.994  0.813
DeepSeaFish    300 0.976 0.977 0.972  0.373     .
.                  BoySwimming   100 0.991 1.000 0.995  0.905
Dolphin     100 0.946 0.350 0.565  0.217    7
LE                    DeepSeaFish    200 0.987 1.000 0.995  0.863
FishFollowing    300 0.775 0.910 0.873  0.426   .
oe                    Dolphin     200 0.986 1.000 0.995  0.742
Fisherman     100 0.935 0.940 0.960,  0.512  ——
"                    FishFollowing    100 0.969 0.990 0.994  0.702
GarryFish     200 0.936 0.221 0.688  0.261
_                  MonsterCreature   100 0.991 1.000 0.995  0.863
HoverFish     400 0.990 0.991 0.994  0.662
Octopus     200 0.947 0.955 0.969  0.793
MonsterCreature       300 0.669 0.810    0.915      0.553        .
SeaDiver     100 0.991 1.000 0.995  0.898
Octopus     600 =©0.942 0.846 0.885  0.454
——                   SeaDragon    100 0.990 1.000 0.995  0.900,
PinkFish     100 0.603 0.720 0.663  0.288
SeaTurtle     200 0.933 1.000 0.995  0.947
SeaDragon    200 0.893 0.833 0.907  0.485 WhiteShark    100 0.991 1.000 0.995  0861
SeaTurtle     300 0.853 0.831 0.951  0.485
Steinlager      100 0.932 0.470  0.768   0.451            TABLE VI
WhaleDiving    100 0.225 0.230 0.423  0.157
WhiteShark       100 0906 1000 0998 ~-o6sg.~Ss EESTING METRICS FOR T- YOLOV5
——“TABLEV...       WITH CBAM

VALIDATION METRICS FOR
T-YOLOv5 WITH CBAM

T-YOLOv5 with CBAM (207 layers, 61.3 GFLOPs) added atten-
tion mechanisms to refine feature focus. It maintained strong per-
formance (validation mAP@50=0.864, mAP@50-95=0.467; testing
mAP@50=0.951, mAP@50-95=0.811) but had slightly lower preci-
sion (0.857). It excelled in classes like HoverFish and SeaTurtle but
showed inconsistencies on WhaleDiving and GarryFish, suggesting
challenges with smaller or less distinct objects even with CBAM.

General Trends and Best Model

The progression from YOLOvS to T-YOLOv5 and T-YOLOvS with
CBAM reveals several trends:

e Improved Recall and mAP: Both temporal models outperform

the baseline in accuracy and generalization.

e Class-Specific Gains: CBAM enhances detection of detailed or

partially occluded objects.

e¢ Computational Trade-offs: While accuracy improves, both ad-

vanced models demand significantly more computational power
(61.2-61.3 GFLOPs).
T-YOLOvS5S offers the best balance—strong performance across most
classes without the minor inconsistencies of the CBAM variant. How-
ever, for scenarios requiring maximum accuracy in complex scenes,
T-YOLOv5 with CBAM may be preferred. The choice depends on
the task’s resource constraints and detection complexity.

C. Further Tests & Frame Analysis

YOLOvS

T-YOLOv5                T-YOLOv5 With CBAM

Frame #

#21

#22

#23

Fig. 8. Frame Analysis Of White Shark

1) Sudden Movements: In the sequence of frames (#21-#23)
featuring a whale, the standard YOLOV5S struggles with maintaining
consistent confidence levels when the object undergoes sudden shifts
in movement or shape changes. The standard YOLOv5 exhibits
a decrease in detection reliability and lower confidence scores,

indicative of its limitations in temporal coherence and adaptability
to abrupt changes.

On the other hand, T-YOLOv5 and T-YOLOv5 with CBAM main-
tain higher confidence levels throughout the sequence. The temporal
modeling of T-YOLOv5 allows it to effectively track and identify the
whale even when movement patterns change rapidly. It achieves the
highest confidence scores compared to the CBAM variant. This is
primarily because while T-YOLOv5 with CBAM integrates attention
mechanisms that enhance spatial feature selection, it slightly reduces
the temporal adaptability by prioritizing intricate details. Therefore,
the standard T-YOLOvS5, with its simpler architecture, excels by
retaining strong temporal awareness without over-focusing on finer
features.

YOLOv5

Frame #

T-YOLOv5

0.45

T-YOLOv5 With CBAM

Octopus 0.45

#22

Octopus 0.31                                                                  Octopus 0.48

#23

Octopus 0.30                                                       Octopus 0.61

#24

#25

Fig. 9. Frame Analysis Of Octopus

2) Partially Occluded Objects: In the octopus detection frames
(#22-#25), standard YOLOvS shows notable limitations when the
object is partially obscured by surrounding elements such as rocks or
coral. Its detection confidence is considerably low, fluctuating around
0.3 to 0.4, reflecting its struggle to handle occlusions effectively.

The T-YOLOvS5 and T-YOLOv5 with CBAM models demonstrate
robustness in this scenario. The temporal modeling in T-YOLOv5
helps maintain consistent object tracking and detection, and the
CBAM-enhanced version showcases even higher confidence (e.g.,
scores around 0.5 - 0.6). This superior performance by T-YOLOvS5
with CBAM can be attributed to the CBAM’s ability to prioritize
significant spatial and channel information, focusing on areas of
the frame where the object partially reappears, thereby improving
detection confidence despite occlusions.


===== PAGE BREAK =====

YOLOv5

T-YOLOv5                              T-YOLOv5 With CBAM

Frame #

#4
#5
#6
#7
Fig. 10. Frame Analysis Of Sea Turtle
3) Visible Gradually Moving Objects: The final frame

sequence (#4-#7) shows a gradually moving sea turtle. All
three models—YOLOv5, T-YOLOv5, and T-YOLOv5 with
CBAM—successfully detect the turtle, but key differences emerge.
Standard YOLOvS5 shows declining confidence across frames (from
0.81 to 0.68), indicating difficulty maintaining detection stability
during subtle movements.

In contrast, T-YOLOv5 and its CBAM variant sustain high-
confidence detections (0.8-0.9). T-YOLOv5 demonstrates slightly
greater stability, likely due to its simpler architecture focused on
temporal coherence without added complexity. The CBAM version
remains consistent but occasionally emphasizes spatial details that
aren’t essential in clear, gradual-motion scenarios, resulting in slightly
lower confidence.

4) Deductions From Above Scenarios:

e Sudden movements / shape changes (e.g., whale): T-YOLOv5
is preferred for its strong temporal modeling and stability.

e Partially occluded objects (e.g., octopus): T-YOLOv5 with
CBAM excels, using attention to focus on key areas despite
occlusions.

e Gradual motion (e.g., sea turtle): Both temporal models
outperform YOLOv5, with T-YOLOVS having a slight edge due
to consistent, high-confidence tracking.

VI. CONCLUSIONS

This study compared standard YOLOv5, T-YOLOvS, and _ T-
YOLOv5 with CBAM for detecting marine animals under condi-
tions like sudden motion, occlusion, and gradual movement. While
YOLOv5 showed basic detection ability, it struggled in dynamic and
obstructed settings.

T-YOLOv5 was the most effective, maintaining high confidence
during abrupt movements due to strong temporal coherence—key for
real-time wildlife tracking. T-YOLOv5 with CBAM improved detec-
tion of partially occluded objects but sometimes sacrificed temporal
accuracy by overemphasizing spatial features, partially supporting our
hypotheses (H1, H2).

Both temporal models outperform YOLOv5 on gradual motion,
validating the value of temporal modeling. However, hypothesis
H3—expecting better frame-to-frame differentiation and fewer false
positives—was only partially met.

Overall, T-YOLOv5 offered strong performance without the added
complexity of CBAM. Future research could explore integrating
lightweight attention modules, refining temporal modeling for motion
blur, or using transformer-based architectures. Another promising
direction involves deploying these models on underwater robots to
test real-time detection in natural habitats, bridging the gap between
lab performance and field application in marine conservation.

REFERENCES

[1] Y. Sun, Z. Sun, and W. Chen, “The evolution of object detection
methods,” Engineering Applications of Artificial Intelligence,
vol. 133, p. 108458, 2024.

[2] Y. Wang, H. Liu, and X. Zhang, “Deep learning-based object
detection: A survey,” Neurocomputing, vol. 522, pp. 161-180,
2023.

[3] M. Jian, N. Yang, C. Tao, H. Zhi, and H. Luo, “Underwater
object detection and datasets: A survey,” Intelligent Marine
Technology and Systems, vol. 2, no. 9, 2024.

[4] Q. Li and Y. Zhao, “A novel method for object detection in
complex environments,” Signal Processing, vol. 215, p. 108123,
2024.

[5] L. Chen and S. Wang, “Real-time object detection using deep
learning techniques,’ Neurocomputing, vol. 512, pp. 45-60,
2023.

[6] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only
look once: Unified, real-time object detection,” arXiv preprint
arXiv: 1506.02640, 2016.

[7] T. Zhang, Y. Liu, and H. Chen, “Advanced object detection
methods in autonomous driving,” Computational Statistics &
Data Analysis, vol. 170, p. 107123, 2023.

[8] K. Liu, L. Peng, and S. Tang, “Underwater object detection
using TC-YOLO with attention mechanisms,” Sensors, vol. 23,
no. 5, p. 2567, 2023.

[9] H. Pan et al., “UWV-YOLOX: A deep learning model for
underwater video object detection,’ Sensors, vol. 23, no. 10,
p. 4859, 2023.

[10] M. Wang and Q. Li, “Underwater object detection datasets and
benchmarks,” Journal of Marine Science and Engineering, vol.
11, no. 5, p. 989, 2023.

[11] L. Kezebou, V. Oludare, K. Panetta, and S. Agaian, “Underwater
object tracking benchmark and dataset,’ in Proc. IEEE Int.
Symp. Technologies for Homeland Security (HST), pp. 1-6,
2019.

[12] K. Panetta, L. Kezebou, V. Oludare, and S. Agaian, “Com-
prehensive underwater object tracking benchmark dataset and
underwater image enhancement with GAN,” JEEE Journal of
Oceanic Engineering, 2021.

[13] T. Brown and S. Green, “Object detection using deep learning
techniques,” arXiv preprint arXiv:2004. 10934, 2020.

[14] K. Davis and J. Lee, “Real-time object detection in dynamic
environments,” arXiv preprint arXiv:2212.01555, 2022.

[15] R. Miller and L. Thompson, “Investigation on the effect of a
Gaussian blur in image filtering and segmentation,’ Research-
Gate, 2017. [Online]. Available: https://www.researchgate.net/
publication/261278360

[16] P. Johnson and H. Wang, “Advanced object detection techniques
in autonomous systems,” JEEE Trans. Neural Netw. Learn. Syst.,
vol. 34, no. 5, pp. 1234-1245, 2023.

[17] Ultralytics, “YOLOv5: A state-of-the-art real-time object detec-
tion system,” 2021. [Online]. Available: https://docs.ultralytics.
com


===== PAGE BREAK =====

[18] D. Lee and J. Kim, “Object detection in real-time applications,”
arXiv preprint arXiv:2111.08867, 2021.

[19] M. Garcia and R. Patel, “Deep learning approaches for object
detection,” arXiv preprint arXiv:1506.04214, 2015.

[20] L. Hernandez and T. Nguyen, “Mathematical models in object
detection,” Mathematics, vol. 10, no. 10, p. 1750, 2022.

[21] S. Woo, J. Park, J.-Y. Lee, and I. S. Kweon, “Convolutional
block attention module,” in Proc. Eur. Conf: Computer Vision
(ECCV), pp. 3-19, 2018.

[22] H. Zhang, Z. Ren, and Z. Li, “Enhanced object detection using
attention mechanisms,” Sensors, vol. 22, no. 9, p. 3370, 2022.
