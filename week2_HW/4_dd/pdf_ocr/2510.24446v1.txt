Viktoriia Zinkovich*

arXiv:2510.24446v1 [cs.CL] 28 Oct 2025

SPARTA: Evaluating Reasoning Segmentation Robustness through
Black-Box Adversarial Paraphrasing in Text Autoencoder Latent Space

Anton Antonov*
Daria Pugacheva Elena Tutubalina

*Equal contribution

Abstract

Multimodal large language models (MLLMs)
have shown impressive capabilities in vision-
language tasks such as reasoning segmentation,
where models generate segmentation masks
based on textual queries. While prior work
has primarily focused on perturbing image
inputs, semantically equivalent textual para-
phrases—crucial in real-world applications
where users express the same intent in varied
ways—remain underexplored. To address this
gap, we introduce a novel adversarial para-
Phrasing task: generating grammatically cor-
rect paraphrases that preserve the original query
meaning while degrading segmentation per-
formance. To evaluate the quality of adver-
sarial paraphrases, we develop a comprehen-
sive automatic evaluation protocol validated
with human studies. Furthermore, we intro-
duce SPARTA—a black-box, sentence-level
optimization method that operates in the low-
dimensional semantic latent space of a text
autoencoder, guided by reinforcement learn-
ing. SPARTA achieves significantly higher suc-
cess rates, outperforming prior methods by up
to 2X on both the ReasonSeg and LLMSeg-
40k datasets. We use SPARTA and competi-
tive baselines to assess the robustness of ad-
vanced reasoning segmentation models. We
reveal that they remain vulnerable to adversar-
ial paraphrasing—even under strict semantic
and grammatical constraints. All code and data
will be released publicly upon acceptance.

1 Introduction

In recent years, foundation models have achieved
significant advances across diverse domains of
deep learning. Advances in image classifica-
tion (Dosovitskiy et al., 2021; Liu et al., 2022;
Woo et al., 2023) and interactive segmentation (Kir-
illov et al., 2023; Ravi et al., 2024), together with
progress in large language models (LLMs) (Brown
et al., 2020; Touvron et al., 2023; Guo et al., 2025;

Andrei Spiridonov*

Denis Shepelev Andrey Moskalenko
Andrey Kuznetsov Vlad Shakhuro!
‘Project leader
Original -- SPARTA --> Adversarial

What is it that the young
girl is using to eat her
dessert?

What might the young
girl be using to eat her
dessert?

Figure 1: Example of an adversarial paraphrase
generated by our proposed SPARTA method. The
SPARTA produces grammatically correct paraphrases
that preserve the original semantic content while signifi-
cantly degrading segmentation performance.

Dubey et al., 2024), have paved the way for mul-
timodal large language models (MLLMs) (Liu
et al., 2023, 2024a; Bai et al., 2023; Wang et al.,
2024b; Li et al., 2023; Peng et al., 2023; Lai et al.,
2024) that seamlessly integrate vision and language.
These models are now integral to diverse applica-
tions, including conversational systems like Chat-
GPT, autonomous driving (Mu et al., 2024; Seff
et al., 2023; Hwang et al., 2024), and robot con-
trol (Driess et al., 2023; Brohan et al., 2022, 2023;
Black et al., 2024). As these models continue to
mature, new tasks are emerging—particularly in
robotics—that require sophisticated visual percep-
tion and reasoning capabilities. One such task is
reasoning segmentation (Lai et al., 2024), where a
model outputs a binary segmentation mask driven
by an implicit text query that requires intricate log-
ical or contextual interpretation.

The quality of a model’s predicted segmentation
mask is expected to remain consistent, even when
users paraphrase their prompts, preserving the orig-


===== PAGE BREAK =====

inal meaning and intent. However, the robustness
of reasoning segmentation models to query para-
phrasing remains largely unexplored.

To address this problem, we propose a novel task,
adversarial paraphrasing, which constrains textual
perturbations according to the following criteria:
(1) the core meaning of the original prompt must
be preserved; (2) the paraphrase must remain gram-
matically correct; and (3) it must lead to a degra-
dation in segmentation mask predictions. These
constraints enable us to evaluate the robustness of
state-of-the-art reasoning-based segmentation mod-
els against adversarially paraphrased queries.

Based on this task, we construct a new bench-
mark to systematically evaluate the robustness
of reasoning segmentation models. We assess
state-of-the-art attack strategies, including gradient-
based and LLM-based methods; however, these ap-
proaches have notable limitations. Gradient-based
methods often produce ungrammatical text (Guo
et al., 2021; Jones et al., 2023), while LLM-based
attacks typically rely on heuristic methods, such as
iterative refinement (Chao et al., 2024).

To overcome the limitations of existing gradient-
based and heuristic methods, we introduce
SPARTA—a novel black-box sentence-level op-
timization method (Figure 1). SPARTA projects
queries into a low-dimensional semantic latent
space of a pretrained autoencoder and employs re-
inforcement learning to identify nearby vectors that
yield effective adversarial paraphrases.

Overall, our contributions are as follows:

¢ We introduce a novel adversarial paraphras-
ing task designed to evaluate the robustness
of reasoning segmentation models against se-
mantically equivalent paraphrased queries. To
facilitate this evaluation, we introduce an au-
tomated evaluation protocol. We conduct a
user study and demonstrate strong alignment
of the proposed scoring method with human
judgment.

We present a new method for generating ad-
versarial paraphrases, leveraging reinforce-
ment learning-based sentence-level optimiza-
tion. Our method outperforms black-box and
white-box baselines by up to 2x on both the
ReasonSeg and LLMSeg-40k datasets, with 2
model-specific exceptions.

We conduct comprehensive experiments to as-
sess the robustness of state-of-the-art reason-

ing segmentation models under both white-
box and black-box adversarial paraphrasing
settings. Our results indicate that, despite the
strict semantic and grammatical constraints,
existing reasoning segmentation models re-
main vulnerable to such attacks.

2 Related Work

2.1 Reasoning Segmentation

In Referring Expression Segmentation (RES), mod-
els output segmentation masks from textual de-
scriptions (Zou et al., 2023; Rasheed et al., 2024;
Wt et al., 2024b,a; Wang et al., 2024c; Liu et al.,
2024b). Expanding on RES, the reasoning segmen-
tation task was introduced to handle prompts requir-
ing world knowledge and logical reasoning (Lai
et al., 2024).

The pioneering reasoning segmentation model
LISA employs an embedding-as-mask paradigm,
decoding a <SEG> token via SAM to produce a seg-
mentation mask. Several LISA-based models fol-
lowed, such as LISA++ (Yang et al., 2024), which
can incorporate segmentation results into text re-
sponses, and GSVA (Xia et al., 2024), which in-
troduces a <REJ> token to explicitly reject absent
objects.

2.2 Adversarial Attacks on Text Modality

Evaluating model robustness often involves adver-
sarial attacks, which are broadly categorized as
white-box or black-box, based on the attacker’s
access to model internals.

White-box attacks leverage gradient informa-
tion to optimize adversarial paraphrases, address-
ing the challenges posed by the discrete nature
of text through techniques such as Taylor expan-
sion (Ebrahimi et al., 2018; Jones et al., 2023) and
Gumbel-Softmax sampling (Jang et al., 2017; Guo
et al., 2021). Among these, we consider two state-
of-the-art methods: GBDA (Guo et al., 2021) and
ARCA (Jones et al., 2023). While ARCA achieves
strong attack success, it lacks semantic regular-
ization, frequently inserting special symbols that
undermine its suitability as a paraphrasing base-
line. In contrast, GBDA incorporates semantic sim-
ilarity constraints; however, it remains limited to
token-level substitutions, constraining paraphrase
diversity.

In the black-box setting, attacks have progressed
from simple word- and character-level manipula-
tions, such as synonym substitution (Jin et al., 2020;


===== PAGE BREAK =====

Ren et al., 2019) and character edits (Gao et al.,
2018), to methods that generate semantically equiv-
alent paraphrases using transformer-based mod-
els (Li et al., 2020; Iyyer et al., 2018). Recent
developments further leverage LLMs to generate
more semantically diverse paraphrases (Yan et al.,
2023; Xu et al., 2023). Among these, we con-
sider PAIR (Chao et al., 2024)—a state-of-the-art
and widely used adversarial method—and Qwen3-
32B (Team, 2025), a leading LLM, as attack base-
lines. While these techniques often produce flu-
ent paraphrases, they typically depend on heuristic
rules or manual trial-and-error, lacking controllable
optimization. To address this gap, we optimize
paraphrases in the low-dimensional semantic latent
space of a pretrained text autoencoder, leveraging
reinforcement learning to maximize degradation
of segmentation performance—thereby enabling
more effective adversarial paraphrasing.

2.3 Evaluation of Adversarial Attacks

Adversarial attack effectiveness is primarily mea-
sured by the attack success rate (SR), where suc-
cess is determined by the model’s output quality
drop crossing a task-specific threshold. For in-
stance, this could be a drop in Intersection over
Union (IoU) for interactive segmentation (Liu et al.,
2025; Huang et al., 2024b) or confidence score
changes in classification task (Guo et al., 2021;
Dong et al., 2019).

To ensure the validity and semantic consistency
of adversarial paraphrases, we consider text quality
metrics. Semantic preservation is commonly as-
sessed via cosine similarity between embeddings of
the original and paraphrased sentences (Guo et al.,
2021; Thieu et al., 2021; Sun and Wang, 2024). In
practice, many recent studies apply a cosine simi-
larity threshold to filter paraphrases, with the cutoff
depending on the embedding model used (Kassem
and Saad, 2024; Herel et al., 2023). Furthermore,
recent works leverage LLMs for evaluation through
GPT-scoring (Fu et al., 2024; Wang et al., 2023;
Chiang and Lee, 2023; Chan et al., 2023), though
the reliability of such metrics remains an open ques-
tion (Wang et al., 2024a). To offer a more trustwor-
thy assessment of paraphrase quality, we combine
cosine-based filtering with LLM-based scoring into
a comprehensive evaluation pipeline.

3 Proposed Method: SPARTA

In this section, we introduce SPARTA—a novel
black-box paraphrasing method that generates
grammatically correct, semantically consistent
paraphrases which degrade segmentation perfor-
mance. The input query is first encoded into a
continuous latent representation using a pretrained
text autoencoder (Section 3.1). A set of candidate
vectors is sampled from a Gaussian distribution in
the latent space, centered at the original latent vec-
tor. These candidates are decoded into paraphrases
and evaluated via a reward function that penalizes
overlap with the original segmentation mask, while
regularization ensures semantic fidelity. The policy
is optimized via Proximal Policy Optimization to
guide the sampling toward more effective adversar-
ial paraphrases (Section 3.2). The full optimization
pipeline is detailed in Algorithm 1.

3.1 Latent Sentence Space

Instead of searching paraphrases over discrete to-
kens, we operate in the continuous latent space of
a pretrained text autoencoder (E£, D). The encoder
FE maps the input query x to a continuous semantic
space, and the decoder D reconstructs the original
sentence from the latent vector z:

z= E(x)€R’, x=D(z),

We adopt SONAR (Duquenne et al., 2023), a 1B-
parameter multilingual model, as the state-of-the-
art text autoencoder (£, D), which is described in
detail in Appendix A. Its training objective includes
translation and MSE losses on sentence embed-
dings, encouraging language-agnostic latent repre-
sentations and a well-aligned cross-lingual embed-
ding space.

For the original input query xg, we obtain the
initial embedding zo = E(xo) and optimize the
latent vector z € R%, initialized with zo.

xx. (1)

3.2 Reinforcement Learning Formulation

SPARTA learns a stochastic policy 79(z | zo) that
perturbs the original latent vector zp to generate
adversarial paraphrases. The policy is modeled as
a diagonal Gaussian distribution in latent space:

z~ T9(z| Zo) =N(u,diag(o*)), 2)

where the learnable parameters 0 = (1, 0) consist
of the mean pp € R?¢ (initialized with zo) and the
standard deviation o € R@.!

'The scale is reparameterized as o = log(1 + exp(A)) to
keep it strictly positive, where A € R? is trainable.


===== PAGE BREAK =====

Algorithm 1 PPO in Latent Sentence Space

Require: Query xo, image I, ground-truth mask
m; autoencoder (F,D); model f; sample
size n; iteration number N; hyperparams
(E, Asim, Nady oO, Vu)

1: Initialize z) + E(xo), uw <— Zo

2: fort = 1 to N do

3: Sample 2; ~ N (4, diag(o”)) for i = 1..n
4. fori =1tondo

5       Decode x; <« D(z;), predict m; <

f(I, &)
6:       Compute R;, Aj, p;, 1; (Eqs. 3-6)
7: end for

8:     Lpolicy   <—   — adv + » bs;      Lyalue
9: Update y,o,~ via Adam on £Lgnai (Eq. 7)
10: | Update old policy: to1q <— 7

li: Savex + D(p)

12: end for

Reward For each sampled vector z, we gener-
ate a candidate paraphrase x = D(z) and pass it
through the attacked reasoning segmentation model
f. The model outputs a segmentation mask m, and
the effectiveness of the adversarial paraphrase is
quantified by the following reward:

R = —IoU(mh, m),             (3)

where m is the ground truth mask. Higher re-
wards correspond to lower Intersection-over-Union,
thereby encouraging paraphrases that most effec-
tively degrade model performance.

Baseline and Advantage To reduce variance in
gradient estimates, we learn a scalar value net-
work V,, as a baseline (Sutton and Barto, 2018).
The advantage A is then the normalized difference
between the observed reward R and the baseline

Vy(z):

_ R-VW — E[R-W]
—— - Std[R— Vy] +e

Optimization via PPO To train the latent-space
policy, we employ the standard clipped surrogate
objective of Proximal Policy Optimization (PPO)
(Schulman et al., 2017; Huang et al., 2024a). At
each update, we sample a batch of n candidate em-
beddings {z;} from the old policy 7,,,, decode
each into a paraphrase, and evaluate its adversar-
ial reward R;. For each sample, we compute the
importance weight:

(4)

ay?) (Z;)
TO ou4 (Z;)

p=           = exp(log ™o(Zi) — log Tora (Zi)

(5)

then form the clipped surrogate:

l= min(p; A;, clip(p;, 1 — «, 1 + €) Aj).   (6)

Here, € = 0.2 is the clip ratio hyperparameter,
which constrains the policy update to a trust region
[1 — «,1 + €]. Clipping p; prevents large updates
that could destabilize training (Schulman et al.,
2015, 2017).

Objective function The final optimization objec-
tive Lena) combines three terms:

1  n     1  n
—_ adv — Soh + So(Ri _ Vu)? + Asim||M ~~ zol|3,
n i=1     n i=1                 Leim

L policy

Lyalue

(7)
where Lyalue trains the baseline and Lim, preserves
semantic fidelity to the original query. Optimiza-
tion is performed with Adam using separate learn-
ing rates for 4, o, and w.

4 Proposed Evaluation Protocol

In this section, we introduce an automatic evalua-
tion protocol for our novel adversarial paraphras-
ing task. We begin by outlining the main steps
of the protocol (Section 4.1). We then examine
the challenges associated with its core component,
LLM-based paraphrase detection, and propose ad-
ditional filtering steps to enhance performance
(Section 4.2). Finally, we evaluate the detection
methods and ablate the proposed improvements
through human studies (Section 4.3).

4.1 Evaluation Protocol

We introduce an automatic evaluation protocol for
the adversarial paraphrasing task. Since existing
attack methods may produce invalid outputs, we
select the best adversarial prompts—based on at-
tack loss and paraphrasing quality—and use them
to evaluate attack performance. Specifically, given
a set of adversarial prompts obtained through an
attack over N iterations, we proceed as follows: (1)
remove duplicate prompts; (2) discard any prompt
that does not reduce the segmentation model’s IoU;
(3) detect which prompts are valid paraphrases; (4)


===== PAGE BREAK =====

Type                                 Text

Original                                     the youngest person

PAIR paraphrase                   considering standard human growth patterns, pinpoint the individual who, if all people in the
image were lined up in order of birth, would be positioned closest to the beginning of the
sequence

Original                                the sauce

PAIR paraphrase                    the component that is neither the main ingredient nor the garnish, but is distributed throughout

the plate in a somewhat fluid form

Table 1: Examples of PAIR-generated paraphrases that are overly verbose or abstract. The first paraphrase
employs indirect and wordy language, while the second describes the sauce ambiguously without explicitly naming
it, leaving it unclear whether it refers to sauce, oil, or dressing. Despite this, LLMs rate such paraphrases as valid.

Qwen3                                                           LLaMA-3.1-Nemotron
Prompt                  LLM                   LLM & RegExp & CosSim                   LLM                  LLM & RegExp & CosSim
Precision Recall F-score Precision Recall -score Precision Recall F-score Precision Recall F-score
1               0.480      0.964 0.641       0.623       0.865 0.725       0.634      0.640 0.637       0.798       0.604 0.687
2                0.480       0.991 0.647        0.601        0.883 0.715        0.520       0.712 0.601        0.664       0.640 0.651
3                 0.530       0.946 0.680        0.671        0.847 0.749        0.552       0.712 0.622        0.726       0.622 0.670

Table 2: Evaluation of paraphrase detection methods. We compare LLM-based detection using the baseline
system prompt | from Michail et al. (2025) against our proposed enhanced system prompts (2 and 3), as well with
additional filtering based on regular expressions and semantic cosine similarity. Best F-scores are shown in bold.

select the paraphrase that yields the greatest relative
IoU drop.

A critical step in this evaluation protocol is para-
phrase detection, which, as we demonstrate in the
following section, presents significant challenges.

4.2 LLM-based Paraphrase Detection Issues

Paraphrasing involves rephrasing a sentence while
preserving its original meaning, intent, and gram-
matical correctness in a clear and concise man-
ner. However, automatically assessing whether
generated prompts meet these criteria remains a
non-trivial task. To address this, we explored a
state-of-the-art LLM-based evaluation approach,
following prior work Michail et al. (2025). Our
initial experiments revealed three key issues:

1. Defining a valid paraphrase for an LLM is
challenging, as commonly accepted defini-
tions like “alternative expressions of the same
meaning” (Xu et al., 2015) are too broad for
reliable automated evaluation.

2. LLMs often fail to capture differences in cap-
italization and terminal punctuation (e.g., “a
person is calling someone” vs. “A person
is calling someone.”). Because ReasonSeg
dataset contain prompts that may be either
fragments or complete sentences, we consider

an adversarial prompt to be a valid paraphrase
only if it preserves both capitalization and ter-
minal punctuation.

3. We observe that some paraphrases become
excessively long or abstract, occasionally re-
sembling riddles or puzzles, which LLMs of-
ten still judge as valid (Table 1). Although
such paraphrases may retain partial semantic
overlap with the original, they obscure the in-
tended meaning and hinder clarity, and thus
should not be regarded as valid.

Our findings are consistent with the recent
work Michail et al. (2025), which demonstrated
that even modern LLMs and specialized classifica-
tion models struggle with the paraphrasing classifi-
cation task.

We address the issues mentioned above as fol-
lows:

1. To mitigate Issue 1, we improve the system
prompt used by the LLM. We consider three
different system prompts. Prompt 1 is a
simple zero-shot binary classification prompt,
which performed best in prior work Michail
et al. (2025). Prompts 2 and 3 provide detailed
task instructions, a 5-point scoring scale, and
10 in-context examples. In the latter two set-
tings, we consider an adversarial prompt a


===== PAGE BREAK =====

valid paraphrase only if it receives an LLM
score of 5. The full prompt templates are in-
cluded in Appendix E.1.

2. To resolve Issue 2, we apply a regular expres-
sion—based filtering to discard paraphrases
that alter capitalization or terminal punctu-
ation.

3. We mitigate Issue 3 by filtering out semanti-
cally distant paraphrases. Specifically, we use
Qwen3-Embedding-8B (Zhang et al., 2025), a
state-of-the-art open-source sentence embed-
ding model, to compute semantic similarity.
Through empirical analysis, we identify an
optimal cosine similarity threshold of 0.825
(see Appendix E.3 for details). This step im-
proves detection performance by removing
overly abstract or indirect prompts.

We evaluated LLM-based detection and ablated our
improvements with human studies.

4.3 Ablation

We sampled a dataset of 310 pairs of original and
adversarial prompts, generated by the proposed
SPARTA and baseline methods (see Section 5.3),
and manually annotated them for paraphrase valid-
ity. For LLM-based detection, we evaluated two
state-of-the-art models: LLaMA-3.1-Nemotron-
70B (Wang et al., 2024d) and Qwen3-32B (Team,
2025). For each LLM, system prompt, and filtering
configuration (with or without regular expressions
and cosine similarity), we measured performance
using the Fl-score to identify the most effective
detection setup.

The results of the human study are summarized
in Table 2. The best detection performance was
achieved using Qwen3-32B with system prompt
3 and filtering based on regular expressions and
cosine similarity, yielding an F1 score of 0.749.
Using system prompt 3 without additional filtering
already led to a notable improvement over system
prompt 1 (F1 score: 0.641 — 0.680), and further
gains were achieved with the full filtering setup
(0.680 — 0.749). This best-performing config-
uration was adopted in the automatic evaluation
protocol described earlier.

5 Implementation Details

5.1 Datasets

We use the ReasonSeg dataset (Lai et al., 2024),
which has become a standard benchmark for evalu-

ating reasoning segmentation models. Addition-
ally, we leverage LLM-Seg40K (Wang and Ke,
2024), the latest large-scale reasoning segmenta-
tion dataset collected using ChatGPT-4. With an
average query length of 15.2 words, LLM-Seg40K
presents more challenging scenarios and greater
linguistic complexity. Due to computational con-
straints, we limit our evaluation to 300 samples
from each dataset.

5.2 Reasoning Models

We evaluated 6 checkpoints of 3 modern reason-
ing segmentation models. Our particular interest is
LISA (Lai et al., 2024), the first and most widely
adopted model in this domain. We also tested
LISA’s successors, LISA++ (Yang et al., 2024) and
GSVA (Xia et al., 2024), which are often used as
strong baselines in reasoning and referring segmen-
tation.

5.3 Attack Baselines

We consider the following attack baselines (see
Appendix B for details): (1) GBDA (Guo et al.,
2021): adapted from text-only adversarial attacks
to the multimodal setting through hyperparameter
tuning; (2) Qwen3-32B, simple prompt (Team,
2025): a naive baseline that prompts the model
to paraphrase the input sentence; (3) PAIR (Chao
et al., 2024): an advanced, iterative method that
was repurposed from LLM jailbreaking with a
paraphrasing-specific prompt and Qwen3-32B as
the language model.

To assess overall robustness, we further intro-
duce a unified attack that, for each sample, selects
the most effective paraphrase from all baselines
and our SPARTA method.

6 Experimental Results

We evaluate adversarial attack performance using
the following procedure: (1) for each dataset sam-
ple, we generate an adversarial paraphrase and com-
pute its relative IoU degradation (AIoU, %) and
its LLM-score, following the evaluation protocol
described in Section 4; (2) we construct the attack
success rate curve SRg, where @ denotes the thresh-
old for AToU; (3) we report the area under the SRg
curve (mSR), as well as the success rates at 9 = 5%
In step 2, an adversarial paraphrase is considered
successful for a given threshold 6 if it achieves
AloU > @ and is rated as valid by the evaluation
protocol (i.e., LLM-score = 5).


===== PAGE BREAK =====

Attacked model             GBDA             Qwen3 (simple)         Qwen3 PAIR             SPARTA (ours)

LISA [7B]             3.2 11.0      84 13.5 30.9 25.1 13.0 25.7 22.5      26.6      48.7       42.4
LISA-exp. [7B]      3.1 10.0     75 11.0 25.0 21.0 161 32.5 25.5     24.6     49.5      42.5
LISA [13B]            2.6 4.5      4.5      9.2 241 18.8 11.0 268 21.0      23.2      46.0       38.4
LISA-exp. [13B] 2.9 7.6     6.2     8.7 246 185 114 27.5 21.8     25.0     47.4      40.8
LISA++ [7B]         0.9 2.1     1.7 107 20.9 19.7     8.8 17.4 14.0     16.2     29.1      23.9
GSVA [13B]         2.2 64     46 15.6 283 23.1 160 31.8 29.5     27.9     53.2      44.5

Table 3: Evaluation results of baselines and the proposed SPARTA on state-of-the-art reasoning segmentation
models on the LLMSeg-40k dataset. mSR refers to the area under the curve of the success rate (SR) versus the
IoU-drop threshold, computed for adversarial paraphrases with an LLM score of 5. SR; and SRjo represent the
success rate for IoU drops greater than 5% and 10%, respectively. Higher values indicate stronger attacks. The best
results are in bold, the second best are underlined.

Attacked model             GBDA             Qwen3 (simple)         Qwen3 PAIR             SPARTA (ours)

LISA [7B]             6.8 18.7 12.6 11.2 32.9 249 146 27.6 23.4      25.8      47.4       40.8
LISA-exp. [7B]      2.8 11.2     5.8 12.5 29.0 24.1 146 268 23.2     14.0     32.3      26.9
LISA [13B]           14 5.7     4.9      9.6 21.55 17.0 13.3 25.1 23.5      16.3      42.7      33.3
LISA-exp. [13B] 34 88    6.7    7.6 254 188 11.3 25.0 19.9    17.5    39.9    32.8
LISA++ [7B]         2.55 84     5.4     9.7 22.8 166 21.1 360 31.8     15.4     28.2      23.1
GSVA [13B]         61 15.3 144 13.1 28.2 23.0 15.1 27.8 25.4     22.7     46.2      37.0

Table 4: Evaluation results of baselines and the proposed SPARTA on state-of-the-art reasoning segmentation
models on the ReasonSeg dataset. mSR refers to the area under the curve of the success rate (SR) versus the
IoU-drop threshold, computed for adversarial paraphrases with an LLM score of 5. SRs5 and SRio represent the
success rate for IoU drops greater than 5% and 10%, respectively. Higher values indicate stronger attacks. The best
results are in bold, the second best are underlined.

The resulting SR curves and metrics are pre-
sented in Table 4, Table 3, and Figure 2. The mSR
measures the average success rate of an adversarial
attack over all IoU thresholds 0, reflecting over-
all attack effectiveness. In Tables 4 and 3, higher
values indicate stronger attacks.

Ss
B

So
N

Success rate (SR), %

Table 5 presents the robustness of each model
against adversarial paraphrasing obtained by the
unified attack. Here, lower values indicate greater
robustness.

tT        t                                     t        +                T
0!0       ‘       O11              0.2           0:0       O11              0.2

loU threshold, %                                        loU threshold, %

ers SPARTA (ours) === GBDA == Qwen (simple) === Qwen PAIR

Figure 2: Success rate (SR) as a function of IoU-drop
threshold for adversarial paraphrases with LLM
score 5. Results are shown for the LISA-7B model on
the ReasonSeg dataset (left) and LLMSeg-40k dataset

(right).

7 Discussion

A review of Tables 3—4, Table 5, and Figure 2 leads
to the following conclusions. First, the proposed
SPARTA attack consistently outperforms all

baselines across reasoning segmentation models on
LLMSeg-40k, achieving an average mSR improve-
ment of about 84% over the strongest baselines. On
ReasonSeg SPARTA continues to improve attack
performance (average mSR gain = 29%), except 2
models (LISA-exp. [7B] and LISA++ [7B]), where

PAIR yields higher mSR. Examples of adversarial
paraphrases generated by the proposed SPARTA
method are presented in Figure 3.

Second, the proposed adversarial paraphras-
ing task presents a significant challenge for cur-
rent reasoning segmentation models. Our task intro-


===== PAGE BREAK =====

‘ted SS       __ IoU = 0.94
¥      DE:84-73        =     “

Original                                    Adversarial                                     Original                                    Adversarial                                   Original                                    Adversarial

something indicating the
identity of the car

What is the item that the man
in the red shirt is throwing?

something indicative of the
car's identity

+                    >.

the object used for stirring
milk or coffee

What's the man in the red
shirt throwing?

object for mixing milk or
coffee

ToU = 0.81                             IoU =0

4

=
something that can make the
food more delicious

Adversarial                                                                                     Adversarial

Original

Original

Original

Adversarial

When a plane is ready to land on
the airport runway, what area
in the picture will it eventually
touch down on?

When a plane is ready to land
on the runway of the airport,

Which object can be used to put What object can be used to put
and place things within arm's      and keep things within reach of
reach when sitting in the living the hand when sitting in the
room?                                    living room?

a thing that can make the
food more delicious

in which area in the picture
will it finally land?

Figure 3: Examples of adversarial paraphrases obtained using the proposed SPARTA method. SPARTA
produces grammatically correct paraphrases that preserve the original query meaning while substantially degrading

segmentation performance.

duces strict constraints on grammatical correctness
and semantic equivalence, making it a significantly
more difficult benchmark for evaluating model ro-
bustness in real-world scenarios. Despite these con-
straints, our unified attack achieves success rates
of up to 68% at a 10% relative IoU drop threshold,
indicating that current reasoning segmentation
models remain vulnerable to well-crafted adver-
sarial paraphrases.

Finally, while a deeper analysis is left for fu-
ture work, unified attacks already offer valuable
insights into robustness differences across models
(Table 5). Notably, LISA++ [7B] demonstrates the
highest robustness on LLMSeg-40k, while LISA-
exp. [13B] achieves the highest robustness on Rea-
sonSeg. This indicates that there is currently no
reasoning segmentation model that is optimal
in terms of robustness on both datasets. LISA
[13B] consistently outperforms its 7B variant, sug-
gesting that increased model capacity enhances
resistance to adversarial paraphrasing. In con-
trast, GSVA [13B] shows the weakest robustness
on LLMSeg-40k, which we attribute to its lower
segmentation performance; our evaluation of the
released checkpoint revealed significantly lower
metrics than reported in the prior work.

8 Conclusion

In this work, we introduced a novel challenging
task that involves generating semantically consis-
tent and grammatically correct paraphrases that sig-
nificantly degrade segmentation performance. To

Attacked model           ReasonSeg (test)             LLMSeg-40k (val)
mSR 1   SR5 1   SRio 1   mSR 1   SR5 1   SRio 1

LISA [7B]             43.6      78.5      68.7       38.8      66.0      58.1
LISA-exp. [7B]     33.2    67.0    55.4     36.6    69.5    59.5
LISA [13B]           30.1      66.0      57.5       33.2      60.3.      53.1
LISA-exp. [13B] 29.6    64.6    54.6     32.4    60.7    52.6
LISA++ [7B]         37.1      63.9     56.0      26.3     44.7     40.0
GSVA [13B]         40.3     72.8      63.8      40.6     68.2     62.4

Table 5: Robustness of state-of-the-art reasoning seg-
mentation models to unified attack. mSR refers to
the area under the curve of the success rate (SR) ver-
sus the IoU-drop threshold, computed for adversarial
paraphrases with an LLM score of 5. SR; and SRjo
represent the success rate for IoU drops greater than 5%
and 10%, respectively. Lower values indicate greater
model robustness. The best results are in bold, the sec-
ond best are underlined.

address this task, we proposed SPARTA, which
leverages a black-box, sentence-level optimiza-
tion in the semantic latent space of the pretrained
text autoencoder, guided by reinforcement learn-
ing. Through comprehensive automatic and human-
validated evaluation protocols, we demonstrate
that SPARTA outperforms state-of-the-art base-
lines, achieving up to a 2X improvement on
LLMSeg-40k; on ReasonSeg, it is better for all
but two models. Despite strict semantic and gram-
matical constraints, our findings reveal that current
reasoning segmentation models remain vulnerable
to adversarial paraphrasing. We believe this work
offers a valuable foundation for future research on
evaluating and enhancing the robustness of multi-
modal vision-language systems.


===== PAGE BREAK =====

9 Limitations and Future Work

While the proposed SPARTA method outperforms
state-of-the-art baselines, several limitations re-
main. First, neither SPARTA nor existing attacks
guarantee that generated prompts are valid para-
phrases. To mitigate this, our evaluation proto-
col selects the best valid adversarial prompts after
generation, though future work could explore in-
corporating validity constraints directly into the
generation process.

Second, while SPARTA generates paraphrases
that are semantically and grammatically correct,
some may appear unnatural to human users. This
reflects broader limitations of current text autoen-
coders (see Appendix A), and future improvements
likely depend on developing models with more
structured and human-aligned latent spaces.

Finally, we focus solely on attack methods, with-
out addressing potential defenses. Exploring ro-
bustness strategies for reasoning segmentation mod-
els is a critical next step toward building more reli-
able multimodal systems.

10. Ethical Considerations

Our work introduces a novel adversarial paraphras-
ing method to evaluate the robustness of reasoning
segmentation models. While this method could
potentially be misused to attack real-world models,
we believe the benefits to the research community
outweigh these risks. By uncovering current vulner-
abilities, we aim to encourage the development of
more robust, interpretable, and trustworthy systems.
To support responsible research, we will release all
code and data under a research-only license, strictly
intended for academic and non-commercial use.

References

Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,
Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou,
and Jingren Zhou. 2023. Qwen-vl: A frontier large
vision-language model with versatile abilities. arXiv
preprint arXiv:2308. 12966.

Kevin Black, Noah Brown, Danny Driess, Adnan Es-
mail, Michael Equi, Chelsea Finn, Niccolo Fusai,
Lachy Groom, Karol Hausman, Brian Ichter, and 1
others. 2024. 79: A Vision-Language-Action Flow
Model for General Robot Control. arXiv preprint.

Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen
Chebotar, Xi Chen, Krzysztof Choromanski, Tianli
Ding, Danny Driess, Avinava Dubey, Chelsea Finn,
and | others. 2023. Rt-2: Vision-language-action

models transfer web knowledge to robotic control.
arXiv preprint arXiv:2307.15818.

Anthony Brohan, Noah Brown, Justice Carbajal, Yev-
gen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana
Gopalakrishnan, Karol Hausman, Alex Herzog, Jas-
mine Hsu, and 1 others. 2022. Rt-1: Robotics trans-
former for real-world control at scale. arXiv preprint
arXiv:2212.06817.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, and 1 others. 2020. Language models are
few-shot learners. Advances in neural information
processing systems, 33:1877-1901.

Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu,
Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan
Liu. 2023. Chateval: Towards better Ilm-based
evaluators through multi-agent debate. Preprint,
arXiv:2308.07201.

Patrick Chao, Alexander Robey, Edgar Dobriban,
Hamed Hassani, George J. Pappas, and Eric Wong.
2024. Jailbreaking black box large language models
in twenty queries. Preprint, arXiv:2310.08419.

Cheng-Han Chiang and Hung-yi Lee. 2023. Can large
language models be an alternative to human evalua-
tions? In Proceedings of the 61st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 15607-15631. Associa-
tion for Computational Linguistics.

Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu.
2019. Evading defenses to transferable adversarial
examples by translation-invariant attacks. In Pro-
ceedings of the IEEE/CVF conference on computer
vision and pattern recognition, pages 4312-4321.

Alexey Dosovitskiy, Lucas Beyer, Alexander
Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias
Minderer, Georg Heigold, Sylvain Gelly, Jakob
Uszkoreit, and Neil Houlsby. 2021. An image
is worth 16x16 words: Transformers for image
recognition at scale. In International Conference on
Learning Representations.

Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,
Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,
Jonathan Tompson, Quan Vuong, Tianhe Yu, and
1 others. 2023. Palm-e: An embodied multimodal
language model. arXiv preprint arXiv:2303.03378.

Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,
Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,
Akhil Mathur, Alan Schelten, Amy Yang, Angela
Fan, and 1 others. 2024. The llama 3 herd of models.
arXiv preprint arXiv:2407.21783.

Paul-Ambroise Duquenne, Holger Schwenk, and Benoit
Sagot. 2023. Sonar: sentence-level multimodal and
language-agnostic representations. arXiv e-prints,
pages arXiv—2308.


===== PAGE BREAK =====

Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing
Dou. 2018. HotFlip: White-box adversarial exam-
ples for text classification. In Proceedings of the 56th
Annual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), pages 3F 1-36.
Association for Computational Linguistics.

Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei
Liu. 2024. GPTScore: Evaluate as you desire. In
Proceedings of the 2024 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(Volume I: Long Papers), pages 6556-6576. Associ-
ation for Computational Linguistics.

Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun
Qi. 2018. Black-box generation of adversarial text se-
quences to evade deep learning classifiers. Preprint,
arXiv:1801.04354.

Chuan Guo, Alexandre Sablayrolles, Hervé Jégou, and
Douwe Kiela. 2021. Gradient-based adversarial at-
tacks against text transformers. In Proceedings of the
2021 Conference on Empirical Methods in Natural
Language Processing, pages 5747-5757. Association
for Computational Linguistics.

Daya Guo, Dejian Yang, Haowei Zhang, Junxiao
Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shi-
rong Ma, Peiyi Wang, Xiao Bi, and | others. 2025.
Deepseek-r1: Incentivizing reasoning capability in
Ilms via reinforcement learning. arXiv preprint
arXiv:2501. 12948.

David Herel, Hugo Cisneros, and Tomas Mikolov. 2023.
Preserving semantics in textual adversarial attacks.
In 26th European Conference on Artificial Intelli-
gence (ECAI 2023), volume 372 of Frontiers in Artifi-
cial Intelligence and Applications, pages 1036-1043.
IOS Press.

Nai-Chieh Huang, Ping-Chun Hsieh, Kuo-Hao Ho, and
I-Chen Wu. 2024a. Ppo-clip attains global optimality:
Towards deeper understandings of clipping. Preprint,
arXiv:2312.12065.

Shize Huang, Qianhui Fan, Zhaoxin Zhang, Xiaowen
Liu, Guanqun Song, and Jinzhe Qin. 2024b. Segment
shards: Cross-prompt adversarial attacks against the
segment anything model. Applied Sciences.

Jyh-Jing Hwang, Runsheng Xu, Hubert Lin, Wei-Chih
Hung, Jingwei Ji, Kristy Choi, Di Huang, Tong
He, Paul Covington, Benjamin Sapp, and 1 others.
2024. Emma: End-to-end multimodal model for au-
tonomous driving. arXiv preprint arXiv:2410.23262.

Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke
Zettlemoyer. 2018. Adversarial example generation
with syntactically controlled paraphrase networks. In
Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume I (Long Papers), pages 1875-1885, New Or-
leans, Louisiana. Association for Computational Lin-
guistics.

10

Eric Jang, Shixiang Gu, and Ben Poole. 2017. Cate-
gorical reparameterization with gumbel-softmax. In
International Conference on Learning Representa-
tions.

Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter
Szolovits. 2020. Is bert really robust? a strong base-
line for natural language attack on text classification
and entailment. Preprint, arXiv:1907.11932.

Erik Jones, Anca Dragan, Aditi Raghunathan, and Ja-
cob Steinhardt. 2023. Automatically auditing large
language models via discrete optimization. In Jn-
ternational Conference on Machine Learning, pages

15307-15329.

Aly M. Kassem and Sherif Saad. 2024. Finding a needle
in the adversarial haystack: A targeted paraphrasing
approach for uncovering edge cases with minimal
distribution distortion. Preprint, arXiv:2401.11373.

Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi
Mao, Chloe Rolland, Laura Gustafson, Tete Xiao,
Spencer Whitehead, Alexander C. Berg, Wan-Yen
Lo, Piotr Dollar, and Ross Girshick. 2023. Segment
anything. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision (ICCV), pages
4015-4026.

Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui
Yuan, Shu Liu, and Jiaya Jia. 2024. Lisa: Reason-
ing segmentation via large language model. In Pro-
ceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 9579-9589.

Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
2023. Blip-2: Bootstrapping language-image pre-
training with frozen image encoders and large lan-
guage models. In International conference on ma-
chine learning, pages 19730-19742. PMLR.

Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue,
and Xipeng Qiu. 2020. Bert-attack: Adversarial at-
tack against bert using bert. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 6193-6202.
Association for Computational Linguistics.

Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae
Lee. 2024a. Improved baselines with visual instruc-
tion tuning. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition,

pages 26296-26306.

Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae
Lee. 2023. Visual instruction tuning. Advances in
neural information processing systems, 36:34892—

34916.

Xiaoliang Liu, Furao Shen, and Jian Zhao. 2025.
Region-guided attack on the segment anything model
(sam). Preprint, arXiv:2411.02974.

Yong Liu, Cairong Zhang, Yitong Wang, Jiahao Wang,
Yujiu Yang, and Yansong Tang. 2024b. Universal
segmentation at arbitrary granularity with language


===== PAGE BREAK =====

instruction. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition,

pages 3459-3469.

Zhuang Liu, Hanzi Mao, Chao- Yuan Wu, Christoph Fe-
ichtenhofer, Trevor Darrell, and Saining Xie. 2022.
A convnet for the 2020s. In Proceedings of the
IEEE/CVF conference on computer vision and pat-
tern recognition, pages 11976-11986.

Andrianos Michail, Simon Clematide, and Juri Opitz.
2025. PARAPHRASUS: A comprehensive bench-
mark for evaluating paraphrase detection models. In
Proceedings of the 31st International Conference on
Computational Linguistics, pages 8749-8762, Abu
Dhabi, UAE. Association for Computational Linguis-
tics.

Norman Mu, Jingwei Ji, Zhenpei Yang, Nate Harada,
Haotian Tang, Kan Chen, Charles R. Qi, Runzhou
Ge, Kratarth Goel, Zoey Yang, Scott Ettinger, Rami
Al-Rfou, Dragomir Anguelov, and Yin Zhou. 2024.
Most: Multi-modality scene tokenization for motion
prediction. Preprint, arXiv:2404.19531.

Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao,
Shaohan Huang, Shuming Ma, and Furu Wei.
2023. Kosmos-2: Grounding multimodal large
language models to the world. arXiv preprint
arXiv:2306. 14824.

Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Ab-
delrahman Shaker, Salman Khan, Hisham Cholakkal,
Rao M Anwer, Eric Xing, Ming-Hsuan Yang, and
Fahad S Khan. 2024. Glamm: Pixel grounding large
multimodal model. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recog-
nition, pages 13009-13018.

Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Rong-
hang Hu, Chaitanya Ryali, Tengyu Ma, Haitham
Khedr, Roman Riadle, Chloe Rolland, Laura
Gustafson, and 1 others. 2024. Sam 2: Segment
anything in images and videos. arXiv preprint
arXiv:2408.00714.

Shuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che.
2019. Generating natural language adversarial exam-
ples through probability weighted word saliency. In
Proceedings of the 57th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1085—-
1097, Florence, Italy. Association for Computational
Linguistics.

John Schulman, Sergey Levine, Pieter Abbeel, Michael
Jordan, and Philipp Moritz. 2015. Trust region policy
optimization. arXiv preprint arXiv: 1502.05477.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
Radford, and Oleg Klimov. 2017. Proximal policy
optimization algorithms. CoRR.

Ari Seff, Brian Cera, Dian Chen, Mason Ng, Aurick
Zhou, Nigamaa Nayakanti, Khaled S. Refaat, Rami
Al-Rfou, and Benjamin Sapp. 2023. Motionlm:
Multi-agent motion forecasting as language model-
ing. Preprint, arXiv:2309.16534.

11

Kun Sun and Rong Wang. 2024. Textual similarity as a
key metric in machine translation quality estimation.
Preprint, arXiv:2406.07440.

Richard S. Sutton and Andrew G. Barto. 2018. Rein-
forcement Learning: An Introduction, 2nd edition.
MIT Press.

Qwen Team. 2025. Qwen3 technical report. Preprint,
arXiv:2505.09388.

Thanh Thieu, Ha Do, Thanh Duong, Shi Pu, Sathya-
narayanan Aakur, and Saad Khan. 2021. Lexdivpara:
A measure of paraphrase quality with integrated sen-
tential lexical complexity. In Proceedings of the Intel-
ligent Systems Conference (IntelliSys 2021), volume
296 of Lecture Notes in Networks and Systems, pages
1-10.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, and | others. 2023. Llama 2: Open foun-
dation and fine-tuned chat models. arXiv preprint
arXiv:2307.09288.

Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui
Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng
Qu, and Jie Zhou. 2023. Is ChatGPT a good NLG
evaluator? a preliminary study. In Proceedings of the
4th New Frontiers in Summarization Workshop, pages
1-11. Association for Computational Linguistics.

Junchi Wang and Lei Ke. 2024. Llm-seg: Bridging
image segmentation and large language model rea-
soning. Preprint, arXiv:2404.08767.

Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu,
Binghuai Lin, Yunbo Cao, Lingpeng Kong, Qi Liu,
Tianyu Liu, and Zhifang Sui. 2024a. Large language
models are not fair evaluators. In Proceedings of the
62nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
9440-9450. Association for Computational Linguis-
tics.

Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhi-
hao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin
Wang, Wenbin Ge, and 1 others. 2024b. Qwen2-
vl: Enhancing vision-language model’s perception
of the world at any resolution. arXiv preprint
arXiv:2409. 12191.

Xudong Wang, Shufan Li, Konstantinos Kallidromitis,
Yusuke Kato, Kazuki Kozuka, and Trevor Darrell.
2024c. Hierarchical open-vocabulary universal im-
age segmentation. Advances in Neural Information
Processing Systems, 36.

Zhilin Wang, Alexander Bukharin, Olivier Delal-
leau, Daniel Egert, Gerald Shen, Jiaqi Zeng, Olek-
sii Kuchaiev, and Yi Dong. 2024d. Helpsteer2-
preference: Complementing ratings with preferences.
Preprint, arXiv:2410.01257.


===== PAGE BREAK =====

Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xin-
lei Chen, Zhuang Liu, In So Kweon, and Saining
Xie. 2023. Convnext v2: Co-designing and scaling
convnets with masked autoencoders. In Proceedings
of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 16133-16142.

Junfeng Wu, Yi Jiang, Qihao Liu, Zehuan Yuan, Xiang
Bai, and Song Bai. 2024a. General object foundation
model for images and videos at scale. In Proceedings
of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 3783-3795.

Tsung-Han Wu, Giscard Biamby, David Chan, Lisa
Dunlap, Ritwik Gupta, Xudong Wang, Joseph E Gon-
zalez, and Trevor Darrell. 2024b. See say and seg-
ment: Teaching Imms to overcome false premises. In
Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, pages 13459-
13469.

Zhuofan Xia, Dongchen Han, Yizeng Han, Xuran Pan,
Shiji Song, and Gao Huang. 2024. Gsva: Gener-
alized segmentation via multimodal large language
models. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition,

pages 3858-3869.

Wei Xu, Chris Callison-Burch, and Bill Dolan. 2015.
SemEval-2015 task 1: Paraphrase and semantic sim-
ilarity in Twitter (PIT). In Proceedings of the 9th
International Workshop on Semantic Evaluation (Se-
mEval 2015), pages 1-11, Denver, Colorado. Associ-
ation for Computational Linguistics.

Xilie Xu, Keyi Kong, Ning Liu, Lizhen Cui, Di Wang,
Jingfeng Zhang, and Mohan Kankanhalli. 2023. An
llm can fool itself: A prompt-based adversarial attack.
Preprint, arXiv:2310.13345.

Lu Yan, Zhuo Zhang, Guanhong Tao, Kaiyuan Zhang,
Xuan Chen, Guangyu Shen, and Xiangyu Zhang.
2023. Parafuzz: An interpretability-driven technique
for detecting poisoned samples in nlp. Preprint,
arXiv:2308.02122.

Senqiao Yang, Tianyuan Qu, Xin Lai, Zhuotao Tian, Bo-
hao Peng, Shu Liu, and Jiaya Jia. 2024. Lisat+: An
improved baseline for reasoning segmentation with
large language model. Preprint, arXiv:2312.17240.

Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang,
Huan Lin, Baosong Yang, Pengjun Xie, An Yang,
Dayiheng Liu, Junyang Lin, Fei Huang, and Jingren
Zhou. 2025. Qwen3 embedding: Advancing text
embedding and reranking through foundation models.
arXiv preprint arXiv:2506.05176.

Xueyan Zou, Zi- Yi Dou, Jianwei Yang, Zhe Gan, Linjie
Li, Chunyuan Li, Xiyang Dai, Harkirat Behl, Jian-
feng Wang, Lu Yuan, and | others. 2023. General-
ized decoding for pixel, image, and language. In Pro-
ceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 15116-15127.

12


===== PAGE BREAK =====

A Autoencoder Analysis

A.1 Overview

In this work, we employed SONAR, a state-of-the-
art pre-trained autoencoder model, to generate se-
mantically equivalent paraphrases (Duquenne et al.,
2023). SONAR constructs a unified fixed-size sen-
tence space by training an encoder-decoder pair
(E, D) with a vector bottleneck z € R¢. The text
backbone is initialized from the NLLB-1B dense
machine translation model (Team et al., 2022),
which consists of a 24-layer Transformer encoder
and a 24-layer Transformer decoder. To ensure
that similar sentences are positioned closer in the
sentence embedding space, SONAR utilizes the
following objective function:

L=Lur + olmse + BLAE/DAE

which integrates translation objective Cyr, auto-
encoding and denoising objectives Laz pag, along
with a cross-lingual similarity objective in the sen-
tence embedding space Lysg. For text decoding
in SONAR, we employ the default beam search
strategy with a beam size of 5.

A.1.1 Embedding Component Analysis

°°?
ar €f00®
Do

2 © cod,

r= —0.956
R? = 0.913

L
°
1

N
°
1

Tokenized text length
WwW
lo}

an
°
1

fo}

0
Embedding value, dim = 654

Figure 4: Scatter plot of SONAR embedding dim
654 versus tokenized text length. A strong negative
correlation (r = —0.956, R? = 0.913) shows that this
dimension encodes sequence length, with shorter sen-
tences having higher embedding values. The red line
indicates a linear fit.

We analyzed the SONAR embedding space,
which features an embedding size of 768, using the
ReasonSeg test split, comprising 790 text samples.
For each sentence, we computed the embedding
and the tokenized sentence length, then normalized
embeddings to remove scale effects.

We computed the Pearson correlation between
each embedding dimension and tokenized text

13

length. One dimension (dim 654) showed a par-
ticularly strong negative correlation (r = —0.956,
R? = 0.913). To ensure this relationship was not
a random artifact, we compared it to a random-
dimension baseline: across 100 randomly selected
embedding dimensions, the mean absolute correla-
tion with text length was |r| = 0.20 + 0.14.

As illustrated in Figure 4, the relationship be-
tween text length and this embedding dimension
is nearly linear: shorter sentences correspond to
higher values of this coordinate, while longer sen-
tences correspond to lower values. This suggests
that the SONAR autoencoder encodes sequence-
length information in a disentangled coordinate.
While this feature can help to decode text more
accurately, it may act as a confounding factor in
semantic similarity tasks, where texts of different
lengths might appear less similar despite being se-
mantically close.

A.1.2 Reconstruction Quality

Method BLEU-4 Rouge-L BETRScore BLEURT
DeCap            0.02             0.20                0.11                -0.75
GVAE             0.22             0.19                0.16                -0.92
SONAR          0.72               0.88                  0.90                  0.70

Table 6: Restoration qualities of DeCap, G-VAE and
SONAR on the ReasonSeg dataset. SONAR substan-
tially outperforms both baselines across all metrics, con-
firming its reliable decoder. It is therefore used as the
autoencoder backbone in the proposed SPARTA attack.

To evaluate the text reconstruction capability of
different autoencoders, we used the test split of
the ReasonSeg dataset. We compared SONAR
with two representative baselines: DeCap (Li et al.,
2023), a decoder designed for CLIP embeddings,
and GVAE (Zhang et al., 2024), a graph-based vari-
ational autoencoder. None of these models were
trained or fine-tuned on ReasonSeg to ensure fair
zero-shot comparison.

For each text sample, we obtained its latent rep-
resentation using the corresponding encoder and
reconstructed it via the paired decoder. Reconstruc-
tion quality was assessed using standard text simi-
larity metrics: BLEU-4, ROUGE-L, BERTScore,
and BLEURT.

As shown in Table 6, SONAR substantially out-
performs both DeCap and GVAE across all met-
rics, achieving high lexical and semantic fidelity
to the original text. This confirms that SONAR
autoencoding framework provides a semantically


===== PAGE BREAK =====

(a) Sentences < 20 Words                            (b) Sentences < 25 Words

({c) Sentences = 30 Words

({d) Sentences =< 35 Words

20                                                  40                 .
4                {                                                               ¥                                                                       40
ts           15                          .          a           30                      ~~ im
¢   -                                      *   a
10             s           BY                        20            <*                    4
2   >                                                                                                            .          othe,                   20
?                                               we
n                         <_                           5             7     +      s *                           ne
v                                                                                        wl *®
c                                                                                                                                .
s                       A                            rt ew                       >     .      .        «
a 0                                                   0                                            2               ‘         a?                 «<         6
2                                                                    e%*          *~         0          ‘      tase,           4
£                                                            *                                                         “      we,       .           .
F                     Y  >                          “5          er            :          &                vot tt      “
w 2                                                       ‘            ~                               -10           ete gy ta whee
Zz                                                                                                                       .       .      vo                 20
a                                                    -10                                                            .        aa               .            7
é                                                                      *                                -20                       ’
a                                                                        nee
-4              «                                  -15                                                                              ee
=     t                   -30                    .    -      ot         -40
“6                          !                       ~20               .                                                                       ™                                       ‘
-6      4      -2      0       2       4              -20       -10        0         lo        20       -40   -30   -20   -10    0     10    20    30           -40       -20        0         20        40
t-SNE Dimension 1
(a) Sentences =< 20 Words                            (b) Sentences = 25 Words                            ({c) Sentences = 30 Words                            (d) Sentences = 35 Words
—                                          *
6                                                                                                     40
20
eo
‘                                  wy                           y                        v
+     20
lo
x            re                                                              -                     .
& ?                                            ]         r                on       .           »                                                                                  Z
e                       ra                            0                       e     a                    0
E 0                                                             ‘               aor,                                                                                              =
2                                           Se                  aw «CT  4                                                                                                           TS,
4                                                 -20              nw                                 40
as                                                                                                                »
6                          aN                                                 w
20

-40 -30 -20 -10 0

t-SNE Dimension 1

Figure 5: t-SNE projections of sentence embeddings from two encoders. Upper: CLIP encoder; bottom:
SONAR encoder. Each grid contains four panels for sentences of length < {20, 25,30, 35} words. Colours
designate paraphrase groups: sentences sharing the same hue are semantically equivalent variants of one another.
See Figure 6 for quantitative cluster quality. Since DeCap and GVAE exhibit extremely low restoration quality
(Table 6), their embedding spaces are omitted from visualization.

meaningful latent space with a high-quality de-
coder. Consequently, in this work SONAR is em-
ployed as the autoencoder backbone in the pro-
posed SPARTA attack, where reliable reconstruc-
tion from perturbed embeddings is essential.

A.1.3. Latent-Space Geometry Study

We utilized the test split of the ReasonSeg dataset,
consisting of 790 text samples, each with up to 5
semantically equivalent paraphrases. For each para-
phrase group, we computed SONAR embeddings
and sentence lengths (in words). A 2D t-SNE pro-
jection of these embeddings was constructed, as de-
picted in Figure 5. The figure includes four panels,
each representing sentences with a maximum of
< {20, 25, 30, 35} words. Different colors denote
paraphrase groups, with semantically equivalent
sentences sharing hues and connected by lines.
For comparison, we focus exclusively on the
CLIP text encoder. As demonstrated in Sec-
tion A.1.2, existing autoencoders such as DeCap
and GVAE exhibit poor text reconstruction qual-
ity, making them unsuitable for analyzing latent-
space organization. In contrast, the CLIP en-
coder—trained with a contrastive learning objec-
tive—is known to produce a well-structured and se-
mantically coherent embedding space. The purpose

14

—_—_—_  steeSSSSSSSSF—7r
_ es

105 texts
21 groups

30 texts                          290 texts

7 | 6 groups                         58 groups

NNR T

wee

480 texts |e
96 groups          650 texts

| —=@= CLIP encoder                                               130 groups)

==@= SONAR encoder

15               20               25

Maximum word count

Lj pet

—=@= CLIP encoder
==@= SONAR encoder

mM

15               20                               30

30                             35

0.40 4

CSR L

0.35 4

0.30 4

25                               35

Maximum word count

Figure 6: Latent-space metrics for SONAR and
CLIP encoders vs. sentence length. (Upper) Nearest-
Neighbour Recall (NNR): Higher values denote bet-
ter local semantic preservation. (Bottom) Cluster-
Separation Ratio (CSR): Lower values indicate better
cluster separation, indicating improved global latent-
space organization.

of this analysis is therefore to examine whether
SONAR preserves a similarly organized latent ge-
ometry while maintaining its ability to reconstruct
text.


===== PAGE BREAK =====

As illustrated in Figure 5, SONAR cluster separa-
bility is comparable to that of CLIP, though it grad-
ually degrades for longer sentences. This observa-
tion is supported both visually and quantitatively
by metrics in Figure 6, which include Nearest-
Neighbour Recall (NNR) for local neighborhood
fidelity and Cluster-Separation Ratio (CSR) for
global latent structure (detailed below).

As noted previously, decoders trained for CLIP,
such as DeCap (Li et al., 2023), show substan-
tially weaker text reconstruction performance (Sec-
tion A.1.2). Therefore, SONAR provides a bal-
anced solution—offering both a semantically mean-
ingful latent space and reasonable text reconstruc-
tion capabilities. Although SONAR limitations
may constrain the effectiveness of the proposed
SPARTA attack, these results highlight promising
directions for future work on designing autoen-
coders that jointly optimize latent structure and
reconstruction fidelity.

Nearest-Neighbour Recall (NNR). For each sen-
tence 7, we normalize embeddings and compute
Euclidean distances to all other samples. Let S; de-
note the set of paraphrases sharing the same label.
Sorting distances yields a neighbour list 7;, and we

define

i.e., the fraction of true paraphrases retrieved
among the |S;| nearest neighbours. Higher values
indicate better local semantic fidelity.

Cluster-Separation Ratio (CSR). For each label
£, we compute the centroid fy and measure the
mean intra-cluster distance

=        1
dintra = yoy [Sel S- S- Iz: _ Mello

L iESe

and mean inter-cluster distance

dinter  =

2
L(L—1) S- Ile — Merl,
l<ll

with L the number of different labels. The ratio

reflects global cluster geometry, where lower val-
ues indicate tighter, better-separated clusters.

B_ Attack Baselines
B.1 GBDA baseline

Preliminary As a white-box baseline, we con-
sider the Gradient-based Distributional Attack
(GBDA) (Guo et al., 2021), which is schematically
illustrated in Figure 7. Let the model’s embedding
matrix be defined as EF = [e; --- ey] € R?*Y,
where V is the size of the model’s vocabulary and
D is the embedding dimension. Given an input to-
ken sequence t = (t; --- t;)', the corresponding
input embedding matrix is Ey = [ez, --- e¢,] €
R?*!. GBDA modifies the model’s input by ap-
proximating FE, with Ep = E Px, where Px is
a matrix of soft token distributions obtained by
applying the Gumbel-Softmax (column-wise) to
a parameter matrix X = [x; --- x] € RY*!.
The matrix X is optimized via gradient descent,
and the Gumbel-Softmax provides a differentiable
approximation of the token selection process, en-
abling smooth gradient-based updates. Once opti-
mized, adversarial prompts can be sampled from
the learned distribution encoded in X.
GBDA loss consists of three components:

L= Ladv + Lim + Lyerp

where similarity loss £,;,, and fluency constraint
Lyerp follow prior work Guo et al. (2021). Our
segmentation-specific adversarial loss Lagy in-
cludes DICE and binary cross-entropy (BCE)
losses as in Lai et al. (2024); Yang et al. (2024).

GBDA’s main limitation is that it only replaces
tokens and cannot be trivially extended to token
insertions and deletions. This limitation may affect
the naturalness of the adversarial paraphrases.

Figure 7: Overview of the GBDA baseline.

Hyperparameter Search The GBDA baseline
was originally developed for text-only attacks,
where the adversarial loss is typically defined to
induce a change in the classification label of a sen-
tence. However, in our work, the objective is to de-
grade the performance of reasoning segmentation


===== PAGE BREAK =====

models, which necessitates adapting the adversarial
loss to be segmentation-specific.

To ensure a fair comparison, we conducted a
comprehensive hyperparameter search to identify
optimal settings for the modified GBDA method.
This search was performed on the validation split
of the ReasonSeg dataset (200 samples). All exper-
iments utilized the LISA-v0O [13B] model, as the vO
versions of LISA were trained without access to the
ReasonSeg validation set, ensuring fair evaluation.

We maintained the original fluency constraint
(via causal language modeling) and BERTScore
similarity constraint, but modified the adversarial
loss to use a linear combination of DICE and BCE
losses with weights (0.5, 2) as in the original pa-
per Lai et al. (2024).

The grid search space, which remains close to
the original GBDA configuration, included:

* Perplexity weight Aperp € {0, 0.5, 1}
° Similarity weight \sim € {0, 100, 150, 200}
¢ Learning rate a € {0.05, 0.1, 0.2}

When varying the learning rate, we adjusted the
number of iterations niters € {400, 200, 100} to
keep their product roughly the same. The optimal
configuration in terms of the mSR metric was found
to be Aperp = 1, Asim = 150, and a learning rate
of a = 0.05.

The hyperparameter search for GBDA required
approximately 31 hours on 24 NVIDIA Tesla A100
GPUs.

B.2. LLM-based baselines
B.2.1 Basic

For the most basic black-box, LLM-based attack
baseline, we employ paraphrases generated by
Qwen3-32B (Team, 2025). With the simple prompt,
we directly ask the model to paraphrase the original
instruction (Figure 9). One paraphrase is generated
for each input sample.

B.2.2. PAIR

As an advanced LLM-based attack baseline, we
leverage the state-of-the-art Prompt Automatic It-
erative Refinement (PAIR) approach (Chao et al.,
2024). PAIR automates jailbreak discovery through
a conversational loop between an attacker LLM
and a target LLM. The attacker generates a prompt,
which is sent to the target model. A separate judge
function then scores the target’s response to de-
termine whether the attack was successful. If the

16

System Prompt: PAIR

You are a helpful red-teaming assistant specializing in testing the robustness of
Vision-Language Models (VLMs). Your task is to generate adversarial text queries for a
reasoning segmentation model. The model is designed to output a segmentation mask based on
the query text and an image.

SETTING:

You will be given an original text query that correctly identifies an object in an image, resulting

in a high Intersection over Union (loU) score. Your goal is to iteratively rephrase this query. The
rephrased query MUST be grammatically correct and semantically equivalent to the original,

but it should cause the VLM to fail, resulting in a very low loU score.

OBJECTIVE:

Your primary objective is to find a paraphrase that minimizes the loU score. A score of 1.0
means your paraphrase failed (the VLM understood it perfectly). A score of 0.0 means your
paraphrase was a perfect success (the VLM was completely confused).

STRATEGY:

To create effective adversarial paraphrases, you should remember:

1. Your paraphrases should be grammatically correct.

2. Your paraphrases should be semantically equivalent to the original query.

3. Do not add extra information that is not mentioned in the original query.

4. Your paraphrase must preserve the original query's capitalization pattern (e.g., initial
uppercase vs. lowercase) and its terminal punctuation (e.g., period, question mark).

INPUT FROM USER:
In each turn, you will receive the original query, the previous adversarial query you generated
and the resulting loU score of the adversarial query from the VLM.

FORMAT:
You MUST format your response as only ‘adversarial_query’.
- The 'adversarial_query' value will contain the new, refined text query.

USER INPUT:
“original_query": "{orig_query}',
"previous_query": "{previous_query}',

"iou_score": {iou_score}

You MUST format your response only as 'adversarial_query’.
Do not add anything before and after it.

YOUR RESPONSE: "adversarial_query":

Figure 8: System Prompt employed in the PAIR attack
using Qwen3-32B as the language model.

System Prompt: Qwen (simple)

You need to make a paraphrase of the original query. It should not be much longer than original one.
Also, it should be natural.

Here is the original query: {orig}
Please provide a paraphrased query that meets the above conditions. Return only modified query
and don't add anything before and after it.

Figure 9: System Prompt employed in Qwen3-32B for
the simple prompt attack.

attack fails, the attacker receives feedback, includ-
ing its own prompt, the target’s refusal, and the
evaluation score, allowing it to iteratively refine its
strategy.

We adapt PAIR to the reasoning segmentation
task as follows: the target is a segmentation model,
the judge computes the Intersection over Union
(IoU) between the predicted and ground truth
masks, and the attacker is Qwen3-32B (Team,
2025). We also modify the prompt to align with
our task (Figure 8). To match the number of attack
iterations with those of SPARTA, we perform 10
refinement iterations per sample to ensure conver-
gence.

C_ Evaluation Setup

We benchmarked 3 reasoning segmentation mod-
els with 6 different checkpoints. To accomplish
this, we spent 1728 GPU hours, which is equiva-
lent to approximately 3 days of compute using 24


===== PAGE BREAK =====

Policy LR (a,.)       5x 1074
Value LR (av)        1x 107+
Log-scale LR (az) 1x 107°
Clip ratio €                    0.2
Adv. weight Aadav            2
Sim. weight Asim      5 x 104
PPO epochs T'               100
Iteration number N         100
Sample size n                32

Table 7: Key hyperparameters of our proposed
SPARTA method.

NVIDIA Tesla A100 GPUs.

All hyperparameters were held constant through-
out our experiments to ensure fair comparison and
reproducibility. The key hyperparameters used for
SPARTA are summarized in Table 7.

D_ Extended Results

Figure 13 complements the main paper by pre-
senting performance curves for the four ad-
ditional checkpoints not shown in Figure 2:
LISA-explanatory [7B], LISA-explanatory [13B],
LISA++ [7B], and GSVA [13B]. Across all check-
points, the observed trends are consistent with
those reported in the main text: with the excep-
tion of a single case, SPARTA consistently out-
performs all baselines, generating adversarial para-
phrases that effectively degrade segmentation per-
formance.

(0)  Simple Prompt

Are the following sentences grammatically correct paraphrases?
Sentence 1: forig}
Sentence 2: {adv}

Answer only with 1 (means yes) or 0 (means no).
Output ONLY a single integer. No words, no spaces, no punctuation.

Figure 10: Prompt 1 (Simple Prompt) used for evalu-
ating paraphrase detection methods.

E LLM-based paraphrase detection

E.1 System Prompts

To ensure high performance in the paraphrase detec-
tion step, we designed and evaluated three distinct
prompt formulations:

¢ Simple Prompt: Prompt | is a concise, zero-
shot instruction for binary paraphrase detec-
tion. It is adapted from the best-performing
prompt in Michail et al. (2025), with the full
text provided in Figure 10.

17

@® Detailed Prompt

You are an impartial grader. Rate the paraphrase on a 1-to-5 scale for
1. semantic equivalence to the original referring-segmentation query and
2. — grammatical correctness.

Output FORMAT
e Output ONLY a single integer 1-5. No words, no spaces, no punctuation.
e — If nothing but symbols/punctuation appears in the <text>...</text> for the paraphrase, output 1.

SCORING GUIDELINES

5 - Meaning fully preserved; no key detail added, removed, or flipped. Grammar natural; no errors that a
native speaker would notice.

4- Meaning fully preserved; only minor grammatical hiccups (e.g., article omission, verb-number slip)
that do not hinder understanding.

3 - Core meaning mostly preserved but some detail weakened/ambiguous OR multiple grammar issues
causing awkwardness.

2 - Meaning noticeably altered, missing, or inverted OR grammar so poor that understanding is difficult.
1- Meaning essentially lost, nonsensical, or mostly symbols/punctuation.

ADDITIONAL NOTES
1. Synonyms, reordering, or pronoun substitution are fine if they keep the meaning.
2. The task is referring-segmentation, so spatial words (left/right, top/bottom,
foreground/background, etc.) and object identity must remain accurate.

EXAMPLES

Example 1:

Original: <text>select the woman on the right in the image</text>
Paraphrase: <text>...</text>

Your mark: 1

Example 2:

Original: <text>select the man on the left in the image</text>
Paraphrase: <text>select the woman on the right in the image</text>
Your mark: 1

<>

Example 10:

Original: <text>select the left car in the image</text>

Paraphrase: <text>select the vehicle shown on the left in the image</text>
Your mark: 5

YOUR TURN

Original: <text>{orig}</text>
Paraphrase: <text>{adv}</text>
Your mark:

Ne

© Balanced Prompt

You are an Al assistant who will help me to evaluate the quality of the following paraphrase on a
scale from 1 to 5 based on its accuracy in preserving the original meaning for the task of referring
segmentation. The paraphrase should be grammatically correct. To mark a response, you should
output a single integer between 1 and 5 (including 1, 5).

All symbols of paraphrase and original text are written inside the <text></text> tag. If there is no
text in the paraphrase and only symbols and punctuation marks, then rate it at 1.

Output only the integer score from 1 to 5, no explanations.

5 means the paraphrase fully retains the original sentence's meaning without losing important
information or altering any key details and does not violate any grammar rules.

1 means the paraphrase significantly distorts the meaning by inverting important parts of the
original sentence or is grammatically incorrect.

Please note in your assessment that the reformulations must be meaningful, i.e. must not contain
inappropriate symbols and punctuation marks.

Example 1:

Original: <text>select the woman on the right in the image</text>
Paraphrase: <text>...</text>

Your mark: 1

Example 2:

Original: <text>select the man on the left in the image</text>
Paraphrase: <text>select the woman on the right in the image</text>
Your mark: 1

<>

Example 10:

Original: <text>select the left car in the image</text>

Paraphrase: <text>select the vehicle shown on the left in the image</text>
Your mark: 5

Your Turn:
Original: <text>{orig}</text>
Paraphrase: <text>{adv}</text>

S]

Figure 11: Prompt 2 (Detailed Prompt) used for eval-
uating paraphrase detection methods.

7S

Figure 12: Prompt 3 (Balanced Prompt) used for eval-
uating paraphrase detection methods.

¢ Detailed Prompt: Prompt 2 is a few-shot
prompt with 10 in-context examples and a
comprehensive 5-point scoring rubric that pro-
vides explicit definitions for each score (1-5),
covering both semantic equivalence and gram-
mar. This prompt is shown in Figure 11.


===== PAGE BREAK =====

LISA-explanatory [7B]

LISA [13B]

'SRs | SRio
04           :         H

Success rate (SR), %

LISA-explanatory [13B]

1SRs | SRio

LISA++ [7B]                                   GSVA [13B]
'SRs | SRio

'SRs | SRio
36}                                            '         '

=== SPARTA (ours)

LISA [13B]

'SRs | SRio                                      {SRs

— GBDA — = Qwen (simple)

LISA-explanatory [13B]

' SRao

0

loU threshold, %

== Qwen PAIR

LISA++ [7B]

GSVA [13B]

'SRs | SRio

x        :
2        i                         i    i                                                 i
2       '                       !    i —“                                   0.20
ra        ‘                          027 |                     e271                         :
8       :                        0.22 0.21                 :    ba0.22                      0-14,
8        ’                         H  =             0.2     H                         H    PC
nO o07,                 t    t                Nine                 i    i
!    .                ee       H    OO —       $0.02 m0 02
0.0     +    t              0.0     t    ‘              0.0     t    ‘              0.0     t    t              0.0
0-0       O11       0.2      0.0       O1       0.2      0:0       O1       0.2      0:0    '   O1       0.2      0.

loU threshold, %

=== SPARTA (ours)

—= GBDA

== Qwen (simple)

== Qwen PAIR

Figure 13: Supplementary success rate (SR) curves as a function of IoU-drop threshold for adversarial
paraphrases with LLM score 5. This figure extends the main paper by presenting results for the four additional
checkpoints not shown in Figure 2: LISA-explanatory [7B], LISA [13B], LISA-explanatory [13B], LISA++ [7B],
and GSVA [13B]. Results are shown for the ReasonSeg dataset (top) and LLMSeg-40k dataset (bottom).

¢ Balanced Prompt: Prompt 3 is a streamlined
version of the Detailed Prompt. It also uses 10
in-context examples and a 5-point scale, but
its key difference is a minimalist rubric that
only defines the criteria for the best (5) and
worst (1) scores, requiring the model to inter-
polate the intermediate values. The prompt is
presented in Figure 12.

For the Detailed and Balanced prompts, we con-
sider an adversarial paraphrase to be valid only if
it receives a perfect LLM score of 5.

Instructions for Annotators

You'll receive two sentences: the Original and its Paraphrase. Return a single digit:

1 — Paraphrase is both grammatically and syntactically correct AND means exactly the same as
the Original.
O — Otherwise (any grammatical, syntactical error or meaning change/addition/omission).

Example 1

Original: "What object would people sit on when eating together at a dining table?"
Paraphrase: "On what object would people sit when eating together at a dining table?"
Return: 1

Example 2

Original: "What object would people sit on when eating together at a dining table?"
Paraphrase: "On what object would people stand when eating together at a dining table?"
Return: 0

Example 3

Original: "What object would people sit on when eating together at a dining table?"
Paraphrase: "What object would people sit on when aating together at a dining table?"
Return: 0

Figure 14: Instruction for annotators.

E.2. Validation Data

To validate the efficiency of the proposed evalua-
tion protocol, we annotated 310 pairs of original

18

and adversarial prompts generated by SPARTA and
baseline methods. The validation subset was anno-
tated by the authors, all of whom have relevant ex-
pertise, with any ambiguous cases resolved through
discussion. The instructions given to the annotators
are detailed in Figure 14.

We randomly sampled 50 examples with an
LLM score of 3, 50 with a score of 4, and 210
with a score of 5. Scores of 3 and 4 included only
four false negatives in total, so we focused on score
5, where the majority of paraphrase detection errors
occurred. In particular, we observed that the main
limitation of the Qwen3-based paraphrase detector
is its low precision (Table 2).

Sampling for LLM score 5 was performed in two
stages. First, we obtained 150 “short” paraphrase
pairs, defined as those where the adversarial para-
phrase was less than twice the length of the origi-
nal prompt. To ensure coverage across attacks, we
sampled 30 examples each from SPARTA, GBDA,
Qwen (simple), Qwen (adversarial), and PAIR.
Next, we sampled an additional 60 “long” para-
phrase pairs, where the adversarial paraphrase ex-
ceeded twice the length of the original prompt. This
was motivated by our observation (see Issue 3 in
Section 4.2) that some paraphrases generated by
the PAIR attack were excessively long or abstract,
occasionally resembling riddles or puzzles.


===== PAGE BREAK =====

E.3 Threshold Validation

To address the low precision of LLM-based para-
phrase detection, we additionally apply a cosine
similarity filter to discard semantically distant para-
phrases. Specifically, in addition to LLM-based de-
tection and regular expression filtering, we applied
cosine similarity filtering by classifying a sample
as a paraphrase if its cosine similarity score ex-
ceeded the threshold, and as a non-paraphrase oth-
erwise. For this, we use embeddings from Qwen3-
Embedding-8B (Zhang et al., 2025). We conducted
an empirical analysis using the annotated dataset
described in the previous section.

We searched for the optimal cosine similarity
threshold in two stages. First, we conducted a
coarse-grained search from 0.5 to 0.9 in increments
of 0.1, which identified 0.8 as the best-performing
threshold based on F1 score. We then refined the
search using a finer granularity around this value,
evaluating thresholds of 0.75, 0.85, 0.775, and
0.825 in a bisection-like manner. This process
yielded two top candidates, 0.8 and 0.825, both
achieving an identical F1 score of 0.749 (Figure 15).
However, the 0.825 threshold provided higher pre-
cision (0.671 vs. 0.655), which we prioritized to
minimize the number of false positives. Therefore,
we selected 0.825 as the final threshold for our
filtering mechanism.

| =e Prompt 1: Simple
== Prompt 2: Detailed
| —e— Prompt 3: Balanced

f
f

' Optimal threshold
| t=0.825

I

T         T         T         T
0.65    0.70    0.75    0.80

Cosine threshold

T
0.60

Figure 15: Determination of the optimal cosine simi-
larity threshold using Qwen3-Embedding-8B embed-
dings. The plot shows the FI score for three different
system prompts as a function of the cosine similarity
threshold. The optimal threshold is selected based on
the maximum F1 score, balancing precision and recall.

F GSVA: Performance Discrepancies

As discussed in the main text, GS VA [13B] exhibits
the weakest robustness on the LLMSeg-40k dataset,
which we hypothesize is linked to its underlying
segmentation performance. To investigate this, we

19

evaluated the publicly released GSVA checkpoint
on the ReasonSeg dataset, strictly following the au-
thors’ original evaluation protocol and script, with-
out modifying any parameters.

Our findings, summarized in Table 8, reveal a
substantial gap between the reported and repro-
duced metrics. Specifically, both the global Inter-
section over Union (gloU) and class-wise Intersec-
tion over Union (cIoU) are notably lower in our
evaluation compared to the original claims. This
discrepancy suggests that the reduced robustness
of GSVA may, at least in part, stem from its lower
segmentation accuracy on the ReasonSeg dataset.

ReasonSeg dataset             gloU cloU
GSVA (reported in paper) 50.5 56.4
GSVA (reproduced)             44.8      40.0

Table 8: Comparison of GSVA performance on the
ReasonSeg dataset: reported results from the original
paper vs. our reproduced results using the released
checkpoint.

References

Patrick Chao, Alexander Robey, Edgar Dobriban,
Hamed Hassani, George J. Pappas, and Eric Wong.
2024. Jailbreaking black box large language models
in twenty queries. Preprint, arXiv:2310.08419.

Paul-Ambroise Duquenne, Holger Schwenk, and Benoit
Sagot. 2023. Sonar: sentence-level multimodal and
language-agnostic representations. arXiv e-prints,
pages arXiv—2308.

Chuan Guo, Alexandre Sablayrolles, Hervé Jégou, and
Douwe Kiela. 2021. Gradient-based adversarial at-
tacks against text transformers. In Proceedings of the
2021 Conference on Empirical Methods in Natural
Language Processing, pages 5747-5757. Association
for Computational Linguistics.

Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui
Yuan, Shu Liu, and Jiaya Jia. 2024. Lisa: Reason-
ing segmentation via large language model. In Pro-
ceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 9579-9589.

Wei Li, Linchao Zhu, Longyin Wen, and Yi Yang. 2023.
Decap: Decoding clip latents for zero-shot captioning
via text-only training. Preprint, arXiv:2303.03032.

Andrianos Michail, Simon Clematide, and Juri Opitz.
2025. PARAPHRASUS: A comprehensive bench-
mark for evaluating paraphrase detection models. In
Proceedings of the 31st International Conference on
Computational Linguistics, pages 8749-8762, Abu
Dhabi, UAE. Association for Computational Linguis-
tics.


===== PAGE BREAK =====

NLLB Team, Marta R. Costa-jussa, James Cross, Onur
Celebi, Maha Elbayad, Kenneth Heafield, Kevin Hef-
fernan, Elahe Kalbassi, Janice Lam, Daniel Licht,
Jean Maillard, Anna Sun, Skyler Wang, Guillaume
Wenzek, Al Youngblood, Bapi Akula, Loic Barrault,
Gabriel Mejia Gonzalez, Prangthip Hansanti, and
20 others. 2022. No language left behind: Scal-
ing human-centered machine translation. Preprint,
arXiv:2207.04672.

Qwen Team. 2025. Qwen3 technical report. Preprint,
arXiv:2505.09388.

Senqiao Yang, Tianyuan Qu, Xin Lai, Zhuotao Tian, Bo-
hao Peng, Shu Liu, and Jiaya Jia. 2024. Lisat+: An
improved baseline for reasoning segmentation with
large language model. Preprint, arXiv:2312.17240.

Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang,
Huan Lin, Baosong Yang, Pengjun Xie, An Yang,
Dayiheng Liu, Junyang Lin, Fei Huang, and Jingren
Zhou. 2025. Qwen3 embedding: Advancing text
embedding and reranking through foundation models.
arXiv preprint arXiv:2506.05176.

Yingji Zhang, Marco Valentino, Danilo Carvalho, Ian
Pratt-Hartmann, and Andre Freitas. 2024. Graph-
induced syntactic-semantic spaces in transformer-
based variational AutoEncoders. In Findings of the
Association for Computational Linguistics: NAACL
2024, pages 474-489.

20
