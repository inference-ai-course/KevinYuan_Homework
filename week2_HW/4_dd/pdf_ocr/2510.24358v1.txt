2510.24358v1 [cs.SE] 28 Oct 2025

arXiv

Automatically Benchmarking LLM Code Agents through
Agent-driven Annotation and Evaluation

Lingyue Fu!, Bolun Zhang}, Hao Guan!, Yaoming Zhu°, Lin Qiu”, Weiwen Liu!,
Xuezhi Cao’, Xunliang Cai’, Weinan Zhang’, Yong Yu'*
Shanghai Jiao Tong University, Shanghai, China
2Meituan, Shanghai, China
3 AGI-Eval, Shanghai, China

ABSTRACT

Recent advances in code agents have enabled automated software
development at the project level, supported by large language mod-
els (LLMs) and widely adopted tools. However, existing benchmarks
for code agent evaluation face two major limitations: high annota-
tion cost and expertise requirements, and rigid evaluation metrics
that rely primarily on unit tests. To address these challenges, we pro-
pose an agent-driven benchmark construction pipeline that lever-
ages human supervision to efficiently generate diverse and chal-
lenging project-level tasks. Based on this approach, we introduce
PRDBench, a novel benchmark comprising 50 real-world Python
projects across 20 domains, each with structured Product Require-
ment Document (PRD) requirements, comprehensive evaluation
criteria, and reference implementations. PRDBench features rich
data sources, high task complexity, and flexible metrics. We fur-
ther employ an Agent-as-a-Judge paradigm to score agent outputs,
enabling the evaluation of various test types beyond unit tests. Ex-
tensive experiments on PRDBench demonstrate its effectiveness
in assessing the capabilities of both code agents and evaluation
agents, providing a scalable and robust framework for annotation
and evaluation.

CCS CONCEPTS

- Computing methodologies — Artificial intelligence; -Human-
centered computing — Human computer interaction (HCI).

KEYWORDS
Code Agent, Agent Evaluation, Large Language Models

1 INTRODUCTION

In recent years, code agents have made significant progress, capable
of solving increasingly complex programming tasks. From initially
focusing on single-file code generation to now advancing towards
complete project-level software development, Code Agents are
demonstrating powerful software development capabilities. LLM-
based Code Agents such as CursorAgent [10], Claude Code [1]
and Gemini CLI [17] have been widely adopted in real-world de-
velopment scenarios, accelerating the proliferation of automated
programming tools.

As code agents continue to advance, a variety of benchmarks
have emerged to evaluate their capabilities. Several works [3, 5, 9]
focus on assessing LLMs and code agents in data science tasks,
including subtasks such as data processing and machine learning

* corresponding author.

training. Other benchmarks [6, 11, 26] select complex code reposi-
tories from GitHub pull requests and transform them, often with
minimal human intervention, into testable scenarios. In addition,
PaperBench [22] evaluates the ability of agents to reproduce paper-
level code implementations, providing a more rigorous assessment
of their research reproducibility.

Despite these advancements, current benchmarks face two major
limitations. First, the creation of high-quality evaluation datasets
requires substantial human effort and expertise. Generating reliable
unit tests or grading tasks often depends on expert annotators,
especially when migrating real-world scenarios such as Kaggle
competitions or GitHub pull requests. These tasks demand domain-
level verification and annotation to produce usable test cases. As
code agents advance rapidly, the requirements for annotators’ ex-
pertise and time investment continue to increase. For example,
PaperBench [22] recruited ICML authors as annotators, with each
annotation task demanding several days of work from PhD-level
experts. This process not only incurs high annotation costs, but
also restricts the diversity of evaluation data, as recruiting experts
from various domains is challenging and often leads to datasets
being sourced from a single domain.

Second, the rigidity of the evaluation limits the broad applicability
of current benchmarks. Existing benchmarks [11] primarily rely on
unit test pass rates to assess code agent performance. While unit
tests are effective for verifying specific functions or components,
they are limited in scope and cannot fully cover the diverse testing
needs encountered in project-level software development. In prac-
tical scenarios, quality assurance involves not only unit testing, but
also integration testing, end-to-end testing, performance testing,
and security testing, which are all essential for robust software de-
livery. Relying solely on unit tests is not only insufficient, but also
overly restrictive, as it enforces strict requirements on project in-
terfaces and implementation details. Many real-world tasks require
broader validation methods. To address these gaps, it is necessary to
introduce additional evaluation strategies such as shell interaction
and file comparison, enabling more comprehensive and flexible
assessment of code agents in complex development environments.

To address the two aforementioned challenges, we propose an
agent-driven construction approach that enables the creation of
project-level benchmarks with flexible metrics at low human cost.
Specifically, a state-of-the-art code agent is used to generate both
the project scaffolding and a Product Requirement Document (PRD)
along with an executable criteria scheme. Human annotators are
only required to verify whether the criteria scheme is compatible
with the scaffolding interfaces and whether the expected outputs are
reasonable, without the need to manually create detailed evaluation


===== PAGE BREAK =====

Benchmark                     Area                 # Project       Annotator       # Metrics       Judger       Claude Code Score
SWE-Bench [11]              Pull Requests                   12            Web Crawler          2,294          Unit Test                 70.3%*
MLEBench [3]               AI Competition                 75                Human                75              Test Set                   51.1%
DevAI [33]                AI Developement             55              Human             365            Agent                73.0%"
PaperBench [22]                   AI Research                          20               Human (PhD.)            8,316          Human/LLM                    21.0%
PRDBench           Engineering Development         50        Agent & Human       1,262           Agent                  45.5%

Table 1: Comparison between PRDBench and project-level code agent benchmarks. We employ the performance of code agents
with Claude as a benchmark indicator for assessing the task difficulty of our proposed evaluation suite. The results from
external report [1] is denoted by «, from our implementation is denoted by +, otherwise we use the best results from the original

paper with author specified Claude-driven agents.

standards or reference solutions. This significantly reduces anno-
tation complexity: for PRDBench, annotators with undergraduate-
level knowledge in software engineering related fields are able
to complete the annotation, with an average of only eight hours
needed to finish the scaffolding and metrics for each project, greatly
improving annotation efficiency.

Based on this approach, we build a Product Requirement Docu-
ment (PRD)-centered benchmark, named PRDBench!. PRDBench
consists of 50 real-world coding tasks, each defined by a structured
PRD, an well-specified and verifiable criteria scheme, and a standard
solution code repository. The coding tasks are sourced from real-
world requirements, academic projects, and thesis work, spanning
20 common subdomains. For each task, the PRD provides a criteria
scheme that facilitates comprehensive human-like quality assur-
ance (QA) checks. We present a comparison between PRDBench
and previous code agent benchmarks in Table 1, where PRDBench
offers a comprehensive, multifaceted, project-level benchmark for
code agents. During evaluation, we employ the Agent-as-a-Judge
paradigm to score code according to the criteria scheme, enabling
the evaluation of a wide range of test types beyond unit tests.

In summary, our contributions include:

e We design an agent-driven data production pipeline, where
human supervision guides agents to efficiently generate chal-
lenging test cases that go beyond current agents capabilities.
This pipeline alleviates the need for expertise annotators.

e We construct PRDBench, which covers 20 common domains in
Python software development. Each benchmark item includes a
PRD and evaluation metrics, and we design three categories of
agent-friendly test types for comprehensive evaluation.

e We adopt the Agent-as-a-Judge paradigm for evaluation, en-
abling flexible adaptation to human-like QA assessment beyond
traditional unit tests.

e We conduct extensive experiments on PRDBench, providing
useful insights into both code agents and the evaluation agent.

2 RELATED WORK

2.1 Code Agent Evaluation

To keep pace with the rapid development of LLMs’ and agents’
coding capabilities, recent benchmarks increasingly emphasize the
construction of executable and complex tasks within real software

1Code of PRDBench Evaluation is available in https://github.com/AGI-Eval-
Official/PRDBench. PRDBench data will be available soon.

projects. These benchmarks are typically built by mining human-
generated data from online platforms or through extensive manual
annotation. Some benchmarks [9, 13] require annotators to manu-
ally create test points for each step, while others [7, 12, 20, 30, 32]
rely on authentic submission records, using the inherent difficulty
of submitted code as a basis for evaluation. Additionally, some
benchmarks [14, 21, 34] leverage existing resources and require
expert-driven rewriting, which demands significant manual effort.
As code agents become more capable, these annotation paradigms
require increasingly specialized expertise, making the recruitment
of expert annotators for each domain costly and limiting the scala-
bility and diversity of benchmark construction.

In terms of evaluation metrics, most existing benchmarks adopt
relatively narrow criteria, such as unit test pass rates [11, 13, 31],
which primarily assess functional correctness. ProjectEval [16] sim-
ulates user interaction to evaluate projects, focusing on specific
types of tasks. However, these evaluation criteria are limited to spe-
cific task types and cannot comprehensively assess diverse, complex
software projects.

2.2 Agent-as-a-Judge

Due to the inherent limitations of unit test-based benchmarks,
recent studies have begun to explore the use of LLMs as judges for
agent evaluation, a paradigm often referred to as LLM-as-a-Judge.
LLMs can provide capabilities such as image recognition [4], code
alignment [29], and quesion-answering [8], breaking the restriction
that benchmark metrics must be executable. With the emergence
of agents equipped with tools, LLMs are able to read large volumes
of files and perform extensive analyses, offering new possibilities
for evaluation. DevAI [33] is the first work to apply the agent-as-a-
judge paradigm to code agent evaluation. It focuses on checking
program execution and output format, but lacks comprehensive
assessment of overall engineering effectiveness. Some studies[27],
have explored LLM-based agent frameworks for evaluating code
generation, improving the accuracy of Agent-as-a-Judge through
architectural innovations. However, the lack of well-defined metrics
and code agents has limited the use of agent-as-a-judge. PRDBench
fills this gap with structured criteria and dedicated tools, enabling
more robust evaluation.


===== PAGE BREAK =====

THESIS

Seed Tasks
Step 1: Initialization     G

sre/          Product Requirement
Document (PRD)

V            Metric Outline
eva             (AAA Method)

Step 2: Generation

PRDBENCH

Step 4: Fix and Refinement

Code Agent
Scaffold
Suggestions
hm Criteria Scheme              Inspector

2

Test Artifacts     Step 3: Human Inspection

Figure 1: Overview of the PRDBench data production workflow. Step1: PRD and Test Plan Initialization; Step 2: Code Scaffold
and Criteria Scheme Generation; Step 3: Human Inspection; Step 4: Agent-based Fix and Refinement; Step 5: Remove scaffold.

3 PRDBENCH

In this section, we describe the construction and design of PRD-
Bench, covering seed task filtering, human annotation, and the
configuration of evaluation agents. We list education background
of hired annotators in Appendix A.

3.1 Seed Tasks

Our seed tasks are sourced from a variety of real-world project
requirements, including user requests from end-to-end AI product
development platforms, academic theses and projects. To ensure
the suitability and consistency of tasks for PRDBench, we apply a
rigorous filtering process. Specifically, selected tasks must satisfy:
(1) the task can be fully implemented in Python; and (2) all datasets
required for the task are publicly accessible. We choose Python
as the programming language for PRDBench primarily due to its
versatility and comprehensive ecosystem, which supports a wide
range of programming paradigms and application domains. This
ensures that PRDBench tasks are representative of diverse, real-
world scenarios.

3.2 Agent-Driven Data Production

Figure 1 illustrates the agent-driven data production workflow of
PRDBench. Throughout the annotation process, we apply SOTA
code agents to generate the project scaffold and criteria scheme.
Human annotators are only required to supervise the quality of
criteria scheme, i-e., whether the criteria aligns with the scaffold
interfaces and whether the expected outputs meet the requirements
specified in the PRD. It is worth mentioning that manual metric
annotation and code modification are not necessary, which greatly
reduces both the complexity and time required for annotation.

Step 1: PRD and Test Plain Initialization. After selecting seed
tasks, we utilize SOTA LLMs (such as GPT-4.1) or code agents (such
as Claude Code) to generate detailed and standardized PRD docu-
ments, ensuring clarity and completeness in task specification. The
PRD serves as the evaluation blueprint for PRDBench, and includes
sections such as Requirement Overview, Functional Requirements,
and Data Requirements. Based on the PRD, we employ GPT-4.1
to generate a corresponding metric outline, structuring test cases

using the Arrange-Act-Assert (AAA) methodology [28]. The Ar-
range step sets up the test case by preparing the necessary files,
input data, and environment configurations. The Act step focuses
on the core behavior to be tested, such as running the program or
performing interaction tests to obtain output results. The Assert
step verifies the expected outcomes by checking the system’s re-
sponse or state, ultimately determining whether the test passes or
fails. This structure is applicable to a wide range of code-related
testing scenarios, providing PRDBench with a comprehensive and
executable testing plan.

Step 2: Scaffold and Criteria Generation. After initializing with
the PRDs and metric outlines, we employ state-of-the-art code
agents to generate the code scaffold for each task. The scaffold
includes both module design and interface design, serving as the
core framework of the entire project. In some cases, particularly for
complex algorithms, the implementation may not be fully correct
as long as it does not affect the annotation process. Building upon
the scaffold, we further utilize code agents to expand and refine
the metric outline into a specific criteria scheme, which includes
necessary test interfaces and test artifacts. The presence of well-
defined code interfaces in the scaffold simplifies and standardizes
the generation of the criteria scheme, making the process more
efficient and consistent.

Step 3: Human Inspection. With the scaffold and the criteria
scheme in place, the inspection process for human annotators be-
comes straightforward. Annotators conduct inspection only by
running the tests to verify whether the interfaces function cor-
rectly and whether the expected outputs in the criteria scheme
align with the PRD requirements. Notably, even for tasks sourced
from expertise projects or academic papers, annotators only need
basic computer science knowledge to perform effective checking
and annotation.

Step 4: Agent-Based Fix and Refinement. If any issues are iden-
tified during step 3, annotators shall provide targeted feedback to
the code agent for refining the scaffold or criteria scheme. The code
agent then revises the relevant components, and the inspection
process is repeated. This iterative process continues until all issues
are resolved, ensuring the quality and correctness of the data.


===== PAGE BREAK =====

Task: Logistics Center Site Selection System

PRD.md (Product Requirement Document)

Requirement Overview.

This project aims to develop an optimized logistics center site selection system based on mathematical modeling and machine
learning, providing a scientific logistics distribution network planning solution for McDonald‘s stores in Anhui Province. The
system integrates the centroid method and K-means clustering algorithm, combined with GIS analysis. Through

multidimensional data modeling and visual presentation, it provides data-driven site selection recommendations for logistics
distribution decision-making, achieving the dual goals of minimizing delivery costs and maximizing service efficiency.

Basic Functional Requirements.
2.1 Data Management Module

Data Requirements.
3.1 Input Data

evaluation/

Detailed_test_plan.json (Criteria Scheme)
- M1: Centroid Method Coordinate Rationality: Verify Centroid Coordinates Rationality

Type: Unit Test
Description:

1. **Arrange:** Ensure evaluation/intput_coordintate.xlsx file exists.

2. **Act:** Execute unit tests to verify coordinate rationality.

3. **Assert:** Verify that the centroid longitude is within the 115-120 range, latitude is within the 30-35 range,
Testcases: “test command”: pytest evaluation/tests/test_centroid_coordinates.py::test_centroid_in_anhui_range

Expected output: pytest passed.

- M2: Centroid Method Coordinate Visualization: Verify Generate Complete Scatter Plot

Type: File Comparison
Description:

1. **Arrange:** Ensure matplotlib can display charts normally and evaluation/intput_coordintate.xlsx file exists.

2. **Act:** Execute scripts to generate visualization charts.

3. Verify that the scatter plot includes blue store scatter points, a red centroid point, axis labels, and legend explanation.
Testcases: “test command”: python src/centroid.py —input_file evaluation/intput_coordintate.xlsx
Expected output: Display a complete scatter plot including blue store scatter points, red centroid point, axis labels, and legend

explanation.

intput_coordintate.xlsx

Figure 2: A task example of PRDBench.

Step 5: Remove Scaffold. In the final stage, the scaffold is removed
and only the criteria scheme, test artifacts, required data, and PRD
are retained in PRDBench. The removal of scaffold ensures that the
code agents to be evaluated generate the code from scratch, thereby
we can assess the end-to-end development capacity of the agents.

An overall example of a PRDBench task is illustrated in Fig-
ure 2. Each PRDBench testcase consists of a PRD document and an
evaluation suite, which includes a criteria scheme and test artifacts.

3.3 EvalAgent and Evaluation

For each item in the criteria scheme, PRDBench provides executable
terminal commands and expected outputs, enabling systematic
verification. PRDBench employs a lightweight evaluation agent,
EvalAgent, to automate the validation of code generated by code
agents against predefined criteria schemes, as illustrated in Figure 3.
EvalAgent is equipped with six core tools, including file reading
and writing, command-line execution, image handling, and a judge
tool. Among these, the dealgraph tool offers a multimodal LLM


===== PAGE BREAK =====

INPUT (PRD.md, detailed_test_plan.json....)
Eval Agent

Tools

Metrics

Unit Test           Shell Interaction       File Comparison

OUTPUT (reports.json)

Figure 3: Overview of PRDBench evaluation. The EvalAgent
executes tests based on the criteria scheme using various
tools, compares outputs (files or results) with expected out-
puts, and generates a report for the submitted code.

interface (GPT-40 in this paper), allowing the agent to process im-

ages by submitting both the image and a descriptive prompt to

obtain results—this is particularly useful for tasks requiring visual
verification. The judge tool is specifically designed for PRDBench.

It accepts files containing simulated user inputs, enabling compre-

hensive terminal log generation for thorough analysis without the

need for manual input specification. By automating the validation
process, EvalAgent significantly reduces manual intervention and
improves evaluation efficiency.

Within PRDBench, the EvalAgent performs three main cate-
gories of tests, which collectively simulate the different stages of
QA in Python projects:

e Unit Test: As in previous benchmark studies [11], unit tests
remain the most effective approach for verifying the functional
correctness of individual components and modules within the
code. PRDBench retains this type of testing and provides aux-
iliary pytest-based test scripts. The EvalAgent simply runs the
pytest command to execute these tests.

e Shell Interaction: For tasks involving command-line interac-
tion or external system operations, the EvalAgent executes pre-
defined shell commands and compares the actual outputs to the
expected results, ensuring the code correctly handles system-
level operations and user inputs. PRDBench supplies compre-
hensive simulated user input files and program entry commands
for this purpose.

e File Comparison: For project-level tasks that generate files or
require specific directory structures, the EvalAgent compares the
produced files against reference solutions, checking for correct-
ness in content, format, and organization. PRDBench provides
the raw data for target file generation, the corresponding Python
commands, and the reference solution files.

By leveraging these tools, the EvalAgent can automatically adapt
to specific code repositories and complete all required tests. All
test categories and supporting files are provided within PRDBench,
which greatly reduces the complexity of evaluation. EvalAgent only

Table 2: Agent Specifications and Open-Source Status.

Agent Type Agent Framework Model                        Agent OSS_ LLM OSS

Qwen3-Coder [25]              v             v

a                   GPT-5 [19]                      V             x
Minimal         Basic code tools         Claude-3.7-Sonnet [1]            VY               x
Gemini-2.5-pro [23]            v            x

Gemini CLI [17]       Gemini-2.5-pro [23]            v             x

C          ial     Claude Code [1]         Claude-4/3.7 Hybrid [2]          x               x
Oomamercla" CodeX [18]            GPT-5 [19]                     v             x
Qwen Code [24]       Qwen3-Coder [25]              v             v

EvalAgent      EvalAgent              Qwen3-Coder [25]              v             v

needs to execute each test and analyze the results. This automated
evaluation process generates detailed reports on the performance
and correctness of code agent submissions.

4 EXPERIMENTS AND RESULTS
4.1 Data Statistics

PRDBench comprises 50 project-level tasks, encompassing a total
of 1,262 scoring points. Specifically, there are 409 unit test points,
729 shell interaction points, and 124 file comparison points. Each
PRD contains, on average, 105.22 lines. Figure 4(a) presents the
distribution of scaffold line counts recorded during the annotation.

For all PRDs, we utilize GPT-4 to assign a domain label to each
task. From these labeled tasks, we select 50 tasks that span 20 dis-
tinct domain labels to construct PRDBench. The distribution of
these domain labels is illustrated in Figure 4(b). To ensure broad
domain coverage, we include at least one representative task from
less common domains. For more prevalent areas of Python develop-
ment, such as data processing and machine learning, we select tasks
that require a variety of technical approaches and sub-skills. This
sampling strategy ensures that PRDBench provides comprehensive
and balanced coverage across different Python application domains.

4.2 Code Agents

We evaluate two categories of code agents in our experiments:

(1) Minimal Agents’, implemented using the Agent Development
Kit (ADK). These agents are equipped with essential tools for
file manipulation, bash scripting, and Python execution. We inte-
grate state-of-the-art LLMs (Claude-3.7-Sonnet, Gemini-2.5-Pro,
Qwen3-Coder-480B-A35B, and OpenAI GPT-5) to systematically
assess their core capabilities in strategy implementation. Here-
after we denote the models as Claude, Gemini, Qwen3-Coder
and GPT-5 respectively for simplicity.

(2) Commercial Code Agents, including advanced CLI-based agents
(Claude Code, CodeX, Gemini CLI, and Qwen Code). These
agents offer enhanced integration with command-line interfaces
and external tools, representing the current state-of-the-art in
code agent development.

Each commercial agent and minimal agent utilizes a correspond-
ing backbone LLM. Commercial agents typically benefit from vendor-
specific fine-tuning and optimization, resulting in more stable and
robust performance within their respective frameworks. While

2Code is available in https://github.com/AGI-Eval-Official/Minimal-CodeAgent.


===== PAGE BREAK =====

Shortest: 188 Lines
Longest: 9185 Lines

Std Dev: 2641.6

—— Average: 2583.8 Lines
— —- Median: 1279 Lines

a
BR

Bb
N

B
oO

Frequency
foe)

a

0        2000       4000       6000       8000
Line Count

(a) Scaffold line counts distribution.

Data Analysis

Information Security

Artificial Intelligence

Information Retrieval
Web Security

Software Engineering  Algorithms Computer Networks

Internet of Things
Web Crawling

Education

Game Development

Operations Research
Compiler Principles

Probability Theory

Computer Science

“r=” Automation
Application Development

(b) Domain distribution of PRDBench.

Figure 4: Data statistics of PRDBench.

minimal agents are designed to facilitate fair comparison across
different LLMs, commercial agents provide an assessment of the
latest advancements in agent capabilities. The mapping between
agent frameworks and backbone models is summarized in Table 2.

4.3 Experimental Setups

We select Qwen3-Coder [24] as the backbone LLM for EvalAgent.
Qwen3-Coder is fully open-source, which facilitates future fine-
tuning and further experiments for Agent-as-a-Judge scenarios
within PRDBench. For code agents, we select the latest model from
each family that is compatible with the agent framework. However,
Claude-4-Sonnet occasionally encounters tool call issues that result
in no code being generated, as frequently reported by the commu-
nity. Therefore, we use Claude-3.7-Sonnet in minimal code agent.
For all LLMs used in our work, we set temperature to be 0.1, max
token identical to their official APIs’ setting. We set top-p to 1.0,
Top-k to be 100, and presence penalty to be default to the API.
Each code agent is executed using a Python virtual environment
that contains necessary and useful packages for the agents. If the
code agent need other packages, we allow it use pip to install them.

4.4 Main Results

Table 3 presents the performance of all evaluated code agents on
PRDBench. In Round 1, we provide the PRD and the relevant evalu-
ation materials, allowing each code agent to implement the entire
project based on the existing interfaces. This round assesses the
development capability of the code agents. In Round 2, we provide
the EvalAgent report from the first round along with the Round 1
code, enabling each code agent to analyze and revise the code ac-
cording to the feedback and identified issues. This round evaluates
the debugging capability of the code agents.

Our main findings are as follows:

(1) The coding ability of the underlying LLM significantly
impacts the development performance of the code agent.
We observe that the relative ranking of minimal agents in round
1 is consistent with that of the commercial agents, indicating a
strong correlation between the LLM’s capabilities and the agent’s
development outcomes.

Table 3: Average pass rate of code agents on PRDBench (in %).
The best results are highlighted in bold, and the second-best
results are underlined.

Agent                   DEV.Tt       DEBUGT       EnhanceT
Minimal Agent
GPT-5                    55.81            60.15                4.34
Claude                   45.50            49.07                3.57
Gemini                   14.27            15.99                1.72
Qwen3-Coder         37.60            46.98                9.38
Commercial Agent
CodeX                   56.23            50.24               -6.99
Claude Code         36.60          45.53             8.93
Gemini CLI        16.35        21.59           5.16
Qwen Code           39.59           35.69              -3.90

(2) Adapting the code agent framework to LLMs can fur-
ther enhance development performance. While commercial
agents typically outperform minimal agents based on the same
LLM, there are reasonable exceptions. For example, since Claude-
3.7-Sonnet does not appear to be specifically optimized for Claude
Code, Claude Code achieves lower scores than the ADK-based
Claude agent in our experiments. Additionally, Gemini-2.5-Pro and
Qwen3-Coder demonstrate better generalization and more stable
performance across different code agent frameworks.

(3) Debugging and initial development require distinct
capabilities from code agents. In the Debug phase (Round 2),
we provide the previous round’s test report to guide code agents
in targeted fixes. Minimal code agents demonstrate more stable
improvements during debugging, consistently increasing their ac-
curacy after applying corrections. In contrast, CodeX experiences a
dramatic drop in performance in the DEBUG phase due to interface
inconsistencies introduced during modification, which led to addi-
tional errors. These observations suggest that effective debugging
requires not only error identification but also careful maintenance
of code structure and interfaces, highlighting the importance of ro-
bust error analysis and correction strategies in code agent designs.


===== PAGE BREAK =====

Table 4: Cost statistics of code agents. Code indicates the number of lines generated in Round 1 (development) and the number
of lines modified in Round 2 (debug). Relevant log data for CodeX is unavailable, so its results are not reported.

Round 1 (DEVELOPMENT)                                            Round 2 (DEBUG)
Time (s)         Input         Output      Code (lines)     Time (s)         Input         Output      A Code (lines)
Minimal Agent
GPT-5               1517.75      1506252.18      43467.62         854.49          1165.25      1888893.43      32358.57          774.04
Claude               717.50       1775631.15      33398.27         1975.35          380.37       599139.68      22760.68          285.96
Gemini               760.54       609661.54      27374.89         667.78          1370.54      3419419.51      32752.20          1579.06
Qwen3-Coder       892.35       1136848.60      17209.19         982.67          495.48       1835992.38      14176.74          917.50
Commercial Agent
CodeX                  -                -                -              651.62             -                -                -               101.18
Claude Code       1309.84     4113143.56     46669.50       1751.80        729.74      2140884.40     19593.26         112.68
Gemini CLI         2740.70       834125.00      20238.96         362.13          2067.61      5776370.08      24611.50          546.12
Qwen Code         1183.42      3108353.62      31400.28         1183.42         9582.81       988041.65       5604.89            31.00
Mi Unit Test (DEV)                 File Comparison (DEV)    49 Shell Interaction (DEBUG)
9 Shell Interaction (DEV)         Unit Test (DEBUG)        Hig File Comparison (DEBUG)

x

®

Pw

0

[4

hee 30

i                i     ]                                                       |

in 20

wi

GPT-5         Claude        Gemini        Qwen3

CodeX        Claude-Code      Gemini-cli       Qwen-code

Figure 5: Error rates of code agents on different types of test cases.

4.5 Code Agent Analysis

4.5.1 Cost of code agents. Table 4 presents the resource consump-
tion statistics for each code agent, including time spent, input and
output token counts, and lines of code generated or modified dur-
ing both development (Round 1) and debugging (Round 2). Overall,
commercial agents tend to consume more time than mini-
mal agents during both phases. For each backbone LLM, the
resource consumption patterns of its minimal and commer-
cial agent implementations are generally consistent. Notably,
Gemini exhibits significantly lower input token consumption in
the development phase compared to other models, whereas Qwen3-
Coder consume the least output token. However, in the debugging
phase, Gemini’s input token usage is more than twice that of other
agents, indicating that the development and debugging phases pose
distinct challenges for code agents in terms of resource demands.
Regarding code modification, Claude Code and Qwen Code made
relatively few changes during debugging, while GPT-5 and Gemini
demonstrated much larger code modifications. This suggests that
different code agents adopt varying strategies or exhibit different
levels of responsiveness when addressing feedback and implement-
ing code revisions. Some agents may focus on targeted, minimal

fixes, while others opt for more extensive refactoring or rewriting
in response to evaluation reports.

In summary, the observed differences in resource consumption
and code modification patterns highlight that development and
debugging are fundamentally different for code agents, requiring
distinct approaches and optimization strategies. The results also
suggest that agent architecture and underlying LLM characteristics
strongly influence how agents allocate resources and perform code
updates during the evaluation process.

4.5.2. Error Analysis of Code Agent. Figure 5 shows the error rates
of code agents across different types of test cases. The error rate
reflects the proportion of failed test cases within each category
for the code generated by the agents. This comparison highlights
the strengths and weaknesses of various agents when handling
diverse evaluation scenarios. We observe two main findings from
the results. First, there is no single category of test cases that is
consistently easy for code agents. The error rates are relatively
uniform across the three types of test cases, indicating that PRD-
Bench’s test case design is well-balanced and effectively covers a
broad spectrum of code implementation scenarios. Second, unit test
cases are noticeably more challenging to debug compared to the
other two types. Most code agents fail to resolve these issues even


===== PAGE BREAK =====

Table 5: Alignment between Human-as-a-Judge and Agent-
as-a-Judge.

Total Unit Test Shell Inter. File Comp.

Human-as-a-Judge

Number           282            107                 149                   26
Alignment 81.56%     79.44%       82.55%        84.62%

after the debugging phase. This is primarily because unit test cases
require agents to read and understand the test function code before
making corrections, whereas the other two test types only require
agents to compare actual and expected outputs, which lowers the
reasoning complexity.

4.6 EvalAgent Analysis

4.6.1. Human Alignment. We assess the reliability of the evaluation
agent through manual annotation. Specifically, we randomly select
16 code submissions, each corresponding to a distinct problem,
and employ two annotators to score the outputs of the code agent
according to the criteria scheme from PRDBench. These 16 codes
are generated by different code agents and span different problems.

The results of human alignment are summarized in Table 5. In
total, we examine 282 distinct test cases. Of these, 230 test cases
(81.56%) achieve perfect score alignment between the annotators
and the EvalAgent, indicating that EvalAgent’s evaluation re-
sults are generally reliable. For 9 cases (3.2%), the absolute differ-
ence in scores is 1, and for 43 cases (15.2%), the absolute difference
is 2. We analyze the alignment rates across different projects and
observe substantial variance (variance = 774.66), indicating signifi-
cant differences in score alignment between projects. The standard
deviation of alignment rates is also relatively high (27.83%), with
values ranging from 0% to 100%. This suggests that the consistency
between machine and human scoring can vary greatly depending
on the specific project or model. The result implies that the stabil-
ity of EvalAgent remains a major challenge, as an error occurs in
one test case, subsequent evaluations within the same project may
be affected due to contextual dependencies, potentially leading to
distorted overall assessment for that problem. This phenomenon is
also observed in other LLM-driven agents [15], and attributed to
propagation chains.

In addition, we examine the alignment rates among the three
major test case types: file comparison (84.62%), shell interaction
(82.55%), and unit test (79.44%). The alignment rates are comparable
across these categories, indicating that the evaluation difficulty
for EvalAgent is similar among the three types. This demonstrates
that our three test formats, as well as the commonly used unit test
benchmarks, are well-suited for reliable assessment by EvalAgent.

4.6.2 Cost of EvalAgent. Table 6 shows the average time and token
cost for EvalAgent. On average, EvalAgent generates a compre-
hensive testing report in just 7 minutes at an API cost of $2.68 per
problem, compared to 0.5 to 1 hour for human annotators (excluding
writing reports). This demonstrates that EvalAgent and PRDBench
greatly improve evaluation efficiency while keeping costs low.

Table 6: Average cost of EvalAgent. The input and output
are quantified by the number of tokens, and the API cost is
estimated according to the official pricing of Qwen3-coder.

Time (s)

EvalAgent 425.62

Input Output API Cost($)

124,2440     8,825          2.68

MM Free Development   149 Fixed Inference

oa

}0.366      0371                jo-396
+          @          s          O
v                        aw          oo

o
oe        Pg       é          Peg

>             (>               ‘2
eo Fe

o.ss4 0558 0.562                        0.863

ass
o.s76
0.203
I
o
<9

re         rv Ss
¢             RS           <            é
cr F

Score

y,
Ce

Figure 6: Comparison of code agent scores under fixed infer-
ence and free development modes on PRDBench.

4.7 Free Development

Currently, PRDBench uses code agents to generate project scaf-
folding and fixes the inference interfaces in the criteria scheme.
Code agents are required to implement code that conforms to these
predefined interfaces, which greatly simplifies the evaluation pro-
cess for EvalAgent. However, in real-world development, free-form
implementation is more common, where only the PRD is provided,
and interfaces are not fixed. To assess code agents in this free
development setting, we modified EvalAgent to accommodate flex-
ible interface designs and evaluated the agents’ ability to develop
projects without fixed scaffolding.

Figure 6 presents a comparison of code agent performance in
the fixed inference and free development modes on PRDBench. Our
results show that, compared to fixed inference development, the
distinction among code agents decreases in the free development
scenario, as reflected by the variance in performance scores: 0.011
for free development versus 0.028 for fixed inference. Nevertheless,
the relative ranking of code agents remains fairly stable between
the two settings. This demonstrates that PRDBench can effectively
evaluate code agents in both constrained and unconstrained devel-
opment environments while maintaining reliable differentiation.

5 CONCLUSION

In this work, we introduce PRDBench, a PRD-centered benchmark
that enables flexible and comprehensive evaluation of code agents
at the project level with reduced annotation costs. Our agent-driven
construction approach greatly reduces annotation cost and enables
efficient creation of project-level benchmarks. We further propose
EvalAgent and a corresponding flexible metric scheme to support
comprehensive and adaptable evaluation of code agents. Based on
these innovations, PRDBench provides a scalable and realistic foun-
dation for advancing research in automated software development
and code agent evaluation.


===== PAGE BREAK =====

REFERENCES

1

[10

[11

[12

[13

[16

[17

[18
[19

[20

[21

[22

Anthropic. 2025. Claude 3.7 Sonnet and Claude Code. https://www.anthropic.
com/news/claude-3-7-sonnet. Accessed: 2025-02-25.

Anthropic. 2025. Introducing Claude 4. https://www.anthropic.com/news/claude-
4. Accessed: 2025-05-23.

Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn,
Evan Mays, Giulio Starace, Kevin Liu, Leon Maksin, Tejal Patwardhan, et al.
2024. Mle-bench: Evaluating machine learning agents on machine learning
engineering. arXiv preprint arXiv:2410.07095 (2024).

Dongping Chen, Ruoxi Chen, Shilin Zhang, Yaochen Wang, Yinuo Liu, Huichi
Zhou, Qihui Zhang, Yao Wan, Pan Zhou, and Lichao Sun. 2024. Mllm-as-a-
judge: Assessing multimodal Ilm-as-a-judge with vision-language benchmark.
In Forty-first International Conference on Machine Learning.

Alireza Daghighfarsoodeh, Chung-Yu Wang, Hamed Taherkhani, Melika
Sepidband, Mohammad Abdollahi, Hadi Hemmati, and Hung Viet Pham.
2025. Deep-Bench: Deep Learning Benchmark Dataset for Code Generation.
arXiv:2502.18726 [cs.SE] https://arxiv.org/abs/2502.18726

Lingyue Fu, Hao Guan, Bolun Zhang, Haowei Yuan, Yaoming Zhu, Jun Xu,
Zongyu Wang, Lin Qiu, Xunliang Cai, Xuezhi Cao, et al. 2025. CoreCodeBench:
A Configurable Multi-Scenario Repository-Level Benchmark. arXiv preprint
arXiv:2507.05281 (2025).

Nam Le Hai, Dung Manh Nguyen, and Nghi D. Q. Bui. 2025. On the Impacts
of Contexts on Repository-Level Code Generation. arXiv:2406.11927 [cs.SE]
https://arxiv.org/abs/2406.11927

Xanh Ho, Jiahao Huang, Florian Boudin, and Akiko Aizawa. 2025. LLM-as-a-
Judge: Reassessing the Performance of LLMs in Extractive QA. arXiv preprint
arXiv:2504.11972 (2025).

Yiming Huang, Jianwen Luo, Yan Yu, Yitong Zhang, Fangyu Lei, Yifan Wei,
Shizhu He, Lifu Huang, Xiao Liu, Jun Zhao, and Kang Liu. 2024. DA-Code:
Agent Data Science Code Generation Benchmark for Large Language Models.
arXiv:2410.07331 [cs.CL] https://arxiv.org/abs/2410.07331

Anysphere Inc. 2025. Cursor: The AI powered Code Editor. https://cursor.com/.
Al assistant code editor announcement, Anysphere blog.

Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir
Press, and Karthik Narasimhan. 2023. Swe-bench: Can language models resolve
real-world github issues? arXiv preprint arXiv:2310.06770 (2023).

Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir
Press, and Karthik Narasimhan. 2024. SWE-bench: Can Language Models Resolve
Real-World GitHub Issues? arXiv:2310.06770 [cs.CL] https://arxiv.org/abs/2310.
06770

Bowen Li, Wenhan Wu, Ziwei Tang, Lin Shi, John Yang, Jinyang Li, Shunyu Yao,
Chen Qian, Binyuan Hui, Qicheng Zhang, Zhiyin Yu, He Du, Ping Yang, Dahua
Lin, Chao Peng, and Kai Chen. 2024. Prompting Large Language Models to Tackle
the Full Software Development Lifecycle: A Case Study. arXiv:2403.08604 [cs.CL]
https://arxiv.org/abs/2403.08604

Jia Li, Ge Li, Xuanming Zhang, Yihong Dong, and Zhi Jin. 2024. EvoCodeBench:
An Evolving Code Generation Benchmark Aligned with Real-World Code Repos-
itories. arXiv:2404.00599 [cs.CL] https://arxiv.org/abs/2404.00599

Xixun Lin, Yucheng Ning, Jingwen Zhang, Yan Dong, Yilong Liu, Yongxuan
Wu, Xiaohua Qi, Nan Sun, Yanmin Shang, Pengfei Cao, Lixin Zou, Xu Chen,
Chuan Zhou, Jia Wu, Shirui Pan, Bin Wang, Yanan Cao, Kai Chen, Songlin Hu,
and Li Guo. 2025. LLM-based Agents Suffer from Hallucinations: A Survey of
Taxonomy, Methods, and Directions. arXiv:2509.18970 [cs.AI] https://arxiv.org/
abs/2509.18970

Kaiyuan Liu, Youcheng Pan, Yang Xiang, Daojing He, Jing Li, Yexing Du, and
Tianrun Gao. 2025. ProjectEval: A Benchmark for Programming Agents Auto-
mated Evaluation on Project-Level Code Generation. arXiv:2503.07010 [cs.SE]
https://arxiv.org/abs/2503.07010

Taylor Mullen and Ryan J. Salva. 2025. Gemini CLI: Your Open Source AI
Agent. https://blog.google/technology/developers/introducing- gemini-cli- open-
source-ai-agent/. Google Developers Blog, Jun 2025.

OpenAl. 2025. CodeX CLI. https://github.com/openai/codex. Accessed: 2025-05-
16.

OpenAI. 2025. Introducing GPT-5. https://openai.com/index/introducing- gpt-5/.
Accessed: 2025-08-07.

Zhenyu Pan, Rongyu Cao, Yongchang Cao, Yingwei Ma, Binhua Li, Fei Huang,
Han Liu, and Yongbin Li. 2024. Codev-Bench: How Do LLMs Understand
Developer-Centric Code Completion? arXiv:2410.01353 [cs.SE] https://arxiv.
org/abs/2410.01353

Qiwei Peng, Yekun Chai, and Xuhong Li. 2024. HumanEval-XL: A Multilingual
Code Generation Benchmark for Cross-lingual Natural Language Generalization.
arXiv:2402.16694 [cs.CL] https://arxiv.org/abs/2402.16694

Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon
Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, et al. 2025.
PaperBench: Evaluating AI’s Ability to Replicate AI Research. arXiv preprint
arXiv:2504.01848 (2025).

23

24

25

26

27

28

29

30

31

32

33

34

Gemini Team. 2025. Gemini 2.5: Pushing the Frontier with Advanced Reason-
ing, Multimodality, Long Context, and Next Generation Agentic Capabilities.
arXiv:2507.06261 [cs.CL] https://arxiv.org/abs/2507.06261

Qwen Team. 2025. Qwen Code brings the capabilities of advanced code models
to your terminal in an interactive Read-Eval-Print Loop (REPL) environment.
https://qwenlm.github.io/qwen-code-docs/en/. Accessed: 2025-07-22.

Qwen Team. 2025. Qwen3-Coder: Agentic Coding in the World. https://qwenlm.
github.io/zh/blog/qwen3-coder/. Accessed: 2025-07-22.

Konstantinos Vergopoulos, Mark Niklas Miller, and Martin Vechev. 2025.
Automated Benchmark Generation for Repository-Level Coding Tasks.
arXiv:2503.07701 [cs.SE] https://arxiv.org/abs/2503.07701

Xinchen Wang, Pengfei Gao, Chao Peng, Ruida Hu, and Cuiyun Gao. 2025.
CodeVisionary: An Agent-based Framework for Evaluating Large Language
Models in Code Generation. arXiv:2504.13472 [cs.SE] https://arxiv.org/abs/2504.
13472

Chenhao Wei, Lu Xiao, Tingting Yu, Sunny Wong, and Abigail Clune. 2025. How
Do Developers Structure Unit Test Cases? An Empirical Analysis of the AAA
Pattern in Open Source Projects. IEEE Transactions on Software Engineering
(2025).

Martin Weyssow, Aton Kamanda, Xin Zhou, and Houari Sahraoui. 2024. CodeUl-
traFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to
Coding Preferences. arXiv:2403.09032 [cs.SE] https://arxiv.org/abs/2403.09032
Qinyun Wu, Chao Peng, Pengfei Gao, Ruida Hu, Haoyu Gan, Bo Jiang, Jinhe
Tang, Zhiwen Deng, Zhanming Guan, Cuiyun Gao, Xia Liu, and Ping Yang.
2024. RepoMasterEval: Evaluating Code Completion via Real-World Repositories.
arXiv:2408.03519 [cs.SE] https://arxiv.org/abs/2408.03519

Yanzheng Xiang, Hanqi Yan, Shuyin Ouyang, Lin Gui, and Yulan He. 2025.
SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Repro-
duction from Research Papers. arXiv:2504.00255 [cs.CL] https://arxiv.org/abs/
2504.00255

Jian Yang, Jiajun Zhang, Jiaxi Yang, Ke Jin, Lei Zhang, Qiyao Peng, Ken
Deng, Yibo Miao, Tianyu Liu, Zeyu Cui, Binyuan Hui, and Junyang Lin.
2024. ExecRepoBench: Multi-level Executable Code Completion Evaluation.
arXiv:2412.11990 [cs.CL] https://arxiv.org/abs/2412.11990

Mingchen Zhuge, Changsheng Zhao, Dylan Ashley, Wenyi Wang, Dmitrii
Khizbullin, Yunyang Xiong, Zechun Liu, Ernie Chang, Raghuraman Krishnamoor-
thi, Yuandong Tian, et al. 2024. Agent-as-a-judge: Evaluate agents with agents.
arXiv preprint arXiv:2410.10934 (2024).

Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira
Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, Simon
Brunner, Chen Gong, Thong Hoang, Armel Randy Zebaze, Xiaoheng Hong,
Wen-Ding Li, Jean Kaddour, Ming Xu, Zhihan Zhang, Prateek Yadav, Naman
Jain, Alex Gu, Zhoujun Cheng, Jiawei Liu, Qian Liu, Zijian Wang, Binyuan Hui,
Niklas Muennighoff, David Lo, Daniel Fried, Xiaoning Du, Harm de Vries, and
Leandro Von Werra. 2025. BigCodeBench: Benchmarking Code Generation with
Diverse Function Calls and Complex Instructions. arXiv:2406.15877 [cs.SE]
https://arxiv.org/abs/2406.15877


===== PAGE BREAK =====

A RECRUITMENT AND COMPENSATION OF
ANNOTATORS

We employ a total of 8 annotators, including five full-time anno-
tators and three part-time annotators. We list their educational
background in Table 7.

Among them, F1 to F6 and P1 participants in PRDBench con-
struction and each of them contribute at least 5 problems.

F1, F2, P2 and P3 participants in data examination.

P2 and P3 participants in EvalAgent and human alignment ex-
periments in section 4.6.1.

The detailed evaluation report must be saved to reports/round2.
jsonl in JSON format. Entries in the report should follow this
structure:
{
"metric": "1.3 Menu Navigation - Export Results Submenu",
"description": "1. *xAct:** Start the program and select main
menu '3' to enter the export results submenu.\n2. **Assert:**
Check whether the submenu displays ‘Export Huffman codes to

CSV', ‘Export Huffman tree to JSON', and ‘Return to main menu
vom

"score": @,

"explanation": "When attempting to export results without

having generated Huffman codes, the program does not enter the
export submenu but instead prompts 'No available Huffman
codes, please generate them first.' and returns to the main
menu, which does not meet the expected behavior."

All annotators receive daily wages (part-time) or monthly salaries (full-   H ,

time) according to local labor regulations.

B FULL PROMPTS

B.1 Prompt of code agents
Prompt for Round 1 (development):

Please develop a complete Python project (ID:{ID}) according to
the requirements specified in the project documentation (src/PRD.
md), and with reference to the expected test metrics (evaluation/
detailed_test_plan.json).

### Requirements

1. Strictly implement all functional requirements described in PRD
-md, ensuring that every feature is fully realized and that no
requirements are omitted.

2. Closely follow the testing schemes defined in
detailed_test_plan.json, ensuring that your implementation process
and interfaces fully comply with the testing specifications, so

that QA testing can be carried out directly using
detailed_test_plan.

3. Submit all project code and related files completely under the
src/ directory, ensuring that the project structure is clear and
maintainable.

4. Do not ask any intermediate questions during the development
process. Complete the entire project and submit directly.

Prompt for Round 2 (debug):

You are provided with the following resources:

- The project requirements and code in src/ (including PRD. md and
source code)

- The evaluation criteria and related files in evaluation/

- The development score report of code in src/ is in reports/
Please analyze the items in the score report where points were

deducted, and modify the code to address these issues. Ensure
that your revised code fully complies with both the evaluation
criteria and the requirements specified in PRD.md.

Prompt for free development in Section 4.7:

Please develop a complete Python project (ID:{ID}) according to
the requirements specified in the project documentation (src/PRD.
md).

### Requirements

1. Strictly implement all functional requirements described in PRD
-md, ensuring that every feature is fully realized and that no
requirements are omitted.

2. Independently design the implementation details, including
process flows and interfaces, based on the requirements in PRD.md,
so that each function can be clearly and completely used via the
cLI.

3. Submit all project code and related files completely under the
src/ directory, ensuring that the project structure is clear and
maintainable.

4. Do not ask any intermediate questions during the development
process. Complete the entire project and submit directly.

B.2. Prompt of EvalAgent
Prompt for EvalAgent:

Please evaluate the {project_dir} project by running its tests and
generating an evaluation report according to the evaluation
criteria. The evaluation criteria are provided in evaluation/
detailed_test_plan.json, and the project code is located in the
src/ directory.

"metric": "3.2 Unit Test - Generate Huffman Codes",
"description": "1. **Pre-check (User Path):** Is there a unit
test for the ~generate_huffman_codes~ function in ~src/tests/~
or a similar directory?\n2. **Arrange:** Prepare test data,
such as a constructed Huffman tree and the expected encoding
dictionary.\n3. **Act:** Run the unit test command “pytest src
/tests/test_huffman.py::TestHuffman::
test_generate_huffman_codes -v~.\n4. **Assert:** Observe
whether the test passes.",
"score": 2,
"explanation": "The test command ~pytest src/tests/
test_huffman.py::TestHuffman::test_generate_huffman_codes -v
executed successfully, and the result was 'PASSED', which
matches the expected output ‘Unit test passed'."

}

Please strictly follow the evaluation criteria in evaluation/
detailed_test_plan.json, run the relevant tests, and generate a
comprehensive evaluation report as described above. Save the
report to reports/round2.json in the specified format.

Prompts for EvalAgent in free development setting:

Please evaluate the {project_dir} project by running its tests and
generating an evaluation report according to the evaluation
criteria. The evaluation criteria are provided in evaluation/
detailed_test_plan.json, and the project code is located in the
src/ directory.
Please carefully read the code, rewrite the test file interface to
adapt to the current code under src/, and offer reliable
evaluation for current code.
The detailed evaluation report must be saved to reports/round2.
jsonl in JSON format. Entries in the report should follow this
structure:
{
"metric": "1.3 Menu Navigation - Export Results Submenu",
"description": "1. *xAct:** Start the program and select main
menu '3' to enter the export results submenu.\n2. **Assert:**
Check whether the submenu displays ‘Export Huffman codes to
CSV', ‘Export Huffman tree to JSON', and ‘Return to main menu
no
"score": @,
"explanation": "When attempting to export results without
having generated Huffman codes, the program does not enter the
export submenu but instead prompts 'No available Huffman
codes, please generate them first.' and returns to the main
menu, which does not meet the expected behavior."

"metric": "3.2 Unit Test - Generate Huffman Codes",
"description": "1. **Pre-check (User Path):** Is there a unit
test for the ~generate_huffman_codes~ function in ~src/tests/~
or a similar directory?\n2. **Arrange:** Prepare test data,
such as a constructed Huffman tree and the expected encoding
dictionary.\n3. **Act:** Run the unit test command “pytest src
/tests/test_huffman.py::TestHuffman::
test_generate_huffman_codes -v~.\n4. **Assert:** Observe
whether the test passes.",
"score": 2,
"explanation": "The test command ~pytest src/tests/
test_huffman.py::TestHuffman::test_generate_huffman_codes -v
executed successfully, and the result was 'PASSED', which
matches the expected output ‘Unit test passed'."

}

Please strictly follow the evaluation criteria in evaluation/
detailed_test_plan.json, rewrite the inference in the evaluation
files, run the relevant tests, and generate a comprehensive
evaluation report as described above. Save the report to reports/
round2.json in the specified format.



===== PAGE BREAK =====

Table 7: Background of annotators.

Type         ID Major                                       Degree                      Years of Experience as Developer
Full-time F1 Computer Science                            Master                                               10
Full-time F2 Software Engineering                     Master                                           6
Full-time F3 Applied Statistics                                Master                                                    1
Full-time F4 Health Service and Management       Bachelor                                           2
Full-time F5 Information Management                 Bachelor                                            4
Full-time F6 Economics and Trade Management Bachelor                                        6
Part-time P1 Computer Science                            Bachelor                                             9
Part-time P2 Computer Science                             Senior Undergraduate                            -
Part-time P3 Computer Science                             Senior Undergraduate                            -

