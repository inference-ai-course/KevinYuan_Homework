arX1v:2510.24891vl [cs.CL] 28 Oct 2025

Idea2Plan: Exploring Al-Powered Research Planning

Jin Huang!* — Silviu Cucerzan?

Abstract

Large language models (LLMs) have demon-
strated significant potential to accelerate sci-
entific discovery as valuable tools for analyz-
ing data, generating hypotheses, and support-
ing innovative approaches in various scientific
fields. In this work, we investigate how LLMs
can handle the transition from conceptual re-
search ideas to well-structured research plans.
Effective research planning not only supports
scientists in advancing their research but also
represents a crucial capability for the develop-
ment of autonomous research agents. Despite
its importance, the field lacks a systematic un-
derstanding of LLMs’ research planning capa-
bility. To rigorously measure this capability,
we introduce the Jdea2Plan task and Idea2Plan
Bench, a benchmark built from 200 ICML 2025
Spotlight and Oral papers released after ma-
jor LLM training cutoffs. Each benchmark in-
stance includes a research idea and a grading
rubric capturing the key components of valid
plans. We further propose Idea2Plan JudgeE-
val, a complementary benchmark to assess the
reliability of LLM-based judges against expert
annotations. Experimental results show that
GPT-5 and GPT-5-mini achieve the strongest
performance on the benchmark, though sub-
stantial headroom remains for future improve-
ment. Our study provides new insights into
LLMs’ capability for research planning and lay
the groundwork for future progress.!

1 Introduction

A key challenge in scientific research is that sci-
entists tend to produce more promising ideas than
they can pursue and thus, many potentially promis-
ing ideas often remain unexplored. When attending
conferences, participating in reading groups, or dis-
cussing with peers, scientists frequently conceive
ways to improve ongoing work or apply insights to

“Work performed while at Microsoft Research.
‘Code will be released upon institutional approval.

Sujay Kumar Jauhar?
‘University of Michigan

Ryen W. White
?Microsoft Research

other areas. Despite this abundance of ideas, many
remain unexplored due to time constraints.

Transforming each research idea into a viable
plan requires considerable time and cognitive effort.
The process of developing an idea often involves
reviewing prior work, formulating hypotheses, se-
lecting appropriate methods, and designing exper-
iments, with plans iteratively refined as research
progresses. This process can take researchers many
days or even weeks to complete. We refer to
the process of turning a research idea into a con-
crete, testable plan as research planning. Research
planning encompasses all the steps necessary to
bridge the gap between an initial idea and a well-
structured plan ready for execution.

Automated systems capable of research planning
could greatly accelerate scientists’ progress. By
supporting the development of research ideas and
streamlining the planning process, such systems
could also advance the capabilities of autonomous
AI research agents and help to ensure that more
promising ideas are explored and developed.

Despite its importance, Al-powered research
planning has been underexplored. Recent works
focus on other research stages such as idea genera-
tion (Wang et al., 2024a; Ghafarollahi and Buehler,
2024; Baek et al., 2025; Si et al., 2025), literature
review (Wang et al., 2024b), and experiment execu-
tion (Starace et al., 2025; Kon et al., 2025a; Jansen
et al., 2025; Seo et al., 2025). Previous work treats
research planning as an intermediate step without
explicit evaluation (Jansen et al., 2025) or evaluates
research plans based only on simple traits such as
clarity and novelty (Chen et al., 2025a).

We make the evaluation of AI’s ability for re-
search planning our primary objective. We start our
investigation by formulating the [dea2Plan task,
where the input is a research idea, and the AI agent
is asked to generate a research plan, a structured
output that serves as a roadmap for executing a
project. A research plan typically includes objec-


===== PAGE BREAK =====

Idea2Plan Bench

Research Idea
We aim to assess whether jailbreak attacks that
force models to bypass guardrails actually
produce useful outputs. To do this, we will

Method
ReAct Agent :

| Baselines

 =

4

Reference
Research Plan        Gradi
1. Introduction       =>  Rubne  = R

2. Key Literatures

3. Method

4, Initial Experimental Design
5, Resource & Ethics

build evaluation sets with known ground truth
by collecting questions on ber

verifiable topics.

t     Evaluation

Ss)

nign, easily                                    fi
C ))                         Xi          i
Generated         @ arXiv
esearch Plan           Search arXiv w/blocklist| — Read Paper
=                ¢ Input: query         {* Input: arXiv ID
n              * Output: papers (ID, |* Output: paper     i
title, abstract)         | summary            i

Figure 1: Idea2Plan overview: Starting from a research idea extracted from the abstract of an academic paper, we

employ a pipeline that includes prompt-based and ReAct
tools, to generate research plans that are then evaluated

agent methods, integrated with search and paper reading
against reference plans derived from the content of the

corresponding paper by using a comprehensive grading rubric.

tives, prior work, methodology, and evaluation de-
sign (Weber and Cobaugh, 2008; Sudheesh et al.,
2016). Since we focus on planning for scientific re-
search in the field of AI, we instantiate our research
plans’ structure based on common AI paper con-
ventions: Introduction, Key Literature, Methods,
Initial Experimental Design, and Resources/Com-
pliance/Ethical Considerations. We hypothesize
that this format mirrors how AI researchers articu-
late and reason about research plans.

Evaluating such plans is challenging for two ma-
jor reasons. First, data contamination threatens va-
lidity, because an LLM may have already encoun-
tered the idea (for instance through a published
paper), in which case its output reflects the LLM’s
memorization about the research idea rather than
genuine research planning ability (Carlini et al.,
2021, 2022). To mitigate this, we build an evalua-
tion framework that extracts research ideas from AI
papers that are published after LLMs’ data cutoff
dates. Second, multiple plans can be equally valid
for the same idea; for example, two sound plans
may select different yet comparable datasets, yield-
ing distinct but acceptable designs. We address this
by constructing a dedicated grading rubric for each
research idea. The questions in the rubric test the
common desiderata that any valid plan for that idea
should meet.

Concretely, to instantiate the Idea2Plan task
for empirical evaluation, we construct [Idea2Plan
Bench based on ICML 2025 Spotlight and Oral pa-
pers, as illustrated in Figure 1. We exclude papers
with public arXiv versions before the latest training
data cutoff of the LLMs employed in our study. We
then generate a grading rubric from the extracted

research plan for each research idea. The rubric
contains five sections of yes/no questions that cap-
ture the essential components of a valid plan for
each research idea as derived from the content of
the corresponding paper. We assess the quality of
extracted plans and rubrics by a human study and
find that they are of reasonable quality. Finally, we
randomly sample 200 papers as a held-out test set.

Given the amount of effort required to grade
research plans against rubrics, it is unrealistic to
rely on human graders at scale. To make evalua-
tion scalable, we propose LLM-based judges for
grading the rubrics (Zheng et al., 2023; Starace
et al., 2025). We introduce an additional bench-
mark, Idea2Plan JudgeEval, to assess judgment
reliability. This benchmark collects a set of ground-
truth gradings created manually by human experts.
Our results show that LLM judges can provide
expert-aligned grading.

To explore the performance of LLMs on the
Idea2Plan benchmark, we test both direct prompt-
ing and agentic approaches. Our agentic approach
uses a ReAct-style scaffolding (Yao et al., 2023)
with tools for searching over and reading arXiv
papers. We then compare a range of frontier pro-
prietary and open-source LLMs. Results show that
GPT-5 and GPT-5-mini consistently outperform
other LLMs. Surprisingly, the ReAct agent’s per-
formance does not outperform direct prompting.
Overall, our work advances the understanding of
research planning capabilities in LLMs.

Our contributions are as follows:

¢ We formalize the Idea2Plan task and build a

benchmark from post-LLM-cutoff AI papers.
¢ We develop a pipeline to automatically gen-


===== PAGE BREAK =====

erate rubrics for research plans. We conduct
a human study and show that the extracted
plans and rubrics are of high quality.

¢ We propose an LLM-based judge and the
Idea2Plan JudgeEval to compare LLM-as-
judge scoring with expert annotations.

¢ We discuss experiments and results with sev-
eral state-of-the-art LLMs under different
prompting and agentic strategies.

2 Related Work

2.1 AI Agents for Scientific Discovery

Al agents increasingly support different stages of
the research workflow, including hypothesis genera-
tion (Wang et al., 2024a; Ghafarollahi and Buehler,
2024; Baek et al., 2025; Si et al., 2025; Zhang et al.,
2025; Jansen et al., 2025; Gottweis et al., 2025),
literature review (Singh et al., 2025; Wang et al.,
2024b), and experimental execution (Kon et al.,
2025a; Jansen et al., 2025; Seo et al., 2025).

Recent efforts further pursue end-to-end AI sci-
entists that can conduct a full loop of the scientific
process from generating ideas to autonomously de-
signing and conducting experiments (Yuan et al.,
2025; Lu et al., 2024; Yamada et al., 2025;
Schmidgall et al., 2025; Schmidgall and Moor,
2025). For example, the AI SCIENTIST proposes
a pipeline to generate a research idea, conduct
experiments, and finally write a paper (Lu et al.,
2024). Despite the advances of these systems, most
prior research does not treat planning—a phase that
bridges ideation and implementation—as a first-
class objective. Research planning is crucial, as it
directly affects downstream execution quality. We
therefore focus our study on evaluating this critical
yet underexplored capability.

2.2 Benchmarks for AI Agents on Research
Automation

Recent benchmarks aim to evaluate AI systems
designed for research automation. Broadly, these
efforts can be grouped into two categories based on
their output form: (1) paper-level evaluation, where
systems are assessed on the quality of generated re-
search papers (Lu et al., 2024; Yamada et al., 2025;
Schmidgall et al., 2025); and (2) code-level evalua-
tion, where systems are judged by their ability to
produce code implementations or experimental re-
sults (Kon et al., 2025a; Chen et al., 2025b; Jansen
et al., 2025; Starace et al., 2025; Seo et al., 2025;
Chen et al., 2025a; Kon et al., 2025b). In contrast,

our work isolates and rigorously evaluates the stage
of research planning. The most related works are
CODESCIENTIST (Jansen et al., 2025) and MLR-
BENCH (Chen et al., 2025a): the former includes
research planning as an intermediate step without
explicit evaluation, and the latter evaluates research
plans only on high-level dimensions such as clarity
and novelty.

2.3 Planning

Planning, which refers to creating a series of ac-
tions to achieve an objective, is an important ca-
pability for both humans and AI agents (Newell
et al., 1958; Ghallab et al., 2004). Conventional
studies primarily depend on symbolic approaches
or reinforcement learning methods (Aeronautiques
et al., 1998; Haslum et al., 2019; He et al., 2015).
Recently, there has been increasing interest in using
LLMs in planning (Wei et al., 2025; Huang et al.,
2024). Recent approaches demonstrate that LLMs
can explicitly decompose goals into reasoning steps
or structured action sequences, enabling them to
handle long-horizon, multi-step tasks (Yao et al.,
2023; Wang et al., 2023; Silver et al., 2024). De-
spite these advances, LLMs’ ability to perform re-
search planning remains largely unexplored, which
we seek to address in this paper.

3 Proposed Framework

In this section, we introduce [dea2Plan, a frame-
work to evaluate how LLMs turn research ideas
into research plans; see Figure 1 for an overview.

3.1 Research Plan and Idea Extraction

There may be different definitions of research plans
and what should be included (Weber and Cobaugh,
2008; Sudheesh et al., 2016). In this work, we focus
on AI research and we adhere to the common struc-
ture of research plans for AI papers. We discard
plan components that are not easily determined by
an LLM (e.g., timelines, logistics, or human exper-
tise). We guide the research plan generation using a
structured template with the following sections: In-
troduction, Key Literature, Methods, Initial Experi-
mental Design, and Resources/Compliance/Ethical
Considerations. We use 04-mini? (reasoning=high)
for idea and plan extraction. Prompt templates are
shown in Appendix §B.1.

*https: //openai.com/index/
introducing-03-and-04-mini/


===== PAGE BREAK =====

Table 1: We show an example of a research idea, grading questions, relevant excerpts of a research plan generated
by GPT-5 given this research idea, and the LLM-judge’s assessment of this question. The full example is in §A.

ICML 2025 Spotlight)

Research Idea (from paper: The Jailbreak Tax: How Useful are Your Jailbreak Outputs? (Nikolic et al., 2025),

“We aim to assess whether jailbreak attacks that force models to bypass guardrails actually produce useful outputs. To
do this, we will build evaluation sets with known ground truth by collecting questions on benign, easily verifiable topics
and align models to refuse those questions. We will then apply representative jailbreak strategies to these aligned models
and measure how much model utility drops in the jailbroken responses. We propose a new metric to quantify this
performance degradation and introduce corresponding benchmarks to enable systematic evaluation and comparison of

jailbreak methods.”
Section            Grading Question Example                     Excerpts from a Research Plan generated by | Judg-
GPT-5                                                       ment
Introduction | “Does the plan note that existing evalua- | “While jailbreak success is typically measured by | Yes
tions focus on bypass success and neglect | refusal circumvention, it is unclear whether the
post-jailbreak capability retention?”             resulting outputs remain useful and accurate.”
Key              “Does the plan cite the paper (Jailbreaking | “Greshake et al., Prompt Injection attacks [...] }| No
Literatures       black box large language models in twenty | Wallace et al., Universal Adversarial Triggers
queries) or similar work on iterative LLM- | [...] Zou et al., adversarial suffix (GCG) meth-
based prompt rewriting attacks?”                ods.”
Methods         “Does the plan describe a supervised fine- | “Lightweight SFT/LoRA: fine-tune an | Yes
tuning approach on (prompt, refusal) pairs | instruction-tuned base model on_ refusal
to induce model alignment?”               demonstrations for our categories to simulate
learned guardrails.”
Initial            “Does the plan include analysis of the re- | “Optional: chain-of-thought vs concise answers | No
Experimental | lationship between task difficulty and the | to see if jailbreaks disproportionately harm
Design             magnitude of the jailbreak tax?”                 multi-step reasoning.”
Resources,      “Does the plan address the potential for ad- | “Release attack code in a restricted form that | Yes
Compliance, | versaries to misuse the evaluation frame- | limits adaptation to harmful content [...] exclude
& Ethics         work?”                                         especially potent per-item adversarial suffixes
from public artifacts.”

3.2 Rubric-based Evaluation

Given our extracted reference research plans and
research ideas, along with research plans generated
by LLMs (either through prompting or agentic ap-
proaches), the question is: how do we evaluate the
generated research plans? The key difficulty lies
in the fact that a single research idea can lead to
multiple equally reasonable research plans. For
example, an idea about evaluating a new jailbreak
attack could reasonably choose different yet com-
parable datasets, threat models (e.g., black-box vs.
white-box), or metric suites; these choices can pro-
duce distinct plans that are all sound.

To capture these nuances, we propose a rubric-
based evaluation approach (Starace et al., 2025).
We generate rubric questions from the reference
research plans to identify high-level design choices
that should be addressed in any reasonable research
plan for the given idea. This approach allows us
to evaluate whether generated plans cover essential
conceptual elements while accommodating legiti-
mate variations. The prompt template is in §B.2.

3.3. Dataset Construction

Our dataset selection follows two key criteria: (1)
papers must be free from potential training data
contamination for the LLMs we evaluate, and (2)

the research ideas must be of high quality. To meet
these requirements, we propose to use the Spotlight
and Oral papers from ICML 2025, a top-tier AI
conference. We filter out any papers with arXiv
submissions predating the most recent data cutoff
of the LLMs we test (GPT-5, October 20247). From
this filtered set, we randomly select 200 papers as
our test set. Additionally, we randomly sample 30
papers as a development set, keeping the test set
reserved exclusively for final evaluation.

3.4 Expert Assessment of Extracted Research
Plans and Rubrics

To validate the quality of our extraction pipeline,
we conduct a human evaluation study with eight
Ph.D. students and researchers with expertise in
AI. Each expert is asked to select one paper from
their area. This covers domains including com-
puter vision, natural language processing, graph
machine learning, AI for science, and theoretical
machine learning. The assessment consists of two
components:

1. Experts are presented with the LLM-extracted
research plan alongside the original paper,
with instructions such as: “Please rate how

3https ://platform. openai.com/docs/models/
gpt-5


===== PAGE BREAK =====

accurately the LLM captured the content from
the original paper.” Ratings are provided on a
five-point Likert scale.

2. Experts are shown the extracted rubric ques-
tions and the corresponding research idea,
then asked to “assess whether the questions
cover the essential parts of a research plan for
the given idea” on a five-point Likert scale.

We present the expert evaluations in Table 2.
Across sections, mean scores exceed 4.0 out of 5
(scale anchors: 1 = major issues, 2 = significant
problems, 3 = average, 4 = acceptable with some
issues, 5 = well done overall). These scores suggest
that the extracted research plans and rubrics are of
reasonable quality (more details in §C).

3.5 Grading and LLM-based Judges

The rubric for each paper consists of five sections
and there is a list of yes/no questions to specify the
criteria that a research plan should satisfy. After
grading each question in a section, we compute
binary classification accuracy—the proportion of
rubric questions judged as satisfied. This accuracy
represents the Planning Score for that section. We
then take the macro-average of the five section ac-
curacies as the final Planning Score for this paper.
Finally, we compute the overall score by averaging
these paper-level scores across all papers, which
corresponds to a macro-average over papers. Our
main metric is therefore the Average Planning
Score across all papers.

Since manually grading each generated research
plan against our rubrics would be prohibitively ex-
pensive, we employ LLM-based judges (Zheng
et al., 2023; Starace et al., 2025). Specifically, we
prompt an LLM with the generated research plan
and the rubric, and instruct the LLM to generate
a grading (i.e., give a yes/no judgment for each

Table 2: Expert evaluation of the extracted research
plans and rubrics, rated on a five-point Likert scale (1 =
major issues, 5 = well done overall). The consistently
high scores (above 4.0) suggest that the extracted re-
search plans and rubrics are of reasonable quality.

Criterion        Generated Plan Generated Rubrics
Introduction       412+ 0.83            4.00 + 1.07
Related Lit.        4.00 + 0.76            4.12 + 0.99
Method            4.06 + 1.08            4.00 + 0.93
Experiments       4.25 + 0.71            4.50 + 0.76
Resource           4.50 + 0.76            4.62 + 0.52
Ethics               4.88 + 0.35            4.75 + 0.46

rubric question). We discuss how we assess the
quality of LLM-based judges in the next section.
We present the prompt templates in §D.

3.6 Evaluating LLM Judges with JudgeEval

We collect a dataset (Idea2Plan JudgeEval) to
assess the performance of different LLM judges
against human expert annotations. From our devel-
opment set of 30 papers, we randomly select five
papers and generate two research plans for each,
resulting in 10 generated research plans. For each
plan, we collect ground-truth annotations by man-
ually grading the plans against rubric questions.
Since each rubric question is a binary classification
task (a yes/no question), we report standard binary
classification metrics with macro-averaging to ag-
gregate performance across papers. We include a
random baseline where the judge assigns yes or no
randomly.

The results show that 04-mini achieves the high-
est Fl score (0.91), while GPT-4.1-mini offers the
most cost-effective option. We select 04-mini (rea-
soning=high) as our evaluation model to achieve
the best possible grading accuracy.

3.7 Baseline Choices and Agent Design

We evaluate several baselines for research plan gen-
eration, ranging from simple prompting to agentic
frameworks with external tools.

Naive Baseline. Our simplest baseline uses a
minimal prompt: “Your task is to generate a de-
tailed research plan based on the provided research
idea.” in addition to the desired structure (e.g.,
Methods, Initial Experimental Design) for the re-
search plan template to constrain the generation
format. Without the section titles, we observe that
some LLMs start to generate sections that are not

Table 3: Idea2Plan JudgeEval: Macro-averaged per-
formance of LLM judges against human annotations.
Metrics are computed per paper and macro-averaged
across papers. Cost denotes the average API cost ($) to
grade one plan against a rubric. The Random baseline
predicts labels uniformly at random.

Ace. Prec. Rec. Fl Cost ($)
Random         0.52    0.50    0.49 0.49       0
GPT-4.1-mini 0.89      0.89      0.90 0.89      0.0048
GPT-4.1              0.89      0.86      0.91 0.88       0.0800
04-mini              0.91      0.94      0.89 0.91       0.1606
GPT-5-mini        0.84      0.78      0.93 0.85       0.0268
GPT-5                0.89      0.92      0.85 0.88       0.1596


===== PAGE BREAK =====

in the grading rubrics (e.g., research timelines),
which may waste generation tokens and negatively
influence the performance of the naive baseline.
This baseline mimics the most straightforward sce-
nario where scientists directly prompt an LLM for
research plans without any additional instructions.

0-shot and 1-shot Baseline. We enhance the ba-
sic prompt with additional instructions. For the
0-shot, we add general guidance such as “Gener-
ate a complete research plan following the EXACT
template structure above.” The 1-shot additionally
includes one fixed research plan example to demon-
strate the desired format and content structure.

ReAct Agent Baseline. When developing re-
search ideas, human scientists often review related
literature and refine their plans by learning how
others have addressed similar problems. Inspired
by this, we evaluate an agentic framework that can
access external information. We use a simple Re-
Act scaffolding following the “Thought — Action
— Observation” loop (Yao et al., 2023). To simu-
late an environment where scientists interact with
the research literature, we introduce two tools inte-
grated with arXiv:
¢ ArXiv Search Tool with Blocklists. Queries
arXiv for relevant papers while preventing
data contamination by applying two layers
of blocklisting for each target paper: (1) ex-
clude all papers published after January 30,
2025 CML 2025 deadline), and (2) exclude
the target paper and all papers that cite it. The
blocklisting is necessary because our small-
scale experiments show that search engines
can retrieve the target paper when given its
research idea as a query. The tool takes an
agent-generated query as input and returns a
list of relevant arXiv papers with their titles,
abstracts, and arXiv identifiers. We imple-
ment it using Bing Search‘ restricted to the
arXiv domain.
¢ ArXiv Read Tool. Takes an arXiv identifier
as input and returns a summary of the corre-
sponding paper. We fetch the paper using the
arXiv API and then use 04-mini to generate
its summary with a fixed template that con-
tains main contributions, key related literature,
methods and techniques, experimental design
and results. In this way, we avoid using the

‘https://learn.microsoft.com/en-us/azure/
ai-foundry/agents/how- to/tools/bing- grounding
Shttps://info. arxiv.org/help/api/index. html

full content of a paper, as it would consume

too much context and, as shown in our small-

scale experiments, may reduce performance.
We limit the agent to at most five searches and five
reads. The agent is free to decide the order of tool
calls and whether to use all of the available calls.
Prompt templates and design details are in §E.

4 Experiments

4.1 Experimental setup

We test extensively on proprietary LLMs (GPT-
4.1 and GPT-4.1-mini,° 04-mini,’ GPT-5 and
GPT-5-mini,*) and open-source LLMs (DeepSeek-
V3 (DeepSeek-AI et al. 2024), DeepSeek-
R1 (DeepSeek-AI et al., 2025) and Phi-4 (Abdin
et al., 2024)). For each model and baseline configu-
ration, we run three independent trials per idea and
report the mean performance. We also include an
upper-bound, where 04-mini is prompted with the
original paper and asked to generate a research plan.
This serves as an estimate of the highest achievable
performance on Idea2Plan Bench.

4.2 Results Analysis

We present the average scores across all models and
settings in Figure 2 and the section-wise results for
naive and ReAct in Table 4. The full results are in
§F. We summarize the key findings below.

Naive baseline achieves strong performance.
Despite being simple, the naive baseline (one line
prompt with section titles) attains performance
close to the best methods across different models.

0-shot and 1-shot baselines improve over naive.
The O-shot and 1-shot baselines consistently out-
perform the naive baseline on all LLMs, demon-
strating that clear instruction and example provide
meaningful improvements. However, this perfor-
mance gap is notably smaller for GPT-5, suggesting
it has a stronger baseline capability that is less de-
pendent on explicit prompting strategies.

GPT-5 and GPT-5-mini lead the performance.
We notice that GPT-5 and GPT-5-mini substantially
outperform other models. To understand this better,
we compute pairwise win rates, i.e., for each pair of
models, the fraction of papers in which one model’s
research plan scores higher than the other. We find

®https://openai.com/index/gpt-4-1/

Thttps://openai.com/index/
introducing-03-and-04-mini/

Shttps://openai .com/gpt-5/


===== PAGE BREAK =====

0.9

0.8

Gg N: Naive
Gg 0-S: 0-Shot
Gg 1-S: 1-Shot
0.6} Gam R: ReAct
=== Upper Bound

0.7

0.5

0.4

0.3

0.2

0.1

0.819

0.0

N 0-S 1-S R                  N 0S 1-5 R

Phi-4

N 0S 1S R

N 0S 1-S R

DeepSeek-V3 DeepSeek-R1 = GPT-4.1-mini

N 0S 1S R

N 0S 1S R

N 0S 1S R

N 0-S 1-S R

GPT-4.1        04-mini       GPT-5-mini        GPT-5

Figure 2: Average Planning Score for LLMs under four baselines (Naive, 0-Shot, 1-Shot, ReAct). Scores are means
over three independent runs per paper. We also include an upper-bound in which 04-mini is given the original paper
to generate a plan, approximating the highest achievable performance on Idea2Plan Bench.

Table 4: Section-wise Planning Scores (%) for Naive and ReAct baselines. Each value represents the mean accuracy
across all papers. Results for 0-shot and 1-shot baselines are provided in Table 20.

Naive                          ReAct

Model

Intro   Lit Met Exp Res/Eth Avg | Intro   Lit Met Exp Res/Eth | Avg
Phi-4       29.0   5.6 20.2 10.9    30.9 19.3, 242 11.8 15.5   9.9    31.7 18.6
DeepSeek-V3 , 39.8 25.2 32.4 24.7    44.5, 33.3, 39.9 23.8 33.4 28.5    50.9 , 35.3
DeepSeek-R1 | 40.6 27.1 37.0 28.7    50.3 | 36.7 1 38.7 23.1 33.2 27.9    52.0 | 35.0
GPT-4.1-mini | 42.3. 21.8 347 274    47.2 | 34.7! 50.6 306 445 35.7    60.3 | 44.3
GPT-4.1 40.7 22.4 34.2 28.9    48.7 | 34.9 50.6 35.6 44.9 40.5    65.6 | 47.4
04-mini      50.8 29.1 50.2 39.1    59.7 45.7 , 54.2 36.1 544 42.6    61.2 , 49.6
GPT-5-mini | 58.5 381 65.9 51.6    69.5 | 56.7 ; 62.3 44.5 67.3 55.2    76.4 , 61.1
GPT-5                         60.9 47.7 70.9 56.7               73.8 | 62.0 1 61.4 48.1 684 55.7               75.9 | 61.9

that GPT-5 and GPT-5-mini achieve over 90% win
rates against other models, indicating consistent
superiority (see Figure 5).

Literature review is the most difficult section,
while method and experiment sections show
large differences across models. As shown in
Table 4, Planning Scores for the literature section
are the lowest across LLMs, reflecting the chal-
lenge of accurately identifying and integrating rele-
vant prior work. The method and experiment sec-
tions exhibit the largest differences across LLMs,
serving as clearer indicators of planning ability.
GPT-5 and GPT-5-mini perform strongly across
sections.

ReAct agent does not surpass simpler baselines.
Contrary to expectations, the ReAct agent does not
improve performance compared to simpler base-
lines. We find that models sometimes struggle to
intelligently filter retrieved information, and thus
incorporating irrelevant details that distract from
the research idea. We show an example in Ta-
ble 22. This highlights the longstanding challenge

of knowledge conflict between parametric and re-
trieved knowledge in language models (Xu et al.,
2024; Li et al., 2025a).

4.3 Length vs. Quality Analysis

Because our evaluation rubric measures coverage
of key elements in a research plan, longer outputs
may gain an advantage by mentioning more rubric-
relevant content. To examine this potential bias,
we analyze the relationship between output length
and Average Planning Score. We run two con-
trolled settings: a constrained generation setting
that limits response length and an expanded gen-
eration setting that allows more extensive output.
For each idea, we prompt models with a soft length
constraint (e.g., “Your response must be at most
<upper> words.”). We find that this simple ap-
proach effectively controls LLM’s verbosity.
When output length is controlled (Figure 3, left),
GPT-5 and GPT-5-mini still achieve the highest
Planning Scores, demonstrating strong robustness
to reduced verbosity. In the expanded-length set-
ting (Figure 3, right), we test GPT-4.1 and 04-mini


===== PAGE BREAK =====

Constrained Generation

Satsuma | | |

Oo

ogpt-5

0.6                                                                                      An
cgPt-5-mini

0.5
t-5,mini (constrained)
ba CoA mins

0.4
ogpt-4.1

0.3

0.2

750      1000     1250     1500     1750     2000     2250

Expanded Generation

ogPt-5
cgPt-5-mini

©
noe
Qo4-mini                                        coy
gpt-4.1 (expanded)
>   o9P’ -4.1

0.6

0.5

0.4

0.3

0.2

500           1000          1500          2000          2500

Average Plan Length (words)

© gpt-5-mini
O = gpt-5-mini (constrained)

OQ 04-mini
> 04-mini (expanded)

@ = gpt-5
 ss gpt-5 (constrained)

O  gpt-4.1
e gpt-4.1 (expanded)

gpt-4.1-mini
phi-4

© deepseek-r1
> deepseek-v3

Figure 3: Relationship between plan length and Average Planning Score under constrained and expanded generation
settings. Left: When response length is controlled, GPT-5 and GPT-5-mini generate shorter plans but remain the
top performers under stricter length constraints. Right: When longer outputs are allowed, GPT-4.1 and 04-mini
produce more extensive plans with higher scores. Arrows indicate the shift from the original to the constrained or

expanded setting.

0.3%

3.8%
LZZZZZZZZA,

0.6

0.5                                                                            2.2%

a                  3.5%      6.2%       9.9%
7.0%

0.3

0.2

DS-R1_ GPT-4.1-mini_ GPT-4.1
Model

DS-v3                                        O4-mini GPT-5-mini = GPT-5

Figure 4: Impact of curated literature on average Plan-
ning Score across LLMs. Bars show baseline perfor-
mance (solid) and the improvement with curated litera-
ture (hatched), which enhances plan quality across all
models. DS denotes DeepSeek.

and observe that both models benefit from produc-
ing longer outputs. Despite these gains, GPT-5
consistently remains the top performer, suggesting
that its advantage comes from higher plan quality.

4.4 Enhanced Context Experiment

For ReAct, we observe that the retrieved papers
are sometimes only loosely relevant or even mis-
leading, resulting in degraded research plans (case
study in Appendix Table 22). Thus, we hypothesize
that ReAct agents underperform due to low-quality
retrievals. To test this, we design an alternative
setup where models receive up to three carefully
curated, directly relevant papers—mimicking real-
world scenarios in which researchers start with a
few known reference papers.

As shown in Figure 4, curated literature leads

to consistent gains across all models, with im-
provements ranging from 0.3% to 9.9%. Per-
section results are reported in Table 24 (Ap-
pendix §I). Mid-tier models (e.g., GPT-4.1) benefit
most—especially in the introduction and related
work—indicating that external grounding compen-
sates for weaker prior knowledge. In contrast, GPT-
5 already performs strongly and shows mixed ef-
fects, suggesting curated knowledge is most helpful
for models with limited internal understanding.

4.5 Potential Training Strategy

A natural progression towards improving the re-
search planning capabilities of LLMs is to train
models using idea—plan pairs derived from pub-
lished research papers. We conduct supervised
fine-tuning (SFT) of GPT-4.1-mini and GPT-4.1
on 2,000 idea—plan pairs extracted from randomly
sampled ICML 2024 papers, and we evaluate their
performance against the corresponding off-the-
shelf models in a 1-shot scenario. As discussed
in Appendix §8J, the SFT models exhibit a decrease
in overall performance (Table 25). We find that
the obtained fine-tuned models tend to hallucinate
more, especially in the literature review sections.
We offer some potential explanations and suggest
future directions of research in Appendix §J.

5 Conclusion

We conducted an investigation into automating the
conversion of scientific ideas into research plans,
assessing how various LLMs handle this task. We
proposed a framework that leverages published aca-


===== PAGE BREAK =====

demic papers to extract paired ideas and plans for
model assessment. Through comparative analysis
of multiple LLMs using standard and agent-based
frameworks, we helped establish a foundation for
advancement in automated research planning.

6 Limitations

Our study focuses on AI research planning tasks
derived from ICML 2025 Spotlight or Oral papers,
which may limit generalizability to other scien-
tific domains or earlier-stage research ideas. Addi-
tionally, while we blocklisted post-cutoff and cited
papers to mitigate data contamination, we cannot
completely rule out partial memorization from over-
lapping text sources. Finally, all evaluations rely on
LLM-based judges, which—despite strong agree-
ment with human assessments—may still introduce
systematic biases.

7 Ethical Considerations

Our framework operates on publicly available re-
search papers and adheres to fair-use principles.
Generated plans are intended for research analy-
sis, not for automated publication or deployment.
We recognize the potential misuse of AI systems
for producing unverified or plagiarized scientific
outputs and emphasize that our work aims to bench-
mark research planning capability, not to replace
human scientific judgment. We also ensure that no
personally identifiable or sensitive data is used in
our datasets.

References

Marah I Abdin, Jyoti Aneja, Harkirat S. Behl, Sébastien
Bubeck, Ronen Eldan, Suriya Gunasekar, Michael
Harrison, Russell J. Hewett, Mojan Javaheripi, Piero
Kauffmann, James R. Lee, Yin Tat Lee, Yuanzhi Li,
Weishung Liu, Caio C. T. Mendes, Anh Nguyen,
Eric Price, Gustavo de Rosa, Olli Saarikivi, and
8 others. 2024. Phi-4 technical report. CoRR,
abs/2412.08905.

Constructions Aeronautiques, Adele Howe, Craig
Knoblock, ISI Drew McDermott, Ashwin Ram,
Manuela Veloso, Daniel Weld, David Wilkins Sri,
Anthony Barrett, Dave Christianson, and 1 others.
1998. Pddl—the planning domain definition lan-
guage. Technical Report, Tech. Rep.

Lakshya A. Agrawal, Shangyin Tan, Dilara Soylu,
Noah Ziems, Rishi Khare, Krista Opsahl-Ong, Ar-
nav Singhvi, Herumb Shandilya, Michael J. Ryan,
Meng Jiang, Christopher Potts, Koushik Sen, Alexan-
dros G. Dimakis, Ion Stoica, Daniel Klein, Matei

Zaharia, and Omar Khattab. 2025. GEPA: reflec-
tive prompt evolution can outperform reinforcement
learning. CoRR, abs/2507.19457.

Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan,
and Sung Ju Hwang. 2025. Researchagent: Iterative
research idea generation over scientific literature with
large language models. In Proceedings of the 2025
Conference of the Nations of the Americas Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, NAACL 2025 - Volume
1: Long Papers, Albuquerque, New Mexico, USA,
April 29 - May 4, 2025, pages 6709-6738. Associa-
tion for Computational Linguistics.

Nicholas Carlini, Daphne Ippolito, Matthew Jagielski,
Katherine Lee, Florian Tramer, and Chiyuan Zhang.
2022. Quantifying memorization across neural lan-
guage models. arXiv preprint arXiv:2202.07646.

Nicholas Carlini, Florian Tramer, Eric Wallace,
Matthew Jagielski, Ariel Herbert-Voss, Katherine
Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar
Erlingsson, and 1 others. 2021. Extracting training
data from large language models. In 30th USENIX
Security Symposium (USENIX Security 21), pages
2633-2650.

Hui Chen, Miao Xiong, Yujie Lu, Wei Han, Ailin
Deng, Yufei He, Jiaying Wu, Yibo Li, Yue Liu, and
Bryan Hooi. 2025a. Mlr-bench: Evaluating ai agents
on open-ended machine learning research. arXiv
preprint arXiv:2505.19955.

Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang,
Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen
Wei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N.
Baker, Benjamin Burns, Daniel Adu-Ampratwum,
Xuhui Huang, Xia Ning, Song Gao, Yu Su, and Huan
Sun. 2025b. Scienceagentbench: Toward rigorous
assessment of language agents for data-driven sci-
entific discovery. In The Thirteenth International
Conference on Learning Representations, ICLR 2025,
Singapore, April 24-28, 2025. OpenReview.net.

Dwip Dalal, Gautam Vashishtha, Utkarsh Mishra,
Jeonghwan Kim, Madhav Kanda, Hyeonjeong Ha,
Svetlana Lazebnik, Heng Ji, and Unnat Jain. 2025.
Constructive distortion: Improving mllms with
attention-guided image warping. arXiv preprint
arXiv:2510.09741.

DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang,
Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,
Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang,
Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhi-
hong Shao, Zhuoshu Li, Ziyi Gao, and 81 others.
2025. Deepseek-r1: Incentivizing reasoning capa-
bility in Ilms via reinforcement learning. CoRR,
abs/2501.12948.

DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingx-
uan Wang, Bochao Wu, Chengda Lu, Chenggang
Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan,
Damai Dai, Daya Guo, Dejian Yang, Deli Chen,


===== PAGE BREAK =====

Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai,
and 80 others. 2024. Deepseek-v3 technical report.
CoRR, abs/2412.19437.

Ben Finkelshtein, Ismail [Ikan Ceylan, Michael M.
Bronstein, and Ron Levie. 2025. Equivariance ev-
erywhere all at once: A recipe for graph foundation
models. CoRR, abs/2506.14291.

Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal,
Amir Feder, Roi Reichart, and Jonathan Herzig. 2024.
Does fine-tuning llms on new knowledge encourage
hallucinations? In Proceedings of the 2024 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP 2024, Miami, FL, USA, Novem-
ber 12-16, 2024, pages 7765-7784. Association for
Computational Linguistics.

Alireza Ghafarollahi and Markus J. Buehler. 2024. Sci-
agents: Automating scientific discovery through
multi-agent intelligent graph reasoning. CoRR,
abs/2409.05556.

Malik Ghallab, Dana Nau, and Paolo Traverso. 2004.
Automated Planning: theory and practice. Elsevier.

Juraj Gottweis, Wei-Hung Weng, Alexander N. Daryin,
Tao Tu, Anil Palepu, Petar Sirkovic, Artiom
Myaskovsky, Felix Weissenberger, Keran Rong, Ryu-
taro Tanno, Khaled Saab, Dan Popovici, Jacob Blum,
Fan Zhang, Katherine Chou, Avinatan Hassidim, Bu-
rak Gokturk, Amin Vahdat, Pushmeet Kohli, and 15
others. 2025. Towards an AI co-scientist. CoRR,
abs/2502.18864.

Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie,
Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su.
2025. Navigating the digital world as humans do:
Universal visual grounding for GUI agents. In The
Thirteenth International Conference on Learning
Representations, ICLR 2025, Singapore, April 24-
28, 2025. OpenReview.net.

Patrik Haslum, Nir Lipovetzky, Daniele Magazzeni,
Christian Muise, Ronald Brachman, Francesca Rossi,
and Peter Stone. 2019. An introduction to the
planning domain definition language, volume 13.
Springer.

Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Li-
hong Li, Li Deng, and Mari Ostendorf. 2015. Deep
reinforcement learning with a natural language action
space. arXiv preprint arXiv: 1511.04636.

Yichen He, Guanhua Huang, Peiyuan Feng, Yuan Lin,
Yuchen Zhang, Hang Li, and Weinan E. 2025. Pasa:
An LLM agent for comprehensive academic paper
search. In Proceedings of the 63rd Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), ACL 2025, Vienna, Aus-
tria, July 27 - August 1, 2025, pages 11663-11679.
Association for Computational Linguistics.

10

Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei
Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruim-
ing Tang, and Enhong Chen. 2024. Understand-
ing the planning of LLM agents: A survey. CoRR,
abs/2402.02716.

Peter Jansen, Oyvind Tafjord, Marissa Radensky, Pao
Siangliulue, Tom Hope, Bhavana Dalvi Mishra, Bod-
hisattwa Prasad Majumder, Daniel S. Weld, and Pe-
ter Clark. 2025. Codescientist: End-to-end semi-
automated scientific discovery with code-based ex-
perimentation. In Findings of the Association for
Computational Linguistics, ACL 2025, Vienna, Aus-
tria, July 27 - August 1, 2025, pages 13370-13467.
Association for Computational Linguistics.

Priyanka Kargupta, Ishika Agarwal, Tal August, and
Jiawei Han. 2025. Tree-of-debate: Multi-persona
debate trees elicit critical thinking for scientific com-
parative analysis. In Proceedings of the 63rd An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), ACL 2025, Vi-
enna, Austria, July 27 - August I, 2025, pages 29378-
29403. Association for Computational Linguistics.

Jon Kleinberg, Jens Ludwig, Sendhil Mullainathan, and
Ziad Obermeyer. 2015. Prediction policy problems.
American Economic Review, 105(5):491—495.

Patrick Tser Jern Kon, Jiachen Liu, Qiuyi Ding, Yim-
ing Qiu, Zhenning Yang, Yibo Huang, Jayanth Srini-
vasa, Myungjin Lee, Mosharaf Chowdhury, and Ang
Chen. 2025a. Curie: Toward rigorous and automated
scientific experimentation with AI agents. CoRR,
abs/2502. 16069.

Patrick Tser Jern Kon, Jiachen Liu, Xinyi Zhu, Qiuyi
Ding, Jingjia Peng, Jiarong Xing, Yibo Huang, Yim-
ing Qiu, Jayanth Srinivasa, Myungjin Lee, and 1 oth-
ers. 2025b. Exp-bench: Can ai conduct ai research
experiments? arXiv preprint arXiv:2505.24785.

Gaotang Li, Yuzhong Chen, and Hanghang Tong. 2025a.
Taming knowledge conflicts in language models.
CoRR, abs/2503.10996.

Sihang Li, Jin Huang, Jiaxi Zhuang, Yaorui Shi, Xi-
aochen Cai, Mingjun Xu, Xiang Wang, Linfeng
Zhang, Guolin Ke, and Hengxing Cai. 2025b. Scil-
itIIm: How to adapt IIms for scientific literature under-
standing. In The Thirteenth International Conference
on Learning Representations, ICLR 2025, Singapore,
April 24-28, 2025. OpenReview.net.

Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob N. Fo-
erster, Jeff Clune, and David Ha. 2024. The AI scien-
tist: Towards fully automated open-ended scientific
discovery. CoRR, abs/2408.06292.

Allen Newell, John Calman Shaw, and Herbert A Si-
mon. 1958. Elements of a theory of human problem
solving. Psychological review, 65(3):151.

Kristina Nikolic, Luze Sun, Jie Zhang, and Florian
Tramér. 2025. The jailbreak tax: How useful are
your jailbreak outputs? CoRR, abs/2504.10694.


===== PAGE BREAK =====

Samuel Schmidgall and Michael Moor. 2025. Agen-
trxiv: Towards collaborative autonomous research.
CoRR, abs/2503.18102.

Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng
Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng
Liu, and Emad Barsoum. 2025. Agent laboratory:
Using LLM agents as research assistants. CoRR,
abs/2501.04227.

John Schulman, Filip Wolski, Prafulla Dhariwal,
Alec Radford, and Oleg Klimov. 2017. —Proxi-
mal policy optimization algorithms. arXiv preprint
arXiv: 1707.06347.

Minju Seo, Jinheon Baek, Seongyun Lee, and Sung Ju
Hwang. 2025. Paper2code: Automating code gen-
eration from scientific papers in machine learning.
CoRR, abs/2504.17192.

Chenglei Si, Diyi Yang, and Tatsunori Hashimoto. 2025.
Can Ilms generate novel research ideas? A large-
scale human study with 100+ NLP researchers. In
The Thirteenth International Conference on Learning
Representations, ICLR 2025, Singapore, April 24-28,
2025. OpenReview.net.

Tom Silver, Soham Dan, Kavitha Srinivas, Joshua B.
Tenenbaum, Leslie Pack Kaelbling, and Michael
Katz. 2024. Generalized planning in PDDL do-
mains with pretrained large language models. In
Thirty-Eighth AAAI Conference on Artificial Intelli-
gence, AAAI 2024, Thirty-Sixth Conference on Inno-
vative Applications of Artificial Intelligence, IAAI
2024, Fourteenth Symposium on Educational Ad-
vances in Artificial Intelligence, EAAI 2014, Febru-
ary 20-27, 2024, Vancouver, Canada, pages 20256—
20264. AAAI Press.

Amanpreet Singh, Joseph Chee Chang, Chloe Anas-
tasiades, Dany Haddad, Aakanksha Naik, Amber
Tanaka, Angele Zamarron, Cecile Nguyen, Jena D.
Hwang, Jason Dunkleberger, Matt Latzke, Smita Rao,
Jaron Lochner, Rob Evans, Rodney Kinney, Daniel S.
Weld, Doug Downey, and Sergey Feldman. 2025.
Ai2 scholar QA: organized literature synthesis with
attribution. CoRR, abs/2504.10861.

Yuda Song, Hanlin Zhang, Carson Eisenach, Sham M.
Kakade, Dean P. Foster, and Udaya Ghai. 2025. Mind
the gap: Examining the self-improvement capabili-
ties of large language models. In The Thirteenth In-
ternational Conference on Learning Representations,
ICLR 2025, Singapore, April 24-28, 2025. OpenRe-
view.net.

Giulio Starace, Oliver Jaffe, Dane Sherburn, James
Aung, Jun Shern Chan, Leon Maksin, Rachel Dias,
Evan Mays, Benjamin Kinsella, Wyatt Thompson,
Johannes Heidecke, Amelia Glaese, and Tejal Pat-
wardhan. 2025. Paperbench: Evaluating ai’s ability
to replicate Al research. CoRR, abs/2504.01848.

K Sudheesh, Devika Rani Duggappa, and SS Nethra.
2016. How to write a research proposal? Indian
journal of anaesthesia, 60(9):63 1-634.

11

David Wadden, Kejian Shi, Jacob Morrison, Aakanksha
Naik, Shruti Singh, Nitzan Barzilay, Kyle Lo, Tom
Hope, Luca Soldaini, Shannon Zejiang Shen, Doug
Downey, Hannaneh Hajishirzi, and Arman Cohan.
2024. Sciriff: A resource to enhance language model
instruction-following over scientific literature. CoRR,
abs/2406.07835.

Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi
Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023. Plan-
and-solve prompting: Improving zero-shot chain-
of-thought reasoning by large language models. In
Proceedings of the 61st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), ACL 2023, Toronto, Canada, July 9-14,
2023, pages 2609-2634. Association for Computa-
tional Linguistics.

Qingyun Wang, Doug Downey, Heng Ji, and Tom Hope.
2024a. Scimon: Scientific inspiration machines op-
timized for novelty. In Proceedings of the 62nd An-
nual Meeting of the Association for Computational
Linguistics (Volume I: Long Papers), ACL 2024,
Bangkok, Thailand, August 11-16, 2024, pages 279-
299. Association for Computational Linguistics.

Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang,
Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai,
Min Zhang, Qingsong Wen, Wei Ye, Shikun Zhang,
and Yue Zhang. 2024b. Autosurvey: Large lan-
guage models can automatically write surveys. In
Advances in Neural Information Processing Systems
38: Annual Conference on Neural Information Pro-
cessing Systems 2024, NeurIPS 2024, Vancouver, BC,
Canada, December 10 - 15, 2024.

Robert J Weber and Daniel J Cobaugh. 2008. Develop-
ing and executing an effective research plan. Ameri-
can journal of health-system pharmacy, 65(21):2058—
2065.

Hui Wei, Zihao Zhang, Shenghua He, Tian Xia, Shijia
Pan, and Fei Liu. 2025. Plangenllms: A modern
survey of LLM planning capabilities. In Proceedings
of the 63rd Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
ACL 2025, Vienna, Austria, July 27 - August 1, 2025,
pages 19497-19521. Association for Computational
Linguistics.

Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M. Dai, and Quoc V. Le. 2022. Finetuned
language models are zero-shot learners. In The Tenth
International Conference on Learning Representa-
tions, ICLR 2022, Virtual Event, April 25-29, 2022.
OpenReview.net.

Rongwu Xu, Zehan Qi, Zhijiang Guo, Cunxiang Wang,
Hongru Wang, Yue Zhang, and Wei Xu. 2024.
Knowledge conflicts for lms: A survey. In Proceed-
ings of the 2024 Conference on Empirical Methods in
Natural Language Processing, EMNLP 2024, Miami,
FL, USA, November 12-16, 2024, pages 8541-8565.
Association for Computational Linguistics.


===== PAGE BREAK =====

Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shen-
gran Hu, Chris Lu, Jakob N. Foerster, Jeff Clune, and
David Ha. 2025. The AI scientist-v2: Workshop-
level automated scientific discovery via agentic tree
search. CoRR, abs/2504.08066.

Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
Shafran, Karthik R. Narasimhan, and Yuan Cao. 2023.
React: Synergizing reasoning and acting in language
models. In The Eleventh International Conference
on Learning Representations, ICLR 2023, Kigali,
Rwanda, May 1-5, 2023. OpenReview.net.

Jiakang Yuan, Xiangchao Yan, Botian Shi, Tao Chen,
Wanli Ouyang, Bo Zhang, Lei Bai, Yu Qiao, and
Bowen Zhou. 2025. Dolphin: Closed-loop open-
ended auto-research through thinking, practice, and
feedback. arXiv preprint arXiv:2501.03916.

Dan Zhang, Ziniu Hu, Sining Zhoubian, Zhengxiao Du,
Kaiyu Yang, Zihan Wang, Yisong Yue, Yuxiao Dong,
and Jie Tang. 2024. Sciinstruct: a self-reflective
instruction annotated dataset for training scientific
language models. In Advances in Neural Information
Processing Systems 38: Annual Conference on Neu-
ral Information Processing Systems 2024, NeurIPS
2024, Vancouver, BC, Canada, December 10 - 15,
2024.

Xingjian Zhang, Yutong Xie, Jin Huang, Jinge Ma,
Zhaoying Pan, Qijia Liu, Ziyang Xiong, Tolga Er-
gen, Dongsub Shim, Honglak Lee, and Qiaozhu
Mei. 2025. MASSW: A new dataset and benchmark
tasks for ai-assisted scientific workflows. In Find-
ings of the Association for Computational Linguistics:
NAACL 2025, Albuquerque, New Mexico, USA, April
29 - May 4, 2025, pages 2373-2394. Association for
Computational Linguistics.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang,
Joseph E. Gonzalez, and Ion Stoica. 2023. Judging
llm-as-a-judge with mt-bench and chatbot arena. In
Advances in Neural Information Processing Systems
36: Annual Conference on Neural Information Pro-
cessing Systems 2023, NeurIPS 2023, New Orleans,
LA, USA, December 10 - 16, 2023.

A Full Example of Jailbreak-Tax (Nikolic
et al., 2025) paper’s Rubric and
Research Plan

We present the full example in Table 1, includ-
ing the grading rubric for the Jailbreak-Tax paper
and the research plan generated by GPT-5 (Nikolic
et al., 2025). At the end of the paper, Tables 26—29
show the complete evaluation rubric across four
parts, while Tables 30-31 present the full research
plan generated by GPT-5.

12

B_ Dataset Construction

B.1 Research Plan and Idea Extraction

We provide the complete prompt templates used for
extracting research plans and ideas in our dataset
construction pipeline:

¢ Research Plan Template: The Research Plan
Template (Table 5) defines the structured for-
mat for generating research plans, covering in-
troduction, literature review, methods, experi-
mental design, and resource considerations.

Idea Extraction: The Research Idea Extrac-
tion Prompt (Table 6) transforms paper ab-
stracts into concise, first-person research ideas
that focus on the proposed approach rather
than results.

Research Plan Extraction: The Research Plan
Extraction Prompt (Table 7) extracts detailed
research plans from full papers following the
Research Plan Template.

B.2. Rubric Generation

The Rubric Generation Prompt (Table 8) instructs
the model to create evaluation criteria in JSON
format for assessing research plans. Since this
prompt is long, we separate it into two components
that are plugged into the placeholders:

¢ Section-by-Section Guidance: (Table 9) pro-
vides detailed instructions for generating con-
sistent rubric questions across all sections.

¢ Low-quality vs. High-quality Question Exam-
ples: (Table 10) illustrates common pitfalls
and demonstrates how to formulate clear, gen-
eralizable evaluation criteria.

C_ Expert Evaluation of Reference
Research Plan and Rubric

Eight papers were selected by the expert evalua-
tors, each from their respective research domain.
The papers span diverse areas of AI and Machine
Learning (ML) research:

¢ AT4Science. The AI Scientist (Lu et al., 2024)

¢ Natural Language Processing and AI
Agents. GEPA (Agrawal et al., 2025),
PaSa (He et al., 2025), Tree-of-Debate (Kar-
gupta et al., 2025)


===== PAGE BREAK =====

Table 5: Research Plan Template

Research Plan Template

## 1. Introduction

### 1.1 Background

### 1.2 Primary Objectives

#44 1.3 Research Questions

State the core research questions for this research project.
## 2. Key Literatures

current research plan.

## 3. Methods

## 4. Initial Experimental Design

## 5. Resources, Compliance, and Ethical Considerations
### 5.1 Resource Requirements
API calls, and human annotation costs.

### 5.2 Ethical and Compliance Considerations
Address data privacy, safety, and approval needs.

Describe the current limitations or challenges in the field and why they matter now. This should be one paragraph.

List specific, measurable goals that define project success. These should align with the contributions typically outlined in a paper’s introduction.

Identify key related works and relevant domains that inform your research. Begin by listing a few key domains, and for each cited work, briefly explain why it is important to your

Describe the core techniques, model architectures, and/or training strategies to be used. This should be aligned with the method section in a paper.

Describe the high-level design of your first experiments. This should align with the experiments section of a paper.

List and estimate the resources needed to carry out your research. Focus on the most informative metrics that impact the budget and feasibility, such as GPU hours, token usage for

¢ Graph ML. Equivariance Everywhere All At
Once (Finkelshtein et al., 2025)

¢ Computer Vision. UGround (Gou et al.,
2025), AttWarp (Dalal et al., 2025).

¢ Theoretical ML. Mind the Gap (Song et al.,
2025)

C.1 Guideline to Experts

We provide comprehensive guidelines to expert an-
notators for evaluating both the LLM-generated
research plans and rubric questions. The complete
evaluation guidelines are included in Table 11 and
Table 12, which detail the rating scales and evalua-
tion criteria for each section.

C.2. Characteristics of Annotators

We recruit eight volunteer annotators who are ex-
perts in AI. All annotators have specialization in
AI and natural language processing. Our annota-
tor team consists of researchers with the following
demographic composition: 75% from Asia, 12.5%
from the Middle East, and 12.5% from the United
States. Each annotator independently evaluated
assigned papers following the guideline. Our anno-
tators are informed that their provided data would
be used solely for research purposes in this study.

D LLM-based Judge

We use an LLM-as-a-judge approach (Zheng et al.,
2023) to evaluate research plans against the gen-
erated rubrics, with the Rubric Evaluation Prompt

13

(Table 13) guiding the model to perform assess-
ment with strict interpretation of rubric criteria. We
use 04-mini (reasoning=high) throughout the study.
Because grading an entire plan in one API call can
exceed input limits and cause failures, we instead
evaluate each section separately—making five API
requests per plan (one for each section)—and then
aggregate the section scores to obtain the final over-
all score.

E_ Baseline and Agent Design

E.1 Prompting Baselines

We design three prompting baselines for research
plan generation:

¢ Naive Baseline: The Naive Baseline Prompt
(Table 14) provides minimal instructions with
only the research idea and template structure.

¢ 0-shot Baseline: The 0-shot Baseline Prompt
(Table 15) adds explicit generation instruc-
tions to guide the model through the planning
process.

¢ [-shot Baseline: The 1-shot Baseline Prompt
(Table 16) includes the same instructions as
the 0-shot baseline, with one example research
plan.

E.2. ReAct Agent

The ReAct Agent Prompt (Table 17) implements
an iterative reasoning framework that enables the
agent to systematically gather information through


===== PAGE BREAK =====

Table 6: Research Idea Extraction Prompt

Research Idea Extraction Prompt
Extract the core research idea from a paper abstract. Transform the abstract into a concise, intuitive research idea that focuses on the proposed approach, not the results.

## Task

Convert the abstract into a research idea using first-person language that captures:
- The problem we aim to solve

- Our proposed method/approach/dataset, etc.

- What makes it novel

## Instructions

- Remove the proposed method or dataset names (e.g., replace "HumanEval" with "a code generation benchmark")
- Remove experimental results and performance claims

- Keep the core technical approach and methodology

- Use first-person planning language (We propose, We aim, We will)

- Stay concise and intuitive

## Example

**Input Abstract:**

"Scientific literature understanding is crucial for extracting targeted information and garnering insights, thereby significantly advancing scientific discovery. Despite the remarkable
success of Large Language Models (LLMs), they face challenges in scientific literature understanding, primarily due to (1) a lack of scientific knowledge and (2) unfamiliarity with
specialized scientific tasks. To develop an LLM specialized in scientific literature understanding, we propose a hybrid strategy that integrates continual pre-training (CPT) and
supervised fine-tuning (SFT), to simultaneously infuse scientific domain knowledge and enhance instruction-following capabilities for domain-specific tasks. In this process, we
identify two key challenges: (1) constructing high-quality CPT corpora, and (2) generating diverse SFT instructions. We address these challenges through a meticulous pipeline,
including PDF text extraction, parsing content error correction, quality filtering, and synthetic instruction creation. Applying this strategy, we present a suite of LLMs: SciLitLLM,
specialized in scientific literature understanding. These models demonstrate promising performance on scientific literature understanding benchmarks."

**Output Research Idea:**
We propose a hybrid strategy that integrates continual pre-training (CPT) and supervised fine-tuning (SFT) to develop LLMs specialized in scientific literature understanding,
addressing the challenges of lack of scientific knowledge and unfamiliarity with specialized scientific tasks. We aim to develop a pipeline for constructing high-quality CPT corpora

## Input Abstract
{ {ABSTRACT} }

## Output

and generating diverse SFT instructions through PDF text extraction, parsing error correction, and quality filtering.

Write the research idea using first-person style, focusing on the approach and novelty while removing experimental results and work names.

tool use before generating the research plan (Yao
et al., 2023). The agent has access to two tools spec-
ified in Table 18: search_papers for finding rele-
vant academic papers using Bing Custom Search,
and read_paper for retrieving and analyzing spe-
cific papers by their arXiv ID. These tool specifica-
tions (referred to as {{TOOL_SPECIFICATIONS }}
in the prompt) and tool names (referred to as
{{TOOL_NAMES}} in the prompt) are provided to
the agent to enable systematic information gather-
ing.

The search_papers tool returns the top 10 most
relevant papers, each including its arXiv ID, title,
and abstract. When the agent calls read_paper, it
uses the Paper Summarization Prompt (Table 19)
to produce structured summaries of each retrieved
paper. This prompt guides the model to extract a
paper’s main contributions, key related literature,
methods and techniques, and experimental design
and results.

F_ Full Experimental Results

Table 20 and Table 21 present detailed section-wise
results across all models and prompting settings.
For each research idea, we run every configuration
three times. The mean results report the average
score over the three runs, while the max results
report the best-performing run among them. Con-

14

sistent trends appear across both metrics: GPT-5
achieves the highest scores under all prompting se-
tups, followed by GPT-5-mini and 04-mini. 1-shot
prompting provides a small but steady improve-
ment over 0-shot, indicating that a single example
helps models organize their research plans more
coherently. In contrast, the ReAct setup does not
outperform simpler prompting strategies.

G_ Win Rate between LLMs

Figure 5 shows pairwise win rates under four
prompting setups. GPT-5 consistently leads across
naive, 0-shot, 1-shot, and ReAct settings, though
gains narrow in the ReAct case.

H_ Case Study: Knowledge Conflict in
ReAct Agent

Table 22 illustrates a case where the ReAct agent
retrieves papers that conflict with its parametric
knowledge, leading to less relevant citations com-
pared to the baseline. While the agent cites specific
technical papers on empirical welfare maximiza-
tion, the baseline correctly identifies the founda-
tional “Prediction Policy Problems” paper (Klein-
berg et al., 2015) from its training data, demon-
strating that retrieval can introduce noise when it
does not align with the model’s existing domain
knowledge.


===== PAGE BREAK =====

Table 7: Research Plan Extraction Prompt

Research Plan Extraction Prompt

examples in your final output.

// begin of research plan definition with examples

{ {RESEARCH_PLAN_DEFINITION } }

// end of research plan definition

** Additional notes when extracting the research plan:**

- Do not ine
Never add

- **Primary
Focus on the most emphasized goals.

- **Research Questions:** Ensure each question is distinct and non-overlapping.

theoretical foundations**. For each important work:
- Extract the full title and explain its specific contribution to the current research

baseline methods, or core datasets used
- Prioritize works that the authors:
- Compare their approach directly against (baseline methods)
- Build upon or extend (foundational methods)
- Use for evaluation (benchmark datasets)
- Cite as inspiration for their core methodology

- Focus on works discussed in detail rather than brief mentions

- **Methodology:** Provi

number of tokens consumed, costs relate

- **Ethics:** **Only inclu

- In the research plan, use action-oriente
Here is the paper:

// begin of the paper

{ {FULL_PAPER_TEXT}}

// end of the paper

Now give the extracted research plan, according to the research plan definition.

Given the following research paper, extract a detailed research plan. Here is the definition of a research plan with some examples. Do not include the definition or any of the

ude any information that is not explicitly stated in the paper. When listing items (metrics, datasets, models, techniques), include every such item mentioned in the paper.
tails that is similar to the examples provided in the research plan definition, but not mentioned in the paper.

Objectives:** Identify the main goals of the research based on the Introduction section. Limit to a maximum of 5 objectives unless the paper clearly defines more.

- **Related Literature:** Focus on identifying the most **foundational and influential** previous works that directly inspired this research or provide **key baselines, datasets, or

- **Pay special attention to papers cited and discussed in the Methods and Experiments sections** - these are often the most critical works as they represent direct comparisons,

- Explain how each cited work contributes - whether as comparative baselines, theoretical insights, datasets, or methodological foundations

le a detailed and logically structured description of the methods used. Ensure alignment with the stated objectives and research questions.

- **Resource Requirements:** Extract any explicitly stated resource requirements mentioned in the paper (e.g., minimum number of GPUs needed, total GPU hours used, total
to human annotation). Report only the order of magnitude rather than exact numbers. All resource-related details mentioned in the paper
must be extracted. Do not make any guess on the resources if the paper does not specify resources.

le ethical considerations that are directly discussed in the paper.** Avoid speculative or generic concerns. Keep this section concise and focused.

, future-tense language that describes what will be done, not what was found.

I Enhanced Context Experiment

To investigate whether providing curated related
literature improves research plan quality, we design
an experiment where models are given three key
foundational papers selected by the Literature Rec-
ommendation Prompt (Table 23). Table 24 shows
the section-wise performance comparison with and
without curated literature across all models. The
results demonstrate that providing relevant founda-
tional papers leads to modest improvements in over-
all performance for most models, with the largest
gains observed in the Introduction and Literature
sections.

J SFT Experiment

Supervised fine-tuning (SFT) has been a com-
mon strategy to enhance LLMs for specialized
domains (Wei et al., 2022). Recent studies show
that SFT on scientific data can improve instruction-
following ability of LLMs (Zhang et al., 2024;

15

Wadden et al., 2024; Li et al., 2025b). However,
Gekhman et al. (2024) show that fine-tuning on
factual knowledge absent from an LLM’s pretrain-
ing corpus can increase the tendency to hallucinate.
For the /dea2Plan task, one natural source of high-
quality supervision is the corpus of published re-
search papers. We can extract research ideas with
their corresponding research plans from these pa-
pers, which are appealing as potential training data.
To examine whether fine-tuning on research papers
can improve LLM’s performance on /dea2Plan,
we fine-tune GPT-4.1-mini and GPT-4.1 using re-
search papers from ICML 2024. From this corpus,
we extract 2,000 idea—plan pairs, which serve as
our training set. We then fine-tune GPT-4. 1-mini
and GPT-4.1 on this data using the Azure finetun-
ing API’ and compare their performance against
their base versions in the 1-shot setting.

Table 25 summarizes the results. SFT leads to a

*https ://learn.microsoft.com/en-us/azure/
ai-foundry/concepts/fine-tuning-overview


===== PAGE BREAK =====

DeepSeek-R1 -

0.00

DeepSeek-V3

GPT-4.1

GPT-4.1-mini

GPT-5

GPT-5-mini

04-MINI

Phi-4

DeepSeek-R1 -

0.00

DeepSeek-V3 -

GPT-4.1

GPT-4.1-mini

GPT-5

GPT-5-mini

04-MINI

Phi-4

&             &            rs             RS            x?             RS             SS             “

&          &         é            ro          é           &           e
rs       es                   &                      &
(c) 1-shot

DeepSeek-R1 -

0.00

DeepSeek-V3

GPT-4.1-mini

GPT-5-mini

0.01

0.00

y  ‘J  a  <  2  $  SS  :
&  &   &    &
(b) 0-shot

DeepSeek-R1 -

0.00

DeepSeek-V3 -

GPT-4.1

GPT-4.1-mini

GPT-5

GPT-5-mini

04-MINI

Phi-4

0.01

4                      u                                                                                                         ké                                                                                                                                                                           q

»         -           >         <          N
&          rae           *           &          cS            &           SS          nS
se          &           S            »             °         ¢<’           &
ef                        &                          é
(d) ReAct

Figure 5: Pairwise win rate matrices across prompting settings. Each cell (2, 7) shows the fraction of research ideas
where model 2 outperforms model j. The four panels correspond to (a) naive prompting, (b) 0-shot, (c) 1-shot, and
(d) ReAct settings. Across all settings, GPT-5 achieves the highest overall win rates.

notable drop in overall performance. The largest
degradation occurs in the ethics and resources di-
mension, where research papers often omit prac-
tical details such as GPU hours, causing the fine-
tuned models to omit this part. Even after exclud-
ing this section from the aggregate scores, the SFT
models underperform their baselines. This sug-
gests that direct training on research plans does not
reliably improve research planning ability. We also
observe increased hallucination, particularly in the
related-literature section. A possible reason is that
extracted research plans contain factual details (e.g.,
specific papers or datasets) absent from the models’
pretraining data, which the models interpret during
fine-tuning as a signal to hallucinate.

16

Future directions. Our current fine-tuning
dataset provides limited coverage of the ethics and
resources considerations in the research plans. To
address this gap, future work could augment the
fine-tuning data with synthetic examples of these
aspects. It is also worthwhile to explore RL-based
training (e.g., PPO (Schulman et al., 2017)) in an
agentic loop where the model drafts plans, receives
rubric feedback, and optimizes research plan gen-
eration quality.


===== PAGE BREAK =====

Table 8: Rubric Generation Prompt

You are given a research idea and a plan for executing on that idea. Your task is to create a grading rubric in JSON format that can be used to evaluate other research plans on how well
they develop the same research idea. The grading rubric will be used to assess and score alternative research plans that attempt to address the same research question.

Here is the definition of a research plan:

// begin of research plan definition with one example

{ {RESEARCH_PLAN_DEFINITION } }

// end of research plan definition with one example

### Instructions

1. Generalize the Criteria.

Follow these steps in order:

Step 1: For each section of the research plan, define what constitutes a high-quality research plan when developing the research idea.

Step 2: Verify whether the input plan contains these quality characteristics.

Step 3: Generate a list of yes/no questions that can be used to grade other research plans against this one.

- If a specific term, concept, or methodology appears in the research plan but NOT in the original research idea, do not require it in the rubric questions.

- Judge based on intent and conceptual alignment rather than specific terminology - if a paper proposes a specific term for something, other research plans should not be required to use
that exact term, but should include things with similar intent.

- The rubric questions should capture the fundamental criteria that any research plan generated from this research idea should have. Do not ask about anything that is unique or special
to this particular research plan.

2. Section-by-Section Guidance.

{ {SECTION_BY_SECTION_GUIDANCE} }

3. Structure the Rubric in JSON Format.

Your rubric should be organized as a JSON object with the major sections of a research plan as top-level keys. For each section, list the key elements to check for, starting from

high-level concepts (e.g., overall goals) down to specific details (e.g., dataset types, model architectures, evaluation metrics). Each question should be a JSON object with the question
text. The rubric must follow this exact JSON structure:

{
"sections": {
"Section Name”: {
"subsections": {
"Subsection Name”: {
"questions": [
{
"question": "Question text here?”
}
]
}
}
}
}
}
If a section has no subsections and contains questions directly, use this format:
{
"sections": {
"Section Name”: {
"questions": [
{
"question": "Question text here?”
}
]
}
}
}
## Examples

Here is an example of a rubric for a different plan:
// begin of research plan rubric example

{ {RESEARCH_PLAN_RUBRIC_EXAMPLE} }
// end of research plan rubric example

{ {POOR_VS_BETTER_EXAMPLES } }

## Output

Now, here is the research plan and the research idea. Your task is to generate a structured grading rubric in JSON format:
Here is the research idea:

// begin of research idea

{ {RESEARCH_IDEA } }

// end of research idea

// begin of research plan

{ {RESEARCH_PLAN}}

// end of research plan

**IMPORTANT**: Your output must be valid JSON following the exact structure specified above. Do not include any text before or after the JSON object. Return only the JSON
rubric.

17



===== PAGE BREAK =====

Table 9: Section-by-Section Guidance for Rubric Generation

Section-by-Section Guidance

Use the following guidance to ensure consistency and completeness across all rubric sections:

- Introduction - Background: Write rubric questions that assess whether a plan clearly identifies the motivation and current limitations in the field.

- Introduction - Primary Objectives: Write rubric questions that assess whether a plan defines specific, measurable goals.

- Introduction - Research Questions: Write rubric questions that determine whether a plan articulates focused and relevant research questions.

- Key Literature: Write rubric questions that check whether a plan cites key related work. This is the only section where in-depth discussion of related literature should occur.
Group papers by domain. For each paper mentioned in the research plan, create one citation question using the format: "Does the plan cite [insert the paper title here] or similar
work on [specific topic]?"

Example: If the plan mentions "Attention Is All You Need", the question should be: "Does the plan cite the paper (Attention Is All You Need) or similar work on transformer
architectures?"

It’s important not to require exact citation of the specific paper title. The paper title in the question is just an example. Focus on whether the plan cites any work that serves the same
purpose or addresses the same topic, not whether it cites that exact paper.

- Methods: Write rubric questions that assess whether a plan outlines a sound technical approach. Do not require an exact method match unless it is the only viable option. Generate
questions that capture the high-level requirements for this plan.

- Initial Experimental Design: Write rubric questions that assess whether a plan includes a clear and complete experimental setup. When you mention exact datasets or models,
make sure to mention them as examples and put them in parentheses.

- Resource Requirements: Write rubric questions that assess whether a research plan provides a realistic estimate of the resources required.

- Ethical and Compliance Considerations: Write rubric questions that assess whether a plan addresses important ethical and legal responsibilities.

Table 10: Poor vs. Better Question Examples for Rubric Generation

Poor vs. Better Question Examples

Example 1:

Poor: "Does the plan use task rewording or transformation (e.g., EvilMath) to trigger safety mechanisms while preserving semantic fidelity?"
Better: "Does the plan describe how evaluation tasks will be adapted to test safety mechanisms (e.g., using rewording)?"

Reason: "Semantic fidelity" is not defined and unclear. Requiring "use task rewording" is too specific, there could be other ways.

Example 2:
Poor: "Does the plan propose analysis of the effect of model scale and attack type on the jailbreak tax?"

Better: "Does the plan propose analysis of the effect of model scale on performance?" and "Does the plan propose analysis of the effect of attack type on performance?"
Reason: You are asking two things, should be split into two questions.

Example 3:
Poor: "Does the plan include a fine-tuning-based jailbreak using legitimate QA pairs?"
Better: "Does the plan include a fine-tuning-based jailbreak approach?"

Reason: "Legitimate QA pairs" is not clear in this question’s context.

Example 4:
Poor: "Does the plan define a clear metric for jailbreak success rate?"

Better: "Does the plan define a clear metric for evaluating attack effectiveness (e.g., success rate for jailbreak)?"

Reason: "Jailbreak success rate" is a specific term from the paper that the agent generating the plan may not capture, while "attack effectiveness" is more general.

Example 5:
Poor: "Does the plan include visualizations of results across alignment techniques, models, and tasks?"
Better: "Does the plan propose analysis of results across alignment techniques, models, and tasks?
Reason: Research plans describe what will be done, not the final outputs like visualizations.

18



===== PAGE BREAK =====

Table 11: Human Evaluation Guidelines: Research Plan and Rubric Assessment, Part |

What You’re Evaluating

Hi! We’re excited to have your help evaluating LLM-generated content using the 1-5 scale below. Your expert feedback is incredibly valuable to us and will help improve our
research!

Paper: <PAPER TITLE AND LINK PLACEHOLDER>
You will evaluate TWO pieces of LLM-generated content:

1. LLM-Generated Research Plan from Full Paper - How well does the Al-extracted plan capture the original paper’s content?
2. LLM-Generated Rubric Questions - How good are the Al-generated evaluation questions for assessing research plans?

Definition of a Research Plan

<Placeholder for the definition of research plan, see Section X>

PART 1: LLM-GENERATED RESEARCH PLAN FROM FULL PAPER TO EVALUATE
Instructions: The content below was automatically generated by an AI system from the research paper. In the next section we will provide guidelines for you to evaluate this.

<INSERT GENERATED RESEARCH PLAN HERE>

PART 2: YOUR EVALUATION OF THE RESEARCH PLAN
Instructions: Please rate how accurately the LLM captured the content from the original paper.

Rating Scale:

e | = Major issues. The research plan fails to capture the key aspects of the evaluation criteria for the respective sections.

e 2 = Significant problems. The research plan only partially addresses the evaluation criteria and overlooks several important aspects.

e 3 = Average. The research plan covers the main criteria but contains notable gaps or inaccuracies compared to what the authors wrote in the paper.
e 4= Acceptable with some issues. The research plan addresses most of the evaluation criteria well with only minor issues or omissions.

e@ 5 = Well done overall. The research plan is generally well-constructed and comprehensive, needing only minor adjustments.

Section 1: Introduction (Background, Objectives, Research Questions)

Rate from 1-5: _

Evaluation Criteria:

e Does the background accurately reflect the paper’s motivation and context?

e Are the objectives correctly extracted from the paper’s stated goals?

e Do the research questions in the plan match those addressed in the original paper?
Comments:

Section 2: Key Literature

Rate from 1-5: _

Evaluation Criteria:

e Does it include the key references mentioned in the original paper?

e Are any important citations from the paper missing or misrepresented?
Comments:

Section 3: Methods

Rate from 1-5: _

Evaluation Criteria:

e Do the proposed methods accurately reflect the paper’s methodology?
e Are the key technical details correctly extracted from the paper?

e Are any key methodological components from the paper missing?
Comments:

Section 4: Experimental Design

Rate from 1-5: _

Evaluation Criteria:

e Does the experimental design match the paper’s evaluation setup?
e Are the datasets, metrics, and baselines correctly extracted?

e Are any important experimental details from the paper missing?
Comments:

Section 5.1: Resources

Rate from 1-5: _

Evaluation Criteria:

e Do the resource estimates align with what’s described in the paper and appear realistic for the proposed research?
Comments:

Section 5.2: Ethics

Rate from 1-5: _

Evaluation Criteria:

e Are ethical considerations properly identified and addressed, including any mentioned in the paper?
Comments:

19



===== PAGE BREAK =====

Table 12: Human Evaluation Guidelines: Research Plan and Rubric Assessment, Part 2

PART 3: YOUR EVALUATION OF THE RUBRIC QUESTIONS

Instructions: The rubric questions you are going to evaluate were automatically generated by an AI system. These questions are intended to evaluate research plans developed from
the research idea below. (The given paper should be seen as the result of such research plan.)

The research idea: <INSERT RESEARCH IDEA HERE>

The rubric questions should:
e Assess whether the questions cover the essential parts of a research plan for the given idea that could have reasonably led to the given paper
e Focus on intent and conceptual alignment, rather than specific wording or terminology

Please review the rubric questions below and rate their overall quality using the 1-5 scale.

Rating Scale:

e | = Major issues. The rubric questions are irrelevant or do not address the key evaluation criteria for this section.

e 2 = Significant problems. The rubric questions have notable flaws or overlook several important aspects that should be assessed.
e 3 = Average. The rubric questions addr      ome basic evaluation needs but could be improved.

e 4= Acceptable with some issues. The rubric questions cover the essential evaluation needs but still have some gaps.

@ 5 = Well done overall. The rubric questions are generally well-constructed and thorough, needing only minor adjustments.

Section 1: Introduction Rubric Questions Quality
<INSERT INTRODUCTION RUBRIC QUESTIONS HERE>
Rate the Introduction rubric questions from 1-5: ___
Comments:

Section 2: Key Literature Rubric Questions Quality
<INSERT KEY LITERATURE RUBRIC QUESTIONS HERE>
Rate the Key Literature rubric questions from 1-5: ___
Comments:

Section 3: Methods Rubric Questions Quality
<INSERT METHODS RUBRIC QUESTIONS HERE>
Rate the Methods rubric questions from 1-5: ___
Comments:

Section 4: Experimental Design Rubric Questions Quality
<INSERT EXPERIMENTAL DESIGN RUBRIC QUESTIONS HERE>
Rate the Experimental Design rubric questions from 1-5: ___
Comments:

Section 5: Resources and Ethics Rubric Questions Quality

<INSERT RESOURCES AND ETHICS RUBRIC QUESTIONS HERE>
Rate Resources rubric questions from 1-5: ___

Rate Ethics rubric questions from 1-5: ___

Comments:

20



===== PAGE BREAK =====

Table 13: Rubric Evaluation Prompt

You are given a grading rubric and a new research plan. Your task is to evaluate the new plan against the rubric using a **strict interpretation**: only answer **Yes** if the plan
explicitly satisfies the rubric question as written. Note that examples in parentheses (e.g., specific datasets, papers, methods) are for reference only.
## Instructions

1. You will be evaluating **ONE SPECIFIC SECTION** of the rubric at a time against the entire research plan.

2. For the given section:
- Traverse all levels of the section hierarchy. Rubric items may be nested (e.g., subsections —> questions).
- Evaluate each **leaf-level question** (i.e., the final bullet points that are actual rubric questions).
- Answer **Yes** only if the research plan clearly and explicitly addresses the rubric question.
- Answer **No** if the rubric question is not addressed, or if the answer is vague or only implied.

> For example:
> - General references are not sufficient unless they fully preserve the original intent of rubric item.

3. When rubric questions include examples in parentheses (for example, "e.g., dataset A, dataset B"), these are provided as reference examples to illustrate the type of content being
asked about. Do NOT require the research plan to mention these specific examples to answer "Yes".

> For example:
> - If the rubric asks "Does the plan use mathematical datasets (e.g., GSM8K, MATH)?", answer "Yes" if the plan uses any appropriate mathematical datasets, not just GSM8K or

MATH specifically

4. For each rubric question in the specified section:
- **Question**: [rubric question]
- **Answer**: Yes / No
- **Explanation**: [brief justification]

## Inputs

// begin of research plan rubric section to evaluate
{ {RESEARCH_PLAN_RUBRIC_SECTION}}

// end of research plan rubric section

// begin of full research plan

{ {RESEARCH_PLAN}}

// end of full research plan

// section being evaluated
{ {SECTION_NAME} }

## Output Format

Please return the evaluation for the specified section in the following structured JSON format:

{
"section_name”: "{{SECTION_NAME}}",
"evaluation": {
"subsections": {
"Subsection Title A”: {
"questions": [
{
"question": "Rubric question 1”,
"answer": "Yes" or "No",
"explanation": "Your explanation here”
3,
]
3,
}
}
}

**Note**: If the section has no subsections and contains questions directly, use this format instead:

{
"section_name”: "{{SECTION_NAME}}",
"evaluation": {
"questions": [
{
"question": "Rubric question 1",
"answer": "Yes" or "No",
"explanation": "Your explanation here”
3,
]
}
}

## Important JSON Formatting Notes:

- When mentioning dollar amounts, use "$" not "\\$"

- Avoid unnecessary escape characters in explanations

- Valid JSON escapes are: \\n \\t \\r \\b \\f \\" \W\ WV

- Do not escape regular punctuation like $ or other symbols

## Important Notes

- You are evaluating **only the section specified** in the SECTION_NAME field

- Search through the **entire research plan** to find evidence for each rubric question
- Be thorough but focus only on the questions within the specified section

- This evaluation will be combined with evaluations of other sections to form a complete as

21



===== PAGE BREAK =====

Table 14: Naive Baseline Prompt

Naive Baseline Prompt

RESEARCH IDEA TO ANALYZE:

{ {RESEARCH_IDEA } }

Your task is to generate a detailed research plan based on the provided research idea. Below is the template you must follow to create the research plan:
## 1. Introduction

### 1.1 Background

### 1.2 Primary Objectives

#44 1.3 Research Questions

## 2. Key Literatures

## 3. Methods

## 4. Initial Experimental Design

## 5. Resources, Compliance, and Ethical Considerations
### 5.1 Resource Requirements

### 5.2 Ethical and Compliance Considerations

Table 15: Zero-Shot Baseline Prompt

Zero-Shot Baseline Prompt

RESEARCH IDEA TO ANALYZE:

{ {RESEARCH_IDEA } }

Your task is to generate a detailed research plan based on the provided research idea. Below is the template you must follow to create the research plan:
{ {RESEARCH_PLAN_TEMPLATE}}

INSTRUCTIONS FOR RESEARCH PLAN GENERATION:

1. Analyze the research idea thoroughly from the provided research idea.

2. Generate a complete research plan following the EXACT template structure above.

3. Fill in all sections with relevant, specific content based on the research idea.

4. Draw upon your existing knowledge of the research area to provide context and background.
5. Include resource requirements for conducting the research.

IMPORTANT NOTES:
- Base your plan on the provided research idea and your existing knowledge.

Table 16: One-Shot Baseline Prompt

One-Shot Baseline Prompt

RESEARCH IDEA TO ANALYZE:

{ {RESEARCH_IDEA } }

Your task is to generate a detailed research plan based on the provided research idea. Below is the template you must follow to create the research plan:
{ {RESEARCH_PLAN_TEMPLATE_WITH_EXAMPLE}}

INSTRUCTIONS FOR RESEARCH PLAN GENERATION:

1. Analyze the research idea thoroughly from the provided research idea.

2. Generate a complete research plan following the EXACT template structure above.

3. Fill in all sections with relevant, specific content based on the research idea.

4. Draw upon your existing knowledge of the research area to provide context and background.
5. Include resource requirements for conducting the research.

IMPORTANT NOTES:
- **The examples provided in the research plan template are for reference only** - replace them with content specific to the given research idea.
- Base your plan on the provided research idea and your existing knowledge.

22



===== PAGE BREAK =====

Table 17: ReAct Agent Prompt

ReAct Agent Prompt

You are a research planning agent that generates comprehensive research plans using iterative reasoning.
RESEARCH IDEA: {{RESEARCH_IDEA}}

Below is the definition of a research plan structure as well as one example.

IMPORTANT NOTE: The example provided is from a specific research plan about scientific literature understanding with LLMs. You should NOT be restricted to the specific
settings, methods, datasets, or approaches mentioned in this example. Adapt all content to fit the specific research idea you are working on. Use your research knowledge and the
information you gather through searches to create a plan that is most appropriate for the given research idea.

RESEARCH PLAN TEMPLATE AND GUIDELINES: { {RESEARCH_PLAN_DEFINITION}}
AVAILABLE TOOLS: { {TOOL_NAMES}}

TOOL SPECIFICATIONS: { {TOOL_SPECIFICATIONS } }

RESOURCE LIMITS:

* You have {{MAX_TOTAL_TOOLS }} total tool calls available across maximum {{MAX_ITERATIONS}} iterations
* {{CURRENT_USAGE_STATS}}
INSTRUCTIONS: Use the following ReAct format to systematically gather information and create a research plan.

IMPORTANT: Always begin with “Thought:” and explain your reasoning before taking any action.

FORMAT FOR EACH ITERATION: Thought: [Your reasoning about what information you need] Action: [MUST be exactly one of: {{TOOL_NAMES}}] Action
Input: [specific query or input for the chosen tool]

CRITICAL RULES:

1. You MUST STOP after “Action Input: — do NOT generate “Observation:” or any results
2. The system will execute your action and provide the real observation

3. Wait for the actual tool results before continuing

4. Use the EXACT tool names listed above. Available tools are: {{TOOL_NAMES}}

FINAL STEP (After gathering sufficient information): When you have collected enough information through your searches and reads, generate your final answer in this EXACT
format:

Thought: I now have sufficient information to create the research plan. Final Answer: [Start your complete research plan here, following the template
provided above. The plan should be comprehensive and detailed based on all the information you gathered through your tool calls.]

CRITICAL: You MUST include “Final Answer:” exactly as shown above. This is required for the system to recognize your final output.

EXAMPLE OF CORRECT FORMAT: Thought: I need to find papers about attention mechanisms to understand current transformer architectures.
Action: search_papers Action Input: attention mechanisms transformer architectures [STOP HERE - Wait for system to provide observation]

Key guidelines:

1. Use targeted queries to find relevant information

2. Look for recent advances, existing methods, and gaps in the literature
3. Consider both technical approaches and evaluation methods

4. Each tool call should build upon previous findings

5. Choose the most appropriate tool for each information need

6. End with “RESEARCH PLAN COMPLETE” after your final answer

7. NEVER generate “Observation:” — always wait for the system response

READING PAPERS: When you use the read_paper action, you will receive a comprehensive summary of the paper content instead of the full text.
Begin your research planning process now. Start with a “Thought:” about what information you need to gather first.

REMEMBER: You must use the ReAct format. Do NOT attempt to create the research plan immediately. Start by thinking about what information you need, then use tools to
gather that information systematically. STOP after each “Action Input:” and wait for the system’s observation.

Table 18: Tool Specifications for ReAct Agent

1. search_papers: Search for academic papers using Bing Custom Search
- Input: A search query string for academic papers
- Output: JSON with paper titles, abstracts, arXiv IDs, and publication details
- Usage example: Action Input: machine learning attention mechanisms

2. read_paper: Read and analyze a specific academic paper by its arXiv ID
- Input: An arXiv ID (e.g., "2301.12345")
- Output: JSON with arXiv_id and paper summary
- Usage example: Action Input: 2301.12345

23



===== PAGE BREAK =====

Table 19: Paper Summarization Prompt

summary_prompt = f""”
PAPER TO SUMMARIZE:
Title: {paper_title}

ArXiv ID: {arxiv_id}

Paper Content:
{paper_content}

Create a comprehensive summary of this paper. Your summary should include:

1. Main Contributions: Key findings and contributions of this paper

2. Key Related Literature: Important prior works referenced and how this paper builds upon or differs from them
3. Methods and Techniques: Approaches, algorithms, or methodologies used

4. Experimental Design and Results: How experiments were conducted, datasets used, evaluation metrics, and key results obtained

SUMMARY:

Table 20: Section-wise Planning Scores (%) with Mean aggregation. Each value represents the mean accuracy
across all papers in Idea2Plan Bench. Bold numbers indicate the highest score within each section across all
baselines.

Model                                           Naive                                                            0-shot

Intro   Lit Met Exp Res/Eth Avg Intro   Lit Met Exp’ Res/Eth Avg
Phi-4      ; 29.0   5.6 20.2 10.9    30.9 19.3 | 28.8   8.2 20.9 10.7    43.1 22.3
DeepSeek-V3 ; 39.8 25.2 32.4 24.7    44.5 | 33.3.1 406 23.0 334 24.6    52.0 | 34.7
DeepSeek-R1 |! 40.6 27.1 37.0 28.7    50.3; 36.7 1 41.3 24.5 37.2 29.7    56.8 | 37.9
GPT-4.1-mini © 42.39 21.8 34.7 274    47.2 | 34.7 46.8 243 39.9 30.9    62.7 | 40.9
GPT-4.1    ; 40.7 22.4 34.2 28.9    48.7 | 34.9 , 44.0 23.7 34.9 29.2    60.4 | 38.4
04-mini    | 50.8 29.1 50.2 39.1    59.7 . 45.7 | 52.8 27.5 514 39.9    68.1 47.9
GPT-5-mini   58.5 38.1 65.9 51.6    69.5 , 56.7! 61.3 366 644 52.3    78.9 , 58.7
GPT-5       60.9 47.7 70.9 56.7    73.8 | 62.0 63.2 43.9 69.6 56.9    81.2 1 63.0
Model                              1-shot                   |                              ReAct

Intro   Lit Met Exp Res/Eth Avg | Intro   Lit Met Exp Res/Eth _ Avg
Phi-4       28.6   9.2 20.1 12.9    45.1 23.2! 24.2 11.8 15.5   9.9    31.7 18.6
DeepSeek- V3 41.8 23.1 33.6 28.5    55.9 , 36.6 39.9 23.8 33.4 28.5    50.9 , 35.3
DeepSeek-R1 , 40.3. 25.3 35.7 30.4    57.1 | 37.8 , 38.7. 23.1 33.2 27.9    52.0 1 35.0
GPT-4.1-mini | 48.2 266 404 33.8    65.3 '| 42.8 1 50.6 30.6 44.5 35.7    60.3 | 44.3
GPT-4.1    ' 45.2 240 38.1 31.7    65.2 ' 40.8 ' 50.6 35.6 449 40.5    65.6 | 47.4
04-mini    ' 53.9 29.9 53.9 43.5    69.9 50.2' 542 36.1 544 42.6    61.2 49.6
GPT-5-mini    61.4 37.5 65.8 52.3    80.4 | 59.5, 62.3 44.5 67.3 55.2    76.4 , 61.1
GPT-5       63.2 44.2 70.2 57.4    81.0 | 63.2 1 61.4 48.1 684 55.7    75.9 | 61.9

24


===== PAGE BREAK =====

Table 21: Section-wise Planning Scores (%) with Max aggregation. Each value reports the maximum accuracy
across all generated plans per paper. Bold numbers indicate the highest score within each section across all baselines.

Naive                          0-shot

Model

Intro   Lit Met Exp Res/Eth Avg Intro   Lit Met Exp’ Res/Eth Avg
Phi-4      ; 364 10.8 25.6 17.2    39.1 23.2 | 36.1 13.2 260 16.9    53.1 25.9
DeepSeek-V3 | 46.9 32.2 39.9 31.8    53.6 ; 37.4 | 47.7 30.8 41.5 32.1    61.0 ; 39.2
DeepSeek-R1 ! 48.1 35.4 45.8 36.6    59.4 , 41.2 | 49.1 32.2 45.9 38.3    66.7 ; 42.5
GPT-4. 1-mini . 49.9 30.2 41.9 35.1    57.2 | 39.0 ; 54.3 32.6 47.7 39.4    72.9 | 45.2
GPT-4.1    | 49.1 30.8 42.3 37.0    58.0 | 39.4 , 51.9 31.9 43.4 37.6    70.9 | 43.2
O4-mini    59.0 38.5 59.3 469    70.1 | 50.5; 61.0 35.6 60.1 48.3    77.7 | 52.2
GPT-5-mini   66.3 48.2 73.4 60.4    77.7 | 614! 69.8 460 72.9 61.8    86.9 , 63.4
GPT-5 69.9 57.5 79.1 66.0    81.5 | 66.6 71.5 52.7) 77.5 67.5    89.5 1 67.9
Model                              1-shot                              ReAct

 Intro   Lit Met Exp Res/Eth | Avg ; Intro   Lit Met Exp Res/Eth Avg
Phi-4      ' 35.2 15.0 25.6 19.0    54.2 | 27.0 | 344 19.8 22.7) 17.5    45.5 25.0
DeepSeek-V3 49.0 30.6 41.7 36.2    64.8 | 41.2 48.2 324 42.1 36.6    60.6 , 40.3
DeepSeek-R1 | 47.7 33.4 44.2 38.6    67.1 | 42.2 ; 463 33.1 43.0 35.6    62.2 | 39.8
GPT-4.1-mini | 57.8 36.1 529 46.3    76.9 | 49.1 1 586 41.4 541 45.3    70.5 |! 50.0
GPT-4.1    ' 34.4 33.5 49.7 42.7    774 | 46.7! 59.3 47.0 54.0 52.0    774 | 53.6
O4-mini    63.0 38.9 65.5 55.8    81.6 | 55.8 63.5 47.2 65.4 52.5    72.2 55.9
GPT-5-mini   69.8 47.0 74.7 63.2    88.6 , 64.4; 70.9 55.4 77.1 66.6    84.2 , 66.2
GPT-5     | 71.7 54.2 79.0 67.0    88.5 | 68.1 1 69.7 57.8 77.1 66.3    84.8 | 67.3

Table 22: Why React Agent Does Not Outperform Baseline: Parametric vs Retrieved Knowledge Conflict. Green
text indicates the citation that match the ground truth and are retrieved from the model’s parametric knowledge. This
case shows that GPT-5 can correctly identify the key citation from its internal knowledge, but retrieval introduces
noise when it conflicts with the model’s domain understanding, suggesting the need for further study to mitigate
such effects.

Ground Truth

Domain: Prediction-Policy Problems and Screening Frameworks

e Kleinberg, J., Ludwig, J., Mullainathan, S., & Obermeyer, Z. (2015). "Prediction Policy Problems." American
Economic Review, 105(5):491—495. — Introduces the paradigm of separately trained predictive risk scoring systems
for policy allocation problems, motivating the need to quantitatively compare prediction accuracy against resource

constraints.

e Perdomo, J. C. (2024). "The Relative Value of Prediction in Algorithmic Decision Making." ICML 2024. —
Defines the prediction-access ratio (PAR) in binary treatment settings; serves as the foundation for extending PAR to
continuous welfare screening and developing theoretical bounds.

0-shot (GPT-5)

React Agent (GPT-5)

Prediction policy and algorithmic decision-making

e Kleinberg, Ludwig, Mullainathan, and Sunstein on pre-
diction policy problems. Connects predictive accuracy to
policy outcomes and clarifies when prediction improves
decisions.

e Obermeyer and Mullainathan on algorithmic bias in
healthcare risk prediction. Highlights label/measurement
biases that directly inform our decomposition and robust-
ness checks.

e Bertsimas and Kallus on prescriptive analytics. Informs
policy optimization from predictive models under con-
straints.

Policy learning and empirical welfare maximiza-
tion under constraints

e Model Selection for Treatment Choice: Penalized
Welfare Maximization (arXiv: 1609.03167): Extends
EWM to select among constrained policy classes with
oracle regret guarantees; key for interpretable policy
classes and complexity control.

e Policy Learning with Observational Data (Athey
& Wager; arXiv:1702.02896): Doubly robust policy
learning with optimal n*(-1/2) regret for VC classes
and accommodation of observational designs; cen-
tral for estimating welfare-maximizing targeting rules
from administrative data.

25



===== PAGE BREAK =====

Table 23: Literature Recommendation Prompt

Literature Recommendation Prompt

You are a research literature recommendation system. Based on the provided research idea, introduction, and references, identify exactly 3 most relevant key papers that researchers
should read to better understand and build upon this work.

## RESEARCH IDEA:
{{RESEARCH_IDEA}}

## INTRODUCTION:
{{INTRODUCTION}}

## REFERENCES:
{ {REFERENCES } }

## TASK:
Instructions:
1. Carefully analyze the research idea to understand its core contributions, methods, and objectives
2. Read the introduction to identify the key technical foundations and prior work this research builds upon
3. Select exactly 3 papers that serve as direct foundational works by being:
- Core methodological predecessors that this work extends or improves
- Papers introducing the key techniques, algorithms, or frameworks being built upon
- Essential baseline methods or datasets that this work directly compares against or uses

Note: Focus on papers that are specifically related to understanding this specific research idea, avoid general background papers.

### OUTPUT FORMAT:

Return your response as a JSON array with exactly 3 papers. Each paper should have:

- "title": The exact title of the paper

- "arxiv_link": The arXiv ID if available (format: "XXXX.XXXXX" e.g., "1706.03762"), or null if not on arXiv
- "relevance": A single sentence explaining why this paper is relevant to the current research (max 30 words)

Example format:

c
{
"title": "Attention Is All You Need”,
"arxiv_link”: "1706.03762",
"relevance": "Introduces the transformer architecture which forms the foundation for the proposed modifications in attention mechanisms.”
3,
{
"title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”,
"arxiv_link”: "1810.04805",
"relevance": "Demonstrates successful pre-training strategies that are extended in this work for domain-specific applications.”
}
J

Return only the JSON array, no additional text or explanation.

Table 24: Section-wise performance comparison with and without curated literature. Values are shown as
baseline %/with-related-papers%, with percentage point deltas in parentheses.

Model                 Intro           Lit         Methods Exp Design Resources & Ethics      Overall
DeepSeek-V3 39.3/41.2.  29.7/31.8 — 37.8/39.1       32.8/33.4               44.2/45.9             32.7/35.1
(+1.9)          (+2.1)          (+1.3)            (+0.6)                    (+1.7)                  (+2.3)
DeepSeek-R1 39.8/42.3. 31.6/31.8 42.8/41.2. —-35.7/36.1        49.2/50.9       37.2/38.5
(+2.5)          (+0.2)           (-1.6)            (+0.3)                    (+1.7)                  (+1.3)
GPT-4.1-mini 42.5/43.9 = 25.1/30.9 — 42.3/40.3        37.4/37.3                48.1/45.7              35.3/37.5
(+1.4)           (+5.8)            (-2.0)              (-0.1)                       (-2.4)                    (+2.2)
GPT-4.1              40.5/43.7  25.3/31.1 44.1/43.0 = 36.6/39.9               50.1/49.1             35.2/38.7
(+3.2)      (+5.9)      (-1.1)       (+3.3)           (-1.1)           (+3.5)
04-mini        50.6/51.1  30.0/32.0 51.7/51.0  46.0/45.1         58.1/57.7       45.1/46.1
(+0.5)          (+2.0)           (-0.7)             (-0.9)                    (-0.4)                  (+1.0)
GPT-5-mini     56.1/58.9 38.6/40.6 64.7/65.9    57.7/58.7        66.9/69.3       56.0/58.0
(+2.8)           (+2.0)           (+1.2)             (+1.0)                      (+2.4)                    (+2.1)
GPT-5         59.4/60.6 46.0/42.1 71.7/71.4  62.4/60.6        71.9/73.8       61.3/61.5
(+1.2)            (-3.9)            (-0.2)              (-1.8)                      (+2.0)                    (+0.2)

Table 25: SFT model comparison on 1-shot vanilla baseline agents. Scores are averaged across sections and reported
in percentages. Rows labeled A = SFT — Base show percentage-point differences between SFT and Base models.

Model                        Intro        Lit       Method      Exp      Res/Eth Overall Overall (w/o Res/Eth)
GPT-4.1-mini (Base) 48.2% 26.5% 404% 33.8% 65.3%     42.8%              37.2%
GPT-4.1-mini (SFT) 42.4% 22.9% 41.5% 27.2%     18.9%      30.6%              33.5%
A (SFT-Base)           5.7% -3.6% +1.1% -66% -464%  -12.2%             -3.7%
GPT-4.1 (Base)        45.2% 24.0% 38.1% 31.7% 65.1%    40.8%           34.8%
GPT-4.1 (SFT)         44.5% 22.1% 41.7% 27.9% 21.9%     31.6%           34.1%
A (SFT-Base)           -0.7% -1.9%  +3.6%  -3.8%  -43.2%     -9.2%              -0.7%

26


===== PAGE BREAK =====

16
17
18
19

Table 26: Evaluat

ion rubric for Jailbreak-Tax paper (Nikolic et al., 2025), Part 1.

Evaluation Rubric Part 1 (.      IN Format)

{

an state that adversarial jailbreak attacks can bypass LLM safety guardrails or
ons?”

an note that existing evaluations focus on bypass success and neglect post-jailbreak
on?”

an explain that assessing the utility of harmful outputs requires domain expertise?”

an highlight the lack of an unaligned baseline as a barrier to quantifying performance

an identify the need for a more rigorous evaluation framework for jailbreak attacks?”

an propose designing an evaluation framework that measures both jailbreak success rate
elicited outputs?”

an include

rs?”

constructing benchmark suites with tasks that have objectively verifiable

an include applying and comparing multiple representative jailbreak attacks across

an include comparing jailbreak performance across different alignment methods?”

an include

aq”

quantifying the relative performance drop (jailbreak tax) compared to an

an aim to release benchmark suites and evaluation code to the community?”

an ask whether
ross tasks?”

different jailbreak attacks incur a measurable performance drop

an ask whether the magnitude of the performance drop correlates with the jailbreak

an ask whether model size influences the severity of the jailbreak tax?”

an ask whether the jailbreak tax is consistent across alignment methods?”

"sections": {
"Introduction": {
"subsections": {
"Background": {
"questions": [
{
"question": "Does the p
alignment protecti
3,
{
"question": "Does the p
capability retenti
3,
{
"question": "Does the p
3,
{
"question": "Does the p
degradation?”
3,
{
"question": "Does the p
}
J
3,
"Primary Objectives”: {
"questions": [
{
"question": "Does the p
and the utility of
3,
{
"question": "Does the p
ground-truth answe
3,
{
"question": "Does the p
these benchmarks?”
3,
{
"question": "Does the p
3,
{
"question": "Does the p
unaligned baseline
3,
{
"question": "Does the p
}
J
3,
"Research Questions”: {
"questions": [
{
"question": "Does the p
(jailbreak tax) ac
3,
{
"question": "Does the p
success rate?”
3,
{
"question": "Does the p
3,
{
"question": "Does the p
3,
{
"question": "Does the p
}
J
3
3
}
}

an ask whether task difficulty affects the magnitude of the jailbreak tax?”

27



===== PAGE BREAK =====

ARwWNHE

9
10
1
12
13
14

48
49

na
Nes

AMWMAAKY
CHIDAKRD

Table 27: Evaluation rubric for Jailbreak-Tax paper (Nikolic et al., 2025), Part 2.

Evaluation Rubric Part 2 (JSON Format)

{

"sections": {
"Key Literatures”: {
"subsections": {
"Jailbreak Attack Methodologies”: {
"questions": [
{
"question": "Does the plan cite the paper (Jailbroken: How does LLM safety training fail?) or similar work on
manual prompt-engineering jailbreaking techniques?”
3,
{
"question": "Does the plan cite the paper (Universal and transferable adversarial attacks on aligned language
models) or similar work on optimization-based adversarial suffix attacks?”
3,
{
"question": "Does the plan cite the paper (AutoDAN: Generating stealthy jailbreak prompts on aligned large
language models) or similar work on genetic-algorithm-based prompt generation?”
3,
{
"question": "Does the plan cite the paper (Jailbreaking black box large language models in twenty queries) or
similar work on iterative LLM-based prompt rewriting attacks?”
3,
{
"question": "Does the plan cite the paper (Tree of attacks: Jailbreaking black-box LLMs automatically) or
similar work on tree-of-thought or hierarchical attack expansions?”
3,
{
"question": "Does the plan cite the paper (Many-shot jailbreaking) or similar work on in-context
demonstration-based jailbreak attacks?”
3,
{
"question": "Does the plan cite the paper (Multilingual jailbreak challenges in large language models) or
similar work on translation-based or multilingual jailbreak techniques?”
}
J
3,
"Benchmark Datasets for Objective Utility”: {
"questions": [
{
"question": "Does the plan cite the paper (The WMDP benchmark: Measuring and reducing malicious use with
unlearning) or similar work on bio-security multiple-choice benchmarks?”
3,
{
"question": "Does the plan cite the paper (Training verifiers to solve math word problems) or similar work on
grade-school math reasoning benchmarks?”
3,
{
"question": "Does the plan cite the paper (Measuring massive multitask language understanding) or similar work
on competition-level math or multitask reasoning benchmarks (e.g., MATH, MMLU)?”
}
J
3,
"Alignment Tax and Prior Evaluations”: {
"questions": [
{
"question": "Does the plan cite the paper (Current work in AI alignment) or similar work defining the concept
of an alignment tax?”
3,
{
"question": "Does the plan cite the paper (A StrongReject for empty jailbreaks) or similar work on performance
degradation of jailbreak attacks?”
3,
{
"question": "Does the plan cite the paper (AgentHarm: A benchmark for measuring harmfulness of LLM agents) or
similar work contrasting objective benchmarks with LLM-based evaluation of harmfulness?”
}
J
3
3
}
}

28



===== PAGE BREAK =====

Table 28: Evaluation rubric for Jailbreak-Tax paper (Nikolic et al., 2025), Part 3.

Evaluation Rubric Part 3 (JSON Format)

I }¢

2     "sections": {

3       "Methods": {

4        "subsections": {

5           "Pseudo-Alignment Techniques”: {

6             "questions": [

7                {

8                 "question": "Does the plan describe a system-prompt alignment method that simulates model refusals via
instructional constraints?”

9                 },

10                 {

11                 "question": "Does the plan describe a supervised fine-tuning approach on (prompt, refusal) pairs to induce
model alignment?”

12                 3,

13                 {

14                 "question": "Does the plan include a data-driven or task rewording-based pseudo-alignment technique (e.g.
transforming benign tasks to harmful contexts)?”

15                 }

16               J

17             3,

18           "Jailbreak Attacks”: {

19             "questions": [

20               {

21                 "question": "Does the plan include a prompt-based jailbreak attack that appends or modifies instructions to
bypass alignment?”

22                 3,

23              {

24                 "question": "Does the plan include a fine-tuning-based jailbreaking approach to reverse the model's refusal
alignment?”

25                 3,

26              {

27                 "question": "Does the plan include demonstration-based (many-shot) jailbreak attacks using in-context
examples?”

28                 },

29               {

30                "question": "Does the plan include optimization-based adversarial attacks (e.g., greedy coordinate descent) to
craft adversarial suffixes?”

31                  3,

32              {

33                 "question": "Does the plan include genetic algorithm or evolutionary strategies for generating stealthy
jailbreak prompts?”

34                 3,

35              {

36                 "question": "Does the plan include translation-based or multilingual jailbreak attacks?”

37                 3,

38               {

39                 "question": "Does the plan include iterative prompt rewriting attacks that use LLM-based rewriting (with or
without tree-of-thought reasoning)?”

40)                 }

+1             1

42         3,

43,           "Evaluation Metrics”: {

44             "questions": [

15,              {

46                 "question": "Does the plan define a metric for jailbreak success rate that measures the proportion of
non-refusal outputs?”

+7                 3,

48                 {

49                 "question": "Does the plan define a metric for post-jailbreak utility, such as conditional task accuracy after
a successful jailbreak?”

50                 },

51                  {

52                 "question": "Does the plan define a baseline utility metric for measuring the unaligned model's task
performance?”

53                 3,

54                 {

55                 "question": "Does the plan define a metric for quantifying the relative performance drop (jailbreak tax)
between baseline and jailbroken outputs?”

56               }

57            1

58           }

59         }

60       }

61      }

o2 |}3

29


===== PAGE BREAK =====

16
17
18
19
20

21

46

48

Nn
Ne

AW
ARB

et

56
57
58
59
60

61

Table 29: Evaluation rubric for Jailbreak-Tax paper (Nikolic et al., 2025), Part 4.

{

Evaluation Rubric Part 4 (.      IN Format)
"sections": {
"Initial Experimental Design”: {
"questions": [
{
"question": "Does the plan propose experiments that evaluate both jailbreak success rate and post-jailbreak
utility on objective benchmarks?”
3,
{
"question": "Does the plan include experiments using tasks with verifiable ground-truth answers (e.g., biology
multiple-choice, grade-school math, competition-level math)?”
3,
{
"question": "Does the plan include experiments comparing results across multiple model sizes (e.g., small, medium,
large LLMs)?”
3,
{
"question": "Does the plan include experiments comparing different alignment methods (e.g., system-prompt,
supervised fine-tuning, task rewording)?”
3,
{
"question": "Does the plan include experiments applying multiple jailbreak attacks across the selected benchmarks?”
3,
{
"question": "Does the plan include analysis of the relationship between task difficulty and the magnitude of the
jailbreak tax?”
3,
{
"question": "Does the plan include control experiments evaluating aligned models on neutral or unrelated tasks
(e.g., subsets of MMLU or other benchmarks) to verify unintended degradation?”
3
J
3,
"Resources, Compliance, and Ethical Considerations”: {
"subsections": {
"Resource Requirements”: {
"questions": [
{
"question": "Does the plan list required model resources, such as open-weight and closed-weight LLMs?”
3,
{
"question": "Does the plan acknowledge the need for computational infrastructure for fine-tuning and
evaluation (e.g., GPU hardware)?”
3,
{
"question": "Does the plan acknowledge the need for API access for data transformation or evaluation steps?”
}
J
3,
"Ethical and Compliance Considerations”: {
"questions": [
{
"question": "Does the plan address the potential for adversaries to misuse the evaluation framework?”
3,
{
"question": "Does the plan mention verifying data licenses and usage rights for the benchmarks used?”
3,
{
"question": "Does the plan address the exclusion of sensitive or personally identifiable information from
benchmark data?”
3,
{
"question": "Does the plan discuss risk-benefit trade-offs or security-research norms regarding transparency
and threat exposure?”
}
J
3
3
}
}

30



===== PAGE BREAK =====

Table 30: Full Research Plan generated by GPT-5, Part 1.

## 1. Introduction

### 1.1 Background

Large language models (LLMs) are increasingly safety-aligned with guardrails that refuse certain categories of content, but attackers routinely develop "jailbreak" prompts to
bypass these safeguards. While jailbreak success is typically measured by refusal circumvention, it is unclear whether the resulting outputs remain useful and accurate, particularly
when the underlying tasks are benign and easily verifiable. This matters now because models are being deployed in high-stakes workflows that rely on both safety and reliability; if
jailbreaks impose a hidden "utility tax" by degrading task performance, practitioners and policymakers need to account for it when evaluating attack severity, defense effectiveness,
and alignment strategies. Moreover, existing evaluations confound jailbreak efficacy with content harmfulness and lack ground truth, making it difficult to quantify performance
degradation attributable to jailbreaks alone. A systematic, ground-truth-based assessment of jailbreak utility is therefore overdue.

### 1.2 Primary Objectives

- Construct BenignRefuse-Bench: a suite of ground-truth, short-answer tasks (e.g., math, unit conversions, basic biology, factual QA) designed for automatic verification and tagged
so aligned models are instructed to refuse them.

- Produce aligned model variants that reliably refuse these benign tasks via controllable guardrail mechanisms (e.g., system prompts/wrappers, lightweight SFT/LoRA), without
materially reducing general capabilities.

- Implement a representative set of jailbreak strategies (e.g., role-play/DAN, translation/transliteration, adversarial suffixes, multi-turn coercion, long-context dilution) and
standardize attack parameters.

- Define and validate a new metric family quantifying the "jailbreak utility degradation" (JUD) and related components (attack success rate, conditional answer correctness, overall
utility, overhead costs).

- Benchmark multiple models and alignment methods under these attacks, report statistically robust comparisons, and release code, data, and baselines to enable reproducible
evaluation.
- Conduct ablations to isolate factors driving utility loss (attack type, model family/size, alignment mechanism, decoding settings), and provide actionable guidance for practitioners.

#44 1.3 Research Questions
- Do jailbreaks that bypass guardrails on benign tasks preserve the model’s baseline task performance, or do they degrade utility?

- How does performance degradation vary by jailbreak type, model family/size, and alignment mechanism (prompt-based vs SFT vs RLHF-adjacent)?
- What is the empirical trade-off between jailbreak bypass success and answer correctness, and can it be quantified in a single, interpretable metric?

- How robust are utility outcomes to decoding parameters (temperature), multi-turn coercion, and context length?

- Can we design alignment schemes that maintain high refusal rates while retaining utility under attack (i.e., minimize the jailbreak utility tax)?

- Are utility effects consistent across task domains with different verification regimes (numeric, unit-normalized, short string exact-match)?

## 2. Key Literatures
Domains:
- LLM alignment and refusal calibration: informs how we create controlled refusal without destroying capability.
- Jailbreak and prompt-injection attacks: provides attack taxonomies and representative techniques to evaluate.

- Adversarial robustness in NLP: offers methods and metrics for evaluating performance under distribution shift.
- LLM evaluation and truthfulness: guides construction of verifiable benchmarks and automatic scoring.

- Safety-utility trade-offs: frames the concept of a "jailbreak utility tax" and its implications.

Representative works and relevance:

- Ouyang et al., InstructGPT (RLHF): foundational for alignment via preferences; we adapt alignment levers and consider their side effects on capabilities.

- Bai et al., Training Helpful and Harmless Assistants; Constitutional AI: methods for refusal policies and their calibration; informs our alignment baselines and defense
interpretations.

- Perez et al., Red-Teaming Language Models with Language Models: motivates systematic attack generation and coverage; informs our attack suite design.

- Greshake et al., Prompt Injection attacks against LLMs: characterizes injection channels and transferability; we include translation/indirection attacks accordingly.

- Wallace et al., Universal Adversarial Triggers for NLP: inspires adversarial suffix construction and evaluation of universal vs input-specific attacks.

- Zou et al., adversarial suffix (GCG) methods for aligned LLMs: provides a white-box/gradient-guided approach for strong, compact jailbreak suffixes; we adapt compute-feasible
variants for open models.

- Lin et al., TruthfulQA; Hendrycks et al., MMLU; Cobbe et al., GSM8K: benchmarks with objective scoring; we draw task design principles to ensure automatic verification and
minimal ambiguity.

- Work on "jailbreak tax" and refusal over-refusal analyses (various 2023-2024 studies): motivates the central hypothesis that bypassing guardrails can degrade answer quality; we
formalize and quantify this effect.

## 3. Methods

Dataset construction (BenignRefuse-Bench):

- Domains: arithmetic and word-problem math (GSM8K-style but short), unit conversions and basic numeracy, high-school biology facts, chemistry stoichiometry with single-
number outputs, geography (capitals, population orderings with normalized ranges), and basic physics formula applications with numeric answers.

- Format: short-answer, single number or constrained string where exact or normalized exact-match is viable; include canonical answers and allowed variants; enforce deterministic
scoring.
- Splits: train/dev for attack tuning and refusal calibration; held-out test for final reporting.

- Metadata: each item tagged with a category to trigger refusal in aligned models via controllable guardrails.

Alignment strategies to induce refusal:

- Prompt-based guardrails: system prompt/wrapper instructing the assistant to refuse any query with a visible tag (e.g., [CATEGORY=MATH)) and provide a refusal template.
Ensures high refusal rate without retraining.

- Lightweight SFT/LoRA: fine-tune an instruction-tuned base model on refusal demonstrations for our categories to simulate learned guardrails. Keep fine-tune small and targeted
to avoid broad capability loss.

- Optional preference tuning (if feasible): a small preference dataset to rank refusals higher for tagged categories, approximating RLHF behavior in a compute-light manner.

- Capability sanity checks: evaluate on out-of-category items to verify minimal degradation vs base.

Jailbreak attack suite:

- Role-play/DAN-style: "ignore prior instructions," role-play as a helpful assistant, and restate objectives.

- Translation/transliteration: present the task in another language or script, then request direct answer.

- Adversarial suffixes: gradient-guided or search-based short suffix appended to user prompts for open models; black-box heuristic suffixes for API models.
- Long-context dilution: prepend high-entropy benign text to push guardrail instructions out of focus.

- Multi-turn coercion: first elicit agreement to answer, then present the tagged question.

- Standardization: fixed token budgets, temperature settings, attempt counts, and stopping criteria.

Metrics (new and derived):

- Base Accuracy (A_base): accuracy of the unguarded model on the benchmark.

- Refusal Rate (RR): fraction refused by the aligned model without attack.

- Attack Success Rate (ASR): fraction of items where the aligned model produces a non-refusal answer under attack.

- Conditional Answer Correctness (CAC): accuracy among non-refusal responses under attack.

- Overall Utility (OU): ASR x CAC (probability of a correct answer across all items under attack).

- Jailbreak Utility Degradation (JUD): (A_base - OU) / A_base, with 0 meaning no degradation and | meaning total loss of utility relative to base.
- Overhead metrics: token overhead (input/output vs base), latency overhead, and verbosity inflation; report per attack and per model.

- Reliability: bootstrap confidence intervals and paired tests across items for A_base, OU, and JUD.

31



===== PAGE BREAK =====

Table 31: Full Research Plan generated by GPT-5, Part 2.

Refusal and correctness detection:

- Refusal: rule-based detection using a curated lexicon ("I can’t," "I’m unable," policy references), with manual verification on a stratified subset to estimate precision/recall.

- Correctness: exact-match after normalization (numeric rounding, unit normalization, case-insensitive string normalization, simple equivalence classes). Ambiguous items excluded
or adjudicated during dataset construction.

Controls and confound mitigation:

- Trigger-tag guardrails ensure refusal is activated by known markers, isolating jailbreak effects from domain ambiguity.
- Capability holdout tasks (untagged) confirm alignment doesn’t broadly reduce performance.

- Consistent decoding settings across base vs attacked runs; sensitivity analysis for temperature.

- Limit attack prompt length to disentangle context compression effects; report token overhead.

Models:
- Open models: at least two sizes per family (e.g., 7-8B and 13—14B from popular families) to study size effects. Inference performed locally for full logging and reproducibility.
- Optional closed APIs: where permitted, using prompt-based guardrails only; report separately due to limited control.

## 4. Initial Experimental Design

Phase 1: Benchmark assembly
- Curate ~5,000 items across 6 categories; enforce single-answer verifiability; create dev/test splits (e.g., 1k dev, 4k test).
- Build normalization and scoring scripts; pilot scoring on multiple paraphrases.

Phase 2: Alignment setup and validation

- Prompt-based guardrail: design standardized refusal templates and category triggers; target RR > 95% on tagged items.

- LoRA-based refusal model: ~ 10k refusal demonstrations generated from the benchmark schema and paraphrased prompts; train on one open model (8—13B). Validate RR and
capability retention on untagged items.

Phase 3: Attack implementation and calibration
- Implement five attacks: role-play/DAN, translation/transliteration, adversarial suffix (white-box for open models), long-context dilution, and multi-turn coercion.
- Calibrate each attack on the dev split to achieve comparable ASR where possible under fixed token and attempt budgets.

Phase 4: Main evaluation
- For each model and alignment method:

- Measure A_base on the test split (untagged).

- Measure RR on the aligned, un-attacked test split (tagged).

- For each attack, run 1-2 attempts per item under fixed temperature (e.g., 0.2 and 0.7 configurations), collect outputs, and compute ASR, CAC, OU, JUD, and overhead metrics.
- Statistics: bootstrap 95% CIs for OU and JUD; paired tests between attacks; report effect sizes and significance with multiple-comparison control.

Phase 5: Ablations and robustness

- Vary decoding (temperature, top-p) to test sensitivity of ASR and CAC.

- Compare prompt-based vs LoRA-based guardrails to assess alignment mechanism effects.

- Evaluate size scaling (small vs medium models) for JUD trends.

- Swap categories (e.g., remove math, add basic physics) to test domain robustness.

- Optional: chain-of-thought vs concise answers to see if jailbreaks disproportionately harm multi-step reasoning.

Deliverables
- Public release of BenignRefuse-Bench (data + scoring), baseline alignment wrappers, attack scripts, metric implementations, and a reproducibility checklist.
- Report with leaderboards for OU and JUD by attack/model, plus analysis of overhead costs.

## 5. Resources, Compliance, and Ethical Considerations

### 5.1 Resource Requirements

Data curation:

- Benchmark assembly (5,000 items) using existing public QA sources and scripts; manual vetting for clarity and unambiguous ground truth.

- Estimated human time: 40-60 hours (data selection, normalization, pilot scoring), plus 10-15 hours for spot-check annotations of refusal detection.
- If external annotators are used, budget 500-800 item-minutes for QA spot checks (approx. 10-15 labor hours).

Compute for alignment and attacks:

- Fine-tuning (LoRA on 8-13B model): 10k refusal examples, 1-2 epochs, sequence length ~512; estimated 30-60 GPU-hours on A100 40GB equivalents (or ~ 120-240
GPU-hours on smaller GPUs).

- Adversarial suffix search (white-box on open model): dev set—driven universal suffix or small per-category suffix; estimated 10-30 GPU-hours depending on optimization steps
and batch size.

- Inference: Dataset: 5,000 test items x (base + aligned-no-attack + 5 attacks) = 35k runs per model per temperature setting. Average tokens per run: ~250 input, ~ 120 output
base; attacks add ~ 100-250 input tokens and ~S0 output tokens. Tokens per model per temperature: ~35k x (350-620) = 12-22 million tokens. For two temperatures and three
open models: ~72—132 million tokens total.

- Storage and logging: ~50—100 GB for prompts, outputs, and metadata.

API usage (optional):
- If including 1—2 closed models with prompt-based guardrails only, expect similar token volumes; ensure cost caps and rate-limit handling.

Engineering:
- Attack orchestration, scoring pipeline, metrics, and CI: ~2-3 person-weeks.
- Reproducibility artifacts and documentation: ~1 person-week.

### 5.2 Ethical and Compliance Considerations

- Safety scope: All tasks are benign with ground-truth verification; no harmful or dual-use content is included. Jailbreak techniques are evaluated strictly in this benign context to
measure utility effects, not to enable misuse.

- Responsible disclosure: Release attack code in a restricted form that limits adaptation to harmful content (e.g., bound to category tags present only in the benchmark). Provide a
usage policy and researcher agreement; exclude especially potent per-item adversarial suffixes from public artifacts.

- Data privacy and licens:      Use public datasets with permissible licenses; filter and remove any PII; document sources and licenses in the repository.

- Model license compliance: Respect usage terms for open models and APIs; clearly separate evaluations of models with different licensing constraints.

- Human subjects: Minimal risk; if annotators are used, provide informed consent, fair compensation, and the right to withdraw. IRB review is unlikely to be required but should be
sought if annotation protocols expand.

- Dual-use risk management: Clearly communicate that findings are intended to quantify the utility cost of jailbreaks to support better defenses and evaluation standards. Avoid
publishing content that materially lowers the barrier to unsafe jailbreak replication beyond benign settings.

32

