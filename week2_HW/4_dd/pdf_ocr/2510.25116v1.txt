arXiv:2510.25116v1 [cs.CL] 29 Oct 2025

Pretraining Strategies using Monolingual and Parallel Data for
Low-Resource Machine Translation

Idriss Nguepi Nguefack
AIMS Senegal
inguepi@aimsammi.org

Abstract

This research article examines the effectiveness
of various pretraining strategies for develop-
ing machine translation models tailored to low-
resource languages. Although this work consid-
ers several low-resource languages, including
Afrikaans, Swahili, and Zulu, the translation
model is specifically developed for Lingala,
an under-resourced African language, building
upon the pretraining approach introduced by
Reid and Artetxe (2021), originally designed
for high-resource languages. Through a series
of comprehensive experiments, we explore dif-
ferent pretraining methodologies, including the
integration of multiple languages and the use of
both monolingual and parallel data during the
pretraining phase. Our findings indicate that
pretraining on multiple languages and leverag-
ing both monolingual and parallel data signifi-
cantly enhance translation quality. This study
offers valuable insights into effective pretrain-
ing strategies for low-resource machine trans-
lation, helping to bridge the performance gap
between high-resource and low-resource lan-
guages. The results contribute to the broader
goal of developing more inclusive and accurate
NLP models for marginalized communities and
underrepresented populations. The code and
datasets used in this study are publicly avail-
able to facilitate further research and ensure
reproducibility, with the exception of certain
data that may no longer be accessible due to
changes in public availability.!

1 Introduction

In recent years, pretraining techniques have gained
significant popularity and transformed the field of
natural language processing (NLP). Pretraining in-
volves training a language model on a large corpus
of text data, enabling it to learn general linguis-
tic patterns and representations. The pretrained
model can then be fine-tuned for specific down-
stream tasks, such as sentiment analysis, named

‘https ://github.com/nguepigit2020/Project1.git

Mara Finkelstein
Google
marafin@google.com

Toadoum Sari Sakayo
AIMS Senegal
tsakayo@aimsammi.org

entity recognition, or machine translation, leading
to substantial performance improvements. While
pretraining has shown remarkable success in high-
resource languages like English, Spanish, and Chi-
nese, applying these techniques to low-resource
languages poses significant challenges. These lan-
guages often lack the extensive datasets required
for effective pretraining, limiting the performance
of NLP models.

Despite these challenges, researchers have ac-
tively explored various pretraining techniques tai-
lored to low-resource languages (Costa-Jussa et al.,
2022). These approaches include unsupervised and
semi-supervised learning, transfer learning, and
cross-lingual pretraining (Khoboko et al., 2025).
Unsupervised methods leverage large amounts of
unlabeled data to learn meaningful representations,
while semi-supervised techniques enhance model
performance by combining limited labeled data
with abundant unlabeled data. Transfer learning
involves pretraining a model on a high-resource
language and fine-tuning it on a low-resource lan-
guage, exploiting linguistic similarities (Zheng
et al., 2021). Cross-lingual pretraining extends this
concept by training models on multiple languages
simultaneously, enabling them to learn shared rep-
resentations.

These techniques hold great promise for advanc-
ing NLP in low-resource languages, which are of-
ten spoken by marginalized and underrepresented
communities. Developing more effective pretrain-
ing strategies can help mitigate data scarcity and
contribute to more accurate, inclusive, and cultur-
ally sensitive NLP models (Adebara et al., 2024).
Ensuring that NLP technologies benefit all lan-
guages and communities, regardless of available
resources, is a crucial step toward linguistic inclu-
sivity (Okolo and Tano, 2024).

In this work, we evaluate the effectiveness of
various pretraining techniques for low-resource
languages, with a particular focus on Lingala, an


===== PAGE BREAK =====

under-resourced African language. To this end,
we combined monolingual and parallel data, hy-
pothesizing that this approach would yield the best
results. We pretrained multiple models using meth-
ods described in Reid et al. (2021) and Reid and
Artetxe (2021), which differ in the types of data
used for pretraining. Specifically, we examine the
impact of incorporating multiple languages and
leveraging both monolingual and parallel data dur-
ing pretraining. Finally, we evaluated these pre-
trained models by fine-tuning them on an English-
Lingala sequence-to-sequence machine translation
task. Our findings offer valuable insights into effec-
tive pretraining strategies for low-resource machine
translation and contribute to the broader goal of de-
veloping more inclusive NLP technologies.

2 Related Work

Most previous studies on multilingual pretraining
have primarily relied on monolingual data (Reid
et al., 2021; Pires et al., 2019; Song et al., 2019;
Liu et al., 2020). While foundational, this ap-
proach does not fully exploit the potential of par-
allel data. Several proposals have attempted to
incorporate parallel data into encoder-only mod-
els by training two models simultaneously: one
encoder-only model trained on the source language
and one decoder-only model trained on the target
language. During training, the encoder-only model
generates hidden representations of the source sen-
tences, which are then used to train the decoder-
only model to generate target sentences (Lample
and Conneau, 2019; Hu et al., 2020). Some ap-
proaches replace words based on a bilingual dictio-
nary, similar to the dictionary denoising objective
(Wu et al., 2019), while others use multilingual dic-
tionaries but focus only on high-resource languages
(Reid and Artetxe, 2021).

However, these methods often fail to effectively
leverage the rich information in parallel data, par-
ticularly for low-resource languages. In contrast,
sequence-to-sequence models provide a more flex-
ible and natural way to integrate parallel data.
Building on the work of Reid and Artetxe (2021),
we incorporated parallel corpora into sequence-
to-sequence pretraining by feeding concatenated
parallel sentences to the encoder and applying
different masking strategies. Unlike Reid and
Artetxe (2021), our approach specifically targets
low-resource languages while maintaining a similar
methodology.

Recent advancements in multilingual pretrain-
ing have further improved the performance of NLP
models for low-resource languages. For example,
(Pires et al., 2019) demonstrated that multilingual
BERT (mBERT) effectively captures cross-lingual
representations, though their study primarily fo-
cused on high-resource languages. Similarly, (Song
et al., 2019) introduced the MASS framework,
which employs a masked sequence-to-sequence
pretraining objective to enhance machine transla-
tion models. However, these studies did not exten-
sively explore the integration of parallel data for
low-resource languages.

Additionally, (Liu et al., 2020) proposed multi-
lingual denoising pretraining for neural machine
translation, highlighting the benefits of training on
multiple languages. Their work underscored the
importance of leveraging both monolingual and par-
allel data to improve translation quality. Building
on these insights, our study extends these pretrain-
ing strategies to low-resource languages.

By integrating both monolingual and parallel
data during pretraining, we aim to overcome the
limitations of existing approaches and develop
more effective strategies for low-resource machine
translation. Our work contributes to the growing
body of research on multilingual pretraining and
provides valuable insights into the development of
inclusive and accurate NLP models for underrepre-
sented languages.

3 Problem

Despite significant advancements in natural lan-
guage processing (NLP) and machine translation,
the benefits of these technologies are not evenly
distributed across all languages. High-resource
languages, such as English, Spanish, and Chinese,
have seen substantial improvements in translation
quality and NLP applications due to the availability
of large datasets and extensive research. However,
low-resource languages, which are often spoken by
marginalized communities and underrepresented
populations, continue to lag behind (Costa-Jussa
et al., 2022). This disparity poses significant chal-
lenges in various domains, including education,
healthcare, and digital communication.

In the context of education, the lack of effec-
tive machine translation tools for low-resource lan-
guages creates a barrier to accessing educational
materials and resources. Students and educators
in regions where these languages are spoken often


===== PAGE BREAK =====

rely on materials in high-resource languages, which
can hinder comprehension and learning outcomes.
Enhancing machine translation for low-resource
languages can facilitate the creation and dissemi-
nation of educational content in native languages
(Steigerwald et al., 2022), thereby improving edu-
cational accessibility and effectiveness.

Moreover, in healthcare settings (Al Shamsi
et al., 2020), accurate communication is crucial for
diagnosing and treating patients. Language barriers
can lead to miscommunication, misdiagnosis, and
inadequate treatment. Machine translation tools
tailored to low-resource languages can help bridge
these gaps, ensuring that healthcare providers can
effectively communicate with patients who speak
these languages.

Additionally, the digital divide is exacerbated by
the lack of support for low-resource languages in
web-centric applications and technologies (Kreien-
brinck et al., 2024). Users who speak these lan-
guages often face difficulties in accessing and in-
teracting with digital content, which limits their
participation in the global digital economy. By
improving machine translation for low-resource
languages, we can make digital platforms more in-
clusive and accessible to a broader range of users
(Bella et al., 2023).

This research aims to address these challenges by
exploring effective pre-training strategies for ma-
chine translation models tailored to low-resource
languages. Specifically, we focus on Lingala, an
under-resourced African language, and investigate
the impact of incorporating multiple languages and
both monolingual and parallel data during the pre-
training phase. Our goal is to develop more accu-
rate and inclusive NLP models that can enhance
communication, education, and digital accessibility
for speakers of low-resource languages. By doing
so, we hope to contribute to the broader goal of
reducing linguistic disparities and promoting equi-
table access to information and services.

4 Dataset

We utilized various datasets in our pretraining pro-
cess for different models, including monolingual
datasets (English, Lingala, Afrikaans, Swahili, and
Zulu), as well as parallel datasets for fine-tuning.
To provide a comprehensive summary of the data
used for each language, please refer to Table 1
below. It should be noted that we use both mono-
lingual and parallel data for pretraining, while for

fine-tuning, only parallel data is used.

4.1 Data Source

We used both parallel and monolingual datasets in
our study. Specifically, the parallel datasets were
obtained from AfroMT (Reid et al., 2021), a com-
prehensive benchmark for African language transla-
tion. The monolingual datasets Afrikaans, English,
Lingala, Swahili, and Zulu were sourced from the
open-source CC-100 dataset, which provides a di-
verse collection of monolingual corpora for various
languages. It is worth noting that some of this data
may no longer be publicly available. The datasets
originate from different sources and vary in size
and the number of sentences per language, as de-
tailed in Table 1.

4.2 Data Quality and Preprocessing

To ensure the robustness of our models, we ap-
plied several preprocessing steps to clean and aug-
ment the data. These steps included tokenization,
normalization, and duplicate removal. Addition-
ally, we addressed data imbalance, particularly for
low-resource languages like Lingala, by employing
techniques such as data augmentation and oversam-
pling. These preprocessing steps were essential
for improving the quality and consistency of the
datasets used in our experiments.

4.3 Data Split

Based on the number of sentences per language
shown in Table 1, we allocated 3,000 sentences
each for testing and validation in the parallel data,
with the remaining sentences used for training dur-
ing both pretraining and fine-tuning. For the mono-
lingual data, we designated 10% for testing, 10%
for validation, and the remaining 80% for train-
ing in each language, but only during the pretrain-
ing phase. This data-splitting strategy ensured a
balanced and representative dataset for both the
pretraining and fine-tuning phases.

5 Models and Methods

This section presents the models and methodolo-
gies used in our study, with a focus on the pretrain-
ing and fine-tuning processes (see Figure 1). Our
goal is to assess the effectiveness of various pre-
training techniques and compare their impact on
the performance of machine translation models for
low-resource languages. Specifically, we pretrain
our models on four African languages (Lingala,


===== PAGE BREAK =====

Language Code _ Parallel data(En-XX) Monolingual data
Size     Sentences     Size    Sentences
Afrikaans Af               7T7MB                   749K                        1.2G                T9T9K
Lingala     Ln     45MB      388K      10.3MB     143K
Swahili     Sw    80MB      706K        2G     12000K
Zulu        Zu     75MB      670K      18.6MB     209K
English     En     77MB        -        27.2MB     259K

Table 1: Dataset Description

Data (English - Lingala)

Pre-trained Multilingual Model
(mBART, mBARTbase)

Objective: Masked Language

Modeling, Denoising Auto-Encoding

African Multilingual Data
(Lingala, Afrikaans,
Swahili, Zulu)

Fine-Tuned Model (          )
Task: Automatic Translation En <-> Li
Objective: Minimize Cross-Entropy

ss

Figure 1: Flowchart from pretraining to finetuning

Afrikaans, Swahili, Zulu) and evaluate them on
English-Lingala machine translation tasks.

5.1 Model Architectures
5.1.1 mBART Architecture

The mBART model (Liu et al., 2020) is a sequence-
to-sequence denoising autoencoder pretrained on
large-scale monolingual corpora spanning 25 lan-
guages. It employs a standard Transformer architec-
ture with 12 encoder layers and 12 decoder layers,
each featuring a hidden dimension of 1024 and 16
attention heads. The model is trained using a de-
noising objective, in which the input is corrupted
by masking, deleting, or permuting tokens, and the
model learns to reconstruct the original sequence.
mBART has demonstrated strong performance in
multilingual machine translation tasks, particularly
in zero-shot and few-shot scenarios.

5.1.2. AfroBART Architecture

The AfroBART model (Reid et al., 2021) is a
variant of the BART model specifically designed
for African languages. It is pretrained on a com-
bination of monolingual and parallel data from

eight African languages, including Lingala. Afro-
BART employs a Transformer architecture similar
to mBART but is adapted to the unique character-
istics and data constraints of African languages.
The model seeks to address the challenges of low-
resource languages by leveraging multilingual pre-
training and transfer learning techniques.

6 Hardware and Schedule

We pre-trained our models on a single machine
equipped with two NVIDIA T4 GPUs, 32 vCPUs,
and 120 GB of RAM, with each pretraining run tak-
ing approximately four days. Fine-tuning was con-
ducted on a machine with one NVIDIA T4 GPU,
32 vCPUs, and 60 GB of RAM, requiring about
one day to complete.

The computational resources used in this study
were sufficient to efficiently handle both pretraining
and fine-tuning. The NVIDIA T4 GPUs acceler-
ated the training processes, enabling us to run mul-
tiple experiments within a reasonable time frame.
Additionally, the ample RAM and vCPUs facil-
itated smooth execution, allowing us to process
large datasets and train complex models without


===== PAGE BREAK =====

significant bottlenecks.

Pretraining was the most time-intensive phase,
requiring up to five days for some experiments, par-
ticularly those involving multiple languages and
large datasets. In contrast, fine-tuning was rela-
tively faster, taking approximately one day to com-
plete. This efficient use of computational resources
and careful time management enabled a thorough
evaluation of various pretraining strategies and
their impact on machine translation performance
for low-resource languages.

6.1 Pretraining

To better understand the impact of different data
types on the pretraining strategy, we conducted
multiple pretraining sessions, making minor modi-
fications such as altering the type of data or the de-
noising task used. The details of these pretraining
approaches are provided in the following sections.

6.1.1 First Pretraining

For the first experiment, we used only monolingual
Lingala data from the AfroMT repository (Reid
et al., 2021) (see the dataset description in Table
1). We tokenized all the data using SentencePiece
(Kudo and Richardson, 2018), employing a multi-
lingual vocabulary of 80k subwords.

We utilized the mBART implementation and the
simple denoising task from the fairseq’ library (Ott
et al., 2019) to train our models. Our setup included
a Transformer-base architecture with a hidden di-
mension of 768, a feed forward size of 3072, and
6 layers for both the encoder and decoder. The
maximum sequence length was set to 1024, and
we trained our models with a batch size of 1024
for 100k iterations on a single NVIDIA T4 GPU,
with 32 vCPUs and 60 GB of RAM. The training
process lasted approximately 24 hours.

6.1.2 Second Pretraining

For this experiment, we used both the monolingual
data of all languages, except English, for the de-
noising task and the parallel data of all languages
for the translation task. This approach combined
two tasks (denoising and translation) on two differ-
ent types of data. While this method was proposed
in (Reid and Artetxe, 2021), it focused primarily
on high-resource languages; we applied the same
approach but focused on low-resource languages.
Regarding the hyper-parameters, we used Sen-
tencePiece (Kudo and Richardson, 2018) to tok-

“https ://github.com/facebookresearch/fairseq

enize all the data, employing a multilingual vocab-
ulary of 80K sub-words. We also used the mBART
implementation from the fairseq2 (Ott et al., 2019)
library to train our model. Our configuration in-
cluded a Transformer-based architecture with a hid-
den dimension of 768, a feed forward size of 3072,
and 6 layers for both the encoder and decoder. The
maximum sequence length was set to 1024, and
we trained our model with a batch size of 1024
for 100K iterations. The model contained approx-
imately 162 million parameters and the training
lasted 4 days.

6.1.3 Third Pretraining

For this experiment, we used monolingual data
from all languages (Afrikaans, English, Lingala,
Swahili and Zulu) for the denoising task, as well as
parallel data from all languages (English-Lingala,
English-Afrikaans, English-Swahili, and English-
Zulu) for the translation task. This experiment com-
bined two pretraining tasks (denoising and transla-
tion) using two different types of data (monolingual
and parallel). It is worth noting that we selected
a random sample of 1OMB of monolingual data
from the English language. By incorporating all
languages in this phase, we aimed to achieve a
more permanent model.

Regarding the hyper-parameters, we used the
same configuration as in the previous experiment.
The model had approximately 162 million param-
eters, and training lasted for 5 days. We later re-
sumed this experiment with the same configura-
tion but increased the English data from 10MB to
112MB.

6.1.4 Fourth Pretraining

We conducted a fourth experiment in which we
used only the parallel and monolingual data of
two languages, English and Lingala. It should
be noted that we used the same tasks and hyper-
parameters as in the previous experiment. Regard-
ing pre-training time, it took about two days.

6.2 Finetuning

This section summarizes the various fine-tuning
experiments we conducted on existing models, in-
cluding those we pre-trained ourselves. We utilized
the parallel data described in Table 1 to fine-tune
all pre-trained models, with a primary focus on
machine translation between English and Lingala.

As a baseline, we started by fine-tuning the
mBART (Liu et al., 2020) model, which is pre-


===== PAGE BREAK =====

trained on 25 high-resource languages but does not
include Lingala. Next, we fine-tuned the Afro-
BART (Reid et al., 2021) model, which is pre-
trained solely on low-resource monolingual data,
including Lingala. Finally, we proceeded to fine-
tune our own pre-trained models.

We evaluated the system outputs using two auto-
matic evaluation metrics: detokenized BLEU (Pap-
ineni et al., 2002) and chrF (Popovié, 2015). While
BLEU is a standard metric for machine translation,
we use chrF to measure performance at the charac-
ter level, given the morphological richness of the
languages in the AfroMT benchmark. Both metrics
were calculated using the SacreBLEU library.

7 Results and Discussion

As shown in Table 2, we performed fine-tuning
on five pretrained models with highly varied struc-
tures. We used the mBART and AfroBART mod-
els as starting points to train an automatic transla-
tion model from English to Lingala. Evaluation
of this model showed that AfroBART outperforms
mBART in terms of both BLEU and chrF scores.
This can be explained by the fact that the monolin-
gual data used to pretrain AfroBART includes Lin-
gala, the target language of the translation, whereas
mBART is pretrained only on high-resource lan-
guages.

The BLEU and chrF scores of Experiment 1 are
significantly lower than those of mBART and Afro-
BART. This can be attributed to the fact that the
data used to pretrain our experimental model con-
sisted solely of monolingual data from a single lan-
guage (Lingala), while mBART is pretrained on 25
languages, and AfroBART is trained on 8 African
languages, including Lingala. Therefore, we can
reasonably conclude that pretraining on multiple
languages positively impacts the performance of
the resulting translation model.

The scores obtained from the evaluation of the
model in Experiment 1 led us to consider another
approach that involved using both monolingual and
parallel data during the pretraining phase. This
approach resulted in a gain of 2 BLEU points and 2
chrF points compared to Experiment 1. Therefore,
we can confidently state that the strategy used in
this experiment is more beneficial in terms of both
BLEU and chrF scores compared to Experiment 1.

We conducted an additional experiment in which
we introduced a 10 MB random sample of mono-
lingual English data. This resulted in notably low

scores, as shown in Table 2. Subsequently, we in-
creased the size of the English monolingual data to
112 MB, which led to a gain of 4 BLEU points and
3 chrF points.

Finally, we conducted a final experiment using
only the parallel and monolingual data of English
and Lingala. We observed a significant decrease
in both the BLEU and chrF scores, as shown in
Table 2. From this, we can affirm that pretraining
a model that includes other African languages is
more effective than pretraining a model solely on
the source and target languages.

8 Conclusion

In conclusion, our study aimed to investigate pre-
training strategies for machine translation models
using low-resource languages. We conducted a
series of experiments, gradually introducing mono-
lingual and parallel data to pretrained models.

We first fine-tuned a pretrained model using only
parallel data from the source and target languages.
Next, we added monolingual data to the pretraining
process, which resulted in a significant improve-
ment in the model’s BLEU and chrF scores.

Then, we introduced a random sample of English
monolingual data, which led to very low scores.
However, when we increased the size of the En-
glish monolingual data, we observed a notable im-
provement in the model’s translation performance.

Finally, we conducted an experiment using par-
allel and monolingual data from both English and
Lingala. We observed a decrease in BLEU and chrF
scores. However, when we pretrained the model
using multiple African languages, including the
low-resource language, we saw a positive impact
on translation performance. Our study underscores
the importance of considering pretraining strategies
for low-resource languages in machine translation.

It is worth noting that the pretraining approach
we used was introduced by Reid and Artetxe
(2021), but it originally focused solely on high-
resource languages. Our study demonstrates that
this approach can also be beneficial for low-
resource languages. Interestingly, mBART, which
was not pretrained on any African languages, still
outperformed our multilingual pretraining setup in
some cases. While the scores were close, mBART
performed slightly better, which may be attributed
to its larger model size, more extensive pretraining
on high-quality data, or architectural advantages.
This suggests that pretraining on high-resource lan-


===== PAGE BREAK =====

Model                      BLEU chrF Pretrained Data            Lingala include
mBART                    28.5       54.03 Monolingual                  x
AfroBART                29.33 54.67 Monolingual                  v
Experiment | 6.1.1     25.34 51.26 Monolingual              v
Experiment 26.1.2. 27.38 53.24 Monolingual & Parallel V
Experiment 3 6.1.3      21.8       48.16 Monolingual & Parallel ¥
Experiment 37 6.1.3. 25.18 51.11 Monolingual & Parallel V
Experiment 46.1.4 21.02 48.92 Monolingual & Parallel V

Table 2: Finetuning on top of English and Lingala

guages may still offer transferable benefits to low-
resource scenarios. Future research in this area
can explore different pretraining techniques and
incorporate more linguistic knowledge to further
improve the performance of machine translation
models for low-resource languages.

Limitations

Despite the valuable insights gained from this study,
there are several limitations to consider. The avail-
ability and quality of data for low-resource lan-
guages, such as Lingala, significantly impact the
effectiveness of the pretraining strategies. Addi-
tionally, the findings may not generalize to all low-
resource languages due to linguistic differences.
Computational resources required for pretraining
and fine-tuning can also be prohibitive, and the re-
liance on BLEU and chrF scores may not fully cap-
ture translation quality, especially for morphologi-
cally rich languages. Future work should explore
more diverse data sources and evaluation methods,
such as human evaluation, to better address these
challenges.

9 Acknowledgments

We would like to extend our heartfelt appreciation
to the African Institute for Mathematical Sciences
(AIMS) and the African Master’s of Machine Intel-
ligence (AMMI) program. Their unwavering sup-
port and provision of top-notch machine learning
training have been instrumental in the success of
our project. We are truly grateful for their guidance
and assistance throughout this endeavor.

In addition, we would like to express our deep
gratitude to Google for generously granting us ac-
cess to the Google Cloud Platform (GCP). This
invaluable resource enabled us to conduct our ex-
periments with utmost efficiency and effectiveness.
We recognize the significant contribution that this
grant has made to our research.

Furthermore, we would like to extend a spe-
cial acknowledgment to the dedicated AMMI staff.
Their exceptional assistance and unwavering sup-
port have been invaluable to us. Their expertise
and commitment have played a crucial role in the
development and execution of our project.

References

Ife Adebara, AbdelRahim Elmadany, and Muhammad
Abdul-Mageed. 2024. Cheetah: Natural language
generation for 517 african languages. arXiv preprint
arXiv:2401.01053.

Hilal Al Shamsi, Abdullah G Almutairi, Sulaiman
Al Mashrafi, and Talib Al Kalbani. 2020. Implica-
tions of language barriers for healthcare: a systematic
review. Oman medical journal, 35(2):e122.

Gabor Bella, Paula Helm, Gertraud Koch, and Fausto
Giunchiglia. 2023. Towards bridging the digital lan-
guage divide. arXiv preprint arXiv:2307.13405.

Marta R Costa-Jussa, James Cross, Onur Celebi, Maha
Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe
Kalbassi, Janice Lam, Daniel Licht, Jean Maillard,
et al. 2022. No language left behind: Scaling
human-centered machine translation. arXiv preprint
arXiv:2207.04672.

Junjie Hu, Melvin Johnson, Orhan Firat, Aditya Sid-
dhant, and Graham Neubig. 2020. Explicit alignment
objectives for multilingual bidirectional encoders.
arXiv preprint arXiv:2010.07972.

Pitso Walter Khoboko, Vukosi Marivate, and Joseph Se-
fara. 2025. Optimizing translation for low-resource
languages: Efficient fine-tuning with custom prompt
engineering in large language models. Machine
Learning with Applications, page 100649.

Annika Kreienbrinck, Saskia Hanft-Robert, and Mike
Mosko. 2024. Usability of technological tools to
overcome language barriers in health care: a scoping
review protocol. BMJ open, 14(3):e0798 14.

Taku Kudo and John Richardson. 2018. Sentencepiece:
A simple and language independent subword tok-
enizer and detokenizer for neural text processing.
arXiv preprint arXiv: 1808.06226.


===== PAGE BREAK =====

Guillaume Lample and Alexis Conneau. 2019. Cross-
lingual language model pretraining. arXiv preprint
arXiv:1901.07291.

Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey
Edunov, Marjan Ghazvininejad, Mike Lewis, and
Luke Zettlemoyer. 2020. Multilingual denoising pre-
training for neural machine translation. Transac-
tions of the Association for Computational Linguis-
tics, 8:726-742.

Chinasa T Okolo and Marie Tano. 2024. Closing the
gap: A call for more inclusive language technologies.

Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,
Sam Gross, Nathan Ng, David Grangier, and Michael
Auli. 2019. fairseq: A fast, extensible toolkit for se-
quence modeling. arXiv preprint arXiv: 1904.01038.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of the
40th annual meeting of the Association for Computa-
tional Linguistics, pages 311-318.

Telmo Pires, Eva Schlinger, and Dan Garrette. 2019.
How multilingual is multilingual bert?   arXiv
preprint arXiv: 1906.01502.

Maja Popovié. 2015. chrf: character n-gram f-score for
automatic mt evaluation. In Proceedings of the tenth
workshop on statistical machine translation, pages

392-395.

Machel Reid and Mikel Artetxe. 2021.   Par-
adise: Exploiting parallel data for multilingual
sequence-to-sequence pretraining. arXiv preprint
arXiv:2108.01887.

Machel Reid, Junjie Hu, Graham Neubig, and Yutaka
Matsuo. 2021. Afromt: Pretraining strategies and
reproducible benchmarks for translation of 8 african
languages. arXiv preprint arXiv:2109.04715.

Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-
Yan Liu. 2019. Mass: Masked sequence to sequence
pre-training for language generation. arXiv preprint
arXiv: 1905.02450.

Emma Steigerwald, Valeria Ramirez-Castafieda, Déb-
ora YC Brandt, Andras Baldi, Julie Teresa Shapiro,
Lynne Bowker, and Rebecca D Tarvin. 2022. Over-
coming language barriers in academia: Machine
translation tools and a vision for a multilingual future.
BioScience, 72(10):988-998.

Shijie Wu, Alexis Conneau, Haoran Li, Luke Zettle-
moyer, and Veselin Stoyanov. 2019. Emerging
cross-lingual structure in pretrained language models.
arXiv preprint arXiv: 1911.01464.

Francis Zheng, Machel Reid, Edison Marrese-Taylor,
and Yutaka Matsuo. 2021. Low-resource machine
translation using cross-lingual language model pre-
training. In Proceedings of the First Workshop on
Natural Language Processing for Indigenous Lan-
guages of the Americas, pages 234—240.
