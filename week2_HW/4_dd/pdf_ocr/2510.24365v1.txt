arX1v:2510.24365v1 [cs.CL] 28 Oct 2025

Text Simplification with Sentence Embeddings

Matthew Shardlow
Manchester Metropolitan University
m.shardlow@mmu.ac.uk

Abstract
Sentence embeddings can be decoded to give approximations of the original texts used to create them. We
explore this effect in the context of text simplification, demonstrating that reconstructed text embeddings
preserve complexity levels. We experiment with a small feed forward neural network to effectively learn a
transformation between sentence embeddings representing high-complexity and low-complexity texts. We
provide comparison to a Seq2Seq and LLM-based approach, showing encouraging results in our much smaller
learning setting. Finally, we demonstrate the applicability of our transformation to an unseen simplification dataset
(MedEASI), as well as datasets from languages outside the training data (ES,DE). We conclude that learning
transformations in sentence embedding space is a promising direction for future research and has potential to un-
lock the ability to develop small, but powerful models for text simplification and other natural language generation tasks.

Keywords: Text Simplification, Readability, Sentence Embeddings

1. Introduction

We investigate the degree to which sentence em-
beddings can be used within the context of the text
simplification task. Text simplification is a monolin-
gual translation task in which complex sentences
are converted into simpler alternatives (Saggion
and Hirst, 2017). In the data-driven paradigm the
notions of complexity, simplicity and the mode of
transition between them is governed by large paral-
lel datasets containing examples of simplifications
(Jiang et al., 2020).

Sentence embeddings provide a distributional
representation of the semantic contents of a text.
For any given sequence of tokens T = ¢1, ta, ...ty
we can apply some function f(a) to transform T
into its corresponding embedding FE where F is a
fixed-size vector of real numbers. This gives the
following formula describing the creation of some
embedding E from any given sentence T:

f[Q)=E

Recently, work has demonstrated that it is possible
to decode the original content of a sentence based
on the sentence embedding, giving the inverse for-
mula f~*(y), or:

f-\(B) =T

Such that f~'(y) reverses the effect of f(x), lead-
ing to f~'(f(T)) = 7", where T = T’

In this work, we investigate the potential of
sentences reconstructed from sentence embed-
dings for the task of sentence simplification. Sen-
tence simplification is a sub-domain of text simpli-
fication which seeks to transform some complex-
original sentence T, into a simple-target sentence
T,. There are many approaches to sentence sim-
plification, including data-driven approaches which

rely on parallel corpora to learn sentence transfor-
mation operations.

In our study, we first investigate the degree to
which reconstructed sentences preserve complex-
ity levels by applying the f-1 f(T.) = T’, transform
and observing the differences between 7, and the
resulting T,. We then seek to learn some transfor-
mation g(x) in sentence embedding space between
the embeddings for complex E, and simple £, sen-
tences:

g(Ec) = Es

This then allows us to perform the following se-
quence of functions:

f-"(g(f(Le))) = Ts

whereby a sentence embedding F, is created for
the complex sentence T;, via the application of f().
Then, EF, is transformed into E via g(x) and finally
T, is given via the application of f—+(2) to E,.

All code and data associated with this research
is released via GitHub. '

2. Related Work

Text simplification is an established field within nat-
ural language processing, seeking to create sim-
pler representations of complex input texts (Ondov
et al., 2022). Typical approaches to text simplifi-
cation range from lexical (North et al., 2025), and
operations-based approaches (Dong et al., 2019)
to sequence-to-sequence translation (Martin et al.,
2022) at the sentence and document level (Sun
et al., 2021; Blinova et al., 2023). Text simplifi-
cation corpora are available for Wikipedia-domain

Thttps://github.com/matt shardlow/
TextSimplificationWithSentenceEmbeddings



===== PAGE BREAK =====

(Coster and Kauchak, 2011; Jiang et al., 2020), as
well as many recent corpora in the medical domain
(Trienes et al., 2022; Luo et al., 2022; Horiguchi
et al., 2024; Campillos-Llanos et al., 2024; Joseph
et al., 2023). Whereas text simplification resources
are generally available in English, there is a grow-
ing trend towards the creation of text simplification
resources for languages outside of English (Trienes
et al., 2022; Horiguchi et al., 2024; Campillos-
Llanos et al., 2024; Joseph et al., 2023; Grabar
and Cardon, 2018; Lee and Vajjala, 2022; Hart-
mann and Aluisio, 2020; Dmitrieva and Tiedemann,
2021).

Whilst our task is primarily focussed on the sim-
plification domain, the methodology we employ is
that of sentence embeddings. Distributional seman-
tic representations of texts at the lexical (Mikolov
et al., 2013; Pennington et al., 2014), document
(Sparck Jones, 1972) and sentence (Reimers and
Gurevych, 2019) levels are vital for modern deep
learning based approaches to NLP. In particular, we
make use of SONAR (SONAR stands for Sentence-
level multimOdal and laNguage-Agnostic Repre-
sentations) embeddings (Duquenne et al., 2023)
which provide a single common 1024-dimensional
embedding space for sentences from diverse lin-
guistic sources and modalities. SONAR embed-
dings are used in multiple downstream NLP tasks
such as machine translation (Seamless, 2025) and
concept modelling (Barrault et al., 2024; Dragunov
et al., 2025).

3. Methodology

3.1.

We make use of the SONAR embedding frame-
work to create fixed-size representations of our
source texts and also to decode embeddings into
target texts. Sonar is multilingual (covering 200
languages) and multi-modal (trained on text and
speech corpora), although we only use it in a mono-
modal capacity in this work (text-embeddings only).
SONAR embeddings and models are available via
GitHub? and we used these in the standard config-
uration following the example code.

SONAR relies on a Transformer encoder-
decoder architecture (Vaswani et al., 2017), which
is initialised with pre-trained MT weights and opti-
mised using parallel data according to a machine-
translation objective, a denoising auto-encoding
objective and a MSE loss objective. Sonar embed-
dings are created via mean-pooling of token-level
encoder outputs in an auto-encoding setting. The
encoder-decoder model was trained for 100K up-
date steps across 200 languages against NLLB

Sentence Embeddings

*https ://github.com/facebookresearch/
SONAR/

(Costa-Jussa et al., 2022).

3.2. Decoding Sentence Embeddings

SONAR introduces random interpolation decoding,
which takes the trained encoder-decoder model,
freezes the encoder and fine-tunes the decoder
weights on the random interpolation task. The task
is defined as follows:

Given a bitext x, y, encode x and y with the
frozen encoder, randomly draw Z as a ran-
dom interpolation of x and y embeddings,
and learn to decode sentence embedding
Z into y.

This effectively combines the translation and auto-
encoding tasks, allowing the decoder to produce
new decoded texts based on a pooled representa-
tion of the encoder weights (the SONAR embed-
ding).

3.3. Datasets

We additionally adopt a number of text simplification
datasets from the text simplification literature for
our task. These are..

ASSET: Comprising 2000 instances of valida-
tion data and 359 test examples, each with 10 ref-
erence simplifications. ASSET is widely used in
text simplification evaluation in conjunction with the
SARI metric. The simplifications in ASSET were
collected via Crowdsourcing. Asset is in English
only. (Alva-Manchego et al., 2020)

WikiAuto: An automatically aligned dataset com-
prising 488,332 parallel sentences from 138,095
article pairs extracted from English Wikipedia and
Simple Wikipedia. WikiAuto is English only (Jiang
et al., 2020).

MedEASI: A crowdsourced dataset of 1,979 par-
allel sentences from the medical domain. English
only. We do not use this dataset for training, but
we do evaluate against it for comparison to other
systems (Basu et al., 2023).

DEPlain (DE): A dataset in the German language
consisting of news domain and web-domain texts
which have been professionally simplified into easy-
to-read German and manually aligned. We adopt
1883 instances of manually aligned web-text for
testing only (Stodden et al., 2023).

CLARA-MeD (ES): A Spanish dataset in the
medical domain with 1200 instances of parallel sen-
tences extracted from 1040 announcements from
the European Clinical Trials Register. All simpli-
fications are produced manually at the sentence
level(Campillos-Llanos et al., 2022).


===== PAGE BREAK =====

3.4. Evaluation Metrics

FKGL: Flesch-Kincaid Reading Grade Level
(Flesch, 1948) is based on a readability
formula that converts the numbers of words
per sentence and syllables per word into a US
educational grade level - indicating that a text
is suitable for a reader at this level.

ARI: Similar to FKGL, the automated readability
index (Senter, 1967) takes into account char-
acters per word and words per sentence and
is aligned to the US reading grade system.

CEFR-Prediction: CEFR grade levels are
an international standard in language
learning, indicating the fluency of a
learner. We make use of a sentence-level
CEFR-prediction model from huggingface
AbdullahBarayan/ModernBERT-base-
doc_sent_en-Cefr, which is an instance
of ModernBert (Warner et al., 2024). To give
a predicted CEFR level at the corpus level,
we take the mean average of CEFR-level
predictions for each corpus instance.

BLEURT: An evaluation metric which is a regres-
sion model trained on human ratings of fluency
and meaning preservation. BLEURT consid-
ers the reference sentences and the system
outputs (Sellam et al., 2020).

SARI: Widely used in text simplification evaluation
and included here for completeness and com-
parison. SARI Score (Xu et al., 2016) is the
aggregated F1-score over the Addition, Keep
and Delete operations as observed between
the original source documents and references
in comparison to the system outputs.

LENS: A learnt metric for simplification quality
(Maddela et al., 2023). LENS considers the
source texts, system outputs and references
and provides scores at the sentence level,
which we average via the mean.

4. Sentence Reconstruction

In this section, we seek to answer the question of
whether reconstructed sentences preserve the com-
plexity level of the original sentences from which
the corresponding embeddings were created. To in-
vestigate this question, we took the validation data
from the Asset corpus (2000 instances) and a sam-
ple of the training data from the WikiAuto corpus
(2000 instances). For Asset, each complex-original
sentence has 10 simple-targets, but for this part
of the study we only considered the first reference
(index 0) for each complex-original sentence. Both
datasets consist of complex-original sentences

C = Ta, Teo, ... and corresponding simple-target
sentences S = 7.1, 72,..., where each TJ. has a
corresponding 7; which is the gold-standard sim-
plified version of the complex-original sentence.

We then applied the transform f—1(f(x)) to each
instance of C and S to give C’ == T!,, T’y, ... and
S’ = T!,, Ti, ... respectively. We report results of
this experiment in Table 1, where we show statis-
tics for the complex-original sentences C, and their
reconstructions C’ as well as the simple-target sen-
tences S and their reconstructions S’ in terms of
FKGL, ARI and CEFR across both datasets. We
additionally report BLEURT-score between C and
C’ and S and S’ in each case.

The results in Table 1 demonstrate that there
is an improvement in reading ease between the
complex-original sentences and the simple-target
sentences. For example in the case of the AS-
SET dataset, the Flesch-Kincaid reading grade
level (FKGL) for the complex-original sentences
is 11.250, whereas the FKGL for the simple-target
sentences is 8.463. This indicates that the simple
sentences are around 2.787 reading grades apart.
We should expect this as the simple-target sen-
tences are designed to be simplified counterparts to
the complex-original sentences. We also note the
same effect for ARI. In WikiAuto, the reading grade
level of the complex-original sentences is higher
than that of those in Asset (12.754 vs. 11.250)
indicating that the WikiAuto complex-original sen-
tences are around 1.504 reading grades higher
in complexity than those in ASSET. The same is
true for the simple-target sentences, where those
in WikiAuto are more than 2 reading grades higher
than in Asset. There is however still a simplifica-
tion effect in terms of FKGL between the complex-
original sentences of WikiAuto (12.754) and the
corresponding simple-target sentences (10.292),
with a decrease of 2.462 reading grades.

We observe similar effects according to the aver-
age predicted CEFR levels for both datasets, cor-
roborating the results derived from the two readabil-
ity formulae employed. For Asset, the CEFR score
is 2.447 for the complex-original data and 2.096 for
the simple-targets, indicating that there is a simplifi-
cation of around 0.351 of a CEFR level (i.e., mid-B1
to low-B1). For WikiAuto, there is a similar drop of
0.375 CEFR levels from 3.092 (low-B2) to 2.717
(mid-B1).

To determine the degree to which the recon-
structed texts represent the original texts , we cal-
culated the difference between the metrics for the
reconstructed sentences (T") and the original Sen-
tences (JT) as AT’. The reconstructed Texts (T’) are
very close to the original texts (T) in terms of com-
plexity levels. For example in the Asset dataset,
the FKGL of the complex-original texts is 11.250
and the FKGL of the reconstructed complex-original


===== PAGE BREAK =====

Metric       C        C’       AC       S        S’       AS
Asset      FKGL     11.250 | 11.175 | -0.075 | 8.463    8.546 | 0.083
ARI      11.458 | 11.375 | -0.083 | 8.149    8.232 | 0.083
CEFR     2.447    2.451     0.004 | 2.096    2.137 | 0.041
BLEURT     —_        —_      0.923      —_        _—     0.924
WikiAuto     FKGL     12.754 | 12.623 | -0.131 | 10.292 | 10.305 | 0.013
ARI      12.896 | 13.035 | 0.138 | 9.849 | 10.095 | 0.246
CEFR     3.092    3.119 | 0.027 | 2.717    2.794 | 0.077
BLEURT     —_        —_      0.824      _—        _—     0.855
Table 1: The results obtained when reconstructing Complex (C) or Simple (S) sentences for the ASSET
Dataset and WikiAuto.
Final          level data from Wiki-Auto and Asset. We make
K       Params | Epochs         Loss             use 2000 instances of WikiAuto for validation data,
256     525,568     8200 | 4.712 x 10-°       the remainder of WikiAuto (n=486,332) for train-
512 | 1,050,112 | 10000 | 4.302 x 107°        ing and also the Asset validation data for training
1024 | 2,099,200 | 10000 | 3.931 x 10°       (n=2000). We test on the ASSET test set (n=357).
2048 | 4,197,376 | 9650 | 3.650 x 10~°       For all datasets, the complex-original source data
4096 | 8,393,728 | 4800 | 3.618 x 10~°       and the simple-target data was transformed into

Table 2: The results of training Neural Networks of
various sizes to learn g(x)

texts is 11.175 indicating a 0.075 drop in reading
grade level. Similarly, there is a 0.083 increase in
FKGL between the simple-target sentences and
their reconstructions. The largest delta is in terms
of ARI for the simple-target sentences of WikiAuto,
where there is a 0.246 increase in ARI reading-level.
This is around 10% of the difference between the
complex-original and simple-target sentences for
WikiAuto.

To summarise our analysis, we demonstrate that
there is a large difference between all metrics for
the complex-original and simple-target sentences
for both datasets. We also show that reconstructed
sentences preserve the complexity level (FKGL,
ARI, CEFR) and semantic content (BLEURT-score)
of the original sentences. Finally, we also observe
that the reconstructed complex-original and simple-
target texts have a similar degree of difference to
the original texts in terms of readability metrics,
indicating that the degree of complexity change
is also preserved. This allows us to confirm that
reconstructed texts preserve complexity level.

5. Sentence Simplification

Given the findings in Section 4, we hypothesise
that it is possible to learn some function g(a) that
will represent the generic transformation between
FE, and E,. To derive this function we make use of
a multi-layer perceptron, trained through backprop-
agation on data from WikiAuto and ASSET with
varying network sizes.

For training our network, we rely on sentence-

SONAR space via the SONAR encoder to give a
1024 dimensional representation of each original
and target sentence.

We configured a 2-layer fully-connected neu-
ral network using Pytorch, which consists of
an input layer representing the embeddings
(d=1024), a hidden layer of K nodes and an out-
put layer of 1024 nodes. We experimented with
K=[256,512,1024,2048,4096], allowing us to ob-
serve the effect of compressing (256,512) and ex-
panding (2048,4096) the representation space. We
used the Adam optimiser (learning rate = 0.001),
with mean-squared error loss calculated between
the model predictions and the reference embed-
dings. We allowed the network to run for 10K
epochs, with checkpointing every 50 epochs and
early stopping implemented for 5 successive check-
points of loss increase on the validation data and
rollback to the previous best model checkpoint. All
experiments were conducted on an A100 GPU with
40GB of RAM via Google Colab.

We present results of training our neural network
which approximates g(a) in Table 2, where we can
observe that (a) the parameter count increases
linearly with the size of K, (b) the parameter count
is in the order of 0.5-8 million, which is much smaller
than other models used for text simplification and (c)
we are able to train a network in which the MSE loss
calculated on the validation data decreases across
epochs until the training ends (10K epochs) or the
early stopping criteria is reached. The network with
the lowest final loss has 4096 hidden nodes. We
did not train any larger networks due to limitations
on GPU RAM.

We then further test our neural network by run-
ning the full pipeline as evaluation f~+(g(f(C))) =
S’, where S = S’. We used the model with 2048
hidden parameters for this evaluation. In this set-


===== PAGE BREAK =====

Metric         C                          TSSE | Seq2Seq | LLM

Asset       FKGL | 11.668 | 8.154 | 8.303        7.126        9.022
ARI | 12.027 | 7.848 | 7.601     6.549    9.113

CEFR | 2.465 | 2.048 | 2.022      2.000      2.114
MedEASi | FKGL | 13.736 | 11.476 | 10.953 | 11.530 | 10.532
ARI | 13.799 | 11.687 | 10.189 | 11.206 | 10.942

CEFR | 3.490 | 3.100 | 3.103    3.173    2.907

Table 3: Metrics demonstrating the measured complexity of the original texts (C), the simple references
(S), our system (TSSE) and two baselines from the literature (Seq2Seq and LLM). We report for the
ASSET Test set and MedEASi, both of which were unseen during model training.

Metric  TSSE | Seq2Seq | LLM
BLEURT | 0.5909 | 0.7159 | 0.7314
LENS        49.723 | 64.551 | 71.739
I] SARI-Add | 5.021  8.108 | 13.996
| SARI-Keep | 40.848 | 58.062 | 58.083
2] SARIDel | 68.316 | 60.698 | 70.328
SARI  38.061 | 42.290 | 47.469
BLEURT | 0.4669 | 0.5722 | 0.5841
LENS  27.473 | 37.809 | 47.342
| SARI-Add | 2.111  2.578  5.834
ti | SARI-Keep | 26.460 | 44.058 | 39.940
8| SARI-Del | 76.568 | 58.812 | 74.374
=  SARI  35.046 | 35.149 | 40.049

Table 4: Semantic metrics demonstrating the sim-
ilarity of the sentences produced by our system
(TSSEO and 2 baselines (Seq2Seq and LLM) to
the reference simplifications for ASSET test (10
refs) and MedEASi (1 ref).

Metric  TSSE
DEPlain (DE) | BLEURT | 0.470
SARI  37.198
ClaraMed (ES) | BLEURT | 0.266
SARI  25.996

Table 5: The results of applying g(x) for embed-
dings constructed from languages outside of the
training data German (DE) and Spanish (ES). Only
Bleurt and SARI are reported as these are language
independent.

ting, we take the complex-original test set of ASSET,
encode it using the SONAR encoder, pass the re-
sulting embeddings through the pre-trained neural
network and then decode using the SONAR de-
coder. We term this approach ‘Text Simplification
with Sentence Embeddings’ or TSSE. The result-
ing decoded texts are then evaluated against the
reference texts for ASSET using FKGL, ARI and
CEFR-score in Table 3 and SARI score, BERTscore
and LENS in Table 4. We include baseline systems
which are taken from the BLESS GitHub repository.
For both systems we made use of the generated
system outputs that are available in the repository,
however we did perform our own evaluation on

each system output to ensure fairness and cover-
age over all metrics we report. The two baseline
systems we use are a Seq2Seq baseline based on
the MUSS system (Martin et al., 2022) anda LLM
system based on GPT-3.5. Full details of system
configurations for these systems are available via
BLESS (Kew et al., 2023).

6. Multilingual Sentence
Simplification

Finally, we experiment within the SONAR embed-
ding space to determine if a transform g(a) learnt
for English could be applicable to other languages.
Effectively, we are testing the degree to which the
representational space of the SONAR embeddings
encodes cross-lingual simplification properties. To
investigate this, we made use of the DEPlain and
CLARA-MeD datasets in German and Spanish re-
spectively. SONAR provides a multilingual encoder
f(x) and decoder f~!(x) which operates in the
same space as for English. This meant that to im-
plement the experiment in Spanish and German,
we changed a single parameter referencing the
language of the encoder/decoder.

For evaluation, we report only on SARI and
BLEURT as these are language independent met-
rics. FKGL and ARI are both designed for English
reading grade levels. The CEFR prediction and
LENS models are both also trained on English
sources only. SARI does not require a base model,
instead relying on reference texts. BLEURT does
make use of a base model, but this is multilingual,
with coverage of German and Spanish.

For the experiment, we collected the complex-
original texts for DEPlain and CLARA-MeD as well
as the simple-target texts for each dataset. We en-
coded the original texts to give an embedding rep-
resentation via SONAR, applied our model trained
on English simplification data to the embeddings to
give embeddings representing simplified versions
of the original texts and finally decoded the result-
ing embeddings via SONAR using the appropriate
language flag. We then compared the complex-
original texts to the system outputs with respect to


===== PAGE BREAK =====

the simple-target texts via SARI and we also com-
pared the system outputs to the simple-target texts
via BLEURT score.

The results in Table 5 show mixed performance.
Whereas the results for DEPlain show similar
scores to those in English, the results for ClaraMed
are much lower. The results for DEPlain are in line
with those reported by the original study which re-
port a SARI score of 34.828 on a fine-tuned MBart
(see Table 5 in (Stodden et al., 2023)). It is no-
table that English and German are more ethnolin-
guistically similar than English and Spanish and it
may be the case that the simplifications produced
by g(x) are more suitable for closely-related lan-
guages than those that are further apart, although
we do not have sufficient data to fully investigate
this claim. It is nonetheless encouraging to note
that some form of simplification appears to have
been produced for German, despite the model be-
ing trained exclusively on English data.

7. Discussion

We intended to investigate the degree to which
sentence embeddings (specifically SONAR embed-
dings) can be used within the context of the text sim-
plification task. Our results have demonstrated that
complexity is preserved through encoding and de-
coding of sentence embeddings (See Table 1 and
that a small neural network (8M params, see Table
2) is capable of learning a mapping between em-
beddings representing complex texts and embed-
dings representing simple texts. We further demon-
strate that texts reconstructed from generated em-
beddings closely match the complexity level of ref-
erence simplifications across 2 datasets, including
one out-of-domain dataset (see Table 3). We also
observed that our model underperforms compared
to baseline models when using metrics that rely on
simplified references — indicating that there is still
a gap in performance in the proposed approach.
Finally, we report promising results on the potential
to transfer the learnt mapping to other languages
by making use of the shared multilingual embed-
ding space, demonstrating encouraging metrics for
German, but not for Spanish (see Table 5).

We have included several randomly sampled sys-
tem outputs for the TSSE system for English, Span-
ish and German in Table 6. These are presented
with both the original complex sentence and the
produced simplification. The simplification quality
is typically higher for shorter sentences that do not
contain too many named entities — which is unsur-
prising given the degree of semantic compression
that is taking place in the transformation to SONAR
embeddings. We also note some failure cases
of hallucinations in simplification, factual inaccura-
cies and at time neural text degeneration failures

(Holtzman et al., 2019). That being said many of
the simplifications that were produced appear to
preserve semantic content whilst transforming the
texts at the syntactic and lexical levels to appear
more simple.

We have not explicitly evaluated the degree to
which simplified texts produced via sentence em-
beddings (or other means) are useful for a reader.
We would note however that the metrics we have
produced are aligned to human judgments and
aligned to the simplified references, indicating that
the texts that are produced are ‘simple’ according to
the definition of simplicity derived from the training
corpora.

Our system is not intended as a new state-of-
the-art for the text simplification task, but rather
as an exploration of the possibilities of transforma-
tions within sentence embedding space as applied
to text simplification. We are able to demonstrate
the production of competitive simplifications with
a very small model using a simple neural architec-
ture. We expect that future work in this new exciting
paradigm of natural language processing will allow
for future improved results over those that we have
produced here.

Limitations

A principal limitation on our work was the capacity to
use high-spec GPU machines at training time. This
limited our study to a maximum size of 4096 hidden
nodes, and we suspect that a larger hidden layer or
a more complex neural architecture may have been
suitable for further improving the performance of
our model. Additionally, this placed a limitation on
how much data we were able to train our model
against.

Our model is arguably also limited by the avail-
ability of high quality text simplification corpora.
The largest such corpus for English is WikiAuto
with 488,332 parallel sentences, however these
are automatically aligned and do not conform to
any definition of ‘simplified language’ beyond that
of guidelines provided to Simple Wikipedia editors.
The provision of large-scale high-quality simplified
corpora would greatly improve data-driven simplifi-
cation approaches such as ours.

Finally, our approach is fundamentally self-
limited by the reliance on a 1024 embedding space.
This creates a fixed-size representational space
over which we have no control — we did not re-
train the encoder or decoder, instead only operat-
ing on the embeddings themselves. Retraining the
encoder-decoder model for the sentence embed-
dings (e.g., to add additional languages), would
be expensive and beyond the capacity of most re-
searchers.


===== PAGE BREAK =====

Corpus | Original                                       System Output

Asset      In its pure form, dextromethorphan oc- | In pure form dextromethorphan is
curs as a white powder.                     present as white powder.
At the Voyager 2 images Ophelia ap- | On the Voyager 2 images Ophelia ap-
pears as an elongated object, the major | pears as the large object with an axis
axis pointing towards Uranus.             that points to Uranus.
Today NRC is organised as an indepen- | Today NRC is organized as an indepen-
dent, private foundation.                    dent, private foundation.
It is situated at the coast of the Baltic | It is located at the coast of Baltic Sea
Sea, where it encloses the city of Stral- | around the city of Stralsund.
sund.
He was also named 1982 "Sportsman | He was named 1982 Sportsman of the
of the Year" by Sports Illustrated.           Year by Sports Illustrated.

DEPlain | Der Generator kann rund sieben Watt | Der Generator kann bis zu 7 Watt Strom
erzeugen — um Mobiltelefone, Taschen- | erzeugen. Er kann Blitzgerate, GPS-
lampen oder GPS-Gerate mit Energie | Gerate und andere Gerate einsetzen.
zu speisen.

Sie sollen die Natur dabei nah, intensiv | Das Ziel ist es, die Natur zu erforschen,

und urspringlich erleben.                die Natur zu erforschen, die Natur Zu
erforschen.

Das Lebendige Museum Online (LeMO) | Das "Lebensmuseum" ist ein Museum

ist ein Gang durch die deutsche | aus dem 19. Jahrhundert.

Geschichte von der Mitte des 19.

Jahrhunderts bis zur Gegenwart.

So schnell kann man zu Fuf3 oder mit | Es ist nicht mdéglich, den Weg zu FuB3

der Bahn keinen Rundgang machen.     zu beschleunigen.

Google hat vor Uber einem Jahr 17 be- | Google hat vor 17 Jahren mehr als ein-

deutende Museen und Galerien im Inter- | tausend Kunstmuseen auf der ganzen

net unter "Art Project" fir alle zuganglich | Welt zuganglich gemacht.

gemacht.

MedEASi | Mujeres no postmenopdausicas (minimo | Las mujeres que no son menopausicas,
12 meses) o quirurgicamente estériles | 0 que tienen un embarazo prematuro,
tienen que obtener un negativo en el | reciben una prueba de deteccidn y una
test de embarazo de la visita de screen- | prueba de detecci6n.
ing y al final del estudio.

Mujeres en edad fértil que aun no es- | Las mujeres en edad fértil que no estén
tan embarazadas en el momento de la | embarazadas estan dispuestas a usar
entrada en el estudio y que no estan | el anticonceptivo hasta los 40 dias de
dispuestas a abstenerse de tener rela- | embarazo.

ciones sexuales con hombres o prac-

ticar métodos anticonceptivos apropia-

dos hasta el dia 140 del estudio

Sujeto no podria recibir medicaci6n que | El paciente podria no recibir tratamiento
interfiera en la coagulacion o la funci6én | con la droga durante el periodo de es-
plaquetaria en los 3 dias previos a la | tudio.

primera dosis del farmaco del estudio

o durante el periodo de tratamiento del

estudio.

Table 6: Examples of system outputs for English, German and Spanish. The outputs demonstrate
simplifications, as well as examples of hallucinations and decoding failures.

8. Bibliographical References         cia Specia. 2020. ASSET: A dataset for tuning
and evaluation of sentence simplification mod-
els with multiple rewriting transformations. In
Proceedings of the 58th Annual Meeting of the
Fernando Alva-Manchego, Louis Martin, Antoine      Association for Computational Linguistics, pages

Bordes, Carolina Scarton, Benoit Sagot, and Lu-


===== PAGE BREAK =====

4668-4679, Online. Association for Computa-
tional Linguistics.

Loic Barrault, Paul-Ambroise Duquenne, Maha EI-
bayad, Artyom Kozhevnikov, Belen Alastruey,
Pierre Andrews, Mariano Coria, Guillaume Coua-
iron, Marta R Costa-jussa, David Dale, et al.
2024. Large concept models: Language mod-
eling in a sentence representation space. arXiv
preprint arXiv:2412.08821.

Chandrayee Basu, Rosni Vasu, Michihiro Ya-
sunaga, and Qian Yang. 2023. Med-easi: Finely
annotated dataset and models for controllable
simplification of medical texts. In Proceedings
of the AAAI Conference on Artificial Intelligence,
volume 37, pages 14093-14101.

Sofia Blinova, Xinyu Zhou, Martin Jaggi, Carsten
Eickhoff, and Seyed Ali Bahrainian. 2023. SIM-
SUM: Document-level text simplification via si-
multaneous summarization. In Proceedings of
the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Pa-
pers), pages 9927-9944, Toronto, Canada. As-
sociation for Computational Linguistics.

Leonardo Campillos-Llanos, Ana R_ Terroba
Reinares, Soffa Zakhir Puig, Ana Valverde-
Mateos, and Adrian Capllonch-Carri6n. 2022.
Building a comparable corpus and a benchmark
for spanish medical text simplification. Proce-
samiento del Lenguaje Natural, 69:189-196.

Leonardo Campillos-Llanos, Ana Rosa Terroba,
Rocio Bartolomé, Ana Valverde-Mateos, Cristina
Gonzalez, Adrian Capllonch-Carrién, and
Jonathan Heras. 2024. Replace, paraphrase or
fine-tune? evaluating automatic simplification
for medical texts in Spanish. In Proceedings
of the 2024 Joint International Conference on
Computational Linguistics, Language Resources
and Evaluation (LREC-COLING 2024), pages
13929-13945, Torino, Italia. ELRA and ICCL.

Marta R Costa-Jussa, James Cross, Onur Celebi,
Maha Elbayad, Kenneth Heafield, Kevin Heffer-
nan, Elahe Kalbassi, Janice Lam, Daniel Licht,
Jean Maillard, et al. 2022. No language left be-
hind: Scaling human-centered machine transla-
tion. arXiv preprint arXiv:2207.04672.

William Coster and David Kauchak. 2011. Simple
English Wikipedia: A new text simplification task.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Hu-
man Language Technologies, pages 665-669,
Portland, Oregon, USA. Association for Compu-
tational Linguistics.

Anna Dmitrieva and J6érg Tiedemann. 2021. Creat-
ing an aligned Russian text simplification dataset

from language learner data. In Proceedings of
the 8th Workshop on Balto-Slavic Natural Lan-
guage Processing, pages 73-79, Kiyv, Ukraine.
Association for Computational Linguistics.

Yue Dong, Zichao Li, Mehdi Rezagholizadeh, and
Jackie Chi Kit Cheung. 2019. EditNTS: An neural
programmer-interpreter model for sentence sim-
plification through explicit editing. In Proceedings
of the 57th Annual Meeting of the Association
for Computational Linguistics, pages 3393-3402,
Florence, Italy. Association for Computational
Linguistics.

Nikita Dragunov, Temurbek Rahmatullaev, Eliza-
veta Goncharova, Andrey Kuznetsov, and An-
ton Razzhigaev. 2025. Sonar-llm: Autoregres-
sive transformer that thinks in sentence em-
beddings and speaks in tokens. arXiv preprint
arXiv:2508.05305.

Paul-Ambroise Duquenne, Holger Schwenk, and
Benoit Sagot. 2023. Sonar: sentence-level mul-
timodal and language-agnostic representations.
arXiv preprint arXiv:2308. 11466.

Rudolph Flesch. 1948. A new readability yardstick.
Journal of applied psychology, 32(3):221.

Natalia Grabar and Rémi Cardon. 2018. CLEAR —
simple corpus for medical French. In Proceed-
ings of the 1st Workshop on Automatic Text Adap-
tation (ATA), pages 3—9, Tilburg, the Netherlands.
Association for Computational Linguistics.

Nathan Siegle Hartmann and Sandra Maria Aluisio.
2020. Adaptagao lexical automatica em textos in-
formativos do portugués brasileiro para o ensino
fundamental. Linguamdatica, 12(2):3—27.

Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes,
and Yejin Choi. 2019. The curious case
of neural text degeneration. arXiv preprint
arXiv:1904.09751.

Koki Horiguchi, Tomoyuki Kajiwara, Yuki Arase, and
Takashi Ninomiya. 2024. Evaluation dataset for
Japanese medical text simplification. In Proceed-
ings of the 2024 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies (Vol-
ume 4: Student Research Workshop), pages
219-225, Mexico City, Mexico. Association for
Computational Linguistics.

Chao Jiang, Mounica Maddela, Wuwei Lan, Yang
Zhong, and Wei Xu. 2020. Neural CRF model
for sentence alignment in text simplification. In
Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics, pages
7943-7960, Online. Association for Computa-
tional Linguistics.


===== PAGE BREAK =====

Sebastian Joseph, Kathryn Kazanas, Keziah Reina,
Vishnesh Ramanathan, Wei Xu, Byron Wallace,
and Junyi Jessy Li. 2023. Multilingual simplifi-
cation of medical texts. In Proceedings of the
2023 Conference on Empirical Methods in Natu-
ral Language Processing, pages 16662-16692,
Singapore. Association for Computational Lin-
guistics.

Tannon Kew, Alison Chi, Laura Vasquez-
Rodriguez, Sweta Agrawal, Dennis Aumiller,
Fernando Alva-Manchego, and Matthew
Shardlow. 2023. BLESS: Benchmarking large
language models on sentence simplification. In
Proceedings of the 2023 Conference on Empiri-
cal Methods in Natural Language Processing,
pages 13291-13309, Singapore. Association for
Computational Linguistics.

Justin Lee and Sowmya Vajjala. 2022. A neural
pairwise ranking model for readability assess-
ment. In Findings of the Association for Computa-
tional Linguistics: ACL 2022, pages 3802-3813,
Dublin, Ireland. Association for Computational
Linguistics.

Junyu Luo, Junxian Lin, Chi Lin, Cao Xiao, Xinning
Gui, and Fenglong Ma. 2022. Benchmarking au-
tomated clinical language simplification: Dataset,
algorithm, and evaluation. In Proceedings of
the 29th International Conference on Computa-
tional Linguistics, pages 3550-3562, Gyeongju,
Republic of Korea. International Committee on
Computational Linguistics.

Mounica Maddela, Yao Dou, David Heineman, and
Wei Xu. 2023. LENS: A learnable evaluation
metric for text simplification. In Proceedings of
the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Pa-
pers), pages 16383-16408, Toronto, Canada.
Association for Computational Linguistics.

Louis Martin, Angela Fan, Eric de la Clergerie, An-
toine Bordes, and Benoit Sagot. 2022. MUSS:
Multilingual unsupervised sentence simplifica-
tion by mining paraphrases. In Proceedings of
the Thirteenth Language Resources and Evalu-
ation Conference, pages 1651-1664, Marseille,
France. European Language Resources Associ-
ation.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv: 1301.3781.

Kai North, Tharindu Ranasinghe, Matthew Shard-
low, and Marcos Zampieri. 2025. Deep learn-
ing approaches to lexical simplification: A sur-
vey. Journal of Intelligent Information Systems,
63(1):111-134.

Brian Ondov, Kush Attal, and Dina Demner-
Fushman. 2022. A survey of automated meth-
ods for biomedical text simplification. Journal of
the American Medical Informatics Association,
29(11):1976—1988.

Jeffrey Pennington, Richard Socher, and Christo-
pher Manning. 2014. GloVe: Global vectors for
word representation. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1532-1543,
Doha, Qatar. Association for Computational Lin-
guistics.

Nils Reimers and Iryna Gurevych. 2019. Sentence-
BERT: Sentence embeddings using Siamese
BERT-networks. In Proceedings of the 2019
Conference on Empirical Methods in Natural
Language Processing and the 9th International
Joint Conference on Natural Language Process-
ing (EMNLP-IJCNLP), pages 3982-3992, Hong
Kong, China. Association for Computational Lin-
guistics.

Horacio Saggion and Graeme Hirst. 2017. Auto-
matic text simplification, volume 32. Springer.

Seamless. 2025. Joint speech and text machine
translation for up to 100 languages. Nature,
637 (8046):587-593.

Thibault Sellam, Dipanjan Das, and Ankur Parikh.
2020. BLEURT: Learning robust metrics for text
generation. In Proceedings of the 58th Annual
Meeting of the Association for Computational Lin-
guistics, pages 7881-7892, Online. Association
for Computational Linguistics.

RJ Senter. 1967. Automated readability index.
Technical report.

Karen Sparck Jones. 1972. A statistical interpre-
tation of term specificity and its application in
retrieval. Journal of Documentation, 28(1):11—
21.

Regina Stodden, Omar Momen, and Laura
Kallmeyer. 2023. DEplain: A German parallel
corpus with intralingual translations into plain
language for sentence and document simplifica-
tion. In Proceedings of the 61st Annual Meeting
of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 16441-16463,
Toronto, Canada. Association for Computational
Linguistics.

Renliang Sun, Hangi Jin, and Xiaojun Wan. 2021.
Document-level text simplification: Dataset, cri-
teria and baseline. In Proceedings of the 2021
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 7997-8013, Online


===== PAGE BREAK =====

and Punta Cana, Dominican Republic. Associa-
tion for Computational Linguistics.

Jan Trienes, J6rg Schlétterer, Hans-Ulrich Schild-
haus, and Christin Seifert. 2022. Patient-friendly
clinical notes: Towards a new text simplification
dataset. In Proceedings of the Workshop on
Text Simplification, Accessibility, and Readability
(TSAR-2022), pages 19-27, Abu Dhabi, United
Arab Emirates (Virtual). Association for Compu-
tational Linguistics.

Ashish Vaswani, Noam Shazeer, Niki Parmar,

Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
tukasz Kaiser, and Illia Polosukhin. 2017. Atten-
tion is all you need. Advances in neural informa-
tion processing systems, 30.

Benjamin Warner, Antoine Chaffin, Benjamin
Clavié, Orion Weller, Oskar Hallstrém, Said
Taghadouini, Alexis Gallagher, Raja Biswas,
Faisal Ladhak, Tom Aarsen, Nathan Cooper, Grif-
fin Adams, Jeremy Howard, and lacopo Poli.
2024. Smarter, better, faster, longer: A modern
bidirectional encoder for fast, memory efficient,
and long context finetuning and inference.

Wei Xu, Courtney Napoles, Ellie Pavlick, Quanze
Chen, and Chris Callison-Burch. 2016. Optimiz-
ing statistical machine translation for text sim-
plification. Transactions of the Association for
Computational Linguistics, 4:401-415.
