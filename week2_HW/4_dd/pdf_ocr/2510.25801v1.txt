arX1v:2510.25801vl [cs.LG] 29 Oct 2025

Preprint.

METIS-SPECS: DECOUPLING LEARNING VIA
SELF-DISTILLED PREFERENCE-BASED COLD START
FOR VLMS

Kun Chen! ?*, Peng Shi**, Haibo Qiu®, Zhixiong Zeng*, Siqi Yang?, Wenji Mao?! /,
Lin Ma?

School of Artificial Intelligence, University of Chinese Academy of Sciences

2MAIS, Institute of Automation, Chinese Academy of Sciences

3Meituan

chenkun2024@ia.ac.cn    shipengl0@meituan.com
wenji.mao@ia.ac.cn    forest.linma@gmail.com

ABSTRACT

Reinforcement learning (RL) with verifiable rewards has recently catalyzed a
wave of “MLLM-r1” approaches that bring RL to vision language models. Most
representative paradigms begin with a cold start, typically employing supervised
fine-tuning (SFT), to initialize the policy before RL. However, SFT-based cold
start adopts the reasoning paradigm intertwined with task solution and output for-
mat, which may induce instruction-style overfitting, weakens out-of-distribution
generalization, and ultimately affects downstream RL. We revisit the cold start
along two views, its training method and data construction, and introduce the
Generalization Factor (GF) coefficient to quantify the generalization capability
under different methods. Our empirical study finds that preference—based train-
ing methods (e.g. DPO) generalizes better than SFT-based methods in cold start.
Motivated by this, we propose SPECS—a Self-distilled, Preference-based Cold
Start framework that decouples multimodal learning: (1) generates introspective
preference data pairs via self-distillation, avoiding reliance on larger teachers or
manual annotation; (2) performs preference—based training to learn, focusing on
shallow, transferable surface-form criteria (format, structure, style) rather than
memorizing content; and (3) hands off to RL with verifiable rewards for deep
reasoning results. Experimental results across multiple multimodal benchmarks
show that our decoupling learning framework yields consistent performance gains
over strong baselines, improving MEGA-BENCH by 4.1% and MATHVISTA by
12.2%. Additional experiments indicate that SPECS contributes to reducing in-
distribution “stuckness,” improving exploration, stabilizing training, and raising
the performance ceiling. |!

1 INTRODUCTION

Recently, inspired by the success of DeepSeek-R1 2025), in effectively enhancing the
reasoning capabilities of large models through reinforcement learning (RL) with verifiable reward

(Lambert et al.|/2024 2025), a growing body of work has begun to apply RL directly to
vision language models (VLMs). This has led to a wave of exciting “MLLM-r1” research (Meng

      ;|Zhou et al.| |2025} |Zhang et al.) |2025a
 2025bja} |Zheng et al.                         , which leverage similar principles to advance
multimodal reasoning.

Previous research has indicated that prior to RL, employing a pre-training or warm-up phase (which
is termed “cold start’’), can significantly improve the readability, stability, and even the final per-

formance of RL training 2025). Currently, the most commonly used cold start strategy

“Equal contribution.   * Corresponding author(s).

‘Project Page: https: //github.com/Kwen-Chen/SPECS-VL



===== PAGE BREAK =====

Preprint.

is supervised fine-tuning (SFT) , where the model is first fine-tuned on a set of high-quality Chain-
of-Thought reasoning data to provide a better initial policy for the subsequent RL phase
 [2025b). This strategy enables the model
to be trained on complex reasoning data during the cold start phase, thereby acquiring reasoning
ability.

The common understanding behind SFT-based cold start is that reasoning abilities, reasoning for-
mat and other learning objectives can be jointly learned during the cold start phase. However, such
an SFT-based joint learning paradigm may largely affect the model’s generalization capability
fet al.| {2025} {Chu et al.|{2025), and consequently degrade subsequent RL 2025a). This
raises an important research issue of quantifying and improving the model’s generalization capabil-
ity during cold start and working in concert with subsequent RL.

To address the above limitations, we consider an alternative learning paradigm, which separates the
learning process into hierarchical stages based on the idea that cold start phase focused more on
shallow, surface-form learning to avoid prematurely getting stuck in in-distribution problem solv-
ing, while subsequent RL focuses on the deep-level learning of a solution to boost the overall perfor-
mance (Bengio et al.|[2009). Thus, the intuition of our adopting decoupling learning for multimodal
reasoning is that the selection of pre-training methods in cold start needs to better support the subse-
quent RL, both in terms of generalization and by having separate objectives to facilitate better final
results.

Another important issue is the generation of cold start data. Previously, the prohibitive cost of human
annotation has motivated a growing body of research to explore the use of synthetic data. This often
involves using a more capable large model as a “teacher” to distill data for a smaller “student”
model. (Zhang-et a 2025b)|Ya0 etal. 2024} [Nu etal. 2004} [Huang et al[2025). However, when
the capability gap between the teacher model and the student model is too large, it can lead to a
decline in model performance (Zhang et al.|{2023). Alternatively, the DeepSeek-R1-Zero paradigm

 2025) first directly applies reinforcement learning to the base model for obtaining R1-
Zero and then generates cold start data by zero model itself. This paradigm has achieved very

remarkable performance; yet it still has the limitation of reliance on the SFT cold start and the
constraints between SFT and subsequent RL, thereby leaving room for further improvement.

In this paper, to examine the suitable cold start training method, we propose the Generalization Fac-
tor (GF) coefficient in Section[2|to quantify the generalization capability of the model and conduct
an empirical study to evaluate different training methods. We identify that Direct Preference Opti-
mization (DPO) based on preference data is a cold start approach that enables
the model to have better performance. On this basis, we present the Self-distilled Preference-based
Cold-Start (SPECS) framework in Section 3} By decoupling the learning objectives during DPO to
focus on output format, we create a pre-aligned model that serves as a superior starting point for the
final RL fine-tuning. Our experiments show that this method leads to more stable, efficient training,
and a higher performance ceiling compared to the advanced and strong baseline.

The main contributions of this paper can be summarized as follows.

1. We present the SPECS framework, a three-stage cold start strategy. It generates preference
data through self-distillation, uses DPO for cold start training, and separates training objectives
so that the model first aligns with output formats, providing a stronger starting point for RL.

2. We propose Generalization Factor as a metric to evaluate a model’s generalization capability
under different cold start training methods by comparing its performance on in-distribution
and out-of-distribution tasks.

3. We reveal the importance of Decoupling Learning between the cold-start and RL phases.
This separation improves exploration and reduces the risk of the model getting stuck on in-
distribution solutions.

4. Our experimental results prove that a preference-based DPO cold start gives the model stronger
generalization ability. In terms of the model’s final results, it achieves consistent performance
gains across benchmarks, improving MEGA-Bench by 4.1% and MathVista by 12.2% over
strong baselines, demonstrating the effectiveness of the SPECS.


===== PAGE BREAK =====

Preprint.

2 EMPIRICAL INVESTIGATION

2.1 EVALUATING DEGREE OF GENERALIZATION

To evaluate the impact of preference-based versus supervised data on a model’s generalization ca-
pabilities under a fixed sample size, we introduce the metric of Generalization Factor (GF).

Setup. We define an evaluation function ~)(f, P) € R that measures the performance of a model f
on a data distribution P. A higher value of ~ indicates better performance.

We consider two types of training data, denoted by + € {pref, sup}, corresponding to preference
data and supervised data. For a given data type 7 and a fixed sample size n, a model f,,,, is obtained
by applying an algorithm A, to a training set S,,,, which consists of n samples drawn from a training
distribution Pain.

frn = Ar(Srn), Where Srn~ Pirain

Generalization Factor. To assess the model’s performance, we measure its effectiveness on both
in-distribution (ID) and out-of-distribution (OOD) data.

¢ ID Performance: The ID performance, U7,(n), is evaluated on a hold-out set from the same

distribution Pi;-cin
Vip (n) = W(frns Prrain)

* OOD Performance: The OOD performance, YO op(7), is the weighted average performance
across a set of m distinct OOD distributions, Q = {Qi,...,Qm}, with weights defined by a

distribution a.
oop (%) = Egral}(frn; Q)]

We establish a baseline model, f;9, which serves as a reference point. This is typically a pre- trained
model that has not been fine-tuned on data of type 7. The performance gains over this baseline are

calculated as:
Gip(m) = Yip(n) — Yip (0)

Goon (n) = Yoon(”) — Yoon (0)
With a fixed number of samples n, we formalize the GF for data type 7, denoted as ',(n), weighted
harmonic mean of OOD performance gain and ID performance gain.

B?Gip(n) + Gip(n)
where the weighting coefficient 6 is generally set to 2 to reflect the importance of the OOD perfor-

mance gain in the generalization capabilities of the model. To ensure that the metric behaves well
and is dimensionless, the evaluation function 7 should be normalized to a consistent range.

2.2 EXPERIMENTAL FINDINGS

ID Performance                             OOD Performance                          Generalization Factor
100                                             100                                             100

80                                                  80

80

60                                                  60

60

40                                                  40

—e— DPO w. sft-loss
—=— DPO
—*— SFT

A(DPO w. sft-loss - SFT)

40                         —*— DPO w. sft-loss
—=— DPO
— SFT

—e— DPO w. sft-loss
—=— DPO
—— SFT

20                                       20

ID Performance (%)
OOD Performance (%)
Generalization Factor

5        10       15       20       25                  5        10       15       20       25                  5        10       15       20       25
Training Step                                        Training Step                                        Training Step

Figure 1: Performance Comparison: DPO vs. SFT on In-Distribution and Out-of-Distribution Task

To preliminarily examine how preference data and supervised data affect model generalization, we
construct a preference dataset Dprer = {(ai, yj’, y; )}*_, . where y;* is the chosen response and
y,_ is the rejected response, and a supervised dataset Dspr = {(xi,yi)}*_, with y; = yj* around
reasoning tasks defined by a specific answer format. Under equal data budgets, we evaluate two


===== PAGE BREAK =====

Preprint.

settings: (i) an in-distribution setting in which the required reasoning format matches that used in
training, and (ii) an out-of-distribution setting in which the required reasoning format differs|“} We
compare DPO training, SFT training, and DPO training augmented with SFT loss (see Section|3.3).
The resulting Gf) (n), GGop(n), and the I’, (n) are reported in Figure]

From the experimental results, it can be observed that SFT achieves the fastest convergence on ID
tasks. However, due to its reliance on a single cross-entropy loss that maximizes the log-likelihood
of the correct answer, it demonstrates poor OOD performance. By contrast, DPO converges more
slowly at the beginning of ID tasks but yields better OOD performance. Remarkably, the model
trained with a combination of DPO with SFT loss achieves the strongest generalization capability
overall. As the number of training steps increases, the GF gap between the SFT training method and
the DPO training method also increases.

3. METHODOLOGY: THE SPECS FRAMEWORK

3.1 SELF-DISTILLED PREFERENCE COLD-START

A model with superior generalization capabilities provides a more effective starting point for RL.
Inspired by the discussion in Section |2} we employ self-distillation to construct preference data
focusing format learning. This data is then used in place of standard SFT data to enhance the
model’s generalization performance during the cold-start phase.

To implement this, we propose SPECS, illustrated in Figure [2| a three-stage training optimization
strategy consisting of 1) Self-Distillation for Preference Data Generation, 2) DPO-based Pre-
Alignment for Cold-Start, and 3) Final GRPO Fine-tuning.

Self-Distillation Preference-based Cold Start |e

Step1 Ours-zero RL Training and Preference Data Generation

:|Step2 DPO-based Cold-Start

¥

    : 100                   Ansewr filter   @                :                     A
:    ap) c>                                   <=>              : ©Format   Preference    Base
:                        format pollution ())                :             Data     Model

} | uu. The response need to enclose within <think> </think> and <answer> </answer> tags...

Base Model :                   .
: Base Model      rejectedx                                 rejected
{J creo  :

                   Ansewr filter   @                :

(29)    :  =>                            =>           :            9            9
:    EI                   format supplement                    :     co:                           Ye
    : Ours-zero      chosen*                           chosen    ‘    eS    {2.2}   RLVR   {2.9}
5    ‘                                     :          —a       AN
‘© Solution warm         Final  al BU

‘Rejected Response Pollution.   & Chosen Response Filtration.         :            Model           Model

Step3 Final GRPO Fine-Tuning

Ours-zero

Figure 2: Method Overview. We propose the SPECS cold-start strategy, a three-stage pipeline to
enhance final RL fine-tuning. Firstly, where we generate a preference dataset focused on teaching
the correct output format by self distillation. Next, The base model is pre-aligned on this data using
DPO to create a format-aware “Warmup Model’. Finally, this pre-aligned model undergoes Final
RL tuning with GRPO, allowing the optimization process to focus on enhancing reasoning.

3.2 SELF-DISTILLATION FOR PREFERENCE DATA GENERATION

Objective: The foundational stage of our framework aims to achieve two interconnected goals:
first, to cultivate a preliminary “seed model” with enhanced reasoning capabilities, and second, to
leverage this model to autonomously generate a high-quality preference dataset through a process
we term self-distillation.

Methodology: A critical initial challenge is that a standard base VLM often lacks the capability
to generate outputs of sufficient reasoning ability. To address this, we first conduct a brief, initial
phase of RL fine-tuning on the base model using GRPO. This step aims not at achieving the final
performance, but at creating an initial policy, denoted 7¢RPO~ zero, Which is more adept at exploring
the solution space.

For more detailed description, see Appendix|A.2


===== PAGE BREAK =====

Preprint.

the length of BC is ?

A
7     As shown in the figure, it is known that 4AED = 4B, DE = 6; AB = 10, AE = 5,
£E
BY      Cc

@)  Since 4AED = 4B and 4A = 4A, we have AADE ~ AACB. So, $\frac{AE}{AB} = \frac{DE}{BC3$.
ao

Given AB = 10, AE = 5, and DE = 6, we can calculate $\frac{5}{10} = \frac{6}{BC3$.

Therefore, $BC = \frac{12}{1} . Thus, the answer is 12.
Base Model

©        <think>Given that 4AED = 4B and 4DAE = 4CAB, we can conclude that AADE ~ AABC (AA similarity

criterion). Therefore, the ratios of corresponding sides are equal, which gives us:

amp } \L \frac{DE}{Bc} = \frac{AE}{AB} \implies \frac{6}{BC} = \frac{5}{10} \implies \frac{6}{Bc} =
=    \frac{1}{2} \implies BC = 12 \]

Ours-zero
Thus, the length of BC is 12. </think>

<answer> The answer is $\boxed{12}$</answer>

Figure 3: Example of a self-distilled preference data pair.

With the exploratory t¢RPO-zero model, we proceed to generate the preference dataset. The data
construction process involves four key steps:

3.3

Response Generation. We prompt two models, our exploratory T¢RPO-zero and Toase, With
specific format instructions ( <think>...</think><answer>...</answer> ) to create a
dataset, which is designed to contain pairs of responses that are both correct in their final answer,
but differ in their reasoning paradigm and answer format.

Chosen Response Filtration. For the chosen response (y;*), we use Gemini-2.5 flash
 2025) as an evaluator. Assesses whether the reasoning path in the t@RPO-_ zero

response aligns correctly with its final answer. Only responses in which the reasoning and the
answer are consistent are retained, forming a high-quality pool of candidates.

Rejected Response Pollution. For the rejected response (y;), we select responses that also
contain the correct answer, but deviate from the required format. Recognizing that some gen-
erated responses might incidentally have the correct format, We randomly apply one of the
following five types of format corruption to these responses to ensure a clear learning signal.

1. Remove all tags (<think>, </think>, <answer>, </answer>).
2. Remove the <answer> and </answer> tags.

3. Remove the <think> and </think> tags.
4

. Remove the <answer> and </answer> tags and move the closing </think> tag to
the end of the response.

5. Replace the <answer> tags with the string Answer: and remove </answer> tags.

Preference Pair Construction via Self-Distillation. We construct the chosen response and
the rejected response into pairs of self-distilled preference data (yt, y~ ). As shown in Figure
  Both Chosen Responses (ys ) and Rejected Responses (y; ) are selected from the filtered
pool and contain the correct final answer. This data set is designed to facilitate decoupled
learning, separating the learning of reasoning paradigms and answer formats from the core
logical reasoning ability. This approach serves as a more effective cold-start method for the
final alignment stage.

DPO-BASED PRE-ALIGNMENT FOR COLD-START

Objective: The primary goal of this stage is to leverage the self-distilled preference dataset gen-
erated in the Stage 1 (Section [3.2) to pre-align the base VLM. This process yields a “cold-start”
model that serves as a significantly improved starting point for the final reinforcement learning fine-
tuning. We conceptualize this phase as a “warm-up,” which shifts the model’s policy into a more
advantageous region of the policy landscape before the intensive final training.

Methodology: To achieve this pre-alignment, we employ DPO (Rafailov et al.||!2023), a powerful

technique that directly optimizes the language model on preference data without the need for an


===== PAGE BREAK =====

Preprint.

explicit reward model. The standard DPO loss function is defined as:

T6(Yw| x) Blo anal

Tret (Yw|2)            Tret (Yul)

Lppo (79; Tref ) =—_ U(0,Ywyyi)~D og 0 (s log

where 7g is the policy being optimized, 7,¢f is the reference policy (the initial base model), { is a
temperature parameter, and (x, yw, yi) represents a triplet of prompt, chosen response, and rejected
response from our self-distilled dataset D.

To augment this process, we incorporate an SFT loss computed on the “chosen” samples, which
serves as a form of regularization. It ensures that while the model learns the directional preference
signal from DPO, it does not drift far from the core distribution of high-quality text embodied by the

chosen responses 2025). The combined loss function is thus:

Lnybrid = LDPO + ALSFT

where Cgrr is the conventional negative log-likelihood loss on the chosen responses, and A is a
weighting coefficient to balance the two objectives. For a discussion for A, see Appendix [A.3]

3.4 FINAL GRPO FINE-TUNING

Objective: To achieve peak performance by fine-tuning the pre-aligned cold-start model, focusing
computational resources on enhancing complex reasoning capabilities.

Methodology: This final stage leverages the cold-start model obtained from Stage 2 as the initial-
ization point for reinforcement learning, rather than starting from the base model or a conventional
SFT model. The pre-alignment from the DPO phase ensures that the model has already mastered the
output format. Consequently, the model is not required to expend resources on learning basic struc-
tural compliance. Instead, credit assignment during RL training can be more accurately attributed
to the core challenge: improving the quality and precision of its reasoning process. This targeted
optimization explains the observed stable convergence in our experiments and the model’s ability to
achieve a higher performance ceiling.

For the final stage of fine-tuning, we employ the GRPO algorithm (Shao et al.|/2024). This process is
guided by a composite reward function that combines format and accuracy components to evaluate

the model’s output, o, for a given question, q.

The total reward Riotai, is the sum of a format reward Reformat, and an accuracy reward Racc:

Rtotal (0, q) = Ryformat (0) + Race(0, q)

The format reward Regormat(0), assigns a fixed value of 0.5 for structurally correct outputs, rein-
forcing the policy’s formatting discipline.

The accuracy reward Racc(0, q), provides a binary signal: 1.0 for a correct answer and 0 other-
wise. We use a hybrid mechanism to determine correctness based on the question type, T'(q):

Rrie(o,q) if T(q) € {Multiple-Choice, Numerical }

Racc(0, q) = ee q)    if T(q) = Short-Answer

For objective types like multiple-choice and numerical questions, a rule-based function assesses
correctness. For subjective short-answer questions, we employ GPT-4o as an external judge.

4 EXPERIMENTS

4.1 EXPERIMENT SETTINGS

Dataset and Benchmark: The data utilized for training t¢RPoO-—zero model in Stage 1 and for
the final GRPO fine-tuning of the cold-started model in Stage 3 is composed of the Orsta47K

 2025) and virl39K (Wang et al.|[2025a) datasets. In Stage 2 of cold start training, we used 9K

self-distilled data. This composition is designed to enhance the model’s general and mathematical


===== PAGE BREAK =====

Preprint.

Table 1: Model performance comparison on MEGA-Bench Core.

Model                                                                                                                      MEGA-Bench                                                                                 MEGA-Bench
Knowledge Mathematics Perception Coding Info. Ex. Planning Science Metrics              Core
Open-Source General Models
QwenVL-2-7B              39.96        25.95        39.99      31.49     40.29      16.64     28.59     43.61         34.47
QwenVL-2.5-7B            38.84        27.67        41.24      28.93     50.23      16.32     36.75     41.64        35.07
InternVL2-8B              33.94        22.08        32.15      24.7      29.13      12.17     24.61     39.96        25.96
InternVL2.5-8B             34.78        25.86        33.27      25.45     35.10      15.97     28.83     44.96        28.34
InternVL3-8B              42.76        34.85        42.76      34.05     44.84      17.10     35.21     49.60        36.02
Llava-OV-7B               31.37        22.11        27.64      13.9       7.07       9.16     24.38     37.31         21.36
Kimi-VL-A3B                          37.63                27.07               39.50          22.30         40.99          22.17         33.94        46.65                34.40
Open-Source Reasoning Models
R1-Onevision+              29.47        20.94        28.65      23.38     43.04      12.67     26.84    42.19        27.18
VLAA-Thinkingt           38.23        28.83        40.73      28.84     44.58      17.05     36.69     45.57        34.86
Kimi-VL-A3B-Thinking     33.45         17.76        28.11      14.69     41.14      12.64     28.60    43.97        27.08
MM-Eureka-7B             40.12        31.59        39.71      28.75     49.32      16.64     37.25,    46.39        35.96
VL-Rethinker-7B                     40.65                30.08               42.02          29.87         52.03           17.83          36.82        46.90                37.25
Orsta-7B                   41.65        31.48        43.84      32.82     54.07      17.83     36.91     41.66        38.31
Ours-zero                  42.44        29.87        43.77      32.80     49.59      17.76     37.39     47.32        37.96
Ours-7B                   42.64        31.71        44.58      34.14     51.68      18.76     38.73    51.87        39.17
A (Ours - Backbone)                  +3.8                   +4.0                  $3.3             +5.2            +14             42.4            +2.0          +10.2                  +4.1

' The tsymbol indicates that the results were evaluated with VLMEvalKi
? The remaining results are from the MEGA-Bench Leaderboard and 2025).

Table 2: Model Performance Comparison On Other Benchmarks

MMMU                        ee                              a            MathVerse Overall
Model                                                    val                MathVision MathVisita         vision only
Backbone
QwenVL-2.5-7B        54.27        25.40         63.70         38.20       45.38
QwenVL-2.5-7B based Reasoning Models
R1-Onevision          49.67+       29.90         64.1          40.0       45.92
VLAA-Thinking        52.677       26.40         68.00        48.20       48.82
MM-Eureka-7B         55.55+       26.90         73.00        47.587      50.76
VL-Rethinker-7B        56.7        29.70         73.60        48.98+      52.25
Orsta-7B+              54.33        25.76         70.20         32.10       45.60
Ours-zero               54.3        26.88         72.90        47.33       50.35
Ours-7B               56.78       29.50         75.90        48.73       52.73
A (Ours - Backbone)     +2.5         +4.1         +12.2        +10.5       +7.3

The +symbol indicates that the results were evaluated with VLMEvalKif']

reasoning capabilities. We conduct evaluations on multiple benchmark datasets, including MEGA-

Bench (Chen et al.|/2025b), MMMU (Yue et al.}/2024), Math Vista 2024b), MATH- Vision
(Wang et al.|!2024a), and MathVerse (Zhang et al.||2024).

Baseline: Our comparative analysis is grounded on two primary categories of models. The first

category comprises open-source general VLMs, including QwenVL-2-7B
QwenVL-2.5-7B (Bai et al.|/2025), InternVL2-8B 2024), InternVL2.5-8B (Chen et al.|
(2024), Kimi-VL-A3B (Team et al.]/2025), and DeepSeek-VL-7B (Lu et al.|/2024a). The second cat-
egory focuses on models specifically engineered for advanced reasoning tasks. This group includes
Kimi-VL-A3B-Thinking 2025), R1-Onevision 2025), VLAA-Thinking
 2025a), MM-Eureka-7B (Meng et al.|{2025), VL-Rethinker-7B (Wang et al.| {2025a),
and Orsta-7B (Ma et al.|/2025).

Implementation Details: We utilize the open-source Multimodal Large Language Model,
Qwen?2.5-VL-7B 2025), as our base model. For the GRPO training in Stage 1 and Stage
3, we employ the MM-EUREKA|?|framework. The rollout and training batch sizes are both set to
128, with 8 rollouts generated per sample. The learning rate is configured to 1 x 10~°. For the DPO
training in Stage 2, as well as for the comparative SFT experiments, we leverage the LlamaFactory|)]
framework. In this configuration, the training batch size is set to 64, the learning rate is maintained

Shttps://github.com/ModalMinds/MM-EUREKA
Shttps://github.com/hiyouga/LLaMA-Factory


===== PAGE BREAK =====

Preprint.

at 1 x 10~°, and the hyperparameter \ for the hybrid loss function is set to 1. The prompt used
during training is shown in Appendix[A.2]

4.2 MAIN RESULTS

Table[I]presents the overall performance of our model on MEGA-Bench Core, along with the scores
for each subtask, in comparison with other baseline models.  Table [2] reports the performance of
various inference models built on the QwenVL-2.5-7B backbone across additional benchmarks.
Our model has improvements in general task benchmarks (MEGA-BENCH core, MMMU) and
mathematical reasoning benchmarks (MathVision, MathVisita, MathVerse), and some benchmarks
are in a leading position among models of the same size, demonstrating the effectiveness of our
approach.

4.3. ABLATION ON SELF-DISTILLATION AND DECOUPLED DATA STRATEGY

Self-distillation proves more effective than external teacher models. First, we evaluate the ef-
fectiveness of the self-distillation mechanism by substituting it with preference data generated from
powerful external teacher models, specifically QwenVL-2.5-32B and QwenVL-2.5-72B. The re-
sults shown in Tabel 3|clearly indicate that our self-distillation approach outperforms both teacher-
based alternatives. We also observe that performance degradation is more pronounced when using
the QwenVL-2.5-32B model, whose output distribution diverges more significantly from our base
model. This finding suggests that preference data closely aligned with the model’s intrinsic capa-
bility distribution is more effective for alignment than guidance from a more capable but dissimilar
external model.

The decoupled data strategy outperforms the coupled approach for DPO cold-starting. Next,
we investigate the impact of our decoupled data strategy for DPO cold-starting. We compare it
against a “coupled” DPO approach, where preference data is mixed, containing pairs that differ
in both answer correctness and reasoning format. In contrast, our decoupled strategy strictly uses
preference pairs where both chosen and rejected responses feature correct answers but diverge in
their reasoning format. The experimental results shown in Table [3]demonstrate the clear superiority
of the decoupled approach.

Table 3: Ablation Results to show the impact of Self Distillation and Decoupled Data

Model                                 Megabench         MMMU          MathVista  MathVision MathVerse             AVG
Qwen-VL-2.5-7B                    35.07                  54.2                  63.70                 25.40                 38.20                 43.31

- Qwen32b Distillation 27.04/29.87 51.44/56.67 66.90/71.50 25.53/28.03 43.53/46.07 42.89 / 46.43
- Qwen72b Distillation 34.00/37.30 53.89/58.56 67.50/73.30 25.62/28.91 43.53/46.83 44.90 / 48.98

- Self Distillation          37.52/39.17 54.89/56.78 72.00/75.90 25.75/29.50 46.19/48.73  47.27/50.02
- Coupled Data             37.02 /38.76 55.44/55.44 71.10/73.10 27.37/28.65 47.46/47.46 47.67/ 48.68
- Decoupled Data          37.52/39.17 54.89/56.78 72.00/75.90 25.75/29.50 46.19/48.73  47.27/50.02

The value on the left side of the slash ’/’ represents the score after cold-start training, and the value on the right side of the slash
represents the score after cold start + RL.

4.4 ANALYSIS OF THE RELATIONSHIP BETWEEN GF AND FINAL PERFORMANCE

Checkpoint Comparison                             Policy Loss Curves                                 Format Rewards

Max A= 1,73
at Checkpoint 800                                                             —_— SFT+GRPO              0.500

0.003                              —— DPO+GRPO

w
©
°
rs
©
a

w
Cy)

0.490

Policy Loss

w
x

°
rs
©
a

=e Ours
—s- SFT Base
A (Ours - SFT Base)

Performance Score
Format Rewards

—— SFT+GRPO
—— DPO+GRPO

0.480
i}       200      400      600      800              0        200       400       600       800            i}        200       400       600      800

Training Steps                                  Training Step                                  Training Step

Figure 4: Impact on RL Training Efficiency and stability.


===== PAGE BREAK =====

Preprint.

We evaluate the correlation between the

A SFT                       DPO w. SFT-loss
model’s GF value during the cold-start phase     48.21) @ pPo                    |
and its final performance (represented here by     g 48.0  2 Trend.ttdegnae 0.864)                :
the average score on MEGA-Bench, MMMU,    <
and MathVerse_Vision_Only). By comparing    a 47.8
three cold-start methods with different GF val-     Ea76                              pbro
ues, presented in Figure |5| we can see that GF     €
and the model’s final performance are corre- = 47-4
lated to a certain extent. This also confirms that = ,,,        a
the stronger the model’s generalization ability           Bey
in the cold-start phase, the more it will con-     47.0\ <= —>5—39 3509 SS
tribute to the model’s improvement in the RL                  Generalization Factor

phase.                                                                               ;                          ;
Figure 5: GF vs. Final Performance
In addition, we can also prove that the cold-start

method based on preference data has higher
generalization ability compared with the traditional SFT cold-start paradigm, thus bringing greater
potential for improvement to subsequent RL.

4.5 ANALYSIS OF THE IMPACT ON RL TRAINING EFFICIENCY AND STABILITY

We examine the downstream effects of our DPO cold-start strategy, assessing its impact on the
efficiency and stability of the final RL phase.

Performance and Training Efficiency. To evaluate performance and training efficiency, we tracked
MEGA-Bench scores throughout the GRPO training process. As illustrated in Figure the
DPO+GRPO model begins with a substantially higher initial score, demonstrating the immediate
benefit of preference-based pre-alignment. Furthermore, it maintains a clear advantage throughout
training, converging more rapidly and ultimately achieving a higher performance ceiling than its
SFT+GRPO counterpart. For more analysis on this content, please refer to Appendix[A.3]

Training Stability. Beyond performance metrics, we analyzed training stability by comparing the
policy loss curves, presented in Figure 4] The curve for DPO+GRPO is visibly smoother and more
stable, indicating a more consistent and reliable optimization trajectory. In contrast, the SFT+GRPO
policy exhibits greater volatility, suggesting that the RL algorithm must make more drastic and
potentially erratic updates. In terms of format rewards, RL based on SFT cold start is also weaker
than RL based on DPO cold start in terms of the stability of format rewards.

5 RELATED WORK

For a more detailed introduction to related work, please refer to Appendix[A.5]

RL for Multimodal Reasoning. The application of RL has proven to be a highly effective method
for enhancing the reasoning earl of large language models, with notable successes such as
DeepSeek-R1 (Guo et al. 2025) which leverages RLVR ( (Lambert et al. [2024] [Guo et al. [2025)
Inspired by these ae in the text-only domain, a substantial and rapidly growing Te of
research has begun to adapt these RL techniques i VEMS-  ms has catalyzed a wave of >MLLM-
rl” studies (Meng et al.|[2025}   Zhou et al.| [2025} [Zhang et al.|
 2       Zheng et al.                ; | (2025),  sini aiming to harness similar

principles to unlock more advanced multimodal reasoning acs

Cold-Start Strategies and Data Generation. A crucial precursor to effective reinforcement learn-
ing is the ’cold-start” phase, which initializes the model Woreean  ns predominant strategy for this

phase is SFT on high-quality instruction-response pairs (Wei et al.} /2025} [Yang et al.| 2025} /Huang|
let al.| [2025] [Deng et al.| [2025b). In parallel, the prohibitive cost of human annotation has driven

the field towards synthetic data generation, often using powerful teacher models to distill data for

smaller student models (Zhang et al.|{2025b 2024 2024 2025).


===== PAGE BREAK =====

Preprint.

6 CONCLUSIONS

In this study, we introduced the Self-Distilled Preference-based Cold-Start framework, a novel three-
stage methodology. By leveraging a self-distillation process to generate preference data, we decou-
ple the learning of shallow objectives, such as output format, from the deep, logical reasoning skills
targeted during the final RL phase. Our method utilizes DPO to pre-align the model, providing
a superior initial policy for RL. The creative insight of decoupling learning objectives solves the
practical problem of SFT-induced overfitting, which often constrains exploration and leads to sub-
optimal performance. Our results demonstrate the practical value of this approach. The introduction
of the Generalization Factor also provides a valuable new metric for quantifying model generaliza-
tion. This work shows considerable application prospects for developing more robust and capable
multimodal reasoning systems.

Despite these promising results, this study has certain limitations that suggest avenues for future
research. Our experiments were focused on the multimodal domain; further studies should be con-
ducted to validate the efficacy of the SPECS framework in text-only reasoning tasks. The gener-
alization of our findings could also be strengthened through more extensive testing across a more
diverse set of out-of-distribution benchmarks. Such investigations would continue to refine our un-
derstanding of how to most effectively structure learning pipelines for complex AI systems.

ETHICS STATEMENT

All of the paper’s authors have read and adhered to the ICLR Code of Ethics. This work focuses on
advancing the reasoning capabilities of multimodal large language models through a novel training
methodology. The core contributions are algorithmic, aimed at improving the efficiency, stability,
and performance ceiling of reinforcement learning pipelines for such models.

Data and Models: The datasets used for training and evaluation, including Orsta47K, virl39K,
MEGA-Bench, MMMU, MathVista, MATH-Vision, and MathVerse, are publicly available datasets
and benchmarks established within the academic community. Our use of these standard datasets is
intended to ensure transparency, facilitate reproducibility, and allow for direct comparison with prior
work. We do not use any private or sensitive user data. The base model used, Qwen2.5-VL-7B, is
an open-source model, promoting accessibility and further research.

External Evaluators: For evaluating subjective short-answer questions where automated rule-based
metrics are insufficient, we employed proprietary models (GPT-40) as external judges. We acknowI-
edge that these models may have their own inherent biases. This approach was chosen to provide a
consistent and scalable evaluation standard for complex, open-ended responses, a common practice
in current AI research. The specific prompts and evaluation criteria were designed to be as objective
as possible to mitigate these potential biases.

Potential Societal Impact: The goal of this research is to enhance the general reasoning abilities of
AI systems. While this can lead to positive applications in fields like education, scientific research,
and accessibility tools, we recognize that, like any powerful technology, it could potentially be
misused. Our work does not introduce any new applications but rather improves the underlying
training methodology. We encourage the responsible development and deployment of AI systems
built upon these foundational research advancements.

Bias and Fairness: Our proposed framework, SPECS, is not designed for a specific downstream
application and was evaluated on broad-domain academic benchmarks. We have not conducted an
in-depth analysis of social or demographic biases, as the datasets primarily consist of math, science,
and general knowledge problems. We acknowledge that the underlying base model and training data
may contain biases, and future work should investigate how different training strategies impact the
propagation or mitigation of such biases.

REPRODUCIBILITY STATEMENT

To ensure the reproducibility of our work, we provide a detailed account of our methodology and ex-
perimental setup. The core SPECS framework, including its three stages of self-distillation for data
generation, DPO-based pre-alignment, and final GRPO fine-tuning, is described in Section [3]  Our
complete experimental settings, including the datasets, benchmarks, and baselines used for evalu-

10


===== PAGE BREAK =====

Preprint.

ation, are detailed in Section [4.1].  This section also specifies crucial implementation details, such
as learning rates and batch sizes for all training stages . We utilized publicly available frameworks,
MM-EUREKA and LlamaFactory, for our implementation. Furthermore, the appendix offers ad-
ditional resources to aid in reproduction, including the exact system prompts used for training and
inference (Appendix[A.2) and a detailed analysis of the hybrid loss coefficient (Appendix[A.3).

REFERENCES

Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang,
Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jiangiang Wan,
Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng,
Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv
preprint arXiv:2502.13923, 2025.

Yoshua Bengio, Jér6me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In
Proceedings of the 26th annual international conference on machine learning, pp. 41-48, 2009.

Hardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, and Cihang
Xie. Sft or rl? an early investigation into training r1-like reasoning large vision-language models.
arXiv preprint arXiv:2504.11468, 2025a.

Jiacheng Chen, Tianhao Liang, Sherman Siu, Zhengqing Wang, Kai Wang, Yubo Wang, Yuansheng
Ni, Wang Zhu, Ziyan Jiang, Bohan Lyu, Dongfu Jiang, Xuan He, Yuan Liu, Hexiang Hu, Xiang
Yue, and Wenhu Chen. Mega-bench: Scaling multimodal evaluation to over 500 real-world tasks.
In International Conference on Learning Representations (ICLR), 2025b.

Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong
Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning
for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 24185-24198, 2024.

Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V
Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: A comparative study of foundation
model post-training. arXiv preprint arXiv:2501.17161, 2025.

Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit
Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the
frontier with advanced reasoning, multimodality, long context, and next generation agentic capa-
bilities. arXiv preprint arXiv:2507.06261, 2025.

Jia Deng, Jie Chen, Zhipeng Chen, Daixuan Cheng, Fei Bai, Beichen Zhang, Yingian Min,
Yanzipeng Gao, Wayne Xin Zhao, and Ji-Rong Wen. From trial-and-error to improvement: A
systematic analysis of Ilm exploration mechanisms in rlvr. arXiv preprint arXiv:2508.07534,
2025a.

Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang. Openvlthinker:
An early exploration to complex vision-language reasoning via iterative self-improvement. arXiv
preprint arXiv:2503.17352, 2025b.

Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,
Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in Ilms
via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.

Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and
Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models.
arXiv preprint arXiv:2503.06749, 2025.

Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brah-

man, Lester James V Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers
in open language model post-training. arXiv preprint arXiv:2411.15124, 2024.

11


===== PAGE BREAK =====

Preprint.

Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren,
Zhuoshu Li, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, and Chong Ruan. Deepseek-
vl: Towards real-world vision-language understanding, 2024a.

Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-
Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of
foundation models in visual contexts. In International Conference on Learning Representations

(ICLR), 2024b.

Yan Ma, Linge Du, Xuyang Shen, Shaoxiang Chen, Pengfei Li, Qibing Ren, Lizhuang Ma, Yuchao
Dai, Pengfei Liu, and Junjie Yan. One rl to see them all: Visual triple unified reinforcement
learning. arXiv preprint arXiv:2505.18129, 2025.

Fanging Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng
Han, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, Ping Luo, Yu Qiao, Qiaosheng
Zhang, and Wenqi Shao. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-
based reinforcement learning. arXiv preprint arXiv:2503.07365, 2025.

Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang,
Xingzhong Xu, Xin Geng, and Xu Yang. Lmm-rl: Empowering 3b Imms with strong reasoning
abilities through two-stage rule-based rl. arXiv preprint arXiv:2503.07536, 2025.

Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea
Finn. Direct preference optimization: Your language model is secretly a reward model. Advances
in neural information processing systems, 36:53728-53741, 2023.

Jun Rao, Zepeng Lin, Xuebo Liu, Xiaopeng Ke, Lian Lian, Dong Jin, Shengjun Cheng, Jun Yu,
and Min Zhang. Apt: Improving specialist lm performance with weakness case acquisition and
iterative preference training. arXiv preprint arXiv:2506.03483, 2025.

Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,
Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathemati-
cal reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.

Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun
Zhang, Kangjia Zhao, Qiangian Zhang, Ruochen Xu, and Tiancheng Zhao. Vlm-rl: A stable and
generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025.

Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin
Zhang, Chenzhuang Du, Chu Wei, Congcong Wang, Dehao Zhang, Dikang Du, Dongliang Wang,
Enming Yuan, Enzhe Lu, Fang Li, Flood Sung, Guangda Wei, Guokun Lai, Han Zhu, Hao Ding,
Hao Hu, Hao Yang, Hao Zhang, Haoning Wu, Haotian Yao, Haoyu Lu, Heng Wang, Hongcheng
Gao, Huabin Zheng, Jiaming Li, Jianlin Su, Jianzhou Wang, Jiaqi Deng, Jiezhong Qiu, Jin Xie,
Jinhong Wang, Jingyuan Liu, Junjie Yan, Kun Ouyang, Liang Chen, Lin Sui, Longhui Yu, Meng-
fan Dong, Mengnan Dong, Nuo Xu, Pengyu Cheng, Qizheng Gu, Runjie Zhou, Shaowei Liu,
Sihan Cao, Tao Yu, Tianhui Song, Tongtong Bai, Wei Song, Weiran He, Weixiao Huang, Weixin
Xu, Xiaokun Yuan, Xingcheng Yao, Xingzhe Wu, Xinxing Zu, Xinyu Zhou, Xinyuan Wang,
Y. Charles, Yan Zhong, Yang Li, Yangyang Hu, Yanru Chen, Yejie Wang, Yibo Liu, Yibo Miao,
Yidao Qin, Yimin Chen, Yiping Bao, Yiqin Wang, Yongsheng Kang, Yuanxin Liu, Yulun Du,
Yuxin Wu, Yuzhi Wang, Yuzi Yan, Zaida Zhou, Zhaowei Li, Zhejun Jiang, Zheng Zhang, Zhilin
Yang, Zhiqi Huang, Zihao Huang, Zijia Zhao, and Ziwei Chen. Kimi-VL technical report, 2025.

URL https://arxiv.org/abs/2504.07491

Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen.  VI-
rethinker: Incentivizing self-reflection of vision-language models with reinforcement learning.
arXiv preprint arXiv:2504.08837, 2025a.

Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and
Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. In The
Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks

Track, 2024a. URL https: //openreview.net/forum?id=QWTCcxMpPA

12


===== PAGE BREAK =====

Preprint.

Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu,
Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng
Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-v1: Enhancing vision-language model’s
perception of the world at any resolution. arXiv preprint arXiv:2409. 12191, 2024b.

Xiyao Wang, Zhengyuan Yang, Chao Feng, Hongjin Lu, Linjie Li, Chung-Ching Lin, Kevin Lin,
Furong Huang, and Lijuan Wang. Sota with less: Mcts-guided sample selection for data-efficient
visual reasoning self-improvement. arXiv preprint arXiv:2504.07934, 2025b.

Lai Wei, Yuting Li, Kaipeng Zheng, Chen Wang, Yue Wang, Linghe Kong, Lichao Sun, and Weiran
Huang. Advancing multimodal reasoning via reinforcement learning with cold start. arXiv
preprint arXiv:2505.22334, 2025.

Yongliang Wu, Yizhou Zhou, Zhou Ziheng, Yingzhe Peng, Xinyu Ye, Xinting Hu, Wenbo Zhu,
Lu Qi, Ming-Hsuan Yang, and Xu Yang. On the generalization of sft: A reinforcement learning
perspective with reward rectification. arXiv preprint arXiv:2508.05629, 2025.

Guowei Xu, Peng Jin, Ziang Wu, Hao Li, Yibing Song, Lichao Sun, and Li Yuan. Llava-cot: Let
vision language models reason step-by-step. arXiv preprint arXiv:241 1.10440, 2024.

Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng
Yin, Fengyun Rao, Minfeng Zhu, et al. Rl-onevision: Advancing generalized multimodal rea-
soning through cross-modal formalization. arXiv preprint arXiv:2503. 10615, 2025.

Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang,
Yuxin Song, Haocheng Feng, Li Shen, et al. Mulberry: Empowering mllm with o1-like reasoning
and reflection via collective monte carlo tree search. arXiv preprint arXiv:2412.18319, 2024.

Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruogi Liu, Ge Zhang, Samuel Stevens,
Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al Mmmu: A massive multi-discipline multi-
modal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 9556-9567, 2024.

Chen Zhang, Dawei Song, Zheyu Ye, and Yan Gao. Towards the law of capacity gap in distilling
language models. arXiv preprint arXiv:2311.07052, 2023.

Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng
Tao. RI-vl: Learning to reason with multimodal large language models via step-wise group
relative policy optimization. arXiv preprint arXiv:2503.12937, 2025a.

Letian Zhang, Quan Cui, Bingchen Zhao, and Cheng Yang. Oasis: One image is all you need for
multimodal instruction data synthesis. arXiv preprint arXiv:2503.08741, 2025b.

Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou,
Pan Lu, Kai-Wei Chang, Peng Gao, et al. Mathverse: Does your multi-modal Ilm truly see the
diagrams in visual math problems? arXiv preprint arXiv:2403. 14624, 2024.

Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and
Xing Yu. Deepeyes: Incentivizing” thinking with images” via reinforcement learning. arXiv
preprint arXiv:2505.14362, 2025.

Hengguang Zhou, Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, and Cho-Jui Hsieh. R1-

zero’s” aha moment” in visual reasoning on a 2b non-sft model. arXiv preprint arXiv:2503.05132,
2025.

13


===== PAGE BREAK =====

Preprint.

A APPENDIX

A.1 THE USE OF LARGE LANGUAGE MODELS

In preparing this manuscript, we utilized the Large Language Model (LLM) Gemini-2.5-pro
 to polishing the text. Its application was strictly limited to correcting spelling
and grammatical errors. The authors manually reviewed and verified all Al-assisted modifications
to ensure factual accuracy. The core ideas, methodologies, and figures presented are entirely the
original work of the human authors.

A.2. PROMPTS

The following are the System Prompts for the model during the cold start training phase and the
RL training phase. In the instruction format generalization experiment in Section [2| the Inference
System Prompt for the ID Task is consistent with these.

System Prompt for Cold Start Training and ID Task Inference

Solve the question. The user asks a question, and you solve it. You
first think about the reasoning process in the mind and then provide
the user with the answer. The answer is in latex format and wrapped
in $...S. The final answer must be wrapped using the \boxed{}
command. The reasoning process and answer ar   nclosed within <think>
</think> and <answer> </answer> tags, respectively, i.e., <think>
Since $1+1=25, so the answer is $2$. </think> <answer> The answer is
S\boxed{2}$ </answer> , which means assistant’s output should start
with <think> and end with </answer>.

The following is the Inference System Prompt for the OOD Task in the instruction format general-
ization experiment of Section 2]

System Prompt for OOD Task Inference

Solve the question. The user asks a question, and you solve it. You
first think about the reasoning process in the mind and then provide
the user with the answer. The answer is in latex format and wrapped
in $...S. The final answer must be wrapped using the \boxed{}

command. The reasoning process and answer ar   nclosed within <cot>
</cot> and <response> </response> tags, respectively, i.e., <cot>
Since $1+1=2S, so the answer is $2$. </cot> <response> The answer is
S\boxed{2}$ </response> , which means assistant’s output should
start with <cot> and end with </response>.

A.3. ANALYSIS OF LOSS BALANCE COEFFICIENTS IN COLD-START TRAINING

A.3.1 PRELIMINARIES

Supervised Fine-Tuning. SFT adapts pre-trained models by optimizing a cross-entropy loss func-
tion to maximize the log-likelihood of a desired output y, for a given prompt x. The loss is defined
as:

Lsrt(mo) = Eve,y.)~pd |— log 70 (Yc|x)]

By training exclusively on positive examples, SFT creates a sharply peaked probability distribu-
tion that closely mimics the training data. While this accelerates model convergence, it can limit
generalization capabilities.

Direct Preference Optimization. DPO(Rafailov et al.||2023) refines models by learning directly

from preference data, consisting of a prompt x, a chosen response y., and a rejected response yr.

14


===== PAGE BREAK =====

Preprint.

The DPO loss function is designed to increase the relative probability of the chosen response over
the rejected one:

Poe) hg Bolle)

Lppo (19; Tref ) =—-E LYosyr)~D og (4 log
(ite¥r)            Tret(Yo|@)      Tret (Yr |X)

where 7g is the policy being optimized, 7,.e¢ is a reference policy, o is the logistic function and
6 is a temperature parameter. This approach maximizes the margin between chosen and rejected
responses, cultivating a smoother and more robust probability distribution that enhances the model’s
generalization performance.

A.3.2 ANALYSIS OF LOSS BALANCE COEFFICIENTS

In Section B-3| we proposed that in our cold start, in addition to the basic DPO loss, we also added
the SFT loss to ensure that the model does not deviate too much from the chosen samples during the
training process. The combined loss function is thus:

Lnybrid = LDPO + ALSFT

Among the Lprysria, the DPO loss function is designed to increase the relative probability of the
chosen response over the rejected one:

Tae) — og Poll)

Lppo(%o3 Tet) = —Ecay.,y,)~D og (s log
(#stles¥r)               Tet (Yo|)        Tret (Yr |£)

We recorded the model’s rewards for chosen, rewards for rejected, and margins during cold start
training when = 0, A = 0.5, and \ = 1, as shown in Figure [6|below.

From the picture, we can see a lot. Since the training loss of DPO can expand the margin between the
chosen and the rejected, when \ = 0, this margin is the largest. However, this margin causes both
the chosen and rejected rewards to decrease (the rejected rewards decrease faster than the chosen
rewards). When A = 0.5 and A = 1, it ensures that the chosen rewards increase during the DPO
training process.

Chosen Rewards                   0            Rejected Rewards                                Reward Margins
g                                         2                                           2”
g                                                      4                                                        = 10
ing                                                                                     =
ao)                                          ZC
3                                         6                                          5 s                               -
G                                                   g                                                    id                                    — reo 5
se
Al
20 40 60 80 100 120 140          20 40 60 80 100 120 140          20 40 60 80 100 120 140
Training Step                                   Training Step                                   Training Step
Figure 6: Effect of \ on Chosen and Rejected Rewards and Reward Margins During Training

A.4. PERFORMANCE POTENTIAL OF DPO-BASED COLD-START VS. SFT

Table 4: Average RBF by Sample Size

Sample Size
“Moder earls See    120 240-480

base        1.7686 1.9166 1.9206
sft        1.8086 1.8961 1.9336

ours                                             1.8216 1.9268 1.9596

To evaluate the effectiveness of our proposed cold-start strategy, we first compare our DPO-based
approach against a conventional SFT baseline. For this comparison, the SFT model was fine-tuned
exclusively on the “chosen” responses from our preference dataset, while our model was trained
using the full preference pairs with the DPO algorithm.

15


===== PAGE BREAK =====

Preprint.

Performance Potential via Pass@K. We as-      90
sessed the performance of both the DPO      = vee    86.88     87.33
and SFT cold-start models on the MMMU             e

benchmark. As delineated in Figure  the
DPO-based model demonstrates superior per-
formance across evaluated metrics, including
Pass@8, and Pass@32. This indicates that the
DPO cold-start method endows the model with
greater initial capabilities and higher potential
for future alignment tasks compared to the stan-
dard SFT approach.

SFT
Exploration Capability via Rollout Branch-                           Model
ing Factor (RBF). Beyond task performance,
we investigated the intrinsic exploratory capac-         Figure 7: MMMU Pass@K Performance

ity of models, a critical factor for successful re-

inforcement learning. We measure this using

the RBF (Deng et al.|/2025a), which quantifies the diversity of a model’s generation by counting the
number of candidate tokens within the probability mass of top p (0.95) during decoding. A higher
RBF signifies greater generation diversity and, consequently, a stronger capacity for exploration.
As shown in Table |4| our DPO-based cold-start method yields a substantially higher RBF than the
SFT baseline. This finding suggests that our approach cultivates a broader exploration space, which
is highly beneficial for the subsequent RL phase, enabling the model to discover more diverse and
potentially higher-quality solutions. This outcome highlights a key advantage of our method in
preparing models for alignment.

A.5 RELATED WORK

The application of RL has emerged as a highly effective method for enhancing the reasoning capabil-
ities of large language models, with notable successes in the text-only domain such as DeepSeek-R1
(Guo et al.|/2025), which leverages RLVR (Lambert et al.|/2024| {Guo et al.|/2025). Inspired by these
advancements, a substantial and rapidly growing body of research has begun adapting RL techniques
for Vision-Language Models (VLMs). This has catalyzed a wave of >MLLM-r1” studies, all aiming
to harness similar principles to unlock more advanced multimodal reasoning abilities. For instance,
specific approaches include exploring the frontiers of multimodal reasoning with rule-based rein-
forcement learning in (GiereraB aalcaox creating a stable and generalizable R1-style
model with VLM-R1 (Shen et al.| |2025), and empowering smaller models using two-stage rule-
based RL as seen in LMM-RI (Peng et al.|{2025). Researchers have also demonstrated significant
visual reasoning on non-SFT models with R1-Zero (Zhou et al.|{2025), learned step-wise reason-

ing with R1-VL (Zhang et al.) /2025a), and achieved data-efficient self-improvement using MCTS-
guided sample selection in SOTA with less (Wang et al.||2025b). Furthermore, innovative methods

seek to incentivize self-reflection with VL-Rethinker (Wang et al.||2025a), encourage thinking

with images” in DeepEyes (Zheng et al.}|2025), or unify various visual tasks under a single RL
framework as proposed in One RL to See Them All 2025).

A crucial precursor to effective reinforcement learning is the “cold-start” phase, which initializes
the model’s policy before the RL stage begins. The predominant strategy for this phase is SFT
on high-quality instruction-response pairs, a foundational step adopted by many leading models to
establish a strong baseline performance
 [2025b). In parallel with refining cold-start methods, the prohibitive cost of human annotation
has driven the field towards synthetic data generation. This approach often involves using powerful
teacher models to distill vast amounts of data for training smaller student models. This trend is
evident in a variety of techniques, such as synthesizing multimodal instruction data from a single
image with Oasis 2025b), empowering models with reflection via collective Monte
Carlo Tree Search in Mulberry (Yao et al. 2024), and enabling models to reason step-by-step with
LLaVA-CoT 2024). These data-centric strategies, including those used to generate data
for models like Vision-R1 (Huang et al.|/2025), are vital for creating the rich, high-quality datasets
needed to effectively train and align modern VLMs for complex reasoning tasks.

16


===== PAGE BREAK =====

Preprint.

A.6 QUALITATIVE EXAMPLES

QA Case Analysis: Case #001

Question: Hint: Please answer the question requiring an integer answer
and provide the final value, e.g., 1, 2, 3, at the end. Question: Subtract all
yellow matte blocks. Subtract all tiny brown cylinders. How many objects
are left?

Ground Truth: 5

Qwen2.5-VL-7B response: 6

Ours-7B response

<think>To solve the problem, first identify all objects in the image:
. Yellow matte cylinder
. Green metallic cylinder (left)
. Green metallic cylinder (front)
. Silver metallic cylinder
. Tiny brown cylinder
. Yellow matte block
. Red matte sphere
Next, follow the instructions:
1. Subtract all yellow matte blocks: Removing the yellow matte block leaves 6
objects.
2. Subtract all tiny brown cylinders: Removing the tiny brown cylinder leaves 5
objects.
Thus, the remaining objects are 5.
</think>
<answer>The answer is | 5 </answer>

Qwen2.5-VL-7B response

To solve this, we first identify all objects in the image: there are 8 objects in total. The
yellow matte block is one object, and the tiny brown cylinder is another. Subtracting
these two objects leaves us with 8 — 1 — 1 = 6 objects.

So, the number of objects left is 6.

17


===== PAGE BREAK =====

Preprint.

QA Case Analysis: Case #002

Web pages visited

Person | Number of Web pages

Question: Hint: Please answer the question requiring an integer answer
‘aston             and provide the final value, e.g., 1, 2, 3, at the end. Question: Several
Percy              people compared how many Web pages they had visited. What is the
Derek                    mean of the numbers?’

—                   Ground Truth: 64

=                  Qwen2.5-VL-7B response: 67.75

Bonnie

Ours-7B response

<think>To find the mean of the numbers, first sum all the numbers and then divide
by the count of numbers.

The numbers are: 84, 56, 70, 68, 60, 47, 75, 52.

Sum of the numbers:

84+ 56 + 70 + 68 + 60 + 47 + 75 4 52
Calculating step by step:
84 + 56 = 140
140 + 70 = 210
210 + 68 = 278
278 + 60 = 338
338 + 47 = 385
385 + 75 = 460
460 + 52 = 512

The sum of the numbers is 512.
Next, count the number of values: There are 8 values.
The mean is the sum divided by the count:

Mean = = = 64

So, the mean of the numbers is 64.
</think>
<answer>The answer is | 64 |</answer>

Qwen2.5-VL-7B response

To find the mean of the numbers, we need to follow these steps:

1. Add up all the numbers in the list. 2. Count how many numbers there are. 3.
Divide the sum by the count.

Let’s start with step 1: Add up all the numbers.

The numbers are: 84, 56, 70, 68, 60, 47, 75, 52.

Sum = 84 + 56 + 70 + 68 + 60 + 47 + 75 + 52

Sum = 542

Mean = 542/8
Mean = 67.75
So, the mean of the numbers is 67.75.

18


===== PAGE BREAK =====

Preprint.

QA Case Analysis: Case #003

Question: Hint: Please answer the question and provide the correct op-
tion letter, e.g., A, B, C, D, at the end. Question: At a certain moment,
there is a passenger ship at sea point P, and lighthouse A is measured in
the direction 30.0 north by east of P, and is 50.0 nautical miles away. The
passenger ship sails at the speed of 60.0 nautical mile/hour in the direction
of 60.0 from north by west for hours to reach point B, then tanZBAP =
0

Choices:

(A) 48) £O 8M

Ground Truth: (A) 2

Qwen2.5-VL-7B response: (B) 2

Ours-7B response

<think> First, we need to determine the coordinates or distances relevant to the prob-
lem. The initial position of the passenger ship is at point P, and it is 50 nautical miles
away from lighthouse A in the direction 30.0 degrees north by east. The passenger
ship then sails in the direction 60.0 degrees from north by west for 2 hours at a speed
of 60 nautical miles per hour, reaching point B.

The distance PB can be calculated as follows:

PB=60 x : = AO nautical miles

The direction 60.0 degrees from north by west means that the angle 7ZBPA =
30.0° + 60.0° = 90.0°. Therefore, AABP is a right triangle with ZBAP = 30.0°.
In a right triangle, the tangent of an angle is the ratio of the opposite side to the
adjacent side. Here, tan 7BAP = of.

Since AP = 50 nautical miles and PB = 40 nautical miles, we can use the
Pythagorean theorem to verify:

AB = VAP? + BP? = \/502 + 402 = 2500 + 1600 = V4100 = 10V41

However, we only need the ratio BE f

BP 40 4
tan ZBAP = —- = —~ =
an          AP 50 5

Thus, the answer is | A |. </think>

<answer>The answer is| A |</answer>

Qwen2.5-VL-7B response

To solve this problem, we need to determine the coordinates of points A and B rela-
tive to point P, and then find the tangent of the angle ZBAP.
1. Determine the coordinates of point A relative to point P:

2. Determine the coordinates of point B relative to point P:

3. Calculate the tangent of ZBAP:
625./3—500—1250V3—1500 _ —925/3-—1000 _

.. - Simplifying the expression, we get

Simplifying     P     ?   8  625/34 1500—625./3+500       2000

—925/3—1000 _ —925./3—1000 _ —925./3—1000 _
2000    ~~           a

2000              Be

Therefore, the correct answer is | B |.

