arXiv:2510.26241vl [cs.CV] 30 Oct 2025

Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for
Vision—Language Models

Shiho Matta’, Lis K. Pereira?>*", Peitao Han?>*, Fei Cheng!, Shigeru Kitazawa?>*
' Kyoto University, Japan
? Center for Information and Neural Networks, Japan
3 National Institute of Information and Communications Technology, Japan
4 The University of Osaka, Japan
“matta@nlp.ist.i.kyoto-u.ac. jp, liskanashiro@nict.go.jp

Abstract

Modern vision—language models (VLMs) ex-
cel at many multimodal tasks, yet their grasp
of temporal information in video remains
weak and, crucially, under-evaluated. We
probe this gap with a deceptively simple but
revealing challenge: judging the arrow of
time (AoT)—whether a short clip is played
forward or backward. We introduce AoT-
PsyPhyBENCH, a psychophysically validated
benchmark that tests whether VLMs can in-
fer temporal direction in natural videos us-
ing the same stimuli and behavioral baselines
established for humans. Our comprehensive
evaluation of open-weight and proprietary, rea-
soning and non-reasoning VLMs reveals that
most models perform near chance, and even the
best lag far behind human accuracy on physi-
cally irreversible processes (e.g., free fall, dif-
fusion/explosion) and causal manual actions
(division/addition) that humans recognize al-
most instantly. These results highlight a fun-
damental gap in current multimodal systems:
while they capture rich visual-semantic corre-
lations, they lack the inductive biases required
for temporal continuity and causal understand-
ing. We release the code and data for AoT-
PsyPhyBENCH to encourage further progress
in the physical and temporal reasoning capabil-
ities of VLMs.

1 Introduction

Vision—language models (VLMs) have recently
achieved remarkable progress in multimodal un-
derstanding, including tasks such as video caption-
ing, retrieval, and question answering (Bai et al.,
2025; Qwen Team, 2024; Liu et al., 2024; Wang
et al., 2024a; Xiao et al., 2021; Wu et al., 2024; Fu
et al., 2025). Yet a fundamental question remains:
do these models possess the implicit physical and
causal assumptions, i.e. the inductive biases, that
humans rely on to interpret the physical world?
Among the most basic of such biases is the arrow

of time (AoT): the implicit assumption that events
unfold irreversibly from past to future, constrained
by gravity, entropy, and causality. Judging whether
a short video plays forward or backward provides
a minimal yet powerful diagnostic of such bias: a
system that has internalized temporal irreversibility
should recognize when physical regularities are vi-
olated, whereas a system that relies only on visual
correlation will fail despite extensive training.

Forward F1

4 4 This is played forward.   a

ao                                                    =
Uh ~ This is played forward.             ‘S   62.5
ap

Is this video played forward or backward? (Answer: Forward)

i  i
*

Is this video played forward or backward? (Answer: Backward)

p*    This is played backward.              Backward F1

Human                                                                                              @ 39.0

é                                                    =
ta    Looks like forward to me! X              ‘e   57.4
an

VLM

Overall Accuracy   a 89.2 ‘e 60.1

Figure 1: Overview of the arrow-of-time (AoT) task.
Humans identify the AoT for both forward and back-
ward playbacks with high accuracy; however, VLMs per-
form substantially worse and exhibit a label-prediction
bias, preferring Forward (see Section 6.2).

To anchor this diagnostic in human performance,
Hanyu et al. (2023) conducted a psychophysical
study using 360 three-second natural video clips
from everyday activities. They found a global
forward bias: participants more often judged re-
versed clips as forward (39% errors on reversed
vs. 9% on forward trials). However, humans de-
tected reversals rapidly and almost flawlessly in
five specific categories: Fall (free fall) and Dif-


===== PAGE BREAK =====

fusion (diffusion/explosion), which involve phys-
ically irreversible processes governed by gravity
and entropy; Proceed (forward locomotion); and
Division (manual division) and Put (manual addi-
tion), which involve agent-driven causal sequences.
In these cases, reversed playback becomes visibly
implausible. For example, when a big chunk of
snow ascends back onto the roof, defying grav-
ity (Figure 1). These results demonstrate that hu-
mans exploit not only low-level motion cues but
prior knowledge of physical regularities and causal
structure, providing a psychophysically validated
foundation for testing AoT in VLMs.

Building on this, we introduce AoT-
PsyPhyBENCH      (pronounced      “AoT-sci-fi
bench’), an evaluation framework that directly
inherits validated stimuli and behavioral baselines
from Hanyu et al. (2023). We systematically test
modern VLMs, from open-weight to proprietary
reasoning/non-reasoning models, on the same
AoT judgment task. Our findings reveal that most
models perform near chance even in "obvious"
irreversible processes, with the best configuration
lagging humans by approximately 29 percentage
points, suggesting that they lack the inductive
biases underpinning human causal and temporal
perception (Figure 1).

Our contributions are threefold. (1) Compre-
hensive evaluation. We present the first system-
atic evaluation of modern VLMs on psychophys-
ically grounded AoT judgment across zero-shot,
few-shot, chain-of-thought prompting, and super-
vised fine-tuning. Despite apparent task simplicity,
the gains from elaborate prompting and fine-tuning
are minimal and inconsistent, indicating that the
bottleneck is temporal and physical understand-
ing, not instruction following. (2) Psychophysi-
cally validated benchmark. We introduce AoT-
PsyPhyBENCH, curated from Hanyu et al.’s psy-
chophysical dataset by retaining only clips with
clear human consensus on irreversible processes
while excluding ambiguous cyclic motions. This
yields a low-ambiguity benchmark enabling direct,
reliable human—model comparison on temporal di-
rection judgment. (3) Open resources. We release
our benchmark, evaluation scripts, and model out-
puts to advance research on temporal reasoning and
physical understanding in VLMs. !

'Ror anonymity, we release the dataset at https:
//anonymous. 4open. science/r/AoT-PsyPhyBENCH- 222F
(anonymized repository) during the reviewing process. We
will update this link in the camera-ready version.

2 Related Work
2.1 Vision—Language Models (VLMs)

Modern VLMs couple a vision encoder with a large
language model (LLM) via a multimodal connector
and are typically aligned through instruction tuning
on image-—/video-text data (Wang et al., 2024b;
Qwen Team, 2024; Azzolini et al., 2025; Bai et al.,
2025). Despite rapid progress, frontier VLMs often
fail on video tasks that depend on event order (Xue
et al., 2025).

We broadly categorized the VLMs used in our
evaluation along two axes: (i) proprietary vs.
open-weight and (ii) reasoning vs. non-reasoning.
Proprietary models are accessible only via APIs
with limited transparency, while open-weight mod-
els release checkpoints and inference scripts, as
well as partial training details. Reasoning models
are trained to generate multi-step deliberation (e.g.,
chain-of-thought reasoning) automatically before
producing the final output, often with controllable
reasoning effort. Non-reasoning models priori-
tise perceptual understanding (e.g., captioning, vi-
sual question-answering) and instruction following,
without dedicated reasoning objectives; they typi-
cally respond directly unless explicitly prompted
for step-by-step reasoning.

Building on this taxonomy, we evaluate the fol-
lowing models. Proprietary non-reasoning: GPT-
4o and GPT-4.1 (OpenAI, 2024, 2025a). Propri-
etary reasoning: 03, 04-mini, and GPT-5 (Ope-
nAI, 2025c,b); Gemini-2.5-Pro (Comanici et al.,
2025). Open-weight non-reasoning: Qwen?2-
VL (Wang et al., 2024b) and Qwen2.5-VL (Bai
et al., 2025). Open-weight reasoning: Cosmos-
Reason! (Azzolini et al., 2025), which combines
explicit AoT supervision and reinforcement learn-
ing on ~30k forward/reverse video clip pairs with
reasoning traces; and QVQ-72B-Preview (Qwen
Team, 2024).

2.2 Temporal Reasoning Benchmarks

We evaluate VLMs on a psychophysically validated
AoT benchmark to address two key limitations in
current temporal reasoning evaluation:

(1) Lack of temporal dependency. Several re-
cent benchmarks claim to assess temporal reason-
ing in VLMs, but many do not demonstrate de-
pendence on event order (Liu et al., 2024; Wang
et al., 2024a; Fu et al., 2025; Xiao et al., 2021;
Wu et al., 2024). Xue et al. (2025) evaluated strong
VLMs on standard temporal benchmarks and found


===== PAGE BREAK =====

a critical failure: models show minimal or no per-
formance degradation when frames are shuffled
or reversed, indicating these tasks can be solved
through scene context without temporal understand-
ing. In contrast, AoT judgment is inherently tempo-
ral—teversed videos violate physical laws, making
temporal order task-critical rather than incidental.

(2) Lack of psychophysical validation. Ex-
isting AoT benchmarks simply reverse video
datasets (Bagad et al., 2023; Wang et al., 2023;
Du et al., 2024; Agarwal et al., 2025; Xue et al.,
2025) without controlled human baselines, for in-
stance, with quantitative measurements of accuracy,
reaction times, and category-specific effects. This
creates systematic evaluation problems: difficulty
remains uncalibrated (videos ambiguous to humans
become spurious failure cases), model—human per-
formance gaps cannot be quantified, systematic bi-
ases (forward bias, category effects) go undetected,
and bidirectionally plausible cases contaminate test
sets. Psychophysical grounding is essential to dis-
tinguish genuine deficiencies from artifacts of am-
biguous stimuli.

To address these limitations, we introduce AoT-
PsyPhyBENCH, a psychophysically validated
benchmark enabling direct human—model compari-
son on temporal direction judgment.

3 Experimental Settings

We evaluate the following models (described in
Section 2) on AoT-PsyPhyBENCH: Proprietary
non-reasoning: GPT-40 and GPT-4.1; propri-
etary reasoning: 03, 04-mini, GPT-5, and Gemini-
2.5-Pro; open-weight non-reasoning: Quwen?2-
VL and Qwen2.5-VL; open-weight reasoning:
Cosmos-Reason1-7B and QVQ-72B-Preview. We
test these models across multiple experimental set-
tings, which are detailed in the following sections.

3.1 Zero-shot Settings

For the zero-shot experiments, we adopted a simple
prompt design. In the system prompt, the model
is asked to output only the F (forward) or B (back-
ward) label as follows:

System prompt You will see videos pro-
vided from the user, played either for-
ward or backward. Finish your answer
with F or B only. F for forward and B for
backward.

In the user prompt, we provide the sampled
video frames with the following instruction:

User prompt [Video frames] Detect
whether the video plays forward or back-
ward with confidence.

3.2 Few-shot Settings

For few-shot experiments, we constructed
four demonstration examples from AoT-
PsyPhyBENCH: two forward and two backward
videos exhibiting visually distinctive temporal
cues (e.g., explosions, falling objects). Each
demonstration consists of sampled video frames,
the user prompt from Section 3.1, and the correct
label (F for forward, B for backward). These four
demonstration videos were excluded from the test
set to prevent data leakage.

3.3 Reasoning Effort Ablation Settings

Reasoning-centric models are trained to generate
internal reasoning chains automatically before pro-
ducing outputs, while non-reasoning models usu-
ally respond directly unless explicitly prompted for
step-by-step reasoning. We ablated the length of
thinking of VLMs by either setting the reasoning ef-
fort parameter of reasoning models, or by manually
prompting chain-of-thought (CoT) reasoning.

Controlling Reasoning Effort for Proprietary
Reasoning Models. We set the reasoning ef-
fort in GPT-5 and Gemini-2.5-Pro to 3 levels: low,
medium, and high. To be noted, the low, medium,
and high reasoning effort of Gemini-2.5-Pro cor-
responds to the thinking budget in its native API
of 1,024, 8,192, and 24,576 tokens, respectively.
These values represent upper bounds; models do
not necessarily use the full allocated budget.

Simulating Reasoning Effort Control with CoT
Reasoning for Non-Reasoning Models. Open-
weight models lack explicit reasoning effort con-
trol available in proprietary reasoning models.
We therefore use few-shot chain-of-thought (CoT)
prompting to simulate varying reasoning depths.
Using two videos from Section 3.2, we created two
CoT variants: Simple CoT with brief reasoning
and Multi-Step CoT with elaborate step-by-step
analysis.

Simple CoT. The model is instructed to focus
on moving elements and identify visual cues indi-
cating playback direction. We manually curated
reasoning traces for each demonstration. Due to
space limitations, we omitted the example in this
version.


===== PAGE BREAK =====

Multi-step CoT. We designed structured instruc-
tions with multi-step reasoning traces divided into
three stages: Observation (objectively describe vis-
ible events without assumptions), Assumption (in-
fer plausible physical or causal sequences), and
Conclusion (determine whether observations align
with or contradict assumptions to judge playback
direction). Due to space limitations, we omitted the
prompts in this version. A model output example
is shown in Figure 2.

3.4 Supervised Fine-Tuning (SFT) Settings

We constructed in-domain fine-tuning datasets to
test whether explicit temporal direction supervision
improves model performance. Note that all videos
in Hanyu et al. (2023), AoT-PsyPhyBENCH, and
these fine-tuning datasets are drawn from Moments
in Time (Monfort et al., 2019). We randomly sam-
pled 500, 1,500, and 2,500 clips distributed across
its 339 action classes in the original dataset. These
clips are entirely distinct from the videos in AoT-
PsychBench. Each video was used to create two
examples: the original (forward, labeled F) and its
temporal reverse (backward, labeled B), yielding
1,000, 3,000, and 5,000 training examples.

We fine-tuned Qwen2-VL-7B and Qwen2.5-VL-
7B using the zero-shot prompt format (Section 3.1)
with standard supervised learning. We then evalu-
ated these models on AoT-PsyPhyBENCH.

3.5 Evaluation Metrics

We evaluate models using overall accuracy and
F1 scores for Forward and Backward classes sepa-
rately. Class-specific F1 scores highlight potential
directional biases in model predictions.

3.6 Hyperparameter Settings

Frame Sampling and FPS Settings. Frames were
sampled at equal temporal intervals without resiz-
ing. Frame rates were set based on model-specific
defaults and training configurations: 2 FPS for
Qwen2-VL, Qwen?2.5-VL, and QVQ-72B-Preview
(standard for these models), and 4 FPS for Cosmos-
Reason1-7B (matching its inference settings rec-
ommendation’) and all proprietary models. Pre-
liminary experiments showed that 4 FPS balances
performance with API costs (Section 5.5).
Inference Parameters. For open-weight mod-
els (Qwen2-VL, Qwen2.5-VL, and QVQ-72B-
Preview), we set temperature to 0.6 and top-p to

*https://huggingface.co/nvidia/Cosmos-Reason1-7B

0.95. For Cosmos-Reason1-7B, we used temper-
ature 0.6, top-p 0.95, and repetition penalty 1.05,
consistent with settings reported in their paper. For
GPT-40 (GPT-40-2024-11-20) and GPT-4.1 (GPT-
4.1-2025-04-14), we applied temperature 0.6, top-p
0.95, and frequency penalty 0. For OpenAI reason-
ing models (03, 04-mini, GPT-5), inference param-
eters such as temperature and top-p are not exposed
via the API and thus remained at default values. For
Gemini-2.5-Pro (accessed 2025-10-02), we used
the OpenAI-compatible API with default values:
temperature 1.0 and top-p 0.95.

SFT Parameters. We froze the vision encoder
and applied LoRA (Hu et al., 2021) (rank 8) to
both the language model and the vision-language
bridging MLP layer. We used a maximum sequence
length of 2,048 tokens, a batch size of 2 per device
with gradient accumulation over 2 steps (effective
batch size of 4), a learning rate of 1 x 10-°, and
trained for 5 epochs with a cosine learning rate
scheduler and 10% warmup.

4 AoT-PsyPhyBENCH: A
Psychophysically Validated
Arrow-of-Time Benchmark

We evaluate whether vision—language models
(VLMs) can infer the arrow of time—that is, dis-
tinguish forward and backward playbacks in every-
day videos. Our work builds on the psychophys-
ical study of Hanyu et al. (2023), enabling direct
comparison with human performance. The source
dataset comprises 360 three-second clips at 29.97
FPS drawn from the Moments in Time dataset
(Monfort et al., 2019), covering a broad range of
daily dynamics. Ten participants viewed every clip
twice—once forward and once reversed—across
two sessions on different days (each session: 180
forward, 180 reversed; order randomized per par-
ticipant). Three raters annotated six motion cate-
gories: (1) Proceed: forward locomotion of peo-
ple, animals, or vehicles; (2) Fall: free-fall/ballistic
motion under gravity; (3) Diffusion: centrifugal
diffusion or small-particle explosions; (4) Divi-
sion: division of material by hand or tool; (5)
Put: addition/construction of material by hand; and
(6) Reciprocal: reciprocating (cyclic) motion (Ta-
ble 1). The first five categories reflect irreversible
processes that, when reversed, tend to violate fun-
damental physical regularities (entropy increase,
gravity, causal ordering), whereas reciprocating
motion is bidirectional and often appears plausi-


===== PAGE BREAK =====

ble in both directions. Categories are not mutually
exclusive. Human performance exhibited a pro-
nounced forward bias: participants made errors on
39% of reversed trials versus 9% of forward trials,
with accuracy strongly modulated by the presence
of motions from the first five categories. Perfor-
mance was significantly lower for the reciprocating
motion category.

In this work,    we introduce AoT-
PsyPhyBENCH, a curated subset of Hanyu
et al. (2023) that excludes the reciprocating (cyclic)
motion category (category 6) and retains only
high-consensus clips from categories 1-5. This
exclusion is justified by human performance
data: the cyclic motion category achieved only
61.1% overall accuracy (accuracy with forward
and backward videos combined) compared to
83.8—93.5% for the other motion categories, sub-
stantially degrading overall benchmark reliability
(77.6% with cyclic motion vs. 89.2% without).
From the original 720 videos (360 clips with
their reversed counterparts), our filtering yields
212 clips (totaling 424 videos with reversals).
This produces a low-ambiguity, psychophysically
validated AoT evaluation benchmark that enables
direct and reliable human—model comparison.
Table 1 provides an overview of the motion
categories with comparative statistics illustrating
how AoT-PsyPhyBENCH filters the original
dataset.

5 Results

5.1 Zero-shot Performance

Table 2 presents zero-shot results, revealing two
key gaps: open vs. proprietary and VLM vs.
human performance gaps. Additionally, we did
not find advantages of reasoning models over non-
reasoning models.

Open vs. Proprietary. Open-weight models
cluster around 50% accuracy (random baseline),
with Cosmos-Reason1-7B slightly higher at 52.1%.
QVQ-72B-Preview, designed for mathematical rea-
soning over single images rather than physical dy-
namics, performs at chance with extreme label bias
(Section 6.2). Cosmos-Reason1-7B, despite be-
ing trained on an undisclosed AoT dataset where
it reportedly achieved 60% accuracy, shows mini-
mal improvement on our benchmark. Proprietary
models consistently exceed random guessing, with
GPT-4.1 achieving the highest zero-shot perfor-
mance (60.1%).

VLM vs. Human. All VLMs lag substantially
behind humans (89.2%), with a gap of 29.1 per-
centage points even for the best zero-shot model,
GPT-4.1.

Reasoning vs. Non-Reasoning: Explicit rea-
soning capability provides no clear advantage.
The non-reasoning model GPT-4.1 (60.1%) out-
performs reasoning models 03 (55.2%), 04-mini
(56.1%), and GPT-5 (56.1%).

5.2 Few-shot Performance

We test whether few-shot examples elicit in-context
learning for AoT (Table 3). Overall, few-shot
prompting does not consistently improve perfor-
mance and often amplifies label prediction bias.
GPT-5 shows modest, stable gains (56.1—57.4%
accuracy; Backward Fl: 26.8-30.9 with 4
shots). In contrast, GPT-4.1 degrades with few-shot
prompts (60.0-+56.0/55.5% accuracy; Backward
F1: 57.4-451.7/49.9). GPT-40 shows severe degra-
dation: accuracy drops (52.6—>50.7%) while Back-
ward F1 collapses catastrophically (24.9-4.6/5.1),
indicating near-complete failure to detect reversed
videos. The open model Qwen2.5-VL-72B ex-
hibits minimal accuracy change (49.5-+51.9%)
but substantially worsened bias (Backward F1:
38.0-418.7/21.1). These results demonstrate that
few-shot prompting provides limited benefits for
temporal reasoning: only one model (GPT-5)
shows modest improvement, while others either
decline or amplify their existing forward bias, sug-
gesting that in-context learning is insufficient to
elicit robust AoT understanding.

5.3. Reasoning Effort Ablation

Controlling reasoning effort in proprietary rea-
soning VLMs. We varied the reasoning effort pa-
rameter (low/medium/high) for GPT-5 and Gemini-
2.5-Pro (Table 4). Surprisingly, increasing reason-
ing effort does not improve performance and of-
ten degrades it. Gemini-2.5-Pro with low effort
achieves 61.1% accuracy, the highest performance
across all our experimental settings, yet medium
and high efforts decrease to 59.9% and 58.7%,
with Backward F1 declining from 52.1 to 51.4 to
49.0. From the low effort setting, we highlight
one correctly predicted example, where we asked
Gemini-2.5-Pro to explain its reasoning in a follow-
up dialogue turn (bottom-right of Figure 2). GPT-5
showed an even more pronounced pattern: low ef-
fort achieves 57.8% accuracy, while medium and


===== PAGE BREAK =====

Reversal is
easy for
humans?

Category &

Description                                                     (F/B)

Human F1 # samples

Included in AoT-
PsyPhyBENCH?

Example 1          Example 2

(1) Proceed: forward           v
locomotion of people,
animals, or vehicles

86.5 /82.5        82

(2) Fall:                    v        86.9/82.8      84
free-fall/ballistic

motion under gravity

(3) Diffusion:                        v
centrifugal diffusion or
small-particle

explosions

84.6/78.7        56

(4) Division: division           v          86.0/80.6        37
of material by hand or

tool

(5) Put:                              v
addition/construction of
material by hand

84.1/77.4          67

(6) Reciprocal:                    x
reciprocating (cyclic)
motion

71.6/38.5      148

Yes

Table 1: Motion categories following Hanyu et al. (2023), with human performance, counts, and visual examples.
Human F1 scores are reported as forward (F)/backward (B) for each motion category. AoT-PsyPhyBENCH includes
categories 1-5 (all clips) and excludes category 6. “ marks categories where reversal is typically easy for humans
(irreversible processes); X marks cyclic, bidirectional motions where reversal is challenging.

high efforts drop to 56.1% and 56.4%. More criti-
cally, Backward F1 collapses with increased effort
(34.4 to 26.8 to 25.7), indicating amplification of
forward bias. These results suggest that extended
reasoning does not elicit better temporal reasoning
and instead reinforces existing directional biases.

Chain-of-thought (CoT) prompting in an
open VLM. We tested explicit step-by-step reason-
ing with Qwen2.5-VL-72B using two CoT variants
(Simple and Multi-step; Section 3.3). As shown
in Table 5, CoT prompting fails to improve per-
formance and amplifies label prediction bias, most
notably with Multi-step CoT (2-shot), where Back-
ward F1 drops by 13.2 points (Section 6.2). In the
model output example using the Multi-step CoT
prompt (top-right in Figure 2), we saw that the
model usually could identify the event in the video,
yet it strongly believed that no physical law was vio-
lated and the video was played forward. Combined
with the reasoning effort results, these findings
demonstrate that additional deliberation, whether
through explicit effort controls or prompted CoT,

does not compensate for the lack of robust temporal
and physical understanding, and often exacerbates
existing biases.

5.4 SFT Performance

We fine-tuned Qwen2-7B and Qwen2.5-VL-7B on
1,000, 3,000, and 5,000 training examples to test
whether explicit supervision improves AoT perfor-
mance (Table 6). Fine-tuning failed to improve ac-
curacy, which remained near chance level (~50%)
across all training set sizes. While fine-tuning re-
duced the extreme forward bias present in vanilla
models, leading to a more balanced forward/back-
ward FI scores, it did not yield genuine task compe-
tence. Performance plateaued after 1,000 examples,
with no gains from additional data. These results
suggest that AoT task might require different train-
ing approaches beyond conventional SFT.

5.5 FPS Ablation

We evaluated GPT-4.1 across frame rates from 2
to 30 FPS and compared results to human per-


===== PAGE BREAK =====

An incorrect output from Qwen2.5VL-72B with Multi-step CoT prompting:
Observation: The video shows two men engaged in arm-wrestling at a desk. (......) Their
hands are clasped together on the desk, and they are pulling against each other.
Assumption: In a typical arm—wrestling match, the participants start with their hands
clasped and then exert force to try to pin the opponent's arm to the table. (......)
Conclusion: The video shows the men in the middle of the struggle, with their hands
clasped and both leaning forward in an effort to win. This matches the expected
sequence of a forward—playing arm—wrestling match. There are no signs of reversed
motion, such as the hands separating or the men leaning backward unexpectedly.
Therefore, the video is played forward.

Gemini-2.5-Pro Self-Explained Reasoning:

The video shows two men engaged in an arm wrestling match.

1. Normal Progression of Arm Wrestling: In a standard arm wrestling match, the
contest begins with both participants’ arms in an upright, near—-vertical position. The goal
is to force the opponent's arm down onto the table. (......)

2. Observed Motion in the Video: The video clip starts with the man in the white shirt in
a Clear losing position, with his arm almost completely pinned to the desk. As the video
plays, his arm moves upward and away from the desk, (......)

3. Conclusion: The action shown is the exact reverse of how an arm wrestling match
progresses. A match ends with an arm going down, it doesn't start there and move up.
Because the video shows the conclusion of the match first and then moves back toward
the starting position, it is playing in reverse.

Figure 2: Left: a backward video clip (category: Put). Top-Right: Qwen2.5VL-72B Multi-step CoT reasoning.
Bottom-Right: Gemini-2.5-Pro’s self-explained reasoning trace. Qwen2.5VL-72B correctly identified the event in
the scene and made a valid assumption, but failed to observe that the event was reversed which led to an incorrect
conclusion. In contrast, Gemini-2.5-Pro correctly detected the reversal of the event in Step 2 based on a valid
assumption it made in Step 1.

Model         EFI B.FI Acc.

Baselines
Random                   —       —      50.0              Model      #Shots FE. F1 B.F1 Ace.
Human                      99.0 89.0 89.2
Open Models                                                        aa A   38.0 49.5
Non-reasoning Models                                                  Qwen2.5          ,           6 4. > 1 8 7 5 0. >
Qwen2-VL-7B           66.7 0.0 50.0               VL-72B           ,

4           65.4 21.1 51.9
Proprietary Models
0           62.3 57.4 60.0

Qwen2.5VL-7B           63.0 19.5 49.3
Qwen2.5VL-72B          574 38.2 49.5
Reasoning Models

QVQ-72B-Preview 66.1 0.0 49.4               GPT4.1 00 2 59.5 51.7 56.0
cosmos-reasonl 7B 31.2 63.3 52.1                                      4         60.0 49.9 55.5
Proprietary Models                                                        0         65.4 249 52.6
Non-reasoning Models                                         GPT-40          2         66.8 4.61 50.7
GPT-4o0                     65.4 249 52.6                                    4         66.5 5.07 50.7
GPT-4.1                  62.5 57.4 60.1                                    0         687 268 561
Reasoning Models                                               GPT-5            2 69.5 29.2 57.4
03    7                     67.2 29.1 55.2                                    4         692 30.9 574
04-mini                              674 33.1 56.1
GPT-5                      68.7     26.8 56.1                        Table 3: Few-shot performance.
Gemini-2.5-pro             65.9 51.4 59.9

Table 2: Zero-shot performance.


===== PAGE BREAK =====

Table 5:

Model       Effort E.F1 B.F1 Acc.
Proprietary Models
Low       68.9 344 57.8
GPT-5      Medium 68.7 26.8 56.1
High       69.1 25.7 56.4
Gemini        Low       67.2 52.1 61.1
35-pro Medium 65.9 514 59.9
High       65.3. 49.0 58.7
Table 4: Reasoning effort ablation.
Setting        Shots F.F1 B.F1 = Acc.
Qwen2.5VL-72B-instruct
No CoT      2    64.2 18.7 50.2
Simple       2    67.3 3.76 51.2
Multi-step       2       66.7 5.48 50.7

Chain-of-Thought performance on the
Qwen2.5VL-72B-instruct model.

Data Size F.F1 B.F1 Ace.
Qwen2-7B Training
Vanilla        66.7      0.0 50.0
1000           64.1 12.2 49.1
3000           54.2 48.2 51.4
5000           594 31.6 49.1
Qwen2.5VL-7B Training
Vanilla        63.0 19.5 49.3
1000           54.8 43.5 49.8
3000           545 43.4 49.5
5000           545 43.4 49.5

Table 6: SFT performance.

Model FPS EFI B.FI1 Acc.
Human         30 «699.0 89.0 89.2
2       62.1 51.2 57.3
4       62.5 574 60.1
GPT-4.1       8       62.9 55.8 59.7

16 624 53.7 58.5

30 «664.4 = =55.9

Table 7: FPS ablation results.

60.6

formance (Table 7). Notably, at 30 FPS, corre-
sponding to the complete video with all frames, the
model achieves only 60.6% accuracy, falling far
short of human performance at 89.2%. This 28.6
percentage point gap demonstrates that the perfor-
mance deficit is not due to insufficient temporal
information. Across tested frame rates, model ac-
curacy varies minimally (57.3-60.6%). Given the
marginal improvement from 4 FPS (60.1%) to 30
FPS and API cost considerations, we selected 4
FPS as the default for all proprietary models.

6 Analyses

6.1 Motion Category Analysis

We analyze per-category performance across the
five motion classes in AoT-PsyPhyBENCH: Pro-
ceed, Fall, Diffusion, Division, and Put. Figure 3
compares humans with three VLMs: Cosmos-
Reason! 7B (best open-weight model in the zero-
shot setting, 52.1%), GPT-4.1 (best proprietary and
overall model in the zero-shot setting, 60.1%), and
Gemini-2.5-Pro (best overall model, 61.1%). Hu-
mans maintain ~80-100% accuracy across all cate-
gories and directions. VLMs show substantial gaps
with striking asymmetry: relatively better on for-
ward videos but dramatically worse on backward
videos, revealing strong label prediction bias.
Category-specific patterns. The largest Fl
deficits appear in Proceed and Fall, where all mod-
els struggle to capture the causal and gravitational
cues defining temporal direction. Diffusion and Put
show moderate F1 scores but remain below human
performance even in forward playback, suggest-
ing incomplete modeling of dispersion and goal-
directed manipulation. Division is comparatively
easier for both humans and models, likely due
to clear temporal discontinuities (e.g., splitting).
Notably, Cosmos-Reason1-7B achieves relatively
strong backward F1, particularly on Fall and Dif-
fusion, consistent with its explicit AoT-oriented
training, whereas Gemini-2.5-Pro and GPT-4.1 per-
form better in forward playback but drop sharply
when the temporal order is reversed. Overall, these
results suggest that current VLMs rely primarily
on directional visual priors and dataset correlations
rather than robust physical causality understanding.

6.2. Label Prediction Bias

We observed that most VLMs exhibit label pre-
diction bias: models strongly favor one label over
the other despite balanced forward/backward video


===== PAGE BREAK =====

Forward Fl

Fall

Backward F1

Fall

Figure 3: Per-category comparison on AoT-PsyPhyBENCH across three representative models. Cosmos-reason1-
7B (zero-shot; best open-weight in this setting), GPT-4.1 (zero-shot; best proprietary and best overall model in
this setting), and Gemini-2.5-Pro (zero-shot, low-reasoning effort; best model across all settings), and humans:
(a) forward F1 (left), (b) backward F1 (middle), and (c) overall accuracy (right). Humans remain consistently
high across all categories and both directions ( 80-100%). In contrast, VLMs show substantial gaps. Backward
detection is the most challenging, revealing a forward-direction bias (with Cosmos-reason1 as a notable exception,

showing comparatively strong backward F1).

distribution in AoT-PsyPhyBENCH. In zero-shot
evaluation (Table 2), GPT-40 predicted 87% of
clips as Forward (only 13% as Backward), while
QVQ-72B-Preview predicted 100% Forward, caus-
ing minority-class Fl scores to drop below 0.4 due
to low recall. Most models exhibit Forward bias
(Qwen2.5-VL series, QVQ-72B-Preview, GPT-40,
03, 04-mini, GPT-5), while Cosmos-Reason1-7B
shows Backward bias, likely due to extensive
reversed-clip exposure during reinforcement learn-
ing.

Furthermore, reasoning amplifies bias: when
models engage in step-by-step reasoning, biases
intensify rather than improve. QVQ-72B-Preview
often correctly identified scene events but failed to
detect reversed motion, with its reasoning process
reinforcing the idea that the video is played forward.
Similarly, increasing reasoning effort (GPT-5 in
Table 4) or enforcing chain-of-thought (Table 5,
Figure 2) further amplified label prediction bias
rather than correcting it.

7 Conclusion

We present AoT-PsyPhyBENCH, a psychophys-
ically validated benchmark designed to assess
whether modern vision—language models (VLMs)
possess a human-like inductive bias for inferring
the arrow of time—the implicit understanding that
physical events unfold irreversibly from past to fu-
ture. Our experiments reveal that current VLMs
perform far below human baselines, even on intu-
itive scenarios such as falling objects, which hu-

mans find easy to interpret. This striking shortfall
cannot be attributed to limited data or lack of rea-
soning depth. Instead, it highlights a fundamental
absence of inductive biases for temporal continuity,
causality, and physical irreversibility—principles
that humans internalize effortlessly through inter-
action with the physical world. We release AoT-
PsyPhyBENCH and its evaluation code to foster
the development of multimodal systems that move
beyond statistical pattern recognition toward gen-
uine physical understanding.

8 Limitations

Our work has two limitations. First, for proprietary
models, even though we report the specific model
names and versions used, exact replication may be
affected by undisclosed updates or API changes
beyond our control. Second, we haven’t been able
to offer a conclusive explanation for why SFT did
not work on AoT. One possible reason is that open-
source VLMs may not effectively encode temporal
information along the frame sequence. Cosmos-
Reason1-7B saw improvement with reinforcement
learning in their work, however, they did not pub-
lish their dataset and we did not see improvement
on our benchmark, either.

References

Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji,
Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay,
Yongxin Chen, Yin Cui, Yifan Ding, and | others.


===== PAGE BREAK =====

2025. Cosmos world foundation model platform for
physical ai. arXiv preprint arXiv:2501.03575.

Alisson Azzolini, Junjie Bai, Hannah Brandon, Jiaxin
Cao, Prithvijit Chattopadhyay, Huayu Chen, Jinju
Chu, Yin Cui, Jenna Diamond, Yifan Ding, and 1
others. 2025. Cosmos-reason1: From physical com-
mon sense to embodied reasoning. arXiv preprint
arXiv:2503. 15558.

Piyush Bagad, Makarand Tapaswi, and Cees GM Snoek.
2023. Test of time: Instilling video-language mod-
els with a sense of time. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition, pages 2503-2516.

Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wen-
bin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie
Wang, Jun Tang, and 1 others. 2025. Qwen2. 5-vl
technical report. arXiv preprint arXiv:2502.13923.

Gheorghe Comanici, Eric Bieber, Mike Schaekermann,
Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Mar-
cel Blistein, Ori Ram, Dan Zhang, Evan Rosen, and
1 others. 2025. Gemini 2.5: Pushing the frontier with
advanced reasoning, multimodality, long context, and
next generation agentic capabilities. arXiv preprint
arXiv:2507.06261.

Yang Du, Yuqi Liu, and Qin Jin. 2024. Reversed in time:
A novel temporal-emphasized benchmark for cross-
modal video-text retrieval. In Proceedings of the
32nd ACM International Conference on Multimedia,
pages 5260-5269.

Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li,
Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu
Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen,
Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu,
Xiawu Zheng, Enhong Chen, Caifeng Shan, and 2
others. 2025. Video-mme: The first-ever comprehen-
sive evaluation benchmark of multi-modal Ilms in
video analysis. Preprint, arXiv:2405.21075.

Nao Hanyu, Kei Watanabe, and Shigeru Kitazawa. 2023.
Ready to detect a reversal of time’s arrow: a psy-
chophysical study using short video clips in daily
scenes. Royal Society open science, 10(4):230036.

Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Lu Wang, and Weizhu Chen.
2021. Lora: Low-rank adaptation of large language
models. arXiv preprint arXiv:2106.09685.

Haowei Liu, Xi Zhang, Haiyang Xu, Yaya Shi, Chaoya
Jiang, Ming Yan, Ji Zhang, Fei Huang, Chunfeng
Yuan, Bing Li, and Weiming Hu. 2024. MIBench:
Evaluating multimodal large language models over
multiple images. In Proceedings of the 2024 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 22417-22428, Miami, Florida, USA.
Association for Computational Linguistics.

Mathew Monfort, Alex Andonian, Bolei Zhou, Kandan
Ramakrishnan, Sarah Adel Bargal, Tom Yan, Lisa
Brown, Quanfu Fan, Dan Gutfreund, Carl Vondrick,

10

and 1 others. 2019. Moments in time dataset: one
million videos for event understanding. [EEE trans-
actions on pattern analysis and machine intelligence,

42(2):502-508.

OpenAI. 2024. Hello gpt-40. https: //openai.com/
index/hello-gpt-40/.

OpenAI. 2025a. Introducing gpt-4.1 in the api.
OpenAI. 2025b. Introducing gpt-5.

OpenAI. 2025c.   Introducing openai o3 and
o04-mini.           https: //openai.com/index/
introducing-03-and-04-mini/.

Qwen Team. 2024. Qvq: Qwen?2-vl with visual question
answering. Technical report, Alibaba Cloud.

Fei Wang, Xingyu Fu, James Y. Huang, Zekun Li, Qin
Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu,
Wenxuan Zhou, Kai Zhang, Tianyi Lorena Yan, Wen-
jie Jacky Mo, Hsiang-Hui Liu, Pan Lu, Chunyuan
Li, Chaowei Xiao, Kai-Wei Chang, Dan Roth, Sheng
Zhang, and 2 others. 2024a. Muirbench: A com-
prehensive benchmark for robust multi-image under-
standing. Preprint, arXiv:2406.09411.

Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhi-
hao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin
Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei
Du, Xuancheng Ren, Rui Men, Dayiheng Liu,
Chang Zhou, Jingren Zhou, and Junyang Lin. 2024b.
Qwenz2-vl: Enhancing vision-language model’s per-
ception of the world at any resolution. arXiv preprint
arXiv:2409. 12191.

Zhenhailong Wang, Ansel Blume, Sha Li, Genglin Liu,
Jaemin Cho, Zineng Tang, Mohit Bansal, and Heng Ji.
2023. Paxion: Patching action knowledge in video-
language foundation models. Advances in Neural
Information Processing Systems, 36:20729-20749.

Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua B Tenen-
baum, and Chuang Gan. 2024. Star: A benchmark
for situated reasoning in real-world videos. Preprint,
arXiv:2405.09711.

Junbin Xiao, Xindi Shang, Angela Yao, and Tat-
Seng Chua. 2021. Next-qa:next phase of question-
answering to explaining temporal actions. Preprint,
arXiv:2105.08276.

Zihui Xue, Mi Luo, and Kristen Grauman. 2025. Seeing
the arrow of time in large multimodal models. arXiv
preprint arXiv:2506.03340.
