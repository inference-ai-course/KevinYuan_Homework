{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de032ea5",
   "metadata": {},
   "source": [
    "# Task 2: Batch OCR for arXiv PDFs\n",
    "\n",
    "**Objective**: Convert PDFs from arXiv (same paper set as Task 1) to text using Tesseract OCR.\n",
    "\n",
    "**Core Tools**: `pytesseract`, `pdf2image`\n",
    "\n",
    "**Deliverables**: \n",
    "- `pdf_ocr/` folder with TXT files (one per paper)\n",
    "- This notebook\n",
    "\n",
    "**Features**:\n",
    "- Downloads PDFs from arXiv based on `arxiv_clean.json`\n",
    "- Converts each PDF page to images using `pdf2image` (requires Poppler)\n",
    "- Runs Tesseract OCR on each page\n",
    "- Preserves layout using page-break markers\n",
    "- Caches downloaded PDFs to avoid re-downloading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657a0649",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "If running for the first time, ensure you have:\n",
    "```bash\n",
    "pip install pytesseract pdf2image pillow requests\n",
    "```\n",
    "\n",
    "**System Requirements**:\n",
    "- **Tesseract OCR**: Download from [GitHub](https://github.com/UB-Mannheim/tesseract/wiki) and install\n",
    "- **Poppler**: This notebook will auto-download Poppler for Windows if not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34acf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import requests\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "\n",
    "print('✓ Imports successful')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c16f4e",
   "metadata": {},
   "source": [
    "## 2. Configure Paths and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916c0d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directories\n",
    "BASE_DIR = Path.cwd()\n",
    "DATA_FILE = BASE_DIR / 'arxiv_clean.json'\n",
    "OUT_DIR = BASE_DIR / 'pdf_ocr'\n",
    "PDF_DIR = OUT_DIR / 'pdfs'\n",
    "\n",
    "# Create output directories\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "PDF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'Output directory: {OUT_DIR}')\n",
    "print(f'PDF cache: {PDF_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c04786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-configure Tesseract (Windows default location)\n",
    "if os.name == 'nt':\n",
    "    default_tesseract = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "    if os.path.exists(default_tesseract):\n",
    "        pytesseract.pytesseract.tesseract_cmd = default_tesseract\n",
    "        print(f'✓ Tesseract found: {default_tesseract}')\n",
    "    else:\n",
    "        print('⚠ Tesseract not found at default location. Please install or set pytesseract.pytesseract.tesseract_cmd')\n",
    "else:\n",
    "    print('✓ Using system Tesseract (Linux/Mac)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de034f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-configure Poppler (Windows)\n",
    "poppler_path = None\n",
    "\n",
    "if os.name == 'nt':\n",
    "    # Check common install locations\n",
    "    candidates = [\n",
    "        BASE_DIR / 'vendor_poppler' / 'poppler-23.08.0' / 'Library' / 'bin',\n",
    "        Path(r\"C:\\ProgramData\\chocolatey\\lib\\poppler\\tools\"),\n",
    "        Path(r\"C:\\Program Files\\poppler\\Library\\bin\"),\n",
    "        Path(r\"C:\\poppler\\Library\\bin\"),\n",
    "    ]\n",
    "    \n",
    "    for candidate in candidates:\n",
    "        if candidate.exists() and (candidate / 'pdftoppm.exe').exists():\n",
    "            poppler_path = str(candidate)\n",
    "            print(f'✓ Poppler found: {poppler_path}')\n",
    "            break\n",
    "    \n",
    "    # If not found, download it\n",
    "    if not poppler_path:\n",
    "        print('⚠ Poppler not found. Downloading...')\n",
    "        import zipfile\n",
    "        \n",
    "        POPPLER_ZIP_URL = 'https://github.com/oschwartz10612/poppler-windows/releases/download/v23.08.0-0/Release-23.08.0-0.zip'\n",
    "        VENDOR_DIR = BASE_DIR / 'vendor_poppler'\n",
    "        VENDOR_DIR.mkdir(exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            tmp_zip = VENDOR_DIR / 'poppler.zip'\n",
    "            print(f'  Downloading to {tmp_zip}...')\n",
    "            \n",
    "            with requests.get(POPPLER_ZIP_URL, stream=True, timeout=120) as r:\n",
    "                r.raise_for_status()\n",
    "                with open(tmp_zip, 'wb') as f:\n",
    "                    for chunk in r.iter_content(8192):\n",
    "                        f.write(chunk)\n",
    "            \n",
    "            print('  Extracting...')\n",
    "            with zipfile.ZipFile(tmp_zip, 'r') as zf:\n",
    "                zf.extractall(VENDOR_DIR)\n",
    "            \n",
    "            # Find pdftoppm.exe\n",
    "            for root, dirs, files in os.walk(VENDOR_DIR):\n",
    "                if 'pdftoppm.exe' in files:\n",
    "                    poppler_path = root\n",
    "                    print(f'✓ Poppler installed: {poppler_path}')\n",
    "                    break\n",
    "            \n",
    "            if not poppler_path:\n",
    "                print('⚠ Could not locate pdftoppm.exe after extraction')\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f'✗ Poppler download failed: {e}')\n",
    "else:\n",
    "    print('✓ Using system Poppler (Linux/Mac)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ed887a",
   "metadata": {},
   "source": [
    "## 3. Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae2b668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OCR configuration for better layout preservation\n",
    "TESS_LANG = 'eng'\n",
    "TESS_CONFIG = '--oem 1 --psm 1 -c preserve_interword_spaces=1'\n",
    "\n",
    "# Limit pages per document (None for all pages)\n",
    "MAX_PAGES_PER_DOC: Optional[int] = None  # Set to e.g., 5 for quick testing\n",
    "\n",
    "# arXiv URL pattern\n",
    "ARXIV_ABS_RE = re.compile(r\"https?://arxiv\\.org/abs/([\\w\\.-]+)\")\n",
    "\n",
    "\n",
    "def derive_id_and_pdf(url: str) -> Tuple[str, str]:\n",
    "    \"\"\"Extract arXiv ID from URL and construct PDF URL.\"\"\"\n",
    "    m = ARXIV_ABS_RE.match(url)\n",
    "    if m:\n",
    "        arxiv_id = m.group(1)\n",
    "    else:\n",
    "        # Assume URL ends with ID\n",
    "        arxiv_id = url.split('/')[-1]\n",
    "    \n",
    "    pdf_url = f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n",
    "    return arxiv_id, pdf_url\n",
    "\n",
    "\n",
    "def safe_filename(name: str) -> str:\n",
    "    \"\"\"Sanitize filename while preserving arXiv ID format.\"\"\"\n",
    "    return re.sub(r\"[^\\w\\.-]+\", \"_\", name).strip('_')\n",
    "\n",
    "\n",
    "def download_pdf(pdf_url: str, dest: Path, retries: int = 3) -> bool:\n",
    "    \"\"\"Download PDF with retry logic.\"\"\"\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            with requests.get(pdf_url, stream=True, timeout=30) as r:\n",
    "                r.raise_for_status()\n",
    "                with open(dest, 'wb') as f:\n",
    "                    for chunk in r.iter_content(chunk_size=8192):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            if i < retries - 1:\n",
    "                time.sleep(2 ** i)  # Exponential backoff\n",
    "            else:\n",
    "                print(f\"  ✗ Download failed: {e}\")\n",
    "    return False\n",
    "\n",
    "\n",
    "def pdf_to_images(pdf_path: Path, dpi: int = 300) -> List[Image.Image]:\n",
    "    \"\"\"Convert PDF pages to images using pdf2image.\"\"\"\n",
    "    kwargs = {'dpi': dpi}\n",
    "    if poppler_path:\n",
    "        kwargs['poppler_path'] = poppler_path\n",
    "    \n",
    "    images = convert_from_path(str(pdf_path), **kwargs)\n",
    "    \n",
    "    if MAX_PAGES_PER_DOC is not None:\n",
    "        images = images[:MAX_PAGES_PER_DOC]\n",
    "    \n",
    "    return images\n",
    "\n",
    "\n",
    "def ocr_image(img: Image.Image) -> str:\n",
    "    \"\"\"Run Tesseract OCR on a single image.\"\"\"\n",
    "    return pytesseract.image_to_string(img, lang=TESS_LANG, config=TESS_CONFIG)\n",
    "\n",
    "\n",
    "def ocr_images_to_text(images: List[Image.Image]) -> str:\n",
    "    \"\"\"OCR all images and combine with page break markers.\"\"\"\n",
    "    texts = []\n",
    "    for idx, img in enumerate(images, 1):\n",
    "        txt = ocr_image(img)\n",
    "        texts.append(f\"\\n{'='*80}\\n PAGE {idx}\\n{'='*80}\\n\\n{txt}\")\n",
    "    return '\\n'.join(texts)\n",
    "\n",
    "\n",
    "print('✓ Helper functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6cd12d",
   "metadata": {},
   "source": [
    "## 4. Batch Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8207adb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_paper_list(data_file: Path) -> List[dict]:\n",
    "    \"\"\"Load paper metadata from arxiv_clean.json.\"\"\"\n",
    "    with open(data_file, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def process_one_paper(item: dict) -> Optional[Path]:\n",
    "    \"\"\"Download, convert, OCR, and save text for one paper.\"\"\"\n",
    "    url = item.get('url') or ''\n",
    "    if not url:\n",
    "        return None\n",
    "    \n",
    "    # Get arXiv ID and PDF URL\n",
    "    arxiv_id, pdf_url = derive_id_and_pdf(url)\n",
    "    base = safe_filename(arxiv_id)\n",
    "    \n",
    "    # Output paths\n",
    "    out_txt = OUT_DIR / f\"{base}.txt\"\n",
    "    \n",
    "    # Skip if already processed\n",
    "    if out_txt.exists():\n",
    "        return out_txt\n",
    "    \n",
    "    # Download PDF if needed\n",
    "    pdf_path = PDF_DIR / f\"{base}.pdf\"\n",
    "    if not pdf_path.exists():\n",
    "        if not download_pdf(pdf_url, pdf_path):\n",
    "            return None\n",
    "    \n",
    "    # Convert PDF to images\n",
    "    try:\n",
    "        images = pdf_to_images(pdf_path)\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ PDF conversion failed: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Run OCR\n",
    "    text = ocr_images_to_text(images)\n",
    "    \n",
    "    # Save text file\n",
    "    out_txt.write_text(text, encoding='utf-8')\n",
    "    \n",
    "    return out_txt\n",
    "\n",
    "\n",
    "def run_batch_ocr(limit: Optional[int] = None):\n",
    "    \"\"\"Process all papers from arxiv_clean.json.\"\"\"\n",
    "    if not DATA_FILE.exists():\n",
    "        raise FileNotFoundError(f\"Missing {DATA_FILE}\")\n",
    "    \n",
    "    items = load_paper_list(DATA_FILE)\n",
    "    if limit is not None:\n",
    "        items = items[:limit]\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Starting batch OCR: {len(items)} papers\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    success, failed = 0, 0\n",
    "    \n",
    "    for i, item in enumerate(items, 1):\n",
    "        title = item.get('title', 'Unknown')[:60]\n",
    "        print(f\"[{i}/{len(items)}] {title}...\")\n",
    "        \n",
    "        try:\n",
    "            result = process_one_paper(item)\n",
    "            if result:\n",
    "                success += 1\n",
    "                print(f\"  ✓ Saved: {result.name}\")\n",
    "            else:\n",
    "                failed += 1\n",
    "                print(\"  ✗ Failed\")\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n⚠ Interrupted by user\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Batch complete: {success} successful, {failed} failed\")\n",
    "    print(f\"Output directory: {OUT_DIR}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "print('✓ Batch processing functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f310df",
   "metadata": {},
   "source": [
    "## 5. Run Batch OCR\n",
    "\n",
    "Execute the cell below to process all papers from `arxiv_clean.json`.\n",
    "\n",
    "**Options**:\n",
    "- Set `LIMIT = None` to process all papers\n",
    "- Set `LIMIT = 5` to test with just 5 papers\n",
    "- Adjust `MAX_PAGES_PER_DOC` in section 3 to limit pages per document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c7d599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run batch OCR\n",
    "LIMIT = None  # Set to a number for testing, or None for all papers\n",
    "\n",
    "run_batch_ocr(limit=LIMIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3145497e",
   "metadata": {},
   "source": [
    "## 6. Verify Results\n",
    "\n",
    "Check the output directory and preview a sample file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9704c6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all TXT files in output directory\n",
    "txt_files = sorted(OUT_DIR.glob('*.txt'))\n",
    "print(f\"Total TXT files generated: {len(txt_files)}\\n\")\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    size_kb = txt_file.stat().st_size / 1024\n",
    "    print(f\"  {txt_file.name:40s} ({size_kb:>8.1f} KB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8663cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview first TXT file (first 2000 characters)\n",
    "if txt_files:\n",
    "    sample_file = txt_files[0]\n",
    "    print(f\"Preview of {sample_file.name}:\")\n",
    "    print(\"=\" * 80)\n",
    "    content = sample_file.read_text(encoding='utf-8')\n",
    "    print(content[:2000])\n",
    "    if len(content) > 2000:\n",
    "        print(f\"\\n... (truncated, total {len(content)} characters)\")\n",
    "else:\n",
    "    print(\"No TXT files found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36231352",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**Task 2 Complete!**\n",
    "\n",
    "✓ Downloaded arXiv PDFs based on `arxiv_clean.json`  \n",
    "✓ Converted each PDF to images using `pdf2image` (Poppler)  \n",
    "✓ Ran Tesseract OCR on all pages  \n",
    "✓ Saved TXT files to `pdf_ocr/` folder with page-break markers for layout preservation  \n",
    "\n",
    "**Output**: `pdf_ocr/` folder containing one `.txt` file per paper\n",
    "\n",
    "**Next Steps**:\n",
    "- Adjust OCR parameters in section 3 if needed (DPI, page limits, Tesseract config)\n",
    "- For better layout preservation, consider using `pytesseract.image_to_pdf_or_hocr()` to generate hOCR files\n",
    "- Process additional papers by updating `arxiv_clean.json`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
