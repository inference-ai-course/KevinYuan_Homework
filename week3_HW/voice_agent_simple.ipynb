{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbafbfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, UploadFile, File\n",
    "from fastapi.responses import FileResponse\n",
    "\n",
    "app = FastAPI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abe1183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "asr_model = whisper.load_model(\"small\")\n",
    "\n",
    "def transcribe_audio(audio_bytes):\n",
    "    with open(\"temp.wav\", \"wb\") as f:\n",
    "        f.write(audio_bytes)\n",
    "    result = asr_model.transcribe(\"temp.wav\")\n",
    "    return result[\"text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9148f01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "conversation_history = []\n",
    "\n",
    "def generate_response(user_text):\n",
    "    # Add user message to history\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_text})\n",
    "    \n",
    "    # Call OpenAI API\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=conversation_history,\n",
    "        max_tokens=150\n",
    "    )\n",
    "    \n",
    "    # Extract LLM response\n",
    "    bot_response = response.choices[0].message.content\n",
    "    \n",
    "    # Add to history\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": bot_response})\n",
    "    \n",
    "    return bot_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c06cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using OpenAI TTS-1 for speech synthesis\n"
     ]
    }
   ],
   "source": [
    "# COSYVOICE TTS (Cosyvoice does not work properly both in this agent and in demo, potentially an issue with Windows, will try WSL in the future.)\n",
    "# import sys\n",
    "# import os\n",
    "# cosyvoice_path = r'C:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\CosyVoice'\n",
    "# sys.path.append(cosyvoice_path)\n",
    "# sys.path.append(os.path.join(cosyvoice_path, 'third_party', 'Matcha-TTS'))\n",
    "# from cosyvoice.cli.cosyvoice import CosyVoice2\n",
    "# from cosyvoice.utils.file_utils import load_wav\n",
    "# import torchaudio\n",
    "# model_path = os.path.join(cosyvoice_path, 'pretrained_models', 'CosyVoice2-0.5B')\n",
    "# cosyvoice = CosyVoice2(model_path, load_jit=False, load_trt=False, load_vllm=False, fp16=False)\n",
    "# reference_audio_path = os.path.join(cosyvoice_path, 'asset', 'ENG_US_M_DAVEL.wav')\n",
    "# # Transcription of the reference audio\n",
    "# prompt_text = \"Being able to communicate positions within a room is critical to our ability to focus light on a certain area and place objects in their proper location on stage. But it goes even deeper than that, this proficiency provides the basic vocabulary in a common language that is spoken by production and staging professionals around the world.\"\n",
    "# prompt_speech_16k = load_wav(reference_audio_path, 16000)\n",
    "# def synthesize_speech(text, output_path=\"response.wav\"):\n",
    "#     # Add text_frontend=False for better English synthesis (per CosyVoice docs)\n",
    "#     for i, result in enumerate(cosyvoice.inference_zero_shot(\n",
    "#         text,              # Text to synthesize\n",
    "#         prompt_text,       # Transcript of reference audio\n",
    "#         prompt_speech_16k, # Reference audio\n",
    "#         stream=False,\n",
    "#         text_frontend=False  # Important for English text\n",
    "#     )):\n",
    "#         torchaudio.save(output_path, result['tts_speech'], cosyvoice.sample_rate)\n",
    "#         break\n",
    "#     return output_path\n",
    "\n",
    "def synthesize_speech(text, output_path=\"response.wav\"):\n",
    "    response = client.audio.speech.create(\n",
    "        model=\"tts-1\",\n",
    "        voice=\"alloy\", \n",
    "        input=text\n",
    "    )\n",
    "    response.stream_to_file(output_path)\n",
    "    return output_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99fda20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/chat/\")\n",
    "async def chat_endpoint(file: UploadFile = File(...)):\n",
    "    audio_bytes = await file.read()\n",
    "    user_text = transcribe_audio(audio_bytes)\n",
    "    bot_text = generate_response(user_text)\n",
    "    audio_path = synthesize_speech(bot_text)\n",
    "    return FileResponse(audio_path, media_type=\"audio/wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a689559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 21:26:06,068 DEBUG Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-11-11 21:26:06,167 DEBUG Using proactor: IocpProactor\n",
      "2025-11-11 21:26:06,284 DEBUG connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 21:26:06,437 DEBUG https://huggingface.co:443 \"HEAD /api/telemetry/https%3A/api.gradio.app/gradio-initiated-analytics HTTP/1.1\" 200 0\n",
      "2025-11-11 21:26:06,443 DEBUG connect_tcp.started host='127.0.0.1' port=7865 local_address=None timeout=None socket_options=None\n",
      "2025-11-11 21:26:06,443 DEBUG connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E70CF90BF0>\n",
      "2025-11-11 21:26:06,444 DEBUG send_request_headers.started request=<Request [b'GET']>\n",
      "2025-11-11 21:26:06,445 DEBUG connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E6CF1A85C0>\n",
      "2025-11-11 21:26:06,445 DEBUG send_request_headers.complete\n",
      "2025-11-11 21:26:06,445 DEBUG start_tls.started ssl_context=<ssl.SSLContext object at 0x000001E6CEFF0DD0> server_hostname='api.gradio.app' timeout=3\n",
      "2025-11-11 21:26:06,446 DEBUG send_request_body.started request=<Request [b'GET']>\n",
      "2025-11-11 21:26:06,447 DEBUG send_request_body.complete\n",
      "2025-11-11 21:26:06,447 DEBUG receive_response_headers.started request=<Request [b'GET']>\n",
      "2025-11-11 21:26:06,447 DEBUG receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Wed, 12 Nov 2025 02:26:06 GMT'), (b'server', b'uvicorn'), (b'content-length', b'4'), (b'content-type', b'application/json')])\n",
      "2025-11-11 21:26:06,448 INFO HTTP Request: GET http://127.0.0.1:7865/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 21:26:06,448 DEBUG receive_response_body.started request=<Request [b'GET']>\n",
      "2025-11-11 21:26:06,449 DEBUG receive_response_body.complete\n",
      "2025-11-11 21:26:06,449 DEBUG response_closed.started\n",
      "2025-11-11 21:26:06,449 DEBUG response_closed.complete\n",
      "2025-11-11 21:26:06,450 DEBUG close.started\n",
      "2025-11-11 21:26:06,450 DEBUG close.complete\n",
      "2025-11-11 21:26:06,451 DEBUG connect_tcp.started host='127.0.0.1' port=7865 local_address=None timeout=3 socket_options=None\n",
      "2025-11-11 21:26:06,452 DEBUG connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E6CF282150>\n",
      "2025-11-11 21:26:06,452 DEBUG send_request_headers.started request=<Request [b'HEAD']>\n",
      "2025-11-11 21:26:06,453 DEBUG send_request_headers.complete\n",
      "2025-11-11 21:26:06,454 DEBUG send_request_body.started request=<Request [b'HEAD']>\n",
      "2025-11-11 21:26:06,454 DEBUG send_request_body.complete\n",
      "2025-11-11 21:26:06,454 DEBUG receive_response_headers.started request=<Request [b'HEAD']>\n",
      "2025-11-11 21:26:06,456 DEBUG receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Wed, 12 Nov 2025 02:26:06 GMT'), (b'server', b'uvicorn'), (b'content-length', b'26503'), (b'content-type', b'text/html; charset=utf-8')])\n",
      "2025-11-11 21:26:06,458 INFO HTTP Request: HEAD http://127.0.0.1:7865/ \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 21:26:06,458 DEBUG receive_response_body.started request=<Request [b'HEAD']>\n",
      "2025-11-11 21:26:06,458 DEBUG receive_response_body.complete\n",
      "2025-11-11 21:26:06,458 DEBUG response_closed.started\n",
      "2025-11-11 21:26:06,459 DEBUG response_closed.complete\n",
      "2025-11-11 21:26:06,459 DEBUG close.started\n",
      "2025-11-11 21:26:06,459 DEBUG close.complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 21:26:06,463 DEBUG Starting new HTTPS connection (1): huggingface.co:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 21:26:06,566 DEBUG start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E70CFD8590>\n",
      "2025-11-11 21:26:06,567 DEBUG send_request_headers.started request=<Request [b'GET']>\n",
      "2025-11-11 21:26:06,567 DEBUG send_request_headers.complete\n",
      "2025-11-11 21:26:06,569 DEBUG send_request_body.started request=<Request [b'GET']>\n",
      "2025-11-11 21:26:06,569 DEBUG send_request_body.complete\n",
      "2025-11-11 21:26:06,569 DEBUG receive_response_headers.started request=<Request [b'GET']>\n",
      "2025-11-11 21:26:06,628 DEBUG receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 12 Nov 2025 02:26:07 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])\n",
      "2025-11-11 21:26:06,629 INFO HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 21:26:06,629 DEBUG receive_response_body.started request=<Request [b'GET']>\n",
      "2025-11-11 21:26:06,630 DEBUG receive_response_body.complete\n",
      "2025-11-11 21:26:06,630 DEBUG response_closed.started\n",
      "2025-11-11 21:26:06,630 DEBUG response_closed.complete\n",
      "2025-11-11 21:26:06,631 DEBUG close.started\n",
      "2025-11-11 21:26:06,631 DEBUG close.complete\n",
      "2025-11-11 21:26:06,782 DEBUG https://huggingface.co:443 \"HEAD /api/telemetry/https%3A/api.gradio.app/gradio-launched-telemetry HTTP/1.1\" 200 0\n",
      "2025-11-11 21:26:08,565 DEBUG close.started\n",
      "2025-11-11 21:26:08,566 DEBUG close.complete\n",
      "2025-11-11 21:26:08,567 DEBUG close.started\n",
      "2025-11-11 21:26:08,567 DEBUG close.complete\n",
      "2025-11-11 21:26:19,110 DEBUG Calling on_part_begin with no data\n",
      "2025-11-11 21:26:19,111 DEBUG Calling on_header_field with data[42:61]\n",
      "2025-11-11 21:26:19,111 DEBUG Calling on_header_value with data[63:108]\n",
      "2025-11-11 21:26:19,112 DEBUG Calling on_header_end with no data\n",
      "2025-11-11 21:26:19,112 DEBUG Calling on_header_field with data[110:122]\n",
      "2025-11-11 21:26:19,113 DEBUG Calling on_header_value with data[124:148]\n",
      "2025-11-11 21:26:19,113 DEBUG Calling on_header_end with no data\n",
      "2025-11-11 21:26:19,113 DEBUG Calling on_headers_finished with no data\n",
      "2025-11-11 21:26:19,114 DEBUG Calling on_part_data with data[152:98304]\n",
      "2025-11-11 21:26:19,115 DEBUG Calling on_part_data with data[0:131072]\n",
      "2025-11-11 21:26:19,116 DEBUG Calling on_part_data with data[0:98922]\n",
      "2025-11-11 21:26:19,116 DEBUG Calling on_part_end with no data\n",
      "2025-11-11 21:26:19,117 DEBUG Calling on_end with no data\n",
      "2025-11-11 21:26:19,130 DEBUG Calling on_part_begin with no data\n",
      "2025-11-11 21:26:19,130 DEBUG Calling on_header_field with data[42:61]\n",
      "2025-11-11 21:26:19,131 DEBUG Calling on_header_value with data[63:108]\n",
      "2025-11-11 21:26:19,131 DEBUG Calling on_header_end with no data\n",
      "2025-11-11 21:26:19,132 DEBUG Calling on_header_field with data[110:122]\n",
      "2025-11-11 21:26:19,132 DEBUG Calling on_header_value with data[124:148]\n",
      "2025-11-11 21:26:19,132 DEBUG Calling on_header_end with no data\n",
      "2025-11-11 21:26:19,133 DEBUG Calling on_headers_finished with no data\n",
      "2025-11-11 21:26:19,134 DEBUG Calling on_part_data with data[152:130400]\n",
      "2025-11-11 21:26:19,136 DEBUG Calling on_part_data with data[0:131072]\n",
      "2025-11-11 21:26:19,138 DEBUG Calling on_part_data with data[0:66826]\n",
      "2025-11-11 21:26:19,138 DEBUG Calling on_part_end with no data\n",
      "2025-11-11 21:26:19,138 DEBUG Calling on_end with no data\n",
      "2025-11-11 21:26:28,082 DEBUG Calling on_part_begin with no data\n",
      "2025-11-11 21:26:28,082 DEBUG Calling on_header_field with data[42:61]\n",
      "2025-11-11 21:26:28,083 DEBUG Calling on_header_value with data[63:108]\n",
      "2025-11-11 21:26:28,083 DEBUG Calling on_header_end with no data\n",
      "2025-11-11 21:26:28,084 DEBUG Calling on_header_field with data[110:122]\n",
      "2025-11-11 21:26:28,084 DEBUG Calling on_header_value with data[124:148]\n",
      "2025-11-11 21:26:28,084 DEBUG Calling on_header_end with no data\n",
      "2025-11-11 21:26:28,085 DEBUG Calling on_headers_finished with no data\n",
      "2025-11-11 21:26:28,086 DEBUG Calling on_part_data with data[152:114688]\n",
      "2025-11-11 21:26:28,087 DEBUG Calling on_part_data with data[0:131072]\n",
      "2025-11-11 21:26:28,088 DEBUG Calling on_part_data with data[0:131072]\n",
      "2025-11-11 21:26:28,089 DEBUG Calling on_part_data with data[0:131072]\n",
      "2025-11-11 21:26:28,089 DEBUG Calling on_part_data with data[0:74412]\n",
      "2025-11-11 21:26:28,089 DEBUG Calling on_part_end with no data\n",
      "2025-11-11 21:26:28,089 DEBUG Calling on_end with no data\n",
      "2025-11-11 21:26:28,103 DEBUG Calling on_part_begin with no data\n",
      "2025-11-11 21:26:28,103 DEBUG Calling on_header_field with data[42:61]\n",
      "2025-11-11 21:26:28,104 DEBUG Calling on_header_value with data[63:108]\n",
      "2025-11-11 21:26:28,105 DEBUG Calling on_header_end with no data\n",
      "2025-11-11 21:26:28,105 DEBUG Calling on_header_field with data[110:122]\n",
      "2025-11-11 21:26:28,106 DEBUG Calling on_header_value with data[124:148]\n",
      "2025-11-11 21:26:28,106 DEBUG Calling on_header_end with no data\n",
      "2025-11-11 21:26:28,107 DEBUG Calling on_headers_finished with no data\n",
      "2025-11-11 21:26:28,108 DEBUG Calling on_part_data with data[152:130400]\n",
      "2025-11-11 21:26:28,111 DEBUG Calling on_part_data with data[0:131072]\n",
      "2025-11-11 21:26:28,113 DEBUG Calling on_part_data with data[0:131072]\n",
      "2025-11-11 21:26:28,114 DEBUG Calling on_part_data with data[0:131072]\n",
      "2025-11-11 21:26:28,115 DEBUG Calling on_part_data with data[0:58700]\n",
      "2025-11-11 21:26:28,116 DEBUG Calling on_part_end with no data\n",
      "2025-11-11 21:26:28,116 DEBUG Calling on_end with no data\n",
      "2025-11-11 21:26:30,141 DEBUG Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-304c2778-5c12-485f-b9ed-4a3d090ada0f', 'json_data': {'messages': [{'role': 'user', 'content': ' Can you please tell me the NBA MVP candidates for the 2021-2022 season?'}, {'role': 'assistant', 'content': 'For the 2021-2022 NBA season, several players stood out as MVP candidates based on their performance throughout the season. The top candidates included:\\n\\n1. **Nikola Jokić** (Denver Nuggets) - The reigning MVP had another stellar season, showcasing his all-around skills, scoring ability, and playmaking.\\n\\n2. **Giannis Antetokounmpo** (Milwaukee Bucks) - Known for his dominant presence on both ends of the floor, Giannis continued to be a force, leading his team and contributing in multiple facets of the game.\\n\\n3. **Joel Embiid** (Philadelphia 76ers) - Embiid had a tremendous season, posting impressive scoring and rebounding numbers while also being a key defensive'}, {'role': 'user', 'content': ' Can you please tell me the MBA MVP candidates for the 2021-2022 season?'}], 'model': 'gpt-4o-mini', 'max_tokens': 150}}\n",
      "2025-11-11 21:26:30,141 DEBUG Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "2025-11-11 21:26:30,142 DEBUG close.started\n",
      "2025-11-11 21:26:30,143 DEBUG close.complete\n",
      "2025-11-11 21:26:30,143 DEBUG connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "2025-11-11 21:26:30,177 DEBUG connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E6CF19E450>\n",
      "2025-11-11 21:26:30,178 DEBUG start_tls.started ssl_context=<ssl.SSLContext object at 0x000001E7CD0D34D0> server_hostname='api.openai.com' timeout=5.0\n",
      "2025-11-11 21:26:30,186 DEBUG start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E6CF228DA0>\n",
      "2025-11-11 21:26:30,187 DEBUG send_request_headers.started request=<Request [b'POST']>\n",
      "2025-11-11 21:26:30,188 DEBUG send_request_headers.complete\n",
      "2025-11-11 21:26:30,188 DEBUG send_request_body.started request=<Request [b'POST']>\n",
      "2025-11-11 21:26:30,188 DEBUG send_request_body.complete\n",
      "2025-11-11 21:26:30,189 DEBUG receive_response_headers.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User said:  Can you please tell me the MBA MVP candidates for the 2021-2022 season?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 21:26:32,430 DEBUG receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 12 Nov 2025 02:26:33 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-exboj2k3mtfkhfyzzdpjqomu'), (b'openai-processing-ms', b'1864'), (b'openai-project', b'proj_01dqWjCBFviU338E5P9LRXGd'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'2135'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'199792'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'62ms'), (b'x-request-id', b'req_0494db97fb31453aaf8c264e6d67e2ea'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'99d295c30e5536fe-YYZ'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "2025-11-11 21:26:32,431 INFO HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 21:26:32,431 DEBUG receive_response_body.started request=<Request [b'POST']>\n",
      "2025-11-11 21:26:32,434 DEBUG receive_response_body.complete\n",
      "2025-11-11 21:26:32,434 DEBUG response_closed.started\n",
      "2025-11-11 21:26:32,435 DEBUG response_closed.complete\n",
      "2025-11-11 21:26:32,435 DEBUG HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Wed, 12 Nov 2025 02:26:33 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'user-exboj2k3mtfkhfyzzdpjqomu', 'openai-processing-ms': '1864', 'openai-project': 'proj_01dqWjCBFviU338E5P9LRXGd', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '2135', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '199792', 'x-ratelimit-reset-requests': '8.64s', 'x-ratelimit-reset-tokens': '62ms', 'x-request-id': 'req_0494db97fb31453aaf8c264e6d67e2ea', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '99d295c30e5536fe-YYZ', 'content-encoding': 'br', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "2025-11-11 21:26:32,435 DEBUG request_id: req_0494db97fb31453aaf8c264e6d67e2ea\n",
      "2025-11-11 21:26:32,436 DEBUG Request options: {'method': 'post', 'url': '/audio/speech', 'headers': {'Accept': 'application/octet-stream'}, 'files': None, 'idempotency_key': 'stainless-python-retry-c6376ca0-5e91-4b51-8a5c-511ce0810d05', 'json_data': {'input': \"Sure! For the 2021-2022 NBA season, several players were prominent candidates for the MVP award based on their outstanding performances. Here are some of the top candidates:\\n\\n1. **Nikola Jokić** (Denver Nuggets) - The reigning MVP had an exceptional season, providing scoring, rebounding, and playmaking, and he was the focal point of the Nuggets' offense.\\n\\n2. **Giannis Antetokounmpo** (Milwaukee Bucks) - Giannis continued to dominate on both ends of the court, leading the Bucks as they aimed to defend their championship title.\\n\\n3. **Joel Embiid** (Philadelphia 76ers) - Embiid had a remarkable season, showcasing his scoring, rebounding,\", 'model': 'tts-1', 'voice': 'alloy'}}\n",
      "2025-11-11 21:26:32,438 DEBUG Sending HTTP Request: POST https://api.openai.com/v1/audio/speech\n",
      "2025-11-11 21:26:32,438 DEBUG send_request_headers.started request=<Request [b'POST']>\n",
      "2025-11-11 21:26:32,439 DEBUG send_request_headers.complete\n",
      "2025-11-11 21:26:32,439 DEBUG send_request_body.started request=<Request [b'POST']>\n",
      "2025-11-11 21:26:32,440 DEBUG send_request_body.complete\n",
      "2025-11-11 21:26:32,440 DEBUG receive_response_headers.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot responds: Sure! For the 2021-2022 NBA season, several players were prominent candidates for the MVP award based on their outstanding performances. Here are some of the top candidates:\n",
      "\n",
      "1. **Nikola Jokić** (Denver Nuggets) - The reigning MVP had an exceptional season, providing scoring, rebounding, and playmaking, and he was the focal point of the Nuggets' offense.\n",
      "\n",
      "2. **Giannis Antetokounmpo** (Milwaukee Bucks) - Giannis continued to dominate on both ends of the court, leading the Bucks as they aimed to defend their championship title.\n",
      "\n",
      "3. **Joel Embiid** (Philadelphia 76ers) - Embiid had a remarkable season, showcasing his scoring, rebounding,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 21:26:35,135 DEBUG receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 12 Nov 2025 02:26:36 GMT'), (b'Content-Type', b'audio/mpeg'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-exboj2k3mtfkhfyzzdpjqomu'), (b'openai-processing-ms', b'983'), (b'openai-project', b'proj_01dqWjCBFviU338E5P9LRXGd'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-648b9546d7-fq7v4'), (b'x-envoy-upstream-service-time', b'1576'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-request-id', b'req_e86573d62e4540c683239d6b6a0ab4fe'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'99d295d11a6836fe-YYZ'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "2025-11-11 21:26:35,136 INFO HTTP Request: POST https://api.openai.com/v1/audio/speech \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 21:26:35,136 DEBUG receive_response_body.started request=<Request [b'POST']>\n",
      "2025-11-11 21:26:42,797 DEBUG receive_response_body.complete\n",
      "2025-11-11 21:26:42,797 DEBUG response_closed.started\n",
      "2025-11-11 21:26:42,798 DEBUG response_closed.complete\n",
      "2025-11-11 21:26:42,798 DEBUG HTTP Response: POST https://api.openai.com/v1/audio/speech \"200 OK\" Headers({'date': 'Wed, 12 Nov 2025 02:26:36 GMT', 'content-type': 'audio/mpeg', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'user-exboj2k3mtfkhfyzzdpjqomu', 'openai-processing-ms': '983', 'openai-project': 'proj_01dqWjCBFviU338E5P9LRXGd', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'via': 'envoy-router-648b9546d7-fq7v4', 'x-envoy-upstream-service-time': '1576', 'x-ratelimit-limit-requests': '500', 'x-ratelimit-remaining-requests': '499', 'x-ratelimit-reset-requests': '120ms', 'x-request-id': 'req_e86573d62e4540c683239d6b6a0ab4fe', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '99d295d11a6836fe-YYZ', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "2025-11-11 21:26:42,799 DEBUG request_id: req_e86573d62e4540c683239d6b6a0ab4fe\n",
      "C:\\Users\\Kevin\\AppData\\Local\\Temp\\ipykernel_27812\\1020545929.py:41: DeprecationWarning: Due to a bug, this method doesn't actually stream the response content, `.with_streaming_response.method()` should be used instead\n",
      "  response.stream_to_file(output_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset file at: .gradio\\flagged\\dataset2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 21:28:12,965 DEBUG Calling on_part_begin with no data\n",
      "2025-11-11 21:28:12,966 DEBUG Calling on_header_field with data[42:61]\n",
      "2025-11-11 21:28:12,966 DEBUG Calling on_header_value with data[63:108]\n",
      "2025-11-11 21:28:12,967 DEBUG Calling on_header_end with no data\n",
      "2025-11-11 21:28:12,967 DEBUG Calling on_header_field with data[110:122]\n",
      "2025-11-11 21:28:12,967 DEBUG Calling on_header_value with data[124:148]\n",
      "2025-11-11 21:28:12,968 DEBUG Calling on_header_end with no data\n",
      "2025-11-11 21:28:12,968 DEBUG Calling on_headers_finished with no data\n",
      "2025-11-11 21:28:12,969 DEBUG Calling on_part_data with data[152:81920]\n",
      "2025-11-11 21:28:12,970 DEBUG Calling on_part_data with data[0:131072]\n",
      "2025-11-11 21:28:12,971 DEBUG Calling on_part_data with data[0:9466]\n",
      "2025-11-11 21:28:12,971 DEBUG Calling on_part_end with no data\n",
      "2025-11-11 21:28:12,971 DEBUG Calling on_end with no data\n",
      "2025-11-11 21:28:12,986 DEBUG Calling on_part_begin with no data\n",
      "2025-11-11 21:28:12,987 DEBUG Calling on_header_field with data[42:61]\n",
      "2025-11-11 21:28:12,987 DEBUG Calling on_header_value with data[63:108]\n",
      "2025-11-11 21:28:12,988 DEBUG Calling on_header_end with no data\n",
      "2025-11-11 21:28:12,988 DEBUG Calling on_header_field with data[110:122]\n",
      "2025-11-11 21:28:12,989 DEBUG Calling on_header_value with data[124:148]\n",
      "2025-11-11 21:28:12,989 DEBUG Calling on_header_end with no data\n",
      "2025-11-11 21:28:12,989 DEBUG Calling on_headers_finished with no data\n",
      "2025-11-11 21:28:12,990 DEBUG Calling on_part_data with data[152:130400]\n",
      "2025-11-11 21:28:12,993 DEBUG Calling on_part_data with data[0:92058]\n",
      "2025-11-11 21:28:12,993 DEBUG Calling on_part_end with no data\n",
      "2025-11-11 21:28:12,993 DEBUG Calling on_end with no data\n",
      "2025-11-11 21:28:21,285 DEBUG Calling on_part_begin with no data\n",
      "2025-11-11 21:28:21,286 DEBUG Calling on_header_field with data[42:61]\n",
      "2025-11-11 21:28:21,286 DEBUG Calling on_header_value with data[63:108]\n",
      "2025-11-11 21:28:21,287 DEBUG Calling on_header_end with no data\n",
      "2025-11-11 21:28:21,287 DEBUG Calling on_header_field with data[110:122]\n",
      "2025-11-11 21:28:21,288 DEBUG Calling on_header_value with data[124:148]\n",
      "2025-11-11 21:28:21,288 DEBUG Calling on_header_end with no data\n",
      "2025-11-11 21:28:21,289 DEBUG Calling on_headers_finished with no data\n",
      "2025-11-11 21:28:21,289 DEBUG Calling on_part_data with data[152:81920]\n",
      "2025-11-11 21:28:21,291 DEBUG Calling on_part_data with data[0:131072]\n",
      "2025-11-11 21:28:21,292 DEBUG Calling on_part_data with data[0:131072]\n",
      "2025-11-11 21:28:21,292 DEBUG Calling on_part_data with data[0:131072]\n",
      "2025-11-11 21:28:21,293 DEBUG Calling on_part_data with data[0:86012]\n",
      "2025-11-11 21:28:21,294 DEBUG Calling on_part_end with no data\n",
      "2025-11-11 21:28:21,294 DEBUG Calling on_end with no data\n",
      "2025-11-11 21:28:21,305 DEBUG Calling on_part_begin with no data\n",
      "2025-11-11 21:28:21,305 DEBUG Calling on_header_field with data[42:61]\n",
      "2025-11-11 21:28:21,306 DEBUG Calling on_header_value with data[63:108]\n",
      "2025-11-11 21:28:21,306 DEBUG Calling on_header_end with no data\n",
      "2025-11-11 21:28:21,307 DEBUG Calling on_header_field with data[110:122]\n",
      "2025-11-11 21:28:21,307 DEBUG Calling on_header_value with data[124:148]\n",
      "2025-11-11 21:28:21,307 DEBUG Calling on_header_end with no data\n",
      "2025-11-11 21:28:21,308 DEBUG Calling on_headers_finished with no data\n",
      "2025-11-11 21:28:21,309 DEBUG Calling on_part_data with data[152:131072]\n",
      "2025-11-11 21:28:21,310 DEBUG Calling on_part_data with data[0:131071]\n",
      "2025-11-11 21:28:21,312 DEBUG Calling on_part_data with data[0:1]\n",
      "2025-11-11 21:28:21,313 DEBUG Calling on_part_data with data[0:131072]\n",
      "2025-11-11 21:28:21,316 DEBUG Calling on_part_data with data[0:131072]\n",
      "2025-11-11 21:28:21,317 DEBUG Calling on_part_data with data[0:36860]\n",
      "2025-11-11 21:28:21,317 DEBUG Calling on_part_end with no data\n",
      "2025-11-11 21:28:21,317 DEBUG Calling on_end with no data\n",
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\uvicorn\\protocols\\http\\httptools_impl.py\", line 409, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\fastapi\\applications.py\", line 1134, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\starlette\\applications.py\", line 113, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\gradio\\brotli_middleware.py\", line 74, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\gradio\\route_utils.py\", line 882, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 63, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\fastapi\\middleware\\asyncexitstack.py\", line 18, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\starlette\\routing.py\", line 716, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\starlette\\routing.py\", line 736, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\starlette\\routing.py\", line 290, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\fastapi\\routing.py\", line 125, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\fastapi\\routing.py\", line 112, in app\n",
      "    await response(scope, receive, send)\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\starlette\\responses.py\", line 365, in __call__\n",
      "    await self._handle_simple(send, send_header_only, send_pathsend)\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\starlette\\responses.py\", line 396, in _handle_simple\n",
      "    await send({\"type\": \"http.response.body\", \"body\": chunk, \"more_body\": more_body})\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 39, in sender\n",
      "    await send(message)\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 39, in sender\n",
      "    await send(message)\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 161, in _send\n",
      "    await send(message)\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\uvicorn\\protocols\\http\\httptools_impl.py\", line 544, in send\n",
      "    raise RuntimeError(\"Response content shorter than Content-Length\")\n",
      "RuntimeError: Response content shorter than Content-Length\n",
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\uvicorn\\protocols\\http\\httptools_impl.py\", line 409, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\fastapi\\applications.py\", line 1134, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\starlette\\applications.py\", line 113, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\gradio\\brotli_middleware.py\", line 74, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\gradio\\route_utils.py\", line 882, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 63, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\fastapi\\middleware\\asyncexitstack.py\", line 18, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\starlette\\routing.py\", line 716, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\starlette\\routing.py\", line 736, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\starlette\\routing.py\", line 290, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\fastapi\\routing.py\", line 125, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\fastapi\\routing.py\", line 112, in app\n",
      "    await response(scope, receive, send)\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\starlette\\responses.py\", line 365, in __call__\n",
      "    await self._handle_simple(send, send_header_only, send_pathsend)\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\starlette\\responses.py\", line 396, in _handle_simple\n",
      "    await send({\"type\": \"http.response.body\", \"body\": chunk, \"more_body\": more_body})\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 39, in sender\n",
      "    await send(message)\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 39, in sender\n",
      "    await send(message)\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 161, in _send\n",
      "    await send(message)\n",
      "  File \"c:\\Users\\Kevin\\Desktop\\ai_class\\KevinYuan_Homework\\week3_HW\\venv_py312_gpu\\Lib\\site-packages\\uvicorn\\protocols\\http\\httptools_impl.py\", line 544, in send\n",
      "    raise RuntimeError(\"Response content shorter than Content-Length\")\n",
      "RuntimeError: Response content shorter than Content-Length\n",
      "2025-11-11 21:28:22,883 DEBUG Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-976c7207-3d1b-433f-abf5-324d2af5d97d', 'json_data': {'messages': [{'role': 'user', 'content': ' Can you please tell me the NBA MVP candidates for the 2021-2022 season?'}, {'role': 'assistant', 'content': 'For the 2021-2022 NBA season, several players stood out as MVP candidates based on their performance throughout the season. The top candidates included:\\n\\n1. **Nikola Jokić** (Denver Nuggets) - The reigning MVP had another stellar season, showcasing his all-around skills, scoring ability, and playmaking.\\n\\n2. **Giannis Antetokounmpo** (Milwaukee Bucks) - Known for his dominant presence on both ends of the floor, Giannis continued to be a force, leading his team and contributing in multiple facets of the game.\\n\\n3. **Joel Embiid** (Philadelphia 76ers) - Embiid had a tremendous season, posting impressive scoring and rebounding numbers while also being a key defensive'}, {'role': 'user', 'content': ' Can you please tell me the MBA MVP candidates for the 2021-2022 season?'}, {'role': 'assistant', 'content': \"Sure! For the 2021-2022 NBA season, several players were prominent candidates for the MVP award based on their outstanding performances. Here are some of the top candidates:\\n\\n1. **Nikola Jokić** (Denver Nuggets) - The reigning MVP had an exceptional season, providing scoring, rebounding, and playmaking, and he was the focal point of the Nuggets' offense.\\n\\n2. **Giannis Antetokounmpo** (Milwaukee Bucks) - Giannis continued to dominate on both ends of the court, leading the Bucks as they aimed to defend their championship title.\\n\\n3. **Joel Embiid** (Philadelphia 76ers) - Embiid had a remarkable season, showcasing his scoring, rebounding,\"}, {'role': 'user', 'content': ' Then who are the fourth and fifth runner-ups for the MVP candidates?'}], 'model': 'gpt-4o-mini', 'max_tokens': 150}}\n",
      "2025-11-11 21:28:22,884 DEBUG Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "2025-11-11 21:28:22,884 DEBUG close.started\n",
      "2025-11-11 21:28:22,885 DEBUG close.complete\n",
      "2025-11-11 21:28:22,885 DEBUG connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "2025-11-11 21:28:22,913 DEBUG connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E6C4015880>\n",
      "2025-11-11 21:28:22,913 DEBUG start_tls.started ssl_context=<ssl.SSLContext object at 0x000001E7CD0D34D0> server_hostname='api.openai.com' timeout=5.0\n",
      "2025-11-11 21:28:22,922 DEBUG start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E6CF005DC0>\n",
      "2025-11-11 21:28:22,922 DEBUG send_request_headers.started request=<Request [b'POST']>\n",
      "2025-11-11 21:28:22,923 DEBUG send_request_headers.complete\n",
      "2025-11-11 21:28:22,923 DEBUG send_request_body.started request=<Request [b'POST']>\n",
      "2025-11-11 21:28:22,924 DEBUG send_request_body.complete\n",
      "2025-11-11 21:28:22,924 DEBUG receive_response_headers.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User said:  Then who are the fourth and fifth runner-ups for the MVP candidates?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 21:28:26,527 DEBUG receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 12 Nov 2025 02:28:27 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-exboj2k3mtfkhfyzzdpjqomu'), (b'openai-processing-ms', b'2676'), (b'openai-project', b'proj_01dqWjCBFviU338E5P9LRXGd'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'3010'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'199612'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'116ms'), (b'x-request-id', b'req_a401767621e34cb2b86b800c6c142efa'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'99d29883ae9778a5-YYZ'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "2025-11-11 21:28:26,528 INFO HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 21:28:26,528 DEBUG receive_response_body.started request=<Request [b'POST']>\n",
      "2025-11-11 21:28:26,532 DEBUG receive_response_body.complete\n",
      "2025-11-11 21:28:26,533 DEBUG response_closed.started\n",
      "2025-11-11 21:28:26,533 DEBUG response_closed.complete\n",
      "2025-11-11 21:28:26,534 DEBUG HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Wed, 12 Nov 2025 02:28:27 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'user-exboj2k3mtfkhfyzzdpjqomu', 'openai-processing-ms': '2676', 'openai-project': 'proj_01dqWjCBFviU338E5P9LRXGd', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '3010', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '199612', 'x-ratelimit-reset-requests': '8.64s', 'x-ratelimit-reset-tokens': '116ms', 'x-request-id': 'req_a401767621e34cb2b86b800c6c142efa', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '99d29883ae9778a5-YYZ', 'content-encoding': 'br', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "2025-11-11 21:28:26,534 DEBUG request_id: req_a401767621e34cb2b86b800c6c142efa\n",
      "2025-11-11 21:28:26,535 DEBUG Request options: {'method': 'post', 'url': '/audio/speech', 'headers': {'Accept': 'application/octet-stream'}, 'files': None, 'idempotency_key': 'stainless-python-retry-95c1a4c1-331f-4fe5-9aaa-caa18f4707ea', 'json_data': {'input': 'For the 2021-2022 NBA season, in addition to Nikola Jokić, Giannis Antetokounmpo, and Joel Embiid as the top three MVP candidates, the fourth and fifth candidates often mentioned in MVP discussions were:\\n\\n4. **Luka Dončić** (Dallas Mavericks) - Dončić had another impressive season, showcasing his scoring, playmaking, and ability to lead the Mavericks, significantly contributing to their success.\\n\\n5. **Devin Booker** (Phoenix Suns) - As a key player for the Suns, who had one of the best records in the league, Booker demonstrated his scoring ability and playmaking skills and was crucial in helping Phoenix secure a top playoff seed.\\n\\nThese players were recognized for', 'model': 'tts-1', 'voice': 'alloy'}}\n",
      "2025-11-11 21:28:26,536 DEBUG Sending HTTP Request: POST https://api.openai.com/v1/audio/speech\n",
      "2025-11-11 21:28:26,537 DEBUG send_request_headers.started request=<Request [b'POST']>\n",
      "2025-11-11 21:28:26,538 DEBUG send_request_headers.complete\n",
      "2025-11-11 21:28:26,538 DEBUG send_request_body.started request=<Request [b'POST']>\n",
      "2025-11-11 21:28:26,539 DEBUG send_request_body.complete\n",
      "2025-11-11 21:28:26,539 DEBUG receive_response_headers.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot responds: For the 2021-2022 NBA season, in addition to Nikola Jokić, Giannis Antetokounmpo, and Joel Embiid as the top three MVP candidates, the fourth and fifth candidates often mentioned in MVP discussions were:\n",
      "\n",
      "4. **Luka Dončić** (Dallas Mavericks) - Dončić had another impressive season, showcasing his scoring, playmaking, and ability to lead the Mavericks, significantly contributing to their success.\n",
      "\n",
      "5. **Devin Booker** (Phoenix Suns) - As a key player for the Suns, who had one of the best records in the league, Booker demonstrated his scoring ability and playmaking skills and was crucial in helping Phoenix secure a top playoff seed.\n",
      "\n",
      "These players were recognized for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 21:28:28,408 DEBUG receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 12 Nov 2025 02:28:29 GMT'), (b'Content-Type', b'audio/mpeg'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-exboj2k3mtfkhfyzzdpjqomu'), (b'openai-processing-ms', b'854'), (b'openai-project', b'proj_01dqWjCBFviU338E5P9LRXGd'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-756db46b57-skd92'), (b'x-envoy-upstream-service-time', b'1275'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-request-id', b'req_3e2639ac19284679922eb4460d741c55'), (b'x-openai-proxy-wasm', b'v0.1'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'99d2989a3cbd78a5-YYZ'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "2025-11-11 21:28:28,408 INFO HTTP Request: POST https://api.openai.com/v1/audio/speech \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 21:28:28,409 DEBUG receive_response_body.started request=<Request [b'POST']>\n",
      "2025-11-11 21:28:34,582 DEBUG receive_response_body.complete\n",
      "2025-11-11 21:28:34,583 DEBUG response_closed.started\n",
      "2025-11-11 21:28:34,583 DEBUG response_closed.complete\n",
      "2025-11-11 21:28:34,584 DEBUG HTTP Response: POST https://api.openai.com/v1/audio/speech \"200 OK\" Headers({'date': 'Wed, 12 Nov 2025 02:28:29 GMT', 'content-type': 'audio/mpeg', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'user-exboj2k3mtfkhfyzzdpjqomu', 'openai-processing-ms': '854', 'openai-project': 'proj_01dqWjCBFviU338E5P9LRXGd', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'via': 'envoy-router-756db46b57-skd92', 'x-envoy-upstream-service-time': '1275', 'x-ratelimit-limit-requests': '500', 'x-ratelimit-remaining-requests': '499', 'x-ratelimit-reset-requests': '120ms', 'x-request-id': 'req_3e2639ac19284679922eb4460d741c55', 'x-openai-proxy-wasm': 'v0.1', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '99d2989a3cbd78a5-YYZ', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "2025-11-11 21:28:34,585 DEBUG request_id: req_3e2639ac19284679922eb4460d741c55\n",
      "C:\\Users\\Kevin\\AppData\\Local\\Temp\\ipykernel_27812\\1020545929.py:41: DeprecationWarning: Due to a bug, this method doesn't actually stream the response content, `.with_streaming_response.method()` should be used instead\n",
      "  response.stream_to_file(output_path)\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def voice_chat(audio):\n",
    "    if audio is None:\n",
    "        return None, \"\"\n",
    "    \n",
    "    with open(audio, \"rb\") as f:\n",
    "        audio_bytes = f.read()\n",
    "    \n",
    "    user_text = transcribe_audio(audio_bytes)\n",
    "    print(f\"User said: {user_text}\")\n",
    "    \n",
    "    bot_text = generate_response(user_text)\n",
    "    print(f\"Bot responds: {bot_text}\")\n",
    "    \n",
    "    audio_path = synthesize_speech(bot_text, \"gradio_response.wav\")\n",
    "    \n",
    "    return audio_path, bot_text\n",
    "\n",
    "gr.Interface(\n",
    "    fn=voice_chat,\n",
    "    inputs=gr.Audio(sources=[\"microphone\"], type=\"filepath\"),\n",
    "    outputs=[\n",
    "        gr.Audio(label=\"Bot Voice Response\"),\n",
    "        gr.Textbox(label=\"Bot Text Response\", lines=5)\n",
    "    ]\n",
    ").launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_py312_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
